<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Doing Computational Social ScienceThe Continuous Development Edition - 25&nbsp; Latent topics in text (LDA)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./26-complex-adaptive-systems.html" rel="next">
<link href="./24-latent-structure-networks.html" rel="prev">
<link href="./media/logo.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="custom.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./20-linear-regression.html">Generative Modeling</a></li><li class="breadcrumb-item"><a href="./25-latent-topics-text-lda.html"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Latent topics in text (LDA)</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./media/logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Doing Computational Social Science<br><span class="small">The <strong>Continuous Development</strong> Edition</span></a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/UWNETLAB/dcss_supplementary/tree/master/book/" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
    </div>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">👋 Hello!</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Learning to Do Computational Social Science</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Research Computing</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-getting-started.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Getting Started</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-python-101.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Python 101</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-python-102.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Python 102</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Obtaining Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-sampling-and-survey-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Sampling and survey data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-web-data-apis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Web data (APIs)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-web-data-scraping.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Web data (Scraping)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-audio-image-and-document-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Audio, image, and document data</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Exploring Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-processing-structured-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Processing Structured Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-exploratory-data-analysis-and-visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Exploratory data analysis and visualization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-association-and-latent-factors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Association and latent factors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-text-as-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Text as Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-text-similarity-and-latent-semantic-space.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Text similarity and latent semantic space</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-social-networks-and-relational-thinking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Networks: Relationships as Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-structural-similarity-and-latent-social-space.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Structural similarity and latent social space</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Prediction and Inference</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-machine-and-statistical-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Machine Learning 101</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-prediction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Probability 101</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-credibility.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Credibility</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Causality</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Generative Modeling</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-linear-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Bayesian Regression Models with Probabilistic Programming</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-multilevel-regression-with-post-stratification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Multilevel regression with post-stratification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-generalized-linear-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Generalized Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-causal-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Causal analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-latent-structure-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Latent structure in networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-latent-topics-text-lda.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Latent topics in text (LDA)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./26-complex-adaptive-systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Agent-based Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./27-developing-agent-based-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Diffusion</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">Deep Learning Demystified</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./28-artificial-neural-networks-fnn-rnn-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Neural networks 101</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./29-processing-natural-language-data-spacy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Processing Natural Language Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./30-transformers-self-attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Transformers, Self-attention architecture</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./31-latent-topics-text-transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Modelling latent topics (Transformers)</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
 <span class="menu-text">Professional Responsibilities</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./32-research-ethics-politics-and-practices.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Research Ethics, Politics, and Practices</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./33-next-steps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Next steps</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Acknowledgements</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./changelog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Changelog</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-centrality-formulas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Centrality Formulas</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-courses.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Courses and Workshops</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives"><span class="header-section-number">25.1</span> LEARNING OBJECTIVES</a></li>
  <li><a href="#learning-materials" id="toc-learning-materials" class="nav-link" data-scroll-target="#learning-materials"><span class="header-section-number">25.2</span> LEARNING MATERIALS</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction"><span class="header-section-number">25.3</span> INTRODUCTION</a></li>
  <li><a href="#generative-topic-models" id="toc-generative-topic-models" class="nav-link" data-scroll-target="#generative-topic-models"><span class="header-section-number">25.4</span> GENERATIVE TOPIC MODELS</a>
  <ul class="collapse">
  <li><a href="#latent-dirichlet-allocation-lda" id="toc-latent-dirichlet-allocation-lda" class="nav-link" data-scroll-target="#latent-dirichlet-allocation-lda"><span class="header-section-number">25.4.1</span> Latent Dirichlet Allocation (LDA)</a></li>
  <li><a href="#lda-as-a-graphical-model" id="toc-lda-as-a-graphical-model" class="nav-link" data-scroll-target="#lda-as-a-graphical-model"><span class="header-section-number">25.4.2</span> LDA as a Graphical Model</a></li>
  <li><a href="#the-dirichlet-in-latent-dirichlet-allocation" id="toc-the-dirichlet-in-latent-dirichlet-allocation" class="nav-link" data-scroll-target="#the-dirichlet-in-latent-dirichlet-allocation"><span class="header-section-number">25.4.3</span> The Dirichlet in Latent Dirichlet Allocation</a></li>
  <li><a href="#variational-inference" id="toc-variational-inference" class="nav-link" data-scroll-target="#variational-inference"><span class="header-section-number">25.4.4</span> Variational Inference</a></li>
  <li><a href="#selecting-the-number-of-topics" id="toc-selecting-the-number-of-topics" class="nav-link" data-scroll-target="#selecting-the-number-of-topics"><span class="header-section-number">25.4.5</span> Selecting the Number of Topics</a></li>
  </ul></li>
  <li><a href="#topic-modelling-with-gensim" id="toc-topic-modelling-with-gensim" class="nav-link" data-scroll-target="#topic-modelling-with-gensim"><span class="header-section-number">25.5</span> TOPIC MODELLING WITH GENSIM</a>
  <ul class="collapse">
  <li><a href="#running-the-topic-model" id="toc-running-the-topic-model" class="nav-link" data-scroll-target="#running-the-topic-model"><span class="header-section-number">25.5.1</span> Running the Topic Model</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">25.6</span> CONCLUSION</a>
  <ul class="collapse">
  <li><a href="#key-points" id="toc-key-points" class="nav-link" data-scroll-target="#key-points"><span class="header-section-number">25.6.1</span> Key Points</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/UWNETLAB/dcss_supplementary/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Latent topics in text (LDA)</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<!-- Variational Bayes and the Craft of Generative Topic Modelling -->
<section id="learning-objectives" class="level2" data-number="25.1">
<h2 data-number="25.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">25.1</span> LEARNING OBJECTIVES</h2>
<ul>
<li>Explain how variational inference differs from Hamiltonian Monte Carlo sampling, conceptually</li>
<li>Describe the distinction between deterministic and stochastic/generative topic models</li>
<li>Explain what Latent Dirichlet Allocation is and how it works</li>
<li>Explore the use of Semantic Coherence as an evaluation metric</li>
</ul>
</section>
<section id="learning-materials" class="level2" data-number="25.2">
<h2 data-number="25.2" class="anchored" data-anchor-id="learning-materials"><span class="header-section-number">25.2</span> LEARNING MATERIALS</h2>
<p>You can find the online learning materials for this chapter in <code>doing_computational_social_science/Chapter_30</code>. <code>cd</code> into the directory and launch your Jupyter Server.</p>
</section>
<section id="introduction" class="level2" data-number="25.3">
<h2 data-number="25.3" class="anchored" data-anchor-id="introduction"><span class="header-section-number">25.3</span> INTRODUCTION</h2>
<p>This chapter serves three purposes: (1) introduce you to generative topic modelling and Bayesian latent variable modelling more generally, (2) explain the role that graphical models can play in developing purpose-made generative models, and (3) introduce you to another computational approach for approximating the posterior called “variational inference.”</p>
<p>We’ll start by introducing the logic behind generative topic modelling. Then, we will discuss the technical details of one of the most widely-used topic models: latent Dirichlet allocation (LDA). Then, we’ll cover the basics of approximating the posterior using an alternative to HMC called variational inference. In the second section, we’ll start developing LDA topic models with Gensim, discuss quantitative measures of coherence, and show how to visualize topic models.</p>
</section>
<section id="generative-topic-models" class="level2" data-number="25.4">
<h2 data-number="25.4" class="anchored" data-anchor-id="generative-topic-models"><span class="header-section-number">25.4</span> GENERATIVE TOPIC MODELS</h2>
<p>You’ve likely noticed that “latent” is a not-so-latent theme in this book. Recall Chapter 9. Sometimes our data has highly correlated variables because they arise from a shared <em>latent</em> factor or process. That may even be <em>by design</em>, like when we collect data on low-level <em>indicators</em> of an abstract and unobservable concept that we want to measure. We later extended that idea to text analysis by discussing latent semantic analysis, which used deterministic matrix factorization methods (truncated SVD) to construct a set of latent thematic dimensions in text data. In other chapters, we’ve touched on “latent” variables in a variety of different ways, including regression models and latent network structure.</p>
<p>Latent variables are central to Bayesian thinking. When we develop models in the Bayesian framework, we define <em>joint</em> probability distributions with both latent and observed variables, and then use an inference algorithm such as HMC or variational inference (introduced below) to approximate the posterior distribution of each latent variable conditional on the evidence provided by the observed variables. This is an extremely flexible and mathematically-principled way of working with latent variables that you can use to develop probabilistic models for just about any research. So, what exactly would a Bayesian approach to modelling latent thematic structure – <em>topics</em> – in text data look like?</p>
<p><strong>Generative topic models</strong> got their start when the computer scientists David Blei, Andrew Ng, and Michael Jordan <span class="citation" data-cites="blei2003latent">(<a href="references.html#ref-blei2003latent" role="doc-biblioref">2003</a>)</span> published a classic paper proposing the model we will focus on in this chapter. This model and many variants of it are widely used in the social sciences and digital humanities, some of them developed by social scientists <span class="citation" data-cites="roberts2013structural roberts2014structural">(e.g., <a href="references.html#ref-roberts2013structural" role="doc-biblioref">M. Roberts et al. 2013</a>; <a href="references.html#ref-roberts2014structural" role="doc-biblioref">M. E. Roberts et al. 2014</a>)</span>. Broadly speaking, generative topic models are a family of Bayesian models that assume, like Latent Semantic Analysis, that documents are collections of thematically linked words. Rather than using matrix factorization methods to understand latent themes, most generative topic models approach this as just another latent variable problem.</p>
<p>Although applicable in many contexts, we should be <em>especially</em> open to using generative topics models if:</p>
<ol type="1">
<li>We have data in the form of many text documents.</li>
<li>We know that each document contains words that represent different themes, or topics, in various proportions.</li>
<li>We want to infer the distribution of latent topics across documents given the words we observe in them.</li>
<li>We have a plausible mechanism, or a “simple generative story,” of the relationship between our latent and observed variables that will help us accomplish (3): documents and their specific combinations of words are “generated from” a mixtures of latent themes, which are themselves mixtures of words.</li>
</ol>
<p>In the case of text data, posing a generative mechanism means thinking through the reasons why some words co-occur in documents while others tend not to. <em>Something</em> influences those relationships; our word choices are not random. What might lead a politician making a speech, or a scientist writing journal articles, to select some combination of words but not others? Why does elephant floral own snowstorm aghast the rock cat? (see what I did there?)</p>
<p>Many generative mechanisms have been posited and tested by different types of topic models, including some that are designed to take information about speakers / authors into account in a regression model-like framework <span class="citation" data-cites="roberts2013structural roberts2014structural rosen2012author">(e.g., <a href="references.html#ref-roberts2013structural" role="doc-biblioref">M. Roberts et al. 2013</a>; <a href="references.html#ref-roberts2014structural" role="doc-biblioref">M. E. Roberts et al. 2014</a>; <a href="references.html#ref-rosen2012author" role="doc-biblioref">Rosen-Zvi et al. 2012</a>)</span> or to account for the ordering / sequence of words <span class="citation" data-cites="blei2006dynamic wang2012continuous">(e.g., <a href="references.html#ref-blei2006dynamic" role="doc-biblioref">D. M. Blei and Lafferty 2006</a>; <a href="references.html#ref-wang2012continuous" role="doc-biblioref">Wang, Blei, and Heckerman 2012</a>)</span>, but the foundational generative mechanism that unites all topic models is that the particular mixture of words that show up in documents are related to a latent set of underlying themes, <strong>latent topics</strong>. The topics we discuss are one of the key factors that determine the probability of selecting one word over another.</p>
<p>We learn about the underlying latent topics by constructing a probabilistic model and then using an inference algorithm to approximate the posterior. We will construct a <strong>latent Dirichlet allocation</strong> topic model (a subtype within the more general class of <strong>mixed membership models</strong>), which revolutionized natural language processing and probabilistic machine learning in the early 2000s. It remains a widely used model, alongside many variations.</p>
<p>The goal of approximating the posterior with an LDA topic model is learning about the distribution of latent topics over: (<em>i</em>) documents and (<em>ii</em>) words. Say we have a journal article about social movements focused on energy transitions. There are likely quite a few latent topics in this article but it’s safe to say that the dominant ones are probably “social movements” and “energy transitions.” Of course, the model doesn’t actually know what “social movements” and “energy transitions” are, so it might tell us that the article in question is 17% “topic 11,” 12% “topic 2,” and then many other topics in much smaller proportions. <em>Note that these are mixtures</em>; documents always consist of multiple topics, though one may be dominant.</p>
<p>Every word in our document has a different probability of appearing in each of the topics we find. The words “movement,” “mobilization,” “collective,” and “protest” may have a high probability of appearing in topic 11 (which we interpret as “social movements”). The words “environmental,” “transition,” “energy,” “oil,” “renewable,” and “pipeline” may have a high probability of appearing in “energy transitions and politics,” “topic 2,” but a relatively low probability of appearing in social movements topics. Other words, such as “medical,” and “healthcare” will have a low probability of appearing in either topic (assuming they appear at all), but they might have a high probability of appearing in a topic about “health” (which itself has a low probability of appearing in the article.</p>
<p>This notion that words have different probabilities of appearing in each topic makes it possible for the same word to have a high probability of appearing in more than one topic depending on its use (we discuss this more in the next chapter). For example, the word “policy” might have a high probability of appearing in both topics. This turns out to be a major benefit of generative topic models. As prominent sociologists of culture have pointed out, this brings the topic modelling framework close to relational theories of language and meaning that have been influential in the social sciences for quite some time <span class="citation" data-cites="dimaggio2013exploiting mohr2013introduction">(e.g. <a href="references.html#ref-dimaggio2013exploiting" role="doc-biblioref">DiMaggio, Nag, and Blei 2013</a>; <a href="references.html#ref-mohr2013introduction" role="doc-biblioref">Mohr and Bogdanov 2013</a>)</span>.</p>
<p>Now that we’ve built a bit of intuition about what a generative topic model might look like, let’s get into some of the technical modelling details. Then we’ll use these models to analyze latent topics in 1,893,372 speeches made in Parliament by Canadian politicians over a thirty-year period (1990 - 2020).</p>
<section id="latent-dirichlet-allocation-lda" class="level3" data-number="25.4.1">
<h3 data-number="25.4.1" class="anchored" data-anchor-id="latent-dirichlet-allocation-lda"><span class="header-section-number">25.4.1</span> Latent Dirichlet Allocation (LDA)</h3>
</section>
<section id="lda-as-a-graphical-model" class="level3" data-number="25.4.2">
<h3 data-number="25.4.2" class="anchored" data-anchor-id="lda-as-a-graphical-model"><span class="header-section-number">25.4.2</span> LDA as a Graphical Model</h3>
<p>In previous chapters, we developed and described our Bayesian models using statistical model notation, and we <em>briefly</em> saw graphical models as an alternative in the context of regression modelling. <strong>Graphical models</strong> are a powerful tool for developing, critiquing, and communicating our probabilistic models in part because they make three key things explicit:</p>
<ol type="1">
<li>the origin of every variable in our model as either observed or latent (we’re always playing the “what’s that?” game introduced in Chapter 27), and</li>
<li>our assumptions about the structure of statistical dependencies between all the variables in our model, and relatedly</li>
<li>our assumptions about the generative processes that give rise to the data we observe.</li>
</ol>
<p>Graphical models are a favoured tool in probabilistic machine learning in particular <span class="citation" data-cites="koller2009probabilistic mcelreath2020statistical pearl2018book jordan2004graphical jordan2003introduction">(<a href="references.html#ref-koller2009probabilistic" role="doc-biblioref">Koller and Friedman 2009</a>; <a href="references.html#ref-mcelreath2020statistical" role="doc-biblioref">McElreath 2020</a>; <a href="references.html#ref-pearl2018book" role="doc-biblioref">Pearl and Mackenzie 2018</a>; <a href="references.html#ref-jordan2004graphical" role="doc-biblioref">Jordan 2004</a>, <a href="references.html#ref-jordan2003introduction" role="doc-biblioref">2003</a>)</span>, and you’ll see them everywhere in the generative topic modelling literature. Though they can be a little confusing at first, they are transparent once you know how to read them. Let’s break down the graphical representation of a vanilla LDA topic model, shown in Figure <span class="quarto-unresolved-ref">?fig-30_01</span>.</p>
<p><img src="figures/lda.png" class="img-fluid"></p>
<blockquote class="blockquote">
<p>It is possible to produce graphical models like this automatically using pymc. Doing so can be very useful if you want to examine how your Bayesian models are structured. Consult the pymc documentation if you want to learn more!</p>
</blockquote>
<p>Now, what does it mean?</p>
<p>First, each node in the graph represents a random variable, with observed variables shaded and latent variables unshaded. The black nodes are model hyperparameters. Each arrow in the graph represents a statistical dependency, or to be precise, conditional independence. The boxes (called <strong>plates</strong>) represent repetition over some set of items (<strong>replicates</strong>), like words or documents. Plates notation is <em>very</em> useful for condensing your graphical models. For example, without it, you would need a node for each document in a text analysis. We will soon analyze 1,893,372 political speeches. Now, imagine how many nodes we would need for the <span class="math inline">\(\text{W}_{d,n}\)</span> node.</p>
<p>Let’s break down this specific graphical model starting with the plates. The large outer plate with the <span class="math inline">\(D\)</span> in the bottom right corner represents all of the documents in our document collection. When we get to our model for political speeches, <span class="math inline">\(D\)</span> will equal 1,893,372. Everything inside the document plate is repeated for each individual document in the document collection. In other words, it pertains to <span class="math inline">\(D_i\)</span>, where the index <span class="math inline">\(_i\)</span> represents any given document in the dataset. The small inner plate with <span class="math inline">\(N\)</span> in the bottom right represents the specific words and their position in the probability distribution for each topic. We’ll come back to this momentarily. The third plate, with <span class="math inline">\(K\)</span> in the bottom right, represents the latent topics whose distributions we are computing. If we model 100 topics, then <span class="math inline">\(\beta_{k}\)</span> would be 100 probability distributions over terms.</p>
<p>Every document in our dataset is composed of a <em>mixture</em> of topics, with each topic being a probability distribution over words. Inside the document plate, then, <span class="math inline">\(\theta_{d}\)</span> represents the topic proportions <em>for each document</em>. Picture a matrix with documents in the rows and latent topics (represented by arbitrary numerical IDs) in the columns. Each document in our collection is made up of words. The gray node <span class="math inline">\(W_{d,n}\)</span> represents each observed word <span class="math inline">\(_n\)</span> for each document <span class="math inline">\(_k\)</span>, while <span class="math inline">\(\text{Z}_{d,n}\)</span> represents the topic assignments for each word in each document for each topic. In other words, each word in each document has a probability associating it with each topic. Imagine a matrix of probabilities with words in the rows and latent topics in the columns. <span class="math inline">\(\beta_{k}\)</span> represents the topics themselves, with <span class="math inline">\(k\)</span> being the number of latent topics to model. The value of <span class="math inline">\(k\)</span> is selected by the researcher; we’ll discuss that process shortly.</p>
<p>That leaves the black nodes <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\eta\)</span>. These are priors for the parameters of the Dirichlet distribution, and we’ll discuss the options for these below. <span class="math inline">\(\alpha\)</span> is the “proportions parameter” and represents text-topic density. Think of this as the prior probability that a document will be associated with a topic. If we set <span class="math inline">\(\alpha\)</span> to a high value – say close to 1 – the probability of texts being associated with topics increases, and when <span class="math inline">\(\alpha\)</span> is set to a low value – say 0.1 – the probability decreases. <span class="math inline">\(\eta\)</span>, on the other hand, represents topic-word densities. It’s known as the “topic parameter. When <span class="math inline">\(\eta\)</span> is set to a high value, the probability of a word being associated with a topic increases. When it is set low, the probability decreases.</p>
<p>Putting this all in one convenient place, then,</p>
<ul>
<li><span class="math inline">\(\beta_{k}\)</span> represents the latent topics themselves;</li>
<li><span class="math inline">\(\theta_{d}\)</span>, inside the document plate, represents the latent topic proportions for each document;</li>
<li><span class="math inline">\(\text{Z}_{d,n}\)</span> represents the latent topic assignments for each word <span class="math inline">\(_n\)</span> in each document <span class="math inline">\(_d\)</span>;</li>
<li><span class="math inline">\(\text{W}_{d,n}\)</span> represents each observed word <span class="math inline">\(_n\)</span> in each document <span class="math inline">\(_d\)</span></li>
<li><span class="math inline">\(\alpha\)</span> represents the portions hyperparameter (the prior probability that a document is associated with a topic), and</li>
<li><span class="math inline">\(\eta\)</span> represents the topic hyperparameter (the prior probability that a word is associated with a topic).</li>
</ul>
<p>We are representing a three-level Hierarchical Bayesian latent variable model with each document in a document collection modelled as a finite mixture of hidden topics in varying proportions, and with each topic modelled as an infinite mixture of words in varying proportions. It posits a generative relationship between these variables in which meaningful patterns of co-occurring words arise from the specific mixtures of latent themes. Altogether, it descibes the <em>joint</em> probability distribution for (a) the latent topics, (2) their distribution over documents, and (3) their distribution of words, or</p>
<p><span class="math display">\[\begin{align}
P(\beta, \theta, Z, W).
\end{align}\]</span></p>
<p>But, we <em>want</em> to know the posterior, which is the probability of the topics, their distribution over documents, their distribution of words <em>conditional</em> on the observed words, or</p>
<p><span class="math display">\[\begin{align}
P(\beta, \theta, Z | W).
\end{align}\]</span></p>
<p>As with other Bayesian models, we can’t derive the posterior from the joint distribution analytically because of the intractable denominator in Bayes theorem, and because the number of potential latent topical structures is exponentially large, so we turn to approximate posterior inference. That’s where variational inference comes in.</p>
</section>
<section id="the-dirichlet-in-latent-dirichlet-allocation" class="level3" data-number="25.4.3">
<h3 data-number="25.4.3" class="anchored" data-anchor-id="the-dirichlet-in-latent-dirichlet-allocation"><span class="header-section-number">25.4.3</span> The Dirichlet in Latent Dirichlet Allocation</h3>
<p>Like the probabilistic models we’ve developed, probabilistic topic models are built out of probability distributions! The ‘Dirichlet’ portion in Latent Dirichlet Allocation (often written as Dir(<span class="math inline">\(\alpha\)</span>) is just another probability distribution of the kind discussed in Chapter 26. It’s a generalization of the idea of a triangle (called the <strong>simplex</strong>), only it can have an arbitrary number of sides… What?</p>
<p>These kinds of descriptions (generalization of a triangle) are useful for those already deeply familiar with mathematical geometry or multidimensional probability distributions, but they’re unlikely to get the rest of us very far. That said, with a little scaffolding, this will quickly make sense. In the probability primer chapter, we established that some probability distributions only cover some parts of the real number line; the exponential distribution, for instance, only supports positive values. The ‘beta distribution’ takes this idea a bit further: it only supports values from 0 to 1, inclusive. It takes two parameters, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, which jointly control the shape of the curve. You can think of the two as representing inversely correlated axes, both trying to pull more of the probability density towards the side of the distribution that they’re more positive in (so <span class="math inline">\(\alpha\)</span> pulls to the right, towards 1, <span class="math inline">\(\beta\)</span> pulls to the left, towards 0). Here’s an example of one where <span class="math inline">\(\beta\)</span> is doing more of the pushing (Figure <span class="quarto-unresolved-ref">?fig-30_02</span>):</p>
<p><img src="figures/beta_distribution.png" class="img-fluid"></p>
<p>The beta distribution is remarkably flexible: you should look up some examples of the shapes it can take!</p>
<p>Since the beta distribution only supports values from 0 to 1, what would it look like if we tacked on a second dimension to this distribution? See for yourself, in Figure <span class="quarto-unresolved-ref">?fig-30_03</span>.</p>
<p><img src="figures/dirichlet_plots.png" class="img-fluid"></p>
<p>Behold the Dirichlet distribution! The Dirichlet is a multi-dimensional generalization of the Beta distribution. In the diagram above, instead of 2 parameters (<span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>) having a tug-of-war along a real number line, we have 3 parameters having a 3-way tug-of-war (the probability is concentrated in areas closer to the red end of the colour spectrum). The shape they’re battling over is a <em>simplex</em> in 2 dimensions (which is just a triangle). If we add a third dimension, then our triangle becomes a pyramid (a 3-dimensional simplex), and we’ll have 4 parameters duking it out in a 4-way tug-of-war. Remember that because the Dirichlet distribution is a probability distribution, its density must integrate to 1; this makes the Dirichlet very useful for describing probability across a large number of mutually-exclusive categorical events.</p>
<p>Like the other Bayesian models we’ve seen, LDA topic models require priors. The <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\eta\)</span> hyperparameters inform the generation of the Dirichlet distribution, and understanding them gives you much greater control over your model. If this discussion of priors reminds you of the chapters on Bayesian Regression, good! LDA models function in a very similar framework. In fact, we can present LDA in a similar format to those chapters!</p>
<p>A few notes first. We’re going to include a long list of variables, including what each of them mean. Normally, we don’t do this kind of thing, because the variables in Linear Regression models are usually self-evident. In the case of LDA, most of the ‘data’ variables we’re using are calculated using some aspect of the corpus and beg explanation. The first three sections that follow (Data, Hyperparameters, and Latent Variables) are all simple descriptions. They all come together in the 4-line Model section at the end.</p>
<p><strong>Data</strong></p>
<p><span class="math display">\[\begin{align}
V &amp;: ~~~\text{integer} &amp;  \text{ [Number of Unique Terms in Vocabulary]}\\
D &amp;: ~~~\text{integer} &amp;  \text{ [Number of Documents]}\\
d &amp;: ~~~\text{integer, values [min:1, max:} D] &amp;  \text{ [Document ID]}\\
N &amp;: ~~~\text{integer} &amp;  \text{ [Total Word Instances]}\\
n &amp;: ~~~\text{integer, values  [min:1, max:} N] &amp;  \text{ [Word Instance]}\\
K &amp;: ~~~\text{integer} &amp;  \text{ [Number of Topics]} \\
k &amp;: ~~~\text{integer, values [min:1, max:}K] &amp;  \text{ [Topic]}\\
\end{align}\]</span></p>
<p><strong>Hyperparameters</strong></p>
<p><span class="math display">\[\begin{align}
\alpha &amp;: ~~~\text{vector of real numbers, length  } K &amp; \text{[Topic-in-Document Prior Hyperparameter]} \\
\eta  &amp;: ~~~\text{vector of real numbers, length  } V &amp; \text{[Term-in-Topic Prior Hyperparameter]} \\
\end{align}\]</span></p>
<p><strong>Latent Variables</strong></p>
<p><span class="math display">\[\begin{align}
\theta_d &amp;: ~~~K\text{-simplex, Dirichlet-distributed}   &amp;  \text{ [Topic Distribution for Document  } d]  \\
\beta_k &amp;: ~~~V\text{-simplex, Dirichlet-distributed}    &amp; \text{[Word Distribution for Topic  } k] \\
\end{align}\]</span></p>
<p><strong>Model</strong></p>
<p><span class="math display">\[\begin{align}
\theta_d &amp;\sim \text{Dirichlet}(\alpha)  &amp;&amp; \text{ for } d \text{ in } 1 ... D  &amp; \text{  [Topic-in-Document Prior]} \\
\beta_k  &amp;\sim \text{Dirichlet}(\eta)   &amp;&amp; \text{ for } k \text{ in } 1 ... K  &amp; \text{  [Term-in-Topic Prior]} \\
z_{d,n}  &amp;\sim \text{Categorical}(\theta_d)  &amp;&amp; \text{ for } d \text{ in } 1 ... D, n \text{ in } 1 ... N &amp;\text{[Document-Topic Probability]} \\
w_{d,n}  &amp;\sim \text{Categorical}(\beta_{z[d,n]}) &amp;&amp; \text{ for } d \text{ in } 1 ... D, n \text{ in } 1 ... N &amp;\text{[Likelihood]}
\end{align}\]</span></p>
<p>Whew, that’s a lot of variables! We’ve already discussed what some of them are (and how they function), but some remain enigmatic. Let’s discuss them in the abstract here.</p>
<section id="understanding-the-alpha-hyperparameter" class="level4" data-number="25.4.3.1">
<h4 data-number="25.4.3.1" class="anchored" data-anchor-id="understanding-the-alpha-hyperparameter"><span class="header-section-number">25.4.3.1</span> Understanding the <span class="math inline">\(\alpha\)</span> hyperparameter</h4>
<p>The <span class="math inline">\(\alpha\)</span> parameter can be a relatively naive setting, or more informed. If it’s a simple scalar (ie. single value) it will be propagated into a matrix of <em>expected</em> topic probabilities for each document. In all cases, this matrix has a shape of <code>n_topics</code> x <code>n_documents</code>. When a single value is used for all of the topic, this is a symmetric prior. As you will soon see, this “a-priori” assumption actually matters, even though the LDA model will modify these values <em>a lot</em>. A symmetric prior essentially tells the model “I expect the probability of each topic being a topic in each document to be the same, and you will have to work very hard to tell me otherwise.” There are times where this assumption might actually be helpful for the model. In most cases, though, we want to use LDA to tell us something we <em>don’t</em> know about a corpus, with an unknown distribution of topics. In this case, an asymmetric prior is essential<span class="citation" data-cites="wallach2009rethinking syed2018selecting">(<a href="references.html#ref-wallach2009rethinking" role="doc-biblioref">H. M. Wallach, Mimno, and McCallum 2009</a>; <a href="references.html#ref-syed2018selecting" role="doc-biblioref">Syed and Spruit 2018</a>)</span>. In the example from <span class="citation" data-cites="hoffman2010online">(<a href="references.html#ref-hoffman2010online" role="doc-biblioref">Hoffman, Bach, and Blei 2010</a>)</span> this value is set at <code>1/num_topics</code>, but they mention that this is for simplicity, and reference <span class="citation" data-cites="wallach2009rethinking">(<a href="references.html#ref-wallach2009rethinking" role="doc-biblioref">H. M. Wallach, Mimno, and McCallum 2009</a>)</span> that asymmetric priors can also be used.</p>
<p>An asymmetric prior tells the LDA model that the probability of each topic in a given document is expected to be different, and that it should work on determining what those differences are. Unlike the symmetric prior, there is a lot of flexibility in the <span class="math inline">\(\alpha\)</span> hyperparameter for an asymmetric prior in Gensim. If a scalar is given, the model will incorporate the values from <code>1 to num_topics</code> when it generates the prior. This means that each document has an array of topic probabilities that are all different, although <em>each document will have the same array</em>. Rather than a single scalar value, it’s also possible to pass an <span class="math inline">\(\alpha\)</span> array of expected probabilities that is informed by prior knowledge. This could be domain knowledge, but then we would again be left wondering whether we want to learn something or just confirm what we already know. Perhaps the most exciting option here, then, is to <em>use Bayes to inform Bayes</em>. A crucial part about the use of asymmetric priors for the <span class="math inline">\(\alpha\)</span> hyperparameter is that the LDA model becomes a lot less sensitive to the number of topics specified. This decreased sensitivity means we should be able to trust the assignment of topics by the model regardless of how much choice we give it.</p>
<p>Of course, there’s more than one way to do this. Gensim implements an automatic hyperparameter tuning method, based on work by <span class="citation" data-cites="ypma1995historical">(<a href="references.html#ref-ypma1995historical" role="doc-biblioref">Ypma 1995</a>)</span>, where the model priors are updated at regular intervals during iterations of model training. This is convenient for a number of reasons, but in particular: we can train an LDA model on a random sample of the data, setting the model to update the priors as frequently and for as many iterations as we have time for. Then, the updated <span class="math inline">\(\alpha\)</span> prior can be used to model the entire corpus. As you will read about in the next chapter, this is a form of transfer learning, and it comes with many advantages.</p>
<p>The posterior results, theta, will be these priors fitted to the corpus, with which we can estimate unique topic probabilities for <em>each</em> document.</p>
</section>
<section id="understanding-the-eta-hyperparameter" class="level4" data-number="25.4.3.2">
<h4 data-number="25.4.3.2" class="anchored" data-anchor-id="understanding-the-eta-hyperparameter"><span class="header-section-number">25.4.3.2</span> Understanding the <span class="math inline">\(\eta\)</span> hyperparameter</h4>
<p>The <span class="math inline">\(\eta\)</span> hyperparameter functions quite similarly to <span class="math inline">\(\alpha\)</span> in terms of technical implementation, but has very different assumptions and conceptual implications. The prior constructed from <span class="math inline">\(\eta\)</span> is the expected probability for <em>each word</em> being a part of each topic. This can, again, be initialized in a relatively simple way by providing a single value - Hoffman and Blei again use <code>1/num_topics</code>. This time, the single value is used to populate an array of shape <code>n_topics</code> x <code>n_words</code>. This again results in a symmetric prior, but the conceptual implication is actually what we want - if we told the model that the probability of each word’s topic contribution should be different from the beginning, we would be directing the model to prefer some words to others. This could bias words away from contributing to topics that they should, or towards topics that they shouldn’t. This issue would tend to smooth out with a large amount of training data, but it’s safer to just start this parameter with uniform word probabilities. In reality, the words we use are versatile and many of them are very likely to be used in <em>all</em> topics. So conceptually, this prior should actually be a symmetric one, and we’ll look at some evidence for this shortly <span class="citation" data-cites="wallach2009rethinking">(<a href="references.html#ref-wallach2009rethinking" role="doc-biblioref">H. M. Wallach, Mimno, and McCallum 2009</a>)</span>.</p>
<p>Nonetheless, there are also asymmetric options for eta, although they’re very similar to alpha, so we won’t spend too much time rehashing the technical details. We can provide an array of probabilities for each word that will be their prior expected probabilities for each topic, or a matrix of shape <code>num_topics</code> x <code>n_words</code> to make the expected word probabilities specific to each word. The last option is to use the same prior update method introduced above for the <span class="math inline">\(\alpha\)</span> prior. We will demonstrate below that the latter method indicates that <span class="math inline">\(\eta\)</span> becomes fairly symmetrical after model training, suggesting that a simple symmetrical prior is the most efficient choice and will not result in a loss of accuracy.</p>
<p>The posterior results, beta, will be these <span class="math inline">\(\eta\)</span> priors fitted to the corpus, which can be used to calculate unique topic probabilities for <em>each</em> word, as well as the top probabilities of words forming each topic.</p>
</section>
</section>
<section id="variational-inference" class="level3" data-number="25.4.4">
<h3 data-number="25.4.4" class="anchored" data-anchor-id="variational-inference"><span class="header-section-number">25.4.4</span> Variational Inference</h3>
<p>We have just described the structure of our generative model. The structure is independent of the inference algorithm that we use to approximate the posterior probabilities for <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\theta\)</span>, and <span class="math inline">\(Z\)</span>. We’ve seen this kind of thing before; in the chapters on Bayesian Linear Regression, we defined our models using priors, likelihood, and a linear model, and then sampled from those models’ posteriors to produce final posterior distributions. We used pymc’s Hamiltonian Monte Carlo-like (HMC) sampler to accomplish this (it’s an easy, efficient, general-purpose approach), but we <em>could</em> have used any number of other techniques, such as a Gibbs Sampler, grid approximation, quadratic approximation, and so on. Our models would have remained the same regardless of approximation techniques.</p>
<p>In this section, we’re going to introduce <strong>Variational Inference</strong> (VI), which is another approach to approximating the posterior of a Bayesian model <span class="citation" data-cites="blei2003latent">(<a href="references.html#ref-blei2003latent" role="doc-biblioref">D. Blei, Ng, and Jordan 2003</a>)</span>. The goal of VI is identical to that of HMC; both seek to efficiently approximate an entire posterior distribution for some set of latent variables. However, whereas HMC is based on the idea that we can learn about posterior distributions by <em>sampling</em> from them, variational inference attempts to approximate posteriors by using a parametric distribution (or some combination thereof) that gets as close as possible. For this brief introduction, the point is that we will still be approximating the posterior, but without imaginary robots hurling imaginary marbles around an unfathomably large sample space. Sampling methods like HMC construct an approximation of the posterior by keeping a ‘tally’ of where the marble ends up in space, building a jagged pyramid of sorts, and then sanding down the edges and filling in the gaps to produce the smooth posterior curves you saw in the model outputs of chapters 28 and 29. It’s worth knowing that VI, by contrast, approaches the problem by doing the rough equivalent of taking a bendy piece of wire and trying to warp it so that it closely matches the posterior. The key here is that VI provides us with a close-as-possible approximation of posterior distributions using a distribution that we can describe mathematically. Remember, at the beginning of Chapter 27, I recounted a parable about there being “more functions than formulae”? The results that we get from a sampling-based approach to approximating the posterior (HMC, MCMC, Gibbs), gives us the equivalent of a function <em>without a formula</em>. We know what those posteriors look like and the values they take on, but we can’t use a mathematical formula to describe them. Variational inference, on the other hand <em>gives us a function <strong>with a formula</strong></em>. It’s not a perfect analogy, but it should help you grasp the difference between the two.</p>
<p>The major breakthroughs in generative topic modelling are due, in part, to variational inference. It provides a proxy which we can use to calculate an <em>exact</em> analytical solution for the (still approximate) posterior distribution of the latent variables <span class="math inline">\(p(Z | X)\)</span>. To do that, we posit a family of distributions with variational parameters over the latent variables in our model, each of which is indexed by the parameter <span class="math inline">\(\nu\)</span>. It’s written like this:</p>
<p><span class="math display">\[\begin{align}
q(Z; \nu)
\end{align}\]</span></p>
<p>We pick some initial value for <span class="math inline">\(\nu\)</span> and then gradually modify it until we find parameter settings that make the distribution as close to the posterior <span class="math inline">\(p(Z | X)\)</span> as possible. We assess closeness by measuring the distance between the two distrubutions using a measure from information theory called KL Divergence. Once we know those parameter settings, we can use <span class="math inline">\(q(Z; \nu)\)</span> as a proxy for the posterior.</p>
<p>This is represented in Figure <span class="quarto-unresolved-ref">?fig-30_04</span>, which is adapted from <span class="citation" data-cites="blei_talk">D. Blei (<a href="references.html#ref-blei_talk" role="doc-biblioref">2017</a>)</span>. We represent the family of distributions <span class="math inline">\(q(Z ; \nu)\)</span> as an ellipse, and every position within that ellipse represents a specific instantiation of the variational family, indexed by <span class="math inline">\(\nu\)</span>. The squiggly gray line represents different realizations along the way to finding the parameterized distribution that is closest to the posterior, measured with KL divergence.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/variational_inference.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">An illustration of the logic of variational inference, adapted from <a href="https://www.youtube.com/watch?v=Dv86zdWjJKQ">David Blei (2017)</a></figcaption>
</figure>
</div>
<p>Remember that, as with HMC, we are <em>approximating</em> the posterior. Only instead of approximating it by drawing samples, we approximate it by finding another <em>very similar but not identical</em> distributions that can serve as exact analytical proxies for the posterior. The general process works as I’ve decscribed above, but the specifics are a thriving area of research in machine learning. Discussions of variational inference in the technical literature involve a healthy dose of dense mathematics, but most of the technical specifics are not really necessary to understand as an applied researcher. It “Just Works<span class="math inline">\(^{(TM)}\)</span>.” It is especially useful when working with very large datasets, as we do in text analysis, and it’s a good bit faster than HMC in cases like these, but is just as accurate.</p>
<p>I have just covered the basic goals and ideas behind LDA topic models and the importance of thinking through the generative mechanisms. You should also understand generative topics models using graphical models with plate notation, and the basics of how variational inference works. There’s one final issues left to address: selecting the number of topics. Here, once again, the emphasis is on iterative multi-method workflows that leverage as much information and careful interpretive and critical work as possible.</p>
<blockquote class="blockquote">
<p><strong>Further Reading</strong></p>
<p>If you want another friendly introduction to LDA topic models, I recommend <span class="citation" data-cites="blei2012probabilistic">D. Blei (<a href="references.html#ref-blei2012probabilistic" role="doc-biblioref">2012</a>)</span>. If you are looking to develop a deeper understanding of variational inference aside from its specific application in LDA topic models, I would recommend Chapters 21 and 22 of Murphy’s <span class="citation" data-cites="murphy2012machine">(<a href="references.html#ref-murphy2012machine" role="doc-biblioref">2012</a>)</span> comprehensive <em>Machine Learning: A Probabilistic Perspective</em>.</p>
</blockquote>
</section>
<section id="selecting-the-number-of-topics" class="level3" data-number="25.4.5">
<h3 data-number="25.4.5" class="anchored" data-anchor-id="selecting-the-number-of-topics"><span class="header-section-number">25.4.5</span> Selecting the Number of Topics</h3>
<p>With LDA topic models, we need to specify the number of topics, <span class="math inline">\(K\)</span>, in advance. We are defining a random variable whose values we will infer from the posterior. Selecting the number of topics in a generative topic model is a bit of a dark art; due to the nature of the problem, there is no “correct” number of topics, although some solutions are certainly better than others. If we tell our topic model to identify 12 topics, it will. It will model the probability distribution of those topics over a set of documents and a probability distribution of words for each of the 12 topics. So how do we know how many topics to look for, and what are the consequences of selecting a number that is too large or too small?</p>
<p>Let’s explore a comparison. Imagine using a simple clustering method like <span class="math inline">\(k\)</span>-means as a rudimentary topic model: you want to identify groups of documents that are thematically similar, so you create a bag-of-words representation of the documents, perform some dimensionality reduction with PCA, and then pass some number of principal components into the <span class="math inline">\(k\)</span>-means algorithm along with the number of clusters to look for. With <span class="math inline">\(k\)</span>-means, each observation (i.e., document) can only be assigned to a single cluster, and if clusters are thematically distinct, then they can only be assigned to a single theme. Continuing with our previous example, a hypothetical article about social movements focused on energy transitions would have to be assigned a single topic (either social movements, energy transitions, or a single topic capturing both of these things), which makes it <em>very</em> likely that documents will be assigned to clusters that don’t fit them very well. There is no “correct” value for <span class="math inline">\(k\)</span>, but solutions that set the value of <span class="math inline">\(k\)</span> too high or too low will result in clusters containing many documents that have no business being there.</p>
<p>Though topic models also require the researcher to choose the number of topics, the consequences of using a sub-optimally calibrated topic model are different from clustering methods like <span class="math inline">\(k\)</span>-means. To reiterate: in topic modelling, documents are <em>always</em> conceptualized as a mixture of topics. If the number of topics that we specify is too small, our model will return extremely general and heterogeneous topics. To a human reader, these topics often appear incoherent. On the other hand, if we set the number of topics too high, then the model will return extremely specific topics. This can seem like taking one topic and splitting it into two topics that are differentiated by things that don’t really matter. It’s the topic modelling version of the narcissism of minor differences. We don’t want that either.</p>
<p>Let’s continue with our hypothetical example for a bit longer. Say we pick a large number of topics and the result is that we split our social movement topic into multiple social movement topics. Is this a good thing or a bad thing? The short answer is “it depends.” If we are lucky, that split may make some sense, such as separating content on resource mobilization theory <span class="citation" data-cites="mccarthy1977resource">(<a href="references.html#ref-mccarthy1977resource" role="doc-biblioref">McCarthy and Zald 1977</a>)</span> from other theoretical perspectives in social movement research, such as frame analysis <span class="citation" data-cites="benford2000framing snow2014emergence benford1993frame">(<a href="references.html#ref-benford2000framing" role="doc-biblioref">Benford and Snow 2000</a>; <a href="references.html#ref-snow2014emergence" role="doc-biblioref">Snow et al. 2014</a>; <a href="references.html#ref-benford1993frame" role="doc-biblioref">Benford 1993</a>)</span>, political process theory <span class="citation" data-cites="mcadam2010political caren2007political">(<a href="references.html#ref-mcadam2010political" role="doc-biblioref">McAdam 2010</a>; <a href="references.html#ref-caren2007political" role="doc-biblioref">Caren 2007</a>)</span>, multi-institutionalism <span class="citation" data-cites="armstrong2008culture">(<a href="references.html#ref-armstrong2008culture" role="doc-biblioref">Armstrong and Bernstein 2008</a>)</span>, or strategic adaptation <span class="citation" data-cites="mccammon2012us mccammon2007movement mccammon2009beyond">(<a href="references.html#ref-mccammon2012us" role="doc-biblioref">H. McCammon 2012</a>, <a href="references.html#ref-mccammon2009beyond" role="doc-biblioref">2009</a>; <a href="references.html#ref-mccammon2007movement" role="doc-biblioref">H. J. McCammon et al. 2007</a>)</span>. Or perhaps it would differentiate between cultural approaches and structural approaches <span class="citation" data-cites="smith2009structural">(<a href="references.html#ref-smith2009structural" role="doc-biblioref">Smith and Fetner 2009</a>)</span>. In reality, we may not find topics that align so neatly with our own mental models but the take home message here is that general (fewer topics) and specific (more topics) solutions can both be good <em>or</em> bad; the “best” solution depends on what we are trying to learn.</p>
<p>Looking for fine distinctions with a small number of topics is like trying to compare pedestrians’ gaits while standing on the rooftop patio of an extremely tall building. It’s not “wrong” but if you really want to analyze gait, you would be better off getting a little closer to the action. On the other hand, if you were looking for a more general perspective on the flow of foot traffic in the neighbourhood, the top of a tall building is a perfectly fine place to be. The key thing to realize here is that <em>your goal makes one vantage point better or worse than the other</em>. Luckily, the same research that found LDA results to be greatly improved by an asymmetric prior also found that the artificial splitting of topics was greatly diminished. This means that, in general, we’re better off choosing too many topics than choosing too few, so long as we’re using an asymmetrical prior. On the other hand, if you’re using LDA on a corpus where you actually do expect a homogenous set of topics to be equally likely in the documents, you might want to use a symmetric prior, in which case you will also want to experiment more with the number of topics. However, if you know enough about the data to determine this is the prior you need, then you probably also have a ballpark idea about how many topics to expect! The two a-priori assumptions go hand-in-hand.</p>
<p>In short, we <em>can</em> make bad decisions when topic modelling, and these bad decisions can have major implications for what we find. But the risks are different than they are for methods like <span class="math inline">\(k\)</span>-means clustering because documents are always a mix of topics. Most of the time, the risk of a bad topic solution is that we will be either too zoomed in or zoomed out to learn what we want to learn. The best course of action here is to develop many different models with different numbers of topics. And the best way to do this is in an iterative framework like Box’s loops, or better yet a framework like computational grounded theory (discussed in Chapter 11) that is designed specifically for multi-method text analysis.</p>
</section>
</section>
<section id="topic-modelling-with-gensim" class="level2" data-number="25.5">
<h2 data-number="25.5" class="anchored" data-anchor-id="topic-modelling-with-gensim"><span class="header-section-number">25.5</span> TOPIC MODELLING WITH GENSIM</h2>
<p>There are a number of options for developing topic models with Python. In this chapter we’ll use Gensim because it’s mature, well-maintained, and has good documentation. It offers some really nice implementations of widely-used models, is computationally efficient, and scales well to large datasets.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dcss <span class="im">import</span> set_style, download_dataset</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>set_style()</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dcss.text <span class="im">import</span> preprocess, bow_to_df</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim <span class="im">import</span> corpora</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pprint <span class="im">import</span> pprint</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> LdaModel</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models.ldamulticore <span class="im">import</span> LdaMulticore</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models.coherencemodel <span class="im">import</span> CoherenceModel</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>ca_hansard_data_url <span class="op">=</span> <span class="st">"https://www.dropbox.com/scl/fo/5voxfrx6qeqgdrjuc979k/AD63UZhKpxF64b58Jp65w18?rlkey=2bbaqw1bwjgvodbwqhox494e2&amp;st=99fldjgr&amp;dl=0"</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>download_dataset(ca_hansard_data_url, <span class="st">'data/canadian_hansard/'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'data/canadian_hansard/lipad/canadian_hansards.csv'</span>, low_memory<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>df.info()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 946686 entries, 0 to 946685
Data columns (total 16 columns):
 #   Column           Non-Null Count   Dtype 
---  ------           --------------   ----- 
 0   index            946686 non-null  int64 
 1   basepk           946686 non-null  int64 
 2   hid              946686 non-null  object
 3   speechdate       946686 non-null  object
 4   pid              824111 non-null  object
 5   opid             787761 non-null  object
 6   speakeroldname   787032 non-null  object
 7   speakerposition  202294 non-null  object
 8   maintopic        932207 non-null  object
 9   subtopic         926996 non-null  object
 10  subsubtopic      163963 non-null  object
 11  speechtext       946686 non-null  object
 12  speakerparty     787692 non-null  object
 13  speakerriding    686495 non-null  object
 14  speakername      923416 non-null  object
 15  speakerurl       763264 non-null  object
dtypes: int64(2), object(14)
memory usage: 115.6+ MB</code></pre>
<p>The text data is stored in the <code>speechtext</code> Series. We’ll use the dcss <code>preprocess()</code> function to perform the same pre-processing steps that we’ve used a few times since. As a reminder, this function passes each document through SpaCy’s nlp pipeline and returns a list of tokens for each document, each token being a lemmatized noun, proper noun, or adjective that is longer than a single character. The function also strips out English language stopwords.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>texts <span class="op">=</span> df[<span class="st">'speechtext'</span>].tolist()</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>processed_text <span class="op">=</span> preprocess(texts, bigrams<span class="op">=</span><span class="va">False</span>, detokenize<span class="op">=</span><span class="va">False</span>, n_process <span class="op">=</span> <span class="dv">32</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(processed_text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>1893372</code></pre>
<p>Since pre-processing 1.8 million speeches takes a good amount of time, we’ll pickle the results. Then we can easily re-load then again later rather than needlessly waiting around.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'data/preprocessed_speeches_canadian_hansards_no_bigrams.pkl'</span>, <span class="st">'wb'</span>) <span class="im">as</span> handle:</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    pickle.dump(processed_text, handle, protocol<span class="op">=</span>pickle.HIGHEST_PROTOCOL)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>processed_text <span class="op">=</span> pickle.load( <span class="bu">open</span>( <span class="st">'data/preprocessed_speeches_canadian_hansards_no_bigrams.pkl'</span>, <span class="st">'rb'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="creating-a-bag-of-words-with-gensim" class="level5" data-number="25.5.0.0.1">
<h5 data-number="25.5.0.0.1" class="anchored" data-anchor-id="creating-a-bag-of-words-with-gensim"><span class="header-section-number">25.5.0.0.1</span> Creating a Bag-of-Words with Gensim</h5>
<p>To topic model our data with Gensim, we need to provide our list of tokenized texts to the <code>Dictionary()</code> class. Gensim uses this to contruct a corpus vocabulary that assigns each unique token in the dataset to an integer. If you run <code>dict(vocab)</code> after creating the <code>vocab</code> object, you’ll see this this is just a Python dictionary that stores a key:value pairing of the integer representation (the key) of each token (the value).</p>
<p>We’ll also create a corpus object using the <code>doc2bow</code> method of the Dictionary class. The resulting object stores information about the specific tokens and token frequencies in each document. If you print the corpus object, you will see a <em>lot</em> of numbers. The corpus object itself is just a list, and each element of the list represents an individual document. Nested inside each document are tuples (e.g.&nbsp;<code>(147, 3</code>)) that represent (a) the unique integer ID for the token (stored in our <code>vocab</code> object) and (b) the number of times the token appears in the document.</p>
<p>We’re going to filter the vocabulary to keep tokens that only appear in 20 or more speeches, as well as tokens that don’t appear in more than 95% of speeches. This is a fairly inclusive filter but still reduces ~160K words to ~36K words. You will want to experiment with this, but one obvious advantage is that a lot of non-words from parts of the text data that are low quality should end up removed, while non-differentiating words that would probably crowd the topic space will also be left out.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> corpora.Dictionary(processed_text) <span class="co"># id2word</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>vocab.save(<span class="st">'../models/lda_vocab.dict'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The file saved above is easy to reload, so you can experiment with different filter parameters at will.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> corpora.Dictionary.load(<span class="st">'../models/lda_vocab.dict'</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>vocab.filter_extremes(no_below<span class="op">=</span><span class="dv">20</span>, no_above<span class="op">=</span><span class="fl">0.95</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>corpus <span class="op">=</span> [vocab.doc2bow(text) <span class="cf">for</span> text <span class="kw">in</span> processed_text]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(vocab)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>36585</code></pre>
<blockquote class="blockquote">
<p>Note that we’re not using TF-IDF weights in our LDA models, whereas we did use them in the context of LSA. While TF-IDF weights are appropriate in some cases (such as LSA), they are not in LDA models <span class="citation" data-cites="blei2009topic">(<a href="references.html#ref-blei2009topic" role="doc-biblioref">D. Blei and Lafferty 2009</a>)</span>. The reason is because of LDA’s generative nature. It makes sense to say that word frequencies are generated from a distribution, as LDA posits, but it does not make sense to say that TF-IDF weights are generated from that distribution. Consequently using TF-IDF weights in a generative topic model generally worsens the results. In general TF-IDF weights work well in deterministic contexts but less so in generative ones.</p>
</blockquote>
</section>
<section id="running-the-topic-model" class="level3" data-number="25.5.1">
<h3 data-number="25.5.1" class="anchored" data-anchor-id="running-the-topic-model"><span class="header-section-number">25.5.1</span> Running the Topic Model</h3>
<p>We are now ready to fit the topic model to our data. We will do so using Gensim’s <code>LdaModel</code> first, rather than <code>LdaMulticore</code> which is designed to speed up computation by using multiprocessing. They do almost the same things, though with one key difference that we’ll discuss shortly.</p>
<p>The code block below estimates a model for 100 topics, which is an initial value that I selected arbitrarily. Later we will discuss other ways of selecting a number of topics. We’re going to start with a random sample from the corpus and the processed list of text, because even with Gensim’s efficiency and using algorithms designed to perform well on large datasets, this can take a while to run, and this will not be our final model.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>sample_corpus, sample_text <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>random.sample(<span class="bu">list</span>(<span class="bu">zip</span>(corpus,processed_text)),<span class="dv">100000</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>ldamod_s <span class="op">=</span> LdaModel(corpus<span class="op">=</span>sample_corpus,</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>                      id2word<span class="op">=</span>vocab,</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>                      num_topics<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>                      random_state<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>                      eval_every<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>                      chunksize<span class="op">=</span><span class="dv">2000</span>,</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>                      alpha<span class="op">=</span><span class="st">'auto'</span>,</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>                      eta<span class="op">=</span><span class="st">'auto'</span>,</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>                      passes<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>                      update_every<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>                      iterations<span class="op">=</span><span class="dv">400</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>                  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We’ll pickle the results to easily load them later without having to wait for our code to run again.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'data/lda_model_sample.pkl'</span>, <span class="st">'wb'</span>) <span class="im">as</span> handle:</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    pickle.dump(ldamod_s, handle, protocol<span class="op">=</span>pickle.HIGHEST_PROTOCOL)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>ldamod_s <span class="op">=</span> pickle.load(<span class="bu">open</span>( <span class="st">'data/lda_model_sample.pkl'</span>, <span class="st">'rb'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Gensim provides a number of useful functions to simplify working with the results of our LDA model. The ones you’ll likely turn to right away are</p>
<ul>
<li><code>.show_topic()</code>, which takes an integer topic ID and returns a list of the words most strongly associated with that topic,</li>
<li><code>.get_term_topics()</code>, which takes a word and, if it’s in the corpus vocabulary, returns the word’s probability for each topic, and</li>
</ul>
<p>As you can see below, we can find the topics that a word is associated with.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>ldamod_s.get_term_topics(<span class="st">'freedom'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>[(53, 0.02927819)]</code></pre>
<p>Gensim provides the weights associated with each of the top words for each topic. The higher the weight, the more strongly associated with the topic the word is. The words in this case make quite a lot of intuitive sense - freedom has to do with the law, rights, principles, society, and is a fundamental concept.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>ldamod_s.show_topic(<span class="dv">53</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>[('right', 0.15800211),
 ('human', 0.04227337),
 ('freedom', 0.029263439),
 ('law', 0.022657597),
 ('Canadians', 0.018386548),
 ('canadian', 0.018030208),
 ('citizen', 0.017851433),
 ('society', 0.015541217),
 ('fundamental', 0.014947715),
 ('principle', 0.013568851)]</code></pre>
<p>When we look at how parliament talks about criminals, we can see that the associated words are pretty intuitive although we might have to dig a bit further into the terms to find more particular term associations.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>ldamod_s.get_term_topics(<span class="st">'criminal'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>[(20, 0.059014548)]</code></pre>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>ldamod_s.show_topic(<span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>[('crime', 0.07494005),
 ('criminal', 0.058972023),
 ('victim', 0.055283513),
 ('justice', 0.047199916),
 ('offence', 0.03621511),
 ('law', 0.03601918),
 ('offender', 0.03377842),
 ('sentence', 0.032146234),
 ('system', 0.022435088),
 ('person', 0.020964943)]</code></pre>
<p>Let’s look a little closer at something that’s a bit more controversial, like ‘marriage’.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>ldamod_s.get_term_topics(<span class="st">'marriage'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>[(28, 0.042418264)]</code></pre>
<p>We can specify the return of a few more terms for a topic by adding an argument for <code>topn</code>. You can see that when marriage is discussed in parliament, it’s around fairly controversial concepts such as equality, gender, tradition, and even abuse.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>ldamod_s.show_topic(<span class="dv">28</span>, topn<span class="op">=</span><span class="dv">30</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>[('woman', 0.26727995),
 ('man', 0.069456935),
 ('violence', 0.06529136),
 ('marriage', 0.04248659),
 ('girl', 0.023184145),
 ('Women', 0.02255594),
 ('equality', 0.021070031),
 ('Canada', 0.019696228),
 ('society', 0.018637668),
 ('gender', 0.01841013),
 ('abuse', 0.015813459),
 ('issue', 0.015377659),
 ('action', 0.012950255),
 ('practice', 0.01211937),
 ('female', 0.011507524),
 ('equal', 0.011195933),
 ('Status', 0.011139394),
 ('medicare', 0.011124585),
 ('group', 0.010348747),
 ('physical', 0.008267313),
 ('psychological', 0.0075966706),
 ('prescription', 0.0070270123),
 ('traditional', 0.006817099),
 ('Speaker', 0.0067508616),
 ('killing', 0.006746756),
 ('status', 0.006714445),
 ('sexual', 0.0065426086),
 ('victim', 0.0060332483),
 ('government', 0.005900839),
 ('country', 0.0058119465)]</code></pre>
<section id="evaluating-the-quality-of-topic-models-by-measuring-semantic-coherence" class="level4" data-number="25.5.1.1">
<h4 data-number="25.5.1.1" class="anchored" data-anchor-id="evaluating-the-quality-of-topic-models-by-measuring-semantic-coherence"><span class="header-section-number">25.5.1.1</span> Evaluating the Quality of Topic Models by Measuring Semantic Coherence</h4>
<p>The model we just estimated found 100 topics <em>because we told it to</em>. Was 100 a good number? How do we pick a good number of topics, at least as a starting point? Again, my advice here is that you develop your model iteratively, by zooming in and zooming out, each time learning a little more about your data. You should supplement this by reading samples of documents from the corpus. All of this will help you develop a better model.</p>
<p>You can also supplement this with quantitative measures. The ideal number of topics ultimately comes down to interpretability and usefulness for the task at hand. Strange as it might seem, there are quantitative approaches to measuring human readability – in this case by measuring “<strong>semantic coherence</strong>.” These measures can be used to help guide our decisions about how many topics to search for in our topic model. The higher the topic coherence, the more human readable the topics in our model should be. Note that quantitative measures of semantic coherence should <em>help</em> you make a decision, not make it for you. You still want to be a human “in the loop,” reading things and thinking deeply about them.</p>
<p>Most semantic coherence measures work by segmenting the corpus into topics, taking the top words in a topic, putting them in pairs, computing their similarity in vector space, and then aggregating those scores to produce an overall summary of the semantic coherence in your model.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> There is a growing technical literature on computing semantic coherence <span class="citation" data-cites="roder2015exploring">(see <a href="references.html#ref-roder2015exploring" role="doc-biblioref">Röder, Both, and Hinneburg 2015</a> for a good introduction to measures used in some Python implementations)</span>. Higher coherence means that the words in a topic are closer to one another in vector space. The closer they are, the more coherent the topic. The general idea here is that words in a topic are likely to come from similar positions in the vector space. (I’m being a bit hand-wavey here because this is essentially what the next three chapters are about.)</p>
<p>Models with high semantic coherence will tend to have a smaller number of junk topics but, ironically, this sometimes comes with a reduction of the <em>quality</em> of the topics in your model. In other words, we can avoid having any really bad topics but the ones we are left with might themselves be middling. Imagine two models. In the first, you have 40 topics, 37 of which seem good, and 3 of which seem like junk. You can ignore the junk topics and focus on the good ones. In the second, you have 33 topics and end up with a higher coherence score. There are no junk topics in this model, but the 33 topics you got are not really as informative as the 37 good topics from your 40 topic model. Which do you prefer? Can you tolerate a few junk topics or not? Personally, I prefer the solution with more interpretable topics and a few junk topics but, again: there is no absolutely correct answer.</p>
<p>Human qualitative evaluation is labour intensive and, yes, inevitably subjective. Quantitative methods for selecting the number of topics sometimes produce models that seem worse, and – like human qualitative evaluation methods – there may not be agreement that the results are more informative even if the model overall has better semantic coherence. What’s the value of eliminating human judgment from a process that is intended to help you iteratively learn things you didn’t know before? Besides, the whole point of Bayesian analysis is to provide principled ways of integrating information from multiple sources. This includes qualitative interpretation <em>and cricitism</em> of the model. Semantic coherence measures are very useful when it comes to making what might seem like arbitrary decisions in developing your topic model, but ultimately you need to use your human abilities and make informed judgements. This is integral to Box’s loop, as I’ve emphasized troughout the book. It’s better to produce many models and look at them all, interpreting, thinking, critiquing. The more you learn, the better! Finally, to repeat a point I have now made many times, <em>you absolutely must read</em>. There’s no getting around it.</p>
<p>Gensim makes it fairly straightforward to compute coherence measures for topic models. There are numerous coherence measures available “out of the box,” each of which works slightly differently. The measure we will use below – <strong><span class="math inline">\(C_v\)</span></strong> – was shown by <span class="citation" data-cites="roder2015exploring">Röder, Both, and Hinneburg (<a href="references.html#ref-roder2015exploring" role="doc-biblioref">2015</a>)</span> to be the most highly correlated with all available data on human interpretation of the output of topic models.</p>
<blockquote class="blockquote">
<p>There are four steps involved in computing the <span class="math inline">\(C_v\)</span> measure. First, it selects the top (i.e.&nbsp;most probable) <span class="math inline">\(n\)</span> words within any given topic. Second, it computes the probability of single top words and the joint probability of pairs of co-occurring top words by counting the number of texts in which the word or word pair occurs, and dividing by the total number of texts. Third, it vectorizes this data using a measure called normalized Pointwise Mutual Information (NPMI), which tells us whether a pair of words <span class="math inline">\(W_i\)</span> and <span class="math inline">\(W_j\)</span> co-occur more than they would if they were independent of one another. Finally, <span class="math inline">\(C_v\)</span> computes the cosine similarity for all vectors. The final coherence score for a topic model is the mean of all of these cosine similarity scores.</p>
</blockquote>
<p>The <code>C_v</code> measure is already implemented in Gensim, so we can compute it with very little code. To start, let’s compute the “coherence” of our 100 topic model. Note that for <code>C_v</code>, coherence scores range from 0 for complete incoherence to 1 for complete coherence. Values above 0.5 are fairly good, while we can’t expect to find values much above 0.8 in real world text data.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>coherence_model_s <span class="op">=</span> CoherenceModel(model<span class="op">=</span>ldamod_s, </span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>                                     texts<span class="op">=</span>sample_text, </span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>                                     dictionary<span class="op">=</span>vocab, </span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>                                     coherence<span class="op">=</span><span class="st">'c_v'</span>)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>coherence_lda_s <span class="op">=</span> coherence_model_s.get_coherence()</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Coherence Score: '</span>, coherence_lda_s)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Coherence Score:  0.3882056267381639</code></pre>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'data/coherence_model_sample.pkl'</span>, <span class="st">'wb'</span>) <span class="im">as</span> handle:</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    pickle.dump(coherence_model_s, handle, protocol<span class="op">=</span>pickle.HIGHEST_PROTOCOL)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>coherence_model_s <span class="op">=</span> pickle.load( <span class="bu">open</span>( <span class="st">'data/coherence_model_sample.pkl'</span>, <span class="st">'rb'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now that we can calculate <span class="math inline">\(C_v\)</span> scores, we can gauge topic solution’s. But what if 21, 37, 42, or <span class="math inline">\(n\)</span> other number of topics would be better? As discussed earlier, we’ve used an asymmetric, trained prior, so even if we selected too many topics, the quality of the best ones should still be pretty good. Let’s take a look at their coherence scores in a dataframe, sorted highest to lowest on coherence, and lowest to highest on standard deviation (although the latter will only have an effect if we have any identical coherence scores).</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>topic_coherence_s <span class="op">=</span> coherence_model_s.get_coherence_per_topic(with_std <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>topic_coherence_df <span class="op">=</span> pd.DataFrame(topic_coherence_s, columns <span class="op">=</span> [<span class="st">'coherence'</span>,<span class="st">'std'</span>])</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>topic_coherence_df <span class="op">=</span> topic_coherence_df.sort_values([<span class="st">'coherence'</span>, <span class="st">'std'</span>], ascending<span class="op">=</span>[<span class="va">False</span>,<span class="va">True</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The top 10 most coherent topics actually have fairly high scores!</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>topic_coherence_df.head(<span class="dv">10</span>).mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>coherence    0.640070
std          0.233939
dtype: float64</code></pre>
<div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>topic_coherence_df.tail(<span class="dv">10</span>).mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>coherence    0.213738
std          0.182295
dtype: float64</code></pre>
<blockquote class="blockquote">
<p><strong>Further Reading</strong></p>
<p><span class="citation" data-cites="wallach2009evaluation">H. Wallach et al. (<a href="references.html#ref-wallach2009evaluation" role="doc-biblioref">2009</a>)</span> and <span class="citation" data-cites="mimno2011optimizing">Mimno et al. (<a href="references.html#ref-mimno2011optimizing" role="doc-biblioref">2011</a>)</span> offer useful guidelines for evaluatating topic models using semantic coherence.</p>
</blockquote>
</section>
<section id="going-further-with-better-priors" class="level4" data-number="25.5.1.2">
<h4 data-number="25.5.1.2" class="anchored" data-anchor-id="going-further-with-better-priors"><span class="header-section-number">25.5.1.2</span> Going Further with Better Priors</h4>
<p>As promised, let’s examine selecting a few different <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\eta\)</span> hyperparameters. We’ll use the same data as before, but rather than a sample, let’s use the whole corpus. For the most part, you can probably feel safe using the “auto” setting for both since your priors will be informed by the data rather than being arbitrary. Unfortunately, with the amount of data in the full corpus, you want to use <code>LdaMulticore</code> rather than the base <code>LdaModel</code>, but the “auto” option is not implemented for <span class="math inline">\(\alpha\)</span> in the <em>much</em> faster multicore option. Your built-in options are either a uniform scalar probability for each topic, or “asymmetrical”. The code block below shows how an asymmetrical <span class="math inline">\(\alpha\)</span> is constructed, as well as the simple scalar value for <span class="math inline">\(\eta\)</span> used in the paper that informs Gensim’s LDA implementation <span class="citation" data-cites="hoffman2010online">(<a href="references.html#ref-hoffman2010online" role="doc-biblioref">Hoffman, Bach, and Blei 2010</a>)</span>.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>alpha_asym <span class="op">=</span> np.fromiter(</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>                    (<span class="fl">1.0</span> <span class="op">/</span> (i <span class="op">+</span> np.sqrt(<span class="dv">100</span>)) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>)),</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>                    dtype<span class="op">=</span>np.float16, count<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>eta_sym <span class="op">=</span> <span class="dv">1</span><span class="op">/</span><span class="dv">100</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>But we can also use the automatically updated <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\eta\)</span> hyperparameters from our earlier model on the sample of the corpus as long as we plan to use the <em>same</em> number of topics. Because we ran <code>LdaModel</code> with 1) frequent model perplexity evaluations 2) frequent hyperparameter updates and 3) two full passes over the sample data along with 400 iterations per document, we can expect the <span class="math inline">\(\alpha\)</span> prior to have a lot of fine-grained nuance. There are more complex ways to sample the data in order to produce trained asymmetric priors, such as the Bayesian slice sampling detailed in,<span class="citation" data-cites="syed2018selecting">(<a href="references.html#ref-syed2018selecting" role="doc-biblioref">Syed and Spruit 2018</a>)</span> but they are outside the scope of this chapter. Taking the output from one Bayesian model to use in another is a lot like the transfer learning methods that are covered in the final chapters of this book.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>alpha_t <span class="op">=</span> ldamod_s.alpha</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>eta_t <span class="op">=</span> ldamod_s.eta</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let’s get a sense of the difference between the options we’ve discussed for priors, by calculating the average of the probabilities as well as the amount of variance they have.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Trained alpha variance: "</span> <span class="op">+</span> <span class="bu">str</span>(np.<span class="bu">round</span>(np.var(alpha_t), <span class="dv">4</span>)))</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Asymmetric alpha variance: "</span> <span class="op">+</span> <span class="bu">str</span>(np.<span class="bu">round</span>(np.var(alpha_asym), <span class="dv">4</span>)))</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Trained alpha avg: "</span> <span class="op">+</span> <span class="bu">str</span>(np.<span class="bu">round</span>(alpha_t.<span class="bu">sum</span>()<span class="op">/</span><span class="bu">len</span>(alpha_t), <span class="dv">4</span>)))</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Asymmetric alpha avg: "</span> <span class="op">+</span> <span class="bu">str</span>(np.<span class="bu">round</span>(alpha_asym.<span class="bu">sum</span>()<span class="op">/</span><span class="bu">len</span>(alpha_asym), <span class="dv">4</span>)))</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Trained eta variance: "</span> <span class="op">+</span> <span class="bu">str</span>(np.<span class="bu">round</span>(np.var(eta_t), <span class="dv">4</span>)))</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Symmetric eta variance: "</span> <span class="op">+</span> <span class="bu">str</span>(np.<span class="bu">round</span>(np.var(eta_sym), <span class="dv">4</span>)))</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Trained eta avg: "</span> <span class="op">+</span> <span class="bu">str</span>(np.<span class="bu">round</span>(eta_t.<span class="bu">sum</span>()<span class="op">/</span><span class="bu">len</span>(eta_t),<span class="dv">4</span>)))</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Symmetric eta avg: "</span> <span class="op">+</span> <span class="bu">str</span>(np.<span class="bu">round</span>(eta_sym, <span class="dv">4</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Trained alpha variance: 0.0006
Asymmetric alpha variance: 0.0004
Trained alpha avg: 0.0304
Asymmetric alpha avg: 0.0244
Trained eta variance: 0.0003
Symmetric eta variance: 0.0
Trained eta avg: 0.0098
Symmetric eta avg: 0.01</code></pre>
<p>As you can see, the trained <span class="math inline">\(\alpha\)</span> prior has around 1.5x the variance of the simpler asymmetric version and around 1.25x the average topic probability. The trained <span class="math inline">\(\eta\)</span> priors, on the other hand, end up with only half the variance of the trained alpha, and the average word probability is <em>very</em> close to the simple <code>1/100</code> symmetrical prior. Although the automatic updates for priors add only linear computation complexity, it’s always nice to trim computation time wherever possible, so you might find that it’s just as good to use a simple scalar for the <span class="math inline">\(\eta\)</span> hyperparameter.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>ldamod_f <span class="op">=</span> LdaMulticore(corpus<span class="op">=</span>corpus,</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>                      id2word<span class="op">=</span>vocab,</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>                      num_topics<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>                      random_state<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>                      chunksize<span class="op">=</span><span class="dv">2000</span>,</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>                      alpha<span class="op">=</span>alpha_t,</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>                      eta<span class="op">=</span>eta_t,</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>                      passes<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>                      iterations<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>                      workers<span class="op">=</span><span class="dv">15</span>,</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>                      per_word_topics<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'data/lda_model_full.pkl'</span>, <span class="st">'wb'</span>) <span class="im">as</span> handle:</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>    pickle.dump(ldamod_f, handle, protocol<span class="op">=</span>pickle.HIGHEST_PROTOCOL)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>ldamod_f <span class="op">=</span> pickle.load( <span class="bu">open</span>( <span class="st">'data/lda_model_full.pkl'</span>, <span class="st">'rb'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>coherence_model_full <span class="op">=</span> CoherenceModel(model<span class="op">=</span>ldamod_f,</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>                                     texts<span class="op">=</span>processed_text,</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>                                     dictionary<span class="op">=</span>vocab,</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>                                     coherence<span class="op">=</span><span class="st">'c_v'</span>)</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>coherence_full <span class="op">=</span> coherence_model_full.get_coherence()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'data/coherence_model_full.pkl'</span>, <span class="st">'wb'</span>) <span class="im">as</span> handle:</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>    pickle.dump(coherence_model_full, handle, protocol<span class="op">=</span>pickle.HIGHEST_PROTOCOL)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>coherence_model_full <span class="op">=</span> pickle.load( <span class="bu">open</span>( <span class="st">'data/coherence_model_full.pkl'</span>, <span class="st">'rb'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We have actually gained a slight amount of topic coherence after training on 1.8 million documents rather than 100,000!</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>coherence_full</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>0.3943192191621368</code></pre>
<p>Let’s look at per-topic results in a dataframe, to compare to the results from the sample corpus. First we’ll compare the average coherence scores for the top 30 and bottom 30 topics.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>topic_coherence_f <span class="op">=</span> coherence_model_full.get_coherence_per_topic(with_std <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>topic_coherence_f_df <span class="op">=</span> pd.DataFrame(topic_coherence_f, columns <span class="op">=</span> [<span class="st">'coherence'</span>,<span class="st">'std'</span>])</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>topic_coherence_f_df <span class="op">=</span> topic_coherence_f_df.sort_values([<span class="st">'coherence'</span>, <span class="st">'std'</span>], ascending<span class="op">=</span>[<span class="va">False</span>,<span class="va">True</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Full model average coherence top 30 topics: "</span> <span class="op">+</span> <span class="bu">str</span>(topic_coherence_f_df[<span class="st">'coherence'</span>].head(<span class="dv">30</span>).mean()))</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Sample model average coherence top 30 topics: "</span> <span class="op">+</span> <span class="bu">str</span>(topic_coherence_df[<span class="st">'coherence'</span>].head(<span class="dv">30</span>).mean()))</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Full model average coherence bottom 30 topics: "</span> <span class="op">+</span> <span class="bu">str</span>(topic_coherence_f_df[<span class="st">'coherence'</span>].tail(<span class="dv">30</span>).mean()))</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Sample model average coherence bottom 30 topics: "</span> <span class="op">+</span> <span class="bu">str</span>(topic_coherence_df[<span class="st">'coherence'</span>].tail(<span class="dv">30</span>).mean()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Full model average coherence top 30 topics: 0.4943150877317519
Sample model average coherence top 30 topics: 0.550659531826426
Full model average coherence bottom 30 topics: 0.2952359669399737
Sample model average coherence bottom 30 topics: 0.2517402193295241</code></pre>
<p>We’ve actually lost a bit of coherence in the top 30 topics, while gaining some in the bottom 30. One thing to keep in mind is that coherence scores are a convenient, objective way to assess a topic model, but they are not a substitute for subjectively inspecting the topics themselves! Below we can see that topic 20 remains the most coherent, while a few others also remain in the top 10 but at different positions.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>topic_coherence_f_df.head(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>    coherence       std
20   0.604434  0.331337
77   0.565511  0.242574
90   0.545323  0.344012
55   0.534403  0.306397
73   0.530510  0.295568
88   0.523907  0.287610
10   0.521217  0.340641
89   0.521041  0.333885
54   0.519937  0.309211
75   0.515672  0.339481</code></pre>
<div class="sourceCode" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>topic_coherence_df.head(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>    coherence       std
20   0.721814  0.228599
65   0.676778  0.116802
12   0.657578  0.283031
55   0.640755  0.331486
74   0.632465  0.197373
42   0.630215  0.211954
17   0.626714  0.253870
80   0.609308  0.241425
4    0.607440  0.215214
75   0.597635  0.259641</code></pre>
<p>Now that we have run through all of these models, let’s actually examine the topics with some visualization of the most coherent ones!</p>
<blockquote class="blockquote">
<p><strong>Further Reading</strong></p>
<p><span class="citation" data-cites="wallach2009rethinking">H. M. Wallach, Mimno, and McCallum (<a href="references.html#ref-wallach2009rethinking" role="doc-biblioref">2009</a>)</span> provides some usful advice on thinking through the use of priors in LDA topic models.</p>
</blockquote>
</section>
<section id="visualizing-topic-model-output-with-pyldavis" class="level4" data-number="25.5.1.3">
<h4 data-number="25.5.1.3" class="anchored" data-anchor-id="visualizing-topic-model-output-with-pyldavis"><span class="header-section-number">25.5.1.3</span> Visualizing Topic Model Output with PyLDAVis</h4>
<p>One way to explore a topic model is to use the pyLDAvis package to interact with dynamic browser-based visualizations. This package is designed to hook into Gensim’s data structures seamlessly. We will provide pyLDAvis with (1) the name of the object storing our LDA model, (2) the corpus object, and (3) the vocab. We will then write the results to an HTML file that you can open in your browser and explore interactively. This can take a while to run, so if you just want to explore the results seen below, you can find the HTML file in the <code>data/misc</code> directory.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pyLDAvis.gensim_models <span class="im">as</span> gensimvis</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyLDAvis <span class="im">import</span> save_html</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>vis <span class="op">=</span> gensimvis.prepare(ldamod_f, corpus, vocab)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>save_html(vis, <span class="st">'data/misc/ldavis_full_model.html'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="figures/ldavis_full_model_topic_20.png" class="img-fluid"></p>
<p>Of course, I can’t show the interactive results here on the printed page, but what you will see in your browser will be something like the graph shown in Figure <span class="quarto-unresolved-ref">?fig-30_05</span>. These maps contain a <em>lot</em> of information, so it’s worth taking your time to explore them fully.</p>
<p>How do you read this? On the left we have a two-dimensional representation of the distances between topics. The distances between topics are computed with Jensen-Shannon divergence (which is a way of measuring the distance between two probability distributions <span class="citation" data-cites="lin1991divergence">(<a href="references.html#ref-lin1991divergence" role="doc-biblioref">Lin 1991</a>)</span>), and then a principal components analysis is performed on the results. The interactive graph says the map is produced “via multidimensional scaling” because MDS is a <em>general</em> class of analysis and PCA is a <em>specific</em> method. Topics that are closer together in the map are more similar. Topics that are further away from one another are dissimilar. <em>However</em>, recall that we don’t know how much variance in the data is actually accounted for with these two dimensions, so we should assume that, like other text analyses, the first two principal components don’t actually account for much variance. Accordingly, we should interpret the spatial relationships between topics with a <em>healthy dose</em> of scepticism. Finally, the size of the topic in this map is related to how common it is. Bigger topics are more common. Smaller topics are rare. Sizing points like this is generally not considered best data visualization practice, but we are not focused on comparing topics on their size, so it’s generally okay.</p>
<p>On the right of the graph, we have horizontal bar graphs that update as you mouse over a topic. These are the words that are most useful for interpreting what a given topic is about. The red shows you how common the word is in the topic, and the blue shows you how common it is in the rest of the corpus. So topics with a lot of red but not a lot of blue are more exclusive to the topic. If you mouse over the words in the bar graphs, the MDS map changes to show you the conditional distribution over topics on the MDS map.</p>
<p>Finally, you can change the meaning of “words that are most useful for interpreting what a given topic is about” by changing the value of the <span class="math inline">\(\lambda\)</span> parameter. You do this moving the slider. If <span class="math inline">\(\lambda\)</span> = 1, then the words provided are ranked in order of their probability of appearing in that specific topic. Setting them at 0 reorders the words displayed by their “lift” score, !keyword(lift scores) which is defined as the ratio of their probability within the topic to its marginal probability across the corpus. The idea, sort of like with TF-IDF, is that words that have a high probability of occurring across the whole corpus are not helpful in interpreting individual topics. You want to find some sort of balance that helps you understand what the topics are about. If you are following along with the code, take some time exploring the results in your browser.</p>
</section>
</section>
</section>
<section id="conclusion" class="level2" data-number="25.6">
<h2 data-number="25.6" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">25.6</span> CONCLUSION</h2>
<section id="key-points" class="level3" data-number="25.6.1">
<h3 data-number="25.6.1" class="anchored" data-anchor-id="key-points"><span class="header-section-number">25.6.1</span> Key Points</h3>
<ul>
<li>Generative topic models are Bayesian models used to understand latent themes in documents containing thematically linked words</li>
<li>Developed latent Dirichlet allocation models using Gensim</li>
<li>Variational inference is an alternative to Hamiltonian Monte Carlo (HMC) that provides a <em>function</em> with an analytical solution for the approximate posterior distribution</li>
<li>Evaluated topic coherence using quantitative measures</li>
<li>Visualized topic models with PyLDAVis</li>
</ul>


<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-armstrong2008culture" class="csl-entry" role="listitem">
Armstrong, Elizabeth, and Mary Bernstein. 2008. <span>“Culture, Power, and Institutions: A Multi-Institutional Politics Approach to Social Movements.”</span> <em>Sociological Theory</em> 26 (1): 74–99.
</div>
<div id="ref-benford1993frame" class="csl-entry" role="listitem">
Benford, Robert. 1993. <span>“Frame Disputes Within the Nuclear Disarmament Movement.”</span> <em>Social Forces</em> 71 (3): 677–701.
</div>
<div id="ref-benford2000framing" class="csl-entry" role="listitem">
Benford, Robert, and David Snow. 2000. <span>“Framing Processes and Social Movements: An Overview and Assessment.”</span> <em>Annual Review of Sociology</em> 26 (1): 611–39.
</div>
<div id="ref-blei2012probabilistic" class="csl-entry" role="listitem">
Blei, David. 2012. <span>“Probabilistic Topic Models.”</span> <em>Communications of the ACM</em> 55 (4): 77–84.
</div>
<div id="ref-blei_talk" class="csl-entry" role="listitem">
———. 2017. <span>“Variational Inference: Foundations and Innovations.”</span> Simons Institute: Computational Challenges in Machine Learning.
</div>
<div id="ref-blei2006dynamic" class="csl-entry" role="listitem">
Blei, David M, and John D Lafferty. 2006. <span>“Dynamic Topic Models.”</span> In <em>Proceedings of the 23rd International Conference on Machine Learning</em>, 113–20.
</div>
<div id="ref-blei2009topic" class="csl-entry" role="listitem">
Blei, David, and John Lafferty. 2009. <span>“Topic Models.”</span> <em>Text Mining: Classification, Clustering, and Applications</em> 10 (71): 34.
</div>
<div id="ref-blei2003latent" class="csl-entry" role="listitem">
Blei, David, Andrew Ng, and Michael I Jordan. 2003. <span>“Latent Dirichlet Allocation.”</span> <em>The Journal of Machine Learning Research</em> 3: 993–1022.
</div>
<div id="ref-caren2007political" class="csl-entry" role="listitem">
Caren, Neal. 2007. <span>“Political Process Theory.”</span> <em>The Blackwell Encyclopedia of Sociology</em>.
</div>
<div id="ref-dimaggio2013exploiting" class="csl-entry" role="listitem">
DiMaggio, Paul, Manish Nag, and David Blei. 2013. <span>“Exploiting Affinities Between Topic Modeling and the Sociological Perspective on Culture: Application to Newspaper Coverage of US Government Arts Funding.”</span> <em>Poetics</em> 41 (6): 570–606.
</div>
<div id="ref-hoffman2010online" class="csl-entry" role="listitem">
Hoffman, Matthew, Francis Bach, and David Blei. 2010. <span>“Online Learning for Latent Dirichlet Allocation.”</span> In <em>Advances in Neural Information Processing Systems</em>, 856–64. Citeseer.
</div>
<div id="ref-jordan2003introduction" class="csl-entry" role="listitem">
Jordan, Michael. 2003. <span>“An Introduction to Probabilistic Graphical Models.”</span> preparation.
</div>
<div id="ref-jordan2004graphical" class="csl-entry" role="listitem">
———. 2004. <span>“Graphical Models.”</span> <em>Statistical Science</em> 19 (1): 140–55.
</div>
<div id="ref-koller2009probabilistic" class="csl-entry" role="listitem">
Koller, Daphne, and Nir Friedman. 2009. <em>Probabilistic Graphical Models: Principles and Techniques</em>. MIT press.
</div>
<div id="ref-lin1991divergence" class="csl-entry" role="listitem">
Lin, Jianhua. 1991. <span>“Divergence Measures Based on the Shannon Entropy.”</span> <em>IEEE Transactions on Information Theory</em> 37 (1): 145–51.
</div>
<div id="ref-mcadam2010political" class="csl-entry" role="listitem">
McAdam, Doug. 2010. <em>Political Process and the Development of Black Insurgency, 1930-1970</em>. University of Chicago Press.
</div>
<div id="ref-mccammon2009beyond" class="csl-entry" role="listitem">
McCammon, Holly. 2009. <span>“Beyond Frame Resonance: The Argumentative Structure and Persuasive Capacity of Twentieth-Century US Women’s Jury-Rights Frames.”</span> <em>Mobilization: An International Quarterly</em> 14 (1): 45–64.
</div>
<div id="ref-mccammon2012us" class="csl-entry" role="listitem">
———. 2012. <em>The US Women’s Jury Movements and Strategic Adaptation: A More Just Verdict</em>. Cambridge University Press.
</div>
<div id="ref-mccammon2007movement" class="csl-entry" role="listitem">
McCammon, Holly J, Courtney Sanders Muse, Harmony D Newman, and Teresa M Terrell. 2007. <span>“Movement Framing and Discursive Opportunity Structures: The Political Successes of the US Women’s Jury Movements.”</span> <em>American Sociological Review</em> 72 (5): 725–49.
</div>
<div id="ref-mccarthy1977resource" class="csl-entry" role="listitem">
McCarthy, John D, and Mayer N Zald. 1977. <span>“Resource Mobilization and Social Movements: A Partial Theory.”</span> <em>American Journal of Sociology</em> 82 (6): 1212–41.
</div>
<div id="ref-mcelreath2020statistical" class="csl-entry" role="listitem">
McElreath, Richard. 2020. <em>Statistical Rethinking: A Bayesian Course with Examples in r and Stan</em>. CRC press.
</div>
<div id="ref-mimno2011optimizing" class="csl-entry" role="listitem">
Mimno, David, Hanna Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. 2011. <span>“Optimizing Semantic Coherence in Topic Models.”</span> In <em>Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</em>, 262–72.
</div>
<div id="ref-mohr2013introduction" class="csl-entry" role="listitem">
Mohr, John, and Petko Bogdanov. 2013. <span>“Introduction—Topic Models: What They Are and Why They Matter.”</span> Elsevier.
</div>
<div id="ref-murphy2012machine" class="csl-entry" role="listitem">
Murphy, Kevin. 2012. <em>Machine Learning: A Probabilistic Perspective</em>. MIT press.
</div>
<div id="ref-pearl2018book" class="csl-entry" role="listitem">
Pearl, Judea, and Dana Mackenzie. 2018. <em>The Book of Why: The New Science of Cause and Effect</em>. Basic books.
</div>
<div id="ref-roberts2014structural" class="csl-entry" role="listitem">
Roberts, Margaret E, Brandon M Stewart, Dustin Tingley, Christopher Lucas, Jetson Leder-Luis, Shana Kushner Gadarian, Bethany Albertson, and David G Rand. 2014. <span>“Structural Topic Models for Open-Ended Survey Responses.”</span> <em>American Journal of Political Science</em> 58 (4): 1064–82.
</div>
<div id="ref-roberts2013structural" class="csl-entry" role="listitem">
Roberts, Margaret, Brandon Stewart, Dustin Tingley, and Edoardo Airoldi. 2013. <span>“The Structural Topic Model and Applied Social Science.”</span> In <em>Advances in Neural Information Processing Systems Workshop on Topic Models: Computation, Application, and Evaluation</em>, 4:1–20. Harrahs; Harveys, Lake Tahoe.
</div>
<div id="ref-roder2015exploring" class="csl-entry" role="listitem">
Röder, Michael, Andreas Both, and Alexander Hinneburg. 2015. <span>“Exploring the Space of Topic Coherence Measures.”</span> In <em>Proceedings of the Eighth ACM International Conference on Web Search and Data Mining</em>, 399–408.
</div>
<div id="ref-rosen2012author" class="csl-entry" role="listitem">
Rosen-Zvi, Michal, Thomas Griffiths, Mark Steyvers, and Padhraic Smyth. 2012. <span>“The Author-Topic Model for Authors and Documents.”</span> <em>arXiv Preprint arXiv:1207.4169</em>.
</div>
<div id="ref-smith2009structural" class="csl-entry" role="listitem">
Smith, Jackie, and Tina Fetner. 2009. <span>“Structural Approaches in the Sociology of Social Movements.”</span> In <em>Handbook of Social Movements Across Disciplines</em>, 13–57. Springer.
</div>
<div id="ref-snow2014emergence" class="csl-entry" role="listitem">
Snow, David, Robert Benford, Holly McCammon, Lyndi Hewitt, and Scott Fitzgerald. 2014. <span>“The Emergence, Development, and Future of the Framing Perspective: 25+ Years Since" Frame Alignment".”</span> <em>Mobilization: An International Quarterly</em> 19 (1): 23–46.
</div>
<div id="ref-syed2018selecting" class="csl-entry" role="listitem">
Syed, Shaheen, and Marco Spruit. 2018. <span>“Selecting Priors for Latent Dirichlet Allocation.”</span> In <em>2018 IEEE 12th International Conference on Semantic Computing (ICSC)</em>, 194–202. IEEE.
</div>
<div id="ref-wallach2009rethinking" class="csl-entry" role="listitem">
Wallach, Hanna M, David M Mimno, and Andrew McCallum. 2009. <span>“Rethinking LDA: Why Priors Matter.”</span> In <em>Advances in Neural Information Processing Systems</em>, 1973–81.
</div>
<div id="ref-wallach2009evaluation" class="csl-entry" role="listitem">
Wallach, Hanna, Iain Murray, Ruslan Salakhutdinov, and David Mimno. 2009. <span>“Evaluation Methods for Topic Models.”</span> In <em>Proceedings of the 26th Annual International Conference on Machine Learning</em>, 1105–12.
</div>
<div id="ref-wang2012continuous" class="csl-entry" role="listitem">
Wang, Chong, David Blei, and David Heckerman. 2012. <span>“Continuous Time Dynamic Topic Models.”</span> <em>arXiv Preprint arXiv:1206.3298</em>.
</div>
<div id="ref-ypma1995historical" class="csl-entry" role="listitem">
Ypma, Tjalling J. 1995. <span>“Historical Development of the Newton–Raphson Method.”</span> <em>SIAM Review</em> 37 (4): 531–51.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>Some alternative approaches are based on computations of conditional probabilities for pairs of words.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./24-latent-structure-networks.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Latent structure in networks</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./26-complex-adaptive-systems.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Agent-based Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>