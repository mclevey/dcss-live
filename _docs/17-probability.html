<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Doing Computational Social ScienceThe Continuous Development Edition - 17&nbsp; Probability 101</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./18-credibility.html" rel="next">
<link href="./16-prediction.html" rel="prev">
<link href="./media/logo.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="custom.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./15-machine-and-statistical-learning.html">Prediction and Inference</a></li><li class="breadcrumb-item"><a href="./17-probability.html"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Probability 101</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./media/logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Doing Computational Social Science<br><span class="small">The <strong>Continuous Development</strong> Edition</span></a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/UWNETLAB/dcss_supplementary/tree/master/book/" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
    </div>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">üëã Hello!</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Learning to Do Computational Social Science</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Research Computing</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-getting-started.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Getting Started</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-python-101.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Python 101</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-python-102.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Python 102</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Obtaining Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-sampling-and-survey-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Sampling and survey data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-web-data-apis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Web data (APIs)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-web-data-scraping.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Web data (Scraping)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-audio-image-and-document-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Audio, image, and document data</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Exploring Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-processing-structured-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Processing Structured Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-exploratory-data-analysis-and-visualization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Exploratory data analysis and visualization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-association-and-latent-factors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Association and latent factors</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-text-as-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Text as Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-text-similarity-and-latent-semantic-space.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Text similarity and latent semantic space</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-social-networks-and-relational-thinking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Networks: Relationships as Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-structural-similarity-and-latent-social-space.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Structural similarity and latent social space</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Prediction and Inference</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-machine-and-statistical-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Machine Learning 101</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-prediction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./17-probability.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Probability 101</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./18-credibility.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Credibility</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./19-causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Causality</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Generative Modeling</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./20-linear-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Bayesian Regression Models with Probabilistic Programming</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./21-multilevel-regression-with-post-stratification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Multilevel regression with post-stratification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./22-generalized-linear-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Generalized Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./23-causal-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Causal analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./24-latent-structure-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Latent structure in networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./25-latent-topics-text-lda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Latent topics in text (LDA)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./26-complex-adaptive-systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Agent-based Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./27-developing-agent-based-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Diffusion</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">Deep Learning Demystified</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./28-artificial-neural-networks-fnn-rnn-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Neural networks 101</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./29-processing-natural-language-data-spacy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Processing Natural Language Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./30-transformers-self-attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Transformers, Self-attention architecture</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./31-latent-topics-text-transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Modelling latent topics (Transformers)</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
 <span class="menu-text">Professional Responsibilities</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./32-research-ethics-politics-and-practices.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Research Ethics, Politics, and Practices</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./33-next-steps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Next steps</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Acknowledgements</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./changelog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Changelog</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-centrality-formulas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Centrality Formulas</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix-courses.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Courses and Workshops</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives"><span class="header-section-number">17.1</span> LEARNING OBJECTIVES</a></li>
  <li><a href="#learning-materials" id="toc-learning-materials" class="nav-link" data-scroll-target="#learning-materials"><span class="header-section-number">17.2</span> LEARNING MATERIALS</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction"><span class="header-section-number">17.3</span> INTRODUCTION</a>
  <ul class="collapse">
  <li><a href="#imports" id="toc-imports" class="nav-link" data-scroll-target="#imports"><span class="header-section-number">17.3.1</span> Imports</a></li>
  </ul></li>
  <li><a href="#foundational-concepts-in-probability-theory" id="toc-foundational-concepts-in-probability-theory" class="nav-link" data-scroll-target="#foundational-concepts-in-probability-theory"><span class="header-section-number">17.4</span> FOUNDATIONAL CONCEPTS IN PROBABILITY THEORY</a></li>
  <li><a href="#probability-distributions-and-likelihood-functions" id="toc-probability-distributions-and-likelihood-functions" class="nav-link" data-scroll-target="#probability-distributions-and-likelihood-functions"><span class="header-section-number">17.5</span> PROBABILITY DISTRIBUTIONS AND LIKELIHOOD FUNCTIONS</a>
  <ul class="collapse">
  <li><a href="#discrete-distributions-probability-mass-functions" id="toc-discrete-distributions-probability-mass-functions" class="nav-link" data-scroll-target="#discrete-distributions-probability-mass-functions"><span class="header-section-number">17.5.1</span> Discrete Distributions, Probability Mass Functions</a></li>
  </ul></li>
  <li><a href="#continuous-distributions-probability-density-functions" id="toc-continuous-distributions-probability-density-functions" class="nav-link" data-scroll-target="#continuous-distributions-probability-density-functions"><span class="header-section-number">17.6</span> CONTINUOUS DISTRIBUTIONS, PROBABILITY DENSITY FUNCTIONS</a>
  <ul class="collapse">
  <li><a href="#the-normal-distribution" id="toc-the-normal-distribution" class="nav-link" data-scroll-target="#the-normal-distribution"><span class="header-section-number">17.6.1</span> The Normal Distribution</a></li>
  </ul></li>
  <li><a href="#joint-and-conditional-probabilities" id="toc-joint-and-conditional-probabilities" class="nav-link" data-scroll-target="#joint-and-conditional-probabilities"><span class="header-section-number">17.7</span> JOINT AND CONDITIONAL PROBABILITIES</a>
  <ul class="collapse">
  <li><a href="#joint-probabilities" id="toc-joint-probabilities" class="nav-link" data-scroll-target="#joint-probabilities"><span class="header-section-number">17.7.1</span> Joint Probabilities</a></li>
  <li><a href="#conditional-probability" id="toc-conditional-probability" class="nav-link" data-scroll-target="#conditional-probability"><span class="header-section-number">17.7.2</span> Conditional Probability</a></li>
  </ul></li>
  <li><a href="#bayesian-inference" id="toc-bayesian-inference" class="nav-link" data-scroll-target="#bayesian-inference"><span class="header-section-number">17.8</span> BAYESIAN INFERENCE</a>
  <ul class="collapse">
  <li><a href="#bayes-theorem" id="toc-bayes-theorem" class="nav-link" data-scroll-target="#bayes-theorem"><span class="header-section-number">17.8.1</span> Bayes‚Äô Theorem</a></li>
  <li><a href="#the-components-of-bayes-theorem" id="toc-the-components-of-bayes-theorem" class="nav-link" data-scroll-target="#the-components-of-bayes-theorem"><span class="header-section-number">17.8.2</span> The Components of Bayes‚Äô Theorem:</a></li>
  </ul></li>
  <li><a href="#posterior-probability" id="toc-posterior-probability" class="nav-link" data-scroll-target="#posterior-probability"><span class="header-section-number">17.9</span> POSTERIOR PROBABILITY</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">17.10</span> CONCLUSION</a>
  <ul class="collapse">
  <li><a href="#key-points" id="toc-key-points" class="nav-link" data-scroll-target="#key-points"><span class="header-section-number">17.10.1</span> Key Points</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/UWNETLAB/dcss_supplementary/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Probability 101</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>::: ## Planned Changes</p>
<p>Full rewrite.</p>
<!-- {{< include _statistical-machine-learning-and-generative-modelling.qmd >}} -->
<p>:::</p>
<section id="learning-objectives" class="level2" data-number="17.1">
<h2 data-number="17.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">17.1</span> LEARNING OBJECTIVES</h2>
<ul>
<li>Explain the meaning of key concepts in probability theory, including
<ul>
<li>random variable</li>
<li>sample space</li>
</ul></li>
<li>Differentiate between (<em>i</em>) discrete probability distributions and Probability Mass Functions (PMFs) and (<em>ii</em>) continuous probability distributions and Probability Density Functions (PDFs)</li>
<li>Learn how to select probability distributions for modelling by thinking through their assumptions and learning about the parameters they take</li>
<li>Explain the differences between marginal, joint, and conditional probabilities, and the notation used to express these probabilities</li>
<li>Understand Bayes‚Äô theorem and its component parts</li>
</ul>
</section>
<section id="learning-materials" class="level2" data-number="17.2">
<h2 data-number="17.2" class="anchored" data-anchor-id="learning-materials"><span class="header-section-number">17.2</span> LEARNING MATERIALS</h2>
<p>You can find the online learning materials for this chapter in <code>doing_computational_social_science/Chapter_26</code>. <code>cd</code> into the directory and launch your Jupyter Server.</p>
</section>
<section id="introduction" class="level2" data-number="17.3">
<h2 data-number="17.3" class="anchored" data-anchor-id="introduction"><span class="header-section-number">17.3</span> INTRODUCTION</h2>
<p>In the previous chapter, we discussed how the Frequentist paradigm differs from the Bayesian interpretation probability and contrasted discriminative models with generative models. Throughout that discussion, we didn‚Äôt actually discuss the mathematics of probability, which are the same regardless of your philosophical persuasion, or the role that probabilities play in the models you develop.</p>
<p>This chapter is a primer on probability theory. Unlike other introductions to probability, we won‚Äôt be getting into any mathematical proofs; there are plenty of those available elsewhere. Instead, we will clarify some foundational concepts and aim to build a bit of intuition about how different types of probability distributions work through simple simulations. Though not especially broad or deep, this introduction will provide enough knowledge of probability that you will be able to understand, develop, critique, interpret, and revise generative models for structured, network / relational, and text data.</p>
<section id="imports" class="level3" data-number="17.3.1">
<h3 data-number="17.3.1" class="anchored" data-anchor-id="imports"><span class="header-section-number">17.3.1</span> Imports</h3>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dcss <span class="im">import</span> set_style</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>set_style()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="foundational-concepts-in-probability-theory" class="level2" data-number="17.4">
<h2 data-number="17.4" class="anchored" data-anchor-id="foundational-concepts-in-probability-theory"><span class="header-section-number">17.4</span> FOUNDATIONAL CONCEPTS IN PROBABILITY THEORY</h2>
<p>Frequentists and Bayesians differ in their philosophies but not in their mathematics. Both paradigms are built on a solid mathematical foundation of <strong>probability distributions</strong> that define the relative probability of <em>all</em> potential events that could result from some pre-defined system, experiment, or trial.</p>
<p>The starting point for probability theory is the <strong>sample space</strong>, sometimes referred to using the symbol <span class="math inline">\(S\)</span>. The sample space is simply an exhaustive list of all the possible <strong>events</strong> (or outcomes) that could result from some trial or experiment. The sample space for a coin toss, for example, consists of Heads and Tails, but not 42 or a bag of catnip. Once we have defined the sample space, we assign probabilities to every possible event.</p>
<p>Probability distributions are just sample spaces where every possible event has an associated probability. They are governed by three axioms:</p>
<ol type="1">
<li>The probability of any event occurring is equal to or greater than 0. There can be no negative probabilities.</li>
<li>The probability of the entire sample space is 1. If the probability distribution is discrete (i.e., distinct events that can be individually counted, like the result of a coin flip) then the sum of those probabilities must equal 1. If the probability distribution is continuous (i.e., you can‚Äôt count the distinct events because they are uncountably infinite) then the probabilities of each event must add up to 1 using an infinite sum operation.</li>
<li>If events A and B are mutually exclusive (a coin can‚Äôt land both Heads <em>and</em> Tails), or <strong>disjoint</strong>, then the probability of either event occurring is equal to the probability of A + B.</li>
</ol>
<p>This third axiom, the ‚Äúadditivity axiom,‚Äù is often expressed using notation from set theory. You may see it expressed in one of two forms, one that is common in more mathematical discussions of probability:</p>
<p><span class="math display">\[
\text{if } P(\text{A} \cap \text{B}) = \emptyset, \text{ then } P(\text{A} \cup \text{B}) = P(A) + P(B)
\]</span></p>
<p>and one that is more common in the statistical discussions of probability:</p>
<p><span class="math display">\[
\cap P(A_{i}) = \emptyset, \text{ then } \cup P(A_i) = P(A_1) + P(A_2) + ... + P(A_n)
\]</span></p>
<p>These two expressions are saying the same thing. The <span class="math inline">\(\cup\)</span> represents the set theoretic concept of the union of two events and the <span class="math inline">\(\cap\)</span> symbol represents their intersection. Consider the simple venn diagram in Figure <a href="#fig-25_01">Figure&nbsp;<span>17.1</span></a>.</p>
<p><embed src="figures/sets.pdf" class="img-fluid"></p>
<p>Two events, A and B, intersect in the top of the figure. The point at which they intersect is the portion of the venn diagram where the two events overlap one another. This intersection is represented by the symbol <span class="math inline">\(\cap\)</span>. If the two events do <em>not</em> intersect, as in the bottom of the figure, then the intersection (<span class="math inline">\(\cap\)</span>) of the two sets is empty. We represent this emptiness with the symbol <span class="math inline">\(\emptyset\)</span>. In essence, all the third axiom is saying is that if two events are disjoint, then the probability of either of those two events happening is the probability of the first event plus the probability of the second event. That‚Äôs it.</p>
<p>These iron-clad rules are paraphrased versions of the original trio, known as ‚ÄòKolmogorov Axioms‚Äô after the mathematician Andrey Kolmogorov. Together, they produce a number of useful features that we‚Äôll explore and exploit throughout the rest of this book.</p>
<p>Another essential concept in probability theory is that of a ‚Äú<strong>random variable</strong>.‚Äù Consider the example of a coin flip once again. There is no inherent numerical value for the outcome of a coin flip. When we flip it, it will either land heads up or tails up. The trouble is that neither ‚Äòheads‚Äô nor ‚Äòtails‚Äô has any inherent mathematical meaning, so it‚Äôs up to us to create something that allows two worlds (the world of the coin and the world of statistics) to come together.</p>
<p>One way to do this is to say that there is a ‚Äúrandom variable‚Äù with values 0 and 1 that represent the outcomes of the coin flip; heads up = 1, tails up = 0. At this point, writing <span class="math inline">\(X = \text{heads}\)</span> means the same thing as <span class="math inline">\(X = 1\)</span>, and writing <span class="math inline">\(X = \text{tails}\)</span> means the same thing as <span class="math inline">\(X = 0\)</span>. What‚Äôs more, we can use probability distributions to describe the probability of the coin flip taking on each value it is capable of producing. Random Variables may take on more than two values; you might use one to describe income, occupation, or height. In short, random variables are what enable us to connect the tangible worlds of coin tosses, income inequality, and so on, with the mathematical world of probability and statistical inference.</p>
<p>Now, let‚Äôs start learning about the properties of some specific probability distributions. I‚Äôll be perfectly honest, most of this is not inherently interesting to applied researchers, but a bit of knowledge here goes a <em>very</em> long way. In the chapters that follow, I will assume you know the contents of this chapter in particular, even though you might be flipping back to it from time to time to remind yourself of the details. That‚Äôs perfectly fine!</p>
<p>Finally, as with other chapters in this book we‚Äôll be discussing some equations. As promised at the start of the book, these equations are not meant to carry the explanatory weight here. If you are used to seeing and thinking about equations, great. If not, that‚Äôs OK too. You still need to understand how the distributions work and what kinds of parameters they take, but it‚Äôs possible to gain that knowledge from the simulations instead of the formulas. Ideally, the simulations will help you understand the equations, and the two can work together.</p>
</section>
<section id="probability-distributions-and-likelihood-functions" class="level2" data-number="17.5">
<h2 data-number="17.5" class="anchored" data-anchor-id="probability-distributions-and-likelihood-functions"><span class="header-section-number">17.5</span> PROBABILITY DISTRIBUTIONS AND LIKELIHOOD FUNCTIONS</h2>
<p>The mathematics of probability vary depending on whether we are working with discrete or continuous distributions, and then there are further differences based on the specific discrete or continuous distributions. To help you understand some of the differences and build some intuition, we‚Äôll walk through a few examples of widely-used distributions. We‚Äôll start with discrete distributions and ‚Äúprobability mass functions,‚Äù which represents the probability that a discrete random variable for a probability distribution is a specific value. Then we‚Äôll move on to continuous distributions and ‚Äúprobability density functions.‚Äù</p>
<section id="discrete-distributions-probability-mass-functions" class="level3" data-number="17.5.1">
<h3 data-number="17.5.1" class="anchored" data-anchor-id="discrete-distributions-probability-mass-functions"><span class="header-section-number">17.5.1</span> Discrete Distributions, Probability Mass Functions</h3>
<p>Discrete distributions are used when we have a limited number of distinct countable outcomes. For example, we can flip Heads or Tails on a coin, but nothing in between. Similarly, we can roll a 3 or a 4 on a regular die, but we can‚Äôt roll a 3.5. Below, we will learn a bit about three discrete probability distributions that are commonly used in models: the Uniform, Bernoulli, and Binomial distributions.</p>
<section id="everything-is-equally-likely-the-uniform-distribution" class="level4" data-number="17.5.1.1">
<h4 data-number="17.5.1.1" class="anchored" data-anchor-id="everything-is-equally-likely-the-uniform-distribution"><span class="header-section-number">17.5.1.1</span> Everything is Equally Likely: The Uniform Distribution</h4>
<p>The uniform distribution is probably familiar to you in practice, if not by name. <strong>A uniform probability distribution describes a collection of possible outcomes for a random process where each outcome is equally (uniformly) likely.</strong> To compute the probability of an event given this assumption that all events are equally likely, we simply divide by the number of possible events,</p>
<p><span class="math display">\[
P(x) = \frac{1}{n}
\]</span></p>
<p>Where <span class="math inline">\(x\)</span> is a particular value within the range of the distribution, and <span class="math inline">\(n\)</span> is the number of possible values in the range. This simple equation is an example of a <strong>probability mass function</strong> (PMF), and in this case it applies <em>only</em> to uniform distributions. If we want to know the probability that our random variable is some value, and we are using a uniform distribution, all we need to know is the number of possible events (<span class="math inline">\(n\)</span>), and that <span class="math inline">\(x\)</span> is one of those possible events.</p>
<p>For example, if we wish to know the probability of rolling a 7 on a die, we need only to know how many sides are on the die (<span class="math inline">\(n\)</span>), and be sure that 7 (<span class="math inline">\(x\)</span>) is on one of those sides. If we are rolling a classic six-sided die, our distribution is defined only for values 1 through 6: you cannot roll a 7. However, if we are rolling a 10-sided die, and 7 is on one of those sides, we can plug those numbers into the above PMF to see that:</p>
<p><span class="math display">\[
P(7) = \frac{1}{10} = 0.1
\]</span></p>
<p>Since the uniform distribution assigns the same probability to all events, all events are equally likely. If we assign values to each event, then the <strong>expected value</strong> is simply the weighted average value of that distribution. It is calculated as follows:</p>
<p><span class="math display">\[
\text{E}[X] = \sum_{i=1}^{n} x_i p_i
\]</span></p>
<p>Where <span class="math inline">\(\text{E}[X]\)</span> represents the expected value of <span class="math inline">\(X\)</span>, <span class="math inline">\(i\)</span> is an iterator, <span class="math inline">\(n\)</span> is the number of different values <span class="math inline">\(X\)</span> can take on, <span class="math inline">\(x_i\)</span> is the value of one of the events represented by <span class="math inline">\(X\)</span>, and <span class="math inline">\(p_i\)</span> is the probability of that event. In this case, because we‚Äôre using the uniform distribution, the weights <span class="math inline">\(p_i\)</span> will all be identical, and <span class="math inline">\(\text{E}[X]\)</span> will just be the average value of <span class="math inline">\(X\)</span>.</p>
<p>Let‚Äôs use numpy‚Äôs random number generators to simulate a uniform distribution with different sample sizes and ranges and then visualize the differences. We‚Äôll simulate rolling a six-sided die 10,000 times by generating an array of 10,000 random integers between 1 and 6 (the number of possible events). If you set the same seed I did (<code>42</code>, in the imports) you should draw the same numbers. If you don‚Äôt set a seed, or you set it to something different, yours will be different.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>n_possible_events <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> np.random.randint(<span class="dv">1</span>, n_possible_events<span class="op">+</span><span class="dv">1</span>, <span class="dv">10_000</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>Counter(samples)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Good! That‚Äôs exactly what we would expect to see (<code>1 / n_possible_events * 10_000 = 1666.666</code>) when drawing so many samples from a uniform distribution. But what if we had drawn a smaller number of samples? Let‚Äôs do few more simulations and visualize the results (Figure <span class="quarto-unresolved-ref">?fig-25_02</span>). We will add a red line showing <span class="math inline">\(E[X]\)</span>, the expected value.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>uniform_sim_1 <span class="op">=</span> np.random.randint(<span class="dv">1</span>, n_possible_events<span class="op">+</span><span class="dv">1</span>, <span class="dv">6</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>uniform_sim_2 <span class="op">=</span> np.random.randint(<span class="dv">1</span>, n_possible_events<span class="op">+</span><span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>uniform_sim_3 <span class="op">=</span> np.random.randint(<span class="dv">1</span>, n_possible_events<span class="op">+</span><span class="dv">1</span>, <span class="dv">1_000</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>uniform_sim_4 <span class="op">=</span> np.random.randint(<span class="dv">1</span>, n_possible_events<span class="op">+</span><span class="dv">1</span>, <span class="dv">10_000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_percentages(simulation_array, n_samples):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> pd.Series(simulation_array).value_counts().div(n_samples)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> s</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>sns.barplot(</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>get_percentages(uniform_sim_1, <span class="dv">6</span>).index,</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>get_percentages(uniform_sim_1, <span class="dv">6</span>), </span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    ax<span class="op">=</span>ax[<span class="dv">0</span>, <span class="dv">0</span>], </span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">'gray'</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">0</span>].axhline(<span class="dv">1</span> <span class="op">/</span> <span class="dv">6</span>, color<span class="op">=</span><span class="st">'crimson'</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">0</span>].<span class="bu">set</span>(title<span class="op">=</span><span class="st">'6 samples'</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>sns.barplot(</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>get_percentages(uniform_sim_2, <span class="dv">100</span>).index,</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>get_percentages(uniform_sim_2, <span class="dv">100</span>), </span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    ax<span class="op">=</span>ax[<span class="dv">0</span>, <span class="dv">1</span>], </span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">1</span>].axhline(<span class="dv">1</span> <span class="op">/</span> <span class="dv">6</span>, color<span class="op">=</span><span class="st">'crimson'</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">1</span>].<span class="bu">set</span>(title<span class="op">=</span><span class="st">'100 samples'</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>sns.barplot(</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>get_percentages(uniform_sim_3, <span class="dv">1_000</span>).index,</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>get_percentages(uniform_sim_3, <span class="dv">1_000</span>), </span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    ax<span class="op">=</span>ax[<span class="dv">1</span>, <span class="dv">0</span>], </span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">'gray'</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">0</span>].axhline(<span class="dv">1</span> <span class="op">/</span> <span class="dv">6</span>, color<span class="op">=</span><span class="st">'crimson'</span>)</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">0</span>].<span class="bu">set</span>(title<span class="op">=</span><span class="st">'1,000 samples'</span>)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>sns.barplot(x<span class="op">=</span>get_percentages(</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>    uniform_sim_4, <span class="dv">10_000</span>).index, </span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>get_percentages(</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>        uniform_sim_4, <span class="dv">10_000</span>), ax<span class="op">=</span>ax[<span class="dv">1</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'gray'</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">1</span>].axhline(<span class="dv">1</span> <span class="op">/</span> <span class="dv">6</span>, color<span class="op">=</span><span class="st">'crimson'</span>)</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">1</span>].<span class="bu">set</span>(title<span class="op">=</span><span class="st">'10,000 samples'</span>)</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>sns.despine()</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">'figures/25_01.png'</span>, dpi<span class="op">=</span><span class="dv">300</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-25_01" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/25_01.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;17.1: png</figcaption>
</figure>
</div>
<p>These four simple simulations show that we get closer and closer to the expected value the more samples we draw. Go ahead and vary the number of samples and re-run the code. You‚Äôll find that the red line is always on the same y-value. Given a uniform distribution and in the case where the number of possible events is 6, the probability of any specific outcome of rolling our die is ~0.1666‚Ä¶, or ~16.67%.</p>
</section>
<section id="the-bernoulli-and-binomial-distributions" class="level4" data-number="17.5.1.2">
<h4 data-number="17.5.1.2" class="anchored" data-anchor-id="the-bernoulli-and-binomial-distributions"><span class="header-section-number">17.5.1.2</span> The Bernoulli and Binomial Distributions</h4>
<p>The <strong>Bernoulli distribution</strong> is a bit different than the other distributions we examine in this chapter. It‚Äôs actually special case of the binomial distribution, which we will discuss in a moment. The Bernoulli distribution describes an experiment <em>with only a single sample</em>, where the outcome of the experiment is binary (e.g., 0 or 1, yes or no, heads or tails, tested positive for COVID-19 or not), and described by a single probability <span class="math inline">\(p\)</span>. Since we only have two possible outcomes, we only need to know the probability of <em>one</em> of those outcomes because the sum of the probabilities of all outcomes must equal 1. Necessarily, if the probability of testing positive for COVID-19 is 20%, the probability of testing negative is 80%.</p>
<p>The <strong>binomial distribution</strong> is the extended case of the Bernoulli distribution. The binomial distribution models observing certain events over some kind of interval. Specifically, the events are Bernoulli trials: events with binary outcomes with a probability <span class="math inline">\(p\)</span> describing one outcome and <span class="math inline">\(q\)</span> describing the other. The interval is a discrete range of number of trials.</p>
<p>The PMF for the binomial distribution models the number of events corresponding to probability <span class="math inline">\(p\)</span> observed over <span class="math inline">\(n\)</span> trials. The formula is:</p>
<p><span class="math display">\[
P(x) = {n \choose x}p^{x}q^{n-x}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(x\)</span> represents observing a specific number of outcomes corresponding to the probability <span class="math inline">\(p\)</span></li>
<li><span class="math inline">\(n\)</span> is the number of trials</li>
<li><span class="math inline">\(p\)</span> is the probability of observing the chosen outcome</li>
<li><span class="math inline">\(q\)</span> is the probability of observing the other outcome and is equal to <span class="math inline">\(1 - p\)</span></li>
</ul>
<p>To make this more concrete, let‚Äôs return to the somewhat tiresome, but useful, example of flipping fair coins. Since this is a binomial distribution, it‚Äôs composed of a series of Bernoulli trials. If we flip the coin 10 times, we are conducting 10 Bernoulli trials (<span class="math inline">\(n=10\)</span>). Across all 10 trials, what‚Äôs the probability of seeing heads <span class="math inline">\(x\)</span> times?</p>
<p>Since we have decided to select heads as our success condition, we shall set the probability of observing heads equal to <span class="math inline">\(p\)</span>. Given that we have a fair coin, both sides are equally likely, so we know that the <span class="math inline">\(p = 0.5\)</span>, and by extension the probability of tails is <span class="math inline">\(1 - p = 0.5\)</span>. We also know that the number of Bernoulli trials is <span class="math inline">\(n = 10\)</span> because we are flipping the coin 10 times.</p>
<p>As with the other PMFs, we can get the probability of seeing heads <span class="math inline">\(x\)</span> times by plugging the value for <span class="math inline">\(x\)</span> into the formula. If we wanted to determine the probability of getting heads 3 times out of the 10 flips:</p>
<p><span class="math display">\[\begin{align}
P(3) &amp;= {10 \choose 3}0.5^{3}0.5^{10-3} \\
P(3) &amp;= {10 \choose 3}0.5^{3}0.5^{7} \\
P(3) &amp;= 0.1171875
\end{align}\]</span></p>
<p>If we flip a fair coin 10 times, we should expect to get exactly 3 heads approximately 12% of the time.</p>
<p>Again, let‚Äôs use simulations to deepen our understanding. We‚Äôll need to provide the number of Bernoulli trials we would like to run. Since we can calculate the one probability using the other, we only need the probability of the ‚Äúsuccess‚Äù outcome, <span class="math inline">\(p\)</span>, of our Bernoulli trial. We also provide the number of random samples we draw from the binomial distribution.</p>
<p>It is worth stressing the difference between the number of Bernoulli trials and the number of samples we draw. In the above example, <span class="math inline">\(n\)</span> is the number of Bernoulli trials, or coin flips. The number of samples we draw does not feature in the equation: each time we draw a sample, we are essentially flipping 10 coins and tallying the results.</p>
<p>Before running the simulations, consider what would you <em>expect</em> to see given different values for the probability of the success condition, <span class="math inline">\(p\)</span>. If <span class="math inline">\(p\)</span> were 0.8, for example, what do you think you might see with an <span class="math inline">\(n\)</span> of 40? How do you think distribution would change with different values for <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>?</p>
<p>The results are shown in Figure <a href="#fig-25_03">Figure&nbsp;<span>17.2</span></a>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>binomial_sim_1 <span class="op">=</span> np.random.binomial(<span class="dv">20</span>, <span class="fl">0.5</span>, <span class="dv">10_000</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>binomial_sim_2 <span class="op">=</span> np.random.binomial(<span class="dv">20</span>, <span class="fl">0.8</span>, <span class="dv">10_000</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>binomial_sim_3 <span class="op">=</span> np.random.binomial(<span class="dv">10</span>, <span class="fl">0.5</span>, <span class="dv">10_000</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>binomial_sim_4 <span class="op">=</span> np.random.binomial(<span class="dv">10</span>, <span class="fl">0.8</span>, <span class="dv">10_000</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>binomial_simulations <span class="op">=</span> pd.DataFrame(</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    [</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        binomial_sim_1, binomial_sim_2, binomial_sim_3, binomial_sim_4</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>).T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>,figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">21</span>))</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>sns.countplot(</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>binomial_simulations[<span class="dv">0</span>], ax<span class="op">=</span>ax[<span class="dv">0</span>, <span class="dv">0</span>], color<span class="op">=</span><span class="st">'gray'</span>, order<span class="op">=</span>t</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].<span class="bu">set</span>(</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    xlabel<span class="op">=</span><span class="st">""</span>, </span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    title <span class="op">=</span> <span class="vs">r'$n=20$ and $p=0.5$'</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>sns.countplot(</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>binomial_simulations[<span class="dv">1</span>], ax<span class="op">=</span>ax[<span class="dv">0</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'gray'</span>, order<span class="op">=</span>t</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">1</span>].<span class="bu">set</span>(</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    xlabel<span class="op">=</span><span class="st">""</span>, </span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    ylabel<span class="op">=</span><span class="st">""</span>, </span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    title <span class="op">=</span> <span class="vs">r'$n=20$ and $p=0.8$'</span>, </span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    xticks<span class="op">=</span><span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">20</span>)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>sns.countplot(</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>binomial_simulations[<span class="dv">2</span>], ax<span class="op">=</span>ax[<span class="dv">1</span>, <span class="dv">0</span>], color<span class="op">=</span><span class="st">'gray'</span>, order<span class="op">=</span>t</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">0</span>].<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">""</span>, title <span class="op">=</span> <span class="vs">r'$n=10$ and $p=0.5$'</span>)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>sns.countplot(</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>binomial_simulations[<span class="dv">3</span>], ax<span class="op">=</span>ax[<span class="dv">1</span>, <span class="dv">1</span>], color<span class="op">=</span><span class="st">'gray'</span>, order<span class="op">=</span>t</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>, <span class="dv">1</span>].<span class="bu">set</span>(xlabel<span class="op">=</span><span class="st">""</span>, ylabel<span class="op">=</span><span class="st">""</span>, title <span class="op">=</span> <span class="vs">r'$n=10$ and $p=0.8$'</span>)</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>sns.despine()</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">'figures/25_03.png'</span>, dpi<span class="op">=</span><span class="dv">300</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-25_03" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/25_03.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;17.2: png</figcaption>
</figure>
</div>
<p>There are, of course, other discrete probability distributions that are commonly used in probabilistic models. There is little to no point in trying to introduce them all here, and there are many fine introductions that go into considerable technical depth. But now you should have a pretty good understanding of the basic concepts and ideas, and you should know to expect unfamiliar distributions to have (<em>i</em>) some set of assumptions that make them more or less appropriate to use given the nature of what you are trying to model, and (<em>ii</em>) some set of parameters that govern the distribution, and which can be used to compute probabilities for different samples.</p>
</section>
</section>
</section>
<section id="continuous-distributions-probability-density-functions" class="level2" data-number="17.6">
<h2 data-number="17.6" class="anchored" data-anchor-id="continuous-distributions-probability-density-functions"><span class="header-section-number">17.6</span> CONTINUOUS DISTRIBUTIONS, PROBABILITY DENSITY FUNCTIONS</h2>
<p>Everything we‚Äôve seen so far in this chapter pertains to discrete distributions. Things are a bit different with continuous distributions, as there are an uncountably infinite number of different values a continuous distribution can take on. Counterintuitively, this means that the probability of any <em>specific</em> value in any continuous distribution (e.g., 8.985452) is 0; instead, we must describe the probability present across a <em>range</em> of values. Instead of using Probability Mass Functions (PMFs) to compute probabilities, we use <strong>Probability Density Functions</strong> (<strong>PDFs</strong>). Let‚Äôs see how this works by focusing on the ubiquitous normal distribution.</p>
<section id="the-normal-distribution" class="level3" data-number="17.6.1">
<h3 data-number="17.6.1" class="anchored" data-anchor-id="the-normal-distribution"><span class="header-section-number">17.6.1</span> The Normal Distribution</h3>
<p>The <strong>normal distribution</strong> (often called the <strong>‚ÄòGaussian‚Äô distribution</strong>) is foundational to traditional statistics. We don‚Äôt have the room to cover why it appears so frequently (in both nature and the scientific study thereof), but you‚Äôve almost certainly seen it many times before. Regardless of the moniker we apply, the Normal Distribution describes a process that trends towards some mean value (<span class="math inline">\(\mu\)</span>) with data evenly spread around it in proportions that diminish the further away from the mean they are. We use the term ‚ÄòStandard Deviation‚Äô (<span class="math inline">\(\sigma\)</span>) to describe how quickly the data diminishes; you might also encounter publications and software that describe it as ‚Äòwidth‚Äô or ‚Äòscale‚Äô. The PDF for the normal distribution is this monstrosity:</p>
<p><span class="math display">\[
P(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\mu\)</span> is the mean of the distribution</li>
<li><span class="math inline">\(\sigma\)</span> is the standard deviation</li>
<li><span class="math inline">\(e\)</span> is the mathematical constant Euler‚Äôs Number (<span class="math inline">\(\approx\)</span> 2.71828‚Ä¶)</li>
<li><span class="math inline">\(\pi\)</span> is the mathematical constant pi (<span class="math inline">\(\approx\)</span> 3.14159‚Ä¶)</li>
</ul>
<p>A standard deviation of 0 indicates that every observation is the same as the mean value. The larger the standard deviation, the further away from the mean the average observation will be.</p>
<p>Let‚Äôs start simulating! We‚Äôll use Numpy‚Äôs <code>random.normal()</code> to perform four simulations, each pulling 10,000 samples from normal distributions with slightly different parameters. The first two arguments indicate the mean and standard deviation for the normal distribution, and the third indicates the number of samples we‚Äôll draw. Results are shown in Figure <a href="#fig-25_04">Figure&nbsp;<span>17.3</span></a>.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>normal_sim_1 <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.1</span>, <span class="dv">10_000</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>normal_sim_2 <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.2</span>, <span class="dv">10_000</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>normal_sim_3 <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.3</span>, <span class="dv">10_000</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>normal_sim_4 <span class="op">=</span> np.random.normal(<span class="fl">0.5</span>, <span class="fl">0.2</span>, <span class="dv">10_000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">30</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>,<span class="dv">2</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>                       figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">4</span>))</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>sns.histplot(normal_sim_1, ax <span class="op">=</span> ax[<span class="dv">0</span>,<span class="dv">0</span>], kde<span class="op">=</span><span class="va">True</span>, bins<span class="op">=</span>b)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">0</span>].set_title(<span class="vs">r'$\mu$ = 0 and $\sigma$ = 0.1'</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>sns.histplot(normal_sim_2, ax <span class="op">=</span> ax[<span class="dv">0</span>,<span class="dv">1</span>], kde<span class="op">=</span><span class="va">True</span>, bins<span class="op">=</span>b)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>,<span class="dv">1</span>].set_title(<span class="vs">r'$\mu$ = 0 and $\sigma$ = 0.2'</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>sns.histplot(normal_sim_3, ax <span class="op">=</span> ax[<span class="dv">1</span>,<span class="dv">0</span>], kde<span class="op">=</span><span class="va">True</span>, bins<span class="op">=</span>b)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">0</span>].set_title(<span class="vs">r'$\mu$ = 0 and $\sigma$ = 0.3'</span>)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>sns.histplot(normal_sim_4, ax <span class="op">=</span> ax[<span class="dv">1</span>,<span class="dv">1</span>], kde<span class="op">=</span><span class="va">True</span>, bins<span class="op">=</span>b)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>,<span class="dv">1</span>].set_title(<span class="vs">r'$\mu$ = 0.5 and $\sigma$ = 0.2'</span>)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>sns.despine(left<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">'figures/25_04.png'</span>, dpi<span class="op">=</span><span class="dv">300</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-25_04" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/25_04.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;17.3: png</figcaption>
</figure>
</div>
<p>As usual, deepen your understanding of how the normal distribution behaves by experimenting with different values of mu (<span class="math inline">\(\mu\)</span>) and sigma (<span class="math inline">\(\sigma\)</span>).</p>
<section id="the-exponential-distribution" class="level4" data-number="17.6.1.1">
<h4 data-number="17.6.1.1" class="anchored" data-anchor-id="the-exponential-distribution"><span class="header-section-number">17.6.1.1</span> The Exponential Distribution</h4>
<p>Another important continuous distribution is the <strong>exponential distribution</strong>. Among other things, the exponential distribution is often used to model the half-life of radionuclide decay, which describes the amount of time it takes for half of a given mass of radioactive atoms to decay into other atoms. Like the other distributions that have shown up so far, we‚Äôll be using it a few times in the chapters that follow.</p>
<p>The PDF of the exponential distribution is normally given as:</p>
<p><span class="math display">\[
P(x) = \lambda e^{\lambda x}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\lambda\)</span> is the rate parameter of the events, and must be greater than 0.</li>
<li><span class="math inline">\(e\)</span> is Euler‚Äôs number (~ 2.71828‚Ä¶)</li>
<li><span class="math inline">\(x\)</span> is the time until the next event</li>
</ul>
<p>Although ‚Äòrate parameter of the events‚Äô is a precise definition, it isn‚Äôt a particularly intuitive one. You might be more familiar with <span class="math inline">\(\lambda\)</span> representing ‚Äòrate of decay‚Äô or ‚Äòhalf-life‚Äô.</p>
<p>Note that what we presented above is not the <em>only</em> formulation of the exponential distribution PDF. These functions can be written in many ways, and for various reasons like interpretability or ease of calculation, some people prefer one over another. This is relevant because we will be drawing samples using numpy‚Äôs exponential distribution function, which uses the scale parameter <span class="math inline">\(\beta = \frac{1}{\lambda}\)</span> rather than <span class="math inline">\(\lambda\)</span>.</p>
<p>This gives the following PDF:</p>
<p><span class="math display">\[
P(x) = \frac{1}{\beta} e^{-\frac{x}{\beta}}
\]</span></p>
<p>Let‚Äôs jump right into the simulations. Results shown in Figure <a href="#fig-25_05">Figure&nbsp;<span>17.4</span></a>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>exponential_sim_1 <span class="op">=</span> np.random.exponential(<span class="dv">1</span>, <span class="dv">10000</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>exponential_sim_2 <span class="op">=</span> np.random.exponential(<span class="dv">2</span>, <span class="dv">10000</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>sns.histplot(</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    exponential_sim_1, color<span class="op">=</span><span class="st">'crimson'</span>, label<span class="op">=</span><span class="vs">r'$\beta = 1$'</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>sns.histplot(</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    exponential_sim_2, color<span class="op">=</span><span class="st">'lightgray'</span>, label<span class="op">=</span><span class="vs">r'$\beta = 2$'</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>sns.despine()</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">'figures/25_05.png'</span>, dpi<span class="op">=</span><span class="dv">300</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-25_05" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/25_05.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;17.4: png</figcaption>
</figure>
</div>
<p>As you can see from the simulations above, the Exponential distribution always assigns the greatest probability density to values closest to 0, with a long tail to the right. The value of <span class="math inline">\(\beta\)</span> or <span class="math inline">\(\lambda\)</span> influences how much of the probability density is in the tail.</p>
<p>The Exponential distribution always assigns a probability of 0 to any events that are less than 0. This is a useful property for us, as it allows us to use an Exponential distribution to describe processes that cannot have negative values.</p>
</section>
</section>
</section>
<section id="joint-and-conditional-probabilities" class="level2" data-number="17.7">
<h2 data-number="17.7" class="anchored" data-anchor-id="joint-and-conditional-probabilities"><span class="header-section-number">17.7</span> JOINT AND CONDITIONAL PROBABILITIES</h2>
<section id="joint-probabilities" class="level3" data-number="17.7.1">
<h3 data-number="17.7.1" class="anchored" data-anchor-id="joint-probabilities"><span class="header-section-number">17.7.1</span> Joint Probabilities</h3>
<p>Up to this point, we‚Äôve been primarily focused on <em>marginal probabilities</em>, though we haven‚Äôt called them that. <strong>Marginal probabilities</strong> describe events that are unconditional on other events (which is why you‚Äôll also see us use <strong>unconditional probability</strong> to refer to the same kind of thing ‚Äì the two terms are interchangeable). <strong>Joint probabilities</strong>, on the other hand, describe two or more events occurring together. Let us consider some simple examples.</p>
<p>Think of a standard deck of cards without jokers: 52 cards with two colours (red and black) divided into 4 suits (Clubs, Diamonds, Hearts, Spades), each with 13 cards having values of Ace through 10, Jack, Queen, and King. If we wanted to know the probability of randomly drawing a single Jack of any suit from the deck, then we are talking about a <em>marginal</em> probability, because the probability of drawing a Jack is independent of other events in this scenario. As there are 4 Jacks in the 52 cards, we can express this probability with the following:</p>
<p><span class="math display">\[\begin{align}
P(\text{Jack}) &amp;= \frac{\text{Number Of Jacks}}{\text{Number Of Cards}} \\
P(\text{Jack}) &amp;= \frac{4}{52} = \frac{1}{13}
\end{align}\]</span></p>
<p>Conversely, if we wanted to know the marginal probability of drawing a Diamond:</p>
<p><span class="math display">\[\begin{align}
P(\text{Diamond}) &amp;= \frac{\text{Number Of Diamonds}}{\text{Number Of Cards}} \\
P(\text{Diamond}) &amp;= \frac{13}{52} = \frac{1}{4}
\end{align}\]</span></p>
<p>Sometimes we want to know the probability of two independent events occurring simultaneously, which again is known as a <em>joint probability</em>. When we want to represent the joint probability of two independent events which we will arbitrarily call <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, we use <span class="math inline">\(P(A \cap B)\)</span>, which represents the <em>intersection</em> of these two events. To get joint probabilities, we multiple the marginal probability of one event by the marginal probability of the other event, which can be expressed as:</p>
<p><span class="math display">\[
P(A \cap B) = P(A) \times P(B)
\]</span></p>
<p>Now consider the probability of drawing the Jack of Diamonds. The event we are interested in can be expressed as two events occurring: drawing a Jack and drawing a Diamond; in order to be both a Jack and a Diamond, our card must be the Jack of Diamonds, of which there is only one in the deck. We know there are 4 Jacks and 13 Diamonds in the 52 cards.</p>
<p><span class="math display">\[\begin{align}
P(\text{Jack} \cap \text{Diamond}) &amp;= P(\text{Jack}) \times P(\text{Diamond}) \\
P(\text{Jack} \cap \text{Diamond}) &amp;= \frac{\text{Number Of Jacks}}{\text{Number Of Cards}} \times \frac{\text{Number Of Diamonds}}{\text{Number Of Cards}} \\
P(\text{Jack} \cap \text{Diamond}) &amp;= \frac{4}{52} \times \frac{13}{52} = \frac{1}{52}
\end{align}\]</span></p>
<p>Finally, we have been representing joint probabilities here with the <span class="math inline">\(\cap\)</span> symbol. You may also see joint probabilities represented with commas, such as <span class="math inline">\(P(\text{Jack},\text{Diamond})\)</span>. There are no differences between the two; they mean the same thing.</p>
</section>
<section id="conditional-probability" class="level3" data-number="17.7.2">
<h3 data-number="17.7.2" class="anchored" data-anchor-id="conditional-probability"><span class="header-section-number">17.7.2</span> Conditional Probability</h3>
<p>Whereas marginal probabilities represent the probability of an event independent of other events and joint probabilities represent the probability of two or more events occurring together, <strong>conditional probabilities</strong> represent the probability of an event occurring <em>given that another has already occurred</em>. You‚Äôll often see this relationship expressed using a statement like ‚Äúthe probability of A conditional upon B‚Äù or ‚Äúthe probability of A given B‚Äù.</p>
<p>Once again we‚Äôll think of drawing Jack of Diamonds from a deck of 52 cards, but under slightly different circumstances. Imagine this time that someone has already drawn a card and informed us that it‚Äôs a Diamond, and we would like to know the the probability that the card in their hand is the <em>Jack</em> of Diamonds. We‚Äôll assuming our friend is honest and the card they‚Äôve removed is indeed a Diamond, which means that <span class="math inline">\(P(Diamond) = 1\)</span>. Now we need to update our probabilities to account for this new certainty. Since that we know we‚Äôre dealing with Diamonds only, there is only one Jack that we could have drawn. But it could only have been drawn from the pool of 13 Diamonds.</p>
<p>To represent the probability of an event, say <span class="math inline">\(A\)</span>, <em>given that another event</em>, say <span class="math inline">\(B\)</span><em>, has occurred,</em> we use the notation <span class="math inline">\(P(A \mid B)\)</span>. You can read the <span class="math inline">\(\mid\)</span> as ‚Äúgiven;‚Äù in this case, the probability of observing a specific value for <span class="math inline">\(A\)</span> given a specific value for <span class="math inline">\(B\)</span> that you have already observed. Knowing these new pieces of information, we can adjust our previous probabilities to the following:</p>
<p><span class="math display">\[\begin{align}
P(\text{Jack} \mid \text{Diamond}) &amp;= \frac{\text{Number Of Jacks That Are Diamonds}}{\text{Number Of Cards That Are Diamonds}} \\
P(\text{Jack} \mid \text{Diamond}) &amp;= \frac{1}{13}
\end{align}\]</span></p>
<p>We‚Äôve used an easy case here. Other data can be much more complicated, making the above process more complicated to puzzle through. Fortunately there is a more formal and generalizable definition we can use. We won‚Äôt discuss the proof here, but know that:</p>
<p><span class="math display">\[
P(\text{Jack} \mid \text{Diamond}) = \frac{P(\text{Jack} \cap \text{Diamond})}{P(\text{Diamond})}
\]</span></p>
<p>Recalling that <span class="math inline">\(P(\text{Jack} \cap \text{Diamond}) = \frac{1}{52}\)</span> and <span class="math inline">\(P(\text{Diamond}) = \frac{1}{4}\)</span> we can plug these probabilities we found earlier into this equation and we should get the same result as above.</p>
<p><span class="math display">\[
P(\text{Jack} \mid \text{Diamond}) = \frac{\frac{1}{52}}{\frac{1}{4}} = \frac{1}{13}
\]</span></p>
</section>
</section>
<section id="bayesian-inference" class="level2" data-number="17.8">
<h2 data-number="17.8" class="anchored" data-anchor-id="bayesian-inference"><span class="header-section-number">17.8</span> BAYESIAN INFERENCE</h2>
<p>So far, we‚Äôve played around with the nuts and bolts of a few discrete and continuous probability distributions to better understand how they work. But all of this is, of course, just a means to an end. We want to understand how these distributions work <em>because we want to use them to develop models!</em> To develop models that are more interesting than the distributions we‚Äôve considered to this point, we need to introduce one more piece of probability theory: Bayes‚Äô theorem.</p>
<section id="bayes-theorem" class="level3" data-number="17.8.1">
<h3 data-number="17.8.1" class="anchored" data-anchor-id="bayes-theorem"><span class="header-section-number">17.8.1</span> Bayes‚Äô Theorem</h3>
<p>The term ‚ÄúBayesian‚Äù is derived from the surname of Reverend Thomas Bayes ‚Äì a British statistician and Presbyterian minister of the first half of the 18th century. He‚Äôs primarily famous for two things:</p>
<ol type="1">
<li>Articulating a special-case solution for finding the probability of an unobserved random variable</li>
<li>His use of probability to describe not just frequencies, but also uncertainty in states of knowledge and belief</li>
</ol>
<p>Of the two, the latter is more distinctly ‚ÄòBayesian‚Äô, and is largely responsible for the move to associate his surname with the Bayesian statistical paradigm. What we now call Bayes‚Äô theorem was originally proposed by Bayes to compute what he called ‚Äòinverse probability,‚Äô a term that has since fallen out of favour. The modern form of the theorem is used for finding the probability of an unknown variable, <span class="math inline">\(P(A|B)\)</span>, given three known variables: <span class="math inline">\(P(A)\)</span>, <span class="math inline">\(P(B)\)</span>, and <span class="math inline">\(P(B|A)\)</span>. It has a very impressive mathematical lineage. Though initially proposed by Bayes, the modern version of the theorem we know and love owes quite a lot to the Welsh mathematician and philosopher Richard Price and the French polymath Pierre-Simon Laplace. Really, it should be probably be named the Bayes-Price-Laplace theorem, but anyway.</p>
<p>If you‚Äôre reading this book, you‚Äôve probably encountered <strong>Bayes‚Äô theorem</strong> at some point.</p>
<p><span class="math display">\[
P(A|B) = \frac{P(B|A)\times P(A)}{P(B)}
\]</span></p>
<p>You can read this as ‚ÄúThe probability of A conditional on B is equal to the probability of B conditional upon A, times the marginal probability of A, all divided by the marginal probability of B.‚Äù</p>
<p>With the theorem introduced, I have some potentially surprising news to share. There‚Äôs nothing uniquely ‚Äì or even distinctly ‚Äì Bayesian about using Bayes‚Äô Theorem! Using it doesn‚Äôt make you a Bayesian. Much of what we cover in later chapters will use Bayes‚Äô theorem in some capacity, <em>but the same would be true if you were using Frequentist methods</em>! Understanding Bayes‚Äô Theorem is an important and necessary stepping stone along the path to working with a more flexible view of probability (which <em>is</em> a distinct feature of Bayesian analysis), but it is not a sufficient one. Not by itself, at least.</p>
<p>Now that I‚Äôve spilled ‚ÄúThe Big Dirty Secret of Bayes‚Äô Theorem,‚Äù the natural next step is to explain what, exactly, we need to do to make Bayes‚Äô theorem ‚ÄúBayesian.‚Äù</p>
<section id="how-to-make-bayes-theorem-bayesian" class="level4" data-number="17.8.1.1">
<h4 data-number="17.8.1.1" class="anchored" data-anchor-id="how-to-make-bayes-theorem-bayesian"><span class="header-section-number">17.8.1.1</span> How To Make Bayes‚Äô Theorem Bayesian</h4>
<p>Simply put, the best way to make Bayes‚Äô theorem Bayesian is to apply it to a hypothesis or a state of knowledge. In other words, we assign a probability to a hypothesis and then use Bayes‚Äô theorem to determine the probability of that hypothesis given the data we‚Äôve observed. Isn‚Äôt that a beautiful idea?</p>
<p>In the Bayesian paradigm, Bayes‚Äô theorem can be applied to hypotheses and data. In other words, just as we might use Bayes‚Äô theorem to compute <span class="math inline">\(P(\text{Jack} \mid \text{Diamond})\)</span>, we can compute <span class="math inline">\(P(\text{Hypothesis} \mid \text{Data})\)</span>, or expressed another way, <span class="math inline">\(P(\text{Hypothesis} \mid \text{Evidence})\)</span>.</p>
<p>In order to introduce a little more clarity into our equations, you‚Äôll often find a slightly different form of notation used for the hypothesis-based version of Bayes‚Äô theorem. Below, we use the symbols <span class="math inline">\(\theta\)</span> or <span class="math inline">\(\text{H}\)</span> to represent a hypothesis. We represent data with <span class="math inline">\(\text{D}\)</span>, or evidence with <span class="math inline">\(\text{E}\)</span>.</p>
<p><span class="math display">\[
P(\theta|D) = \frac{P(D|\theta)\times P(\theta)}{P(D)}
\]</span></p>
<p>Another equivalent rendition:</p>
<p><span class="math display">\[
P(H|E) = \frac{P(E|H)\times P(H)}{P(E)}
\]</span></p>
<p>You can read either of these versions of the theorem in a very similar way as the form we described earlier. In this case, one might read: ‚Äúthe probability of a specific hypothesis conditional upon the data/evidence is equal to the probability of that data conditioned upon the hypothesis, times the unconditional probability of the hypothesis divided by the unconditional probability of the data.‚Äù Whew! That was a mouthful. We‚Äôre going to be referring back to the first of these forms of Bayes‚Äô theorem a whole lot (the one with <span class="math inline">\(\theta\)</span> and <span class="math inline">\(D\)</span>), so it might be a good idea to jot it down in your notes or take a picture of it. To reiterate, the reason why this particular form of Bayes‚Äô theorem can be considered ‚ÄòBayesian‚Äô is because we‚Äôre using a fixed interpretation of data to update probabilistically-described hypotheses.</p>
<p>This hypothesis form of Bayes Theorm has several components, and each component has a specific name that you‚Äôll need to know if you want to be conversant in Bayesian inference and data analysis, and to think deeply and systematically about probabilistic / generative modelling. We‚Äôll cover each of them shortly, but first, an apology: the terminology we must cover now is, for lack of a better word, <em>tragic</em>. Among other things it will involve drawing a distinction between two words that are near-perfect synonyms in colloquial English. The distinction between them only matters in the specialized setting we currently operate within, and the differences in their meanings are confusing and oblique. I‚Äôll do my best to differentiate between them clearly, as the distinction is vitally important, but I‚Äôm sorry to have to ruin two perfectly good words for you. Those words are ‚ÄòProbability‚Äô and ‚ÄòLikelihood‚Äô.</p>
</section>
</section>
<section id="the-components-of-bayes-theorem" class="level3" data-number="17.8.2">
<h3 data-number="17.8.2" class="anchored" data-anchor-id="the-components-of-bayes-theorem"><span class="header-section-number">17.8.2</span> The Components of Bayes‚Äô Theorem:</h3>
<p>In this final part of the chapter I‚Äôm going to walk through each of the components of Bayes‚Äô Theorem. Specifically, the</p>
<ol type="1">
<li>the <strong>prior probability</strong>, or ‚Äú<strong>priors</strong>,‚Äù</li>
<li>the <strong>likelihood</strong>, and</li>
<li>the <strong>normalizing constant</strong></li>
</ol>
<p>Together, these three components are used to compute something called the ‚Äúposterior probability.‚Äù <em>Everything we do is in search of understanding the posterior.</em></p>
<p>When people think of the Bayesian paradigm, they generally think of two things: (<em>i</em>) Bayes‚Äô theorem (which, as we‚Äôve established, isn‚Äôt especially Bayesian), and (<em>ii</em>) the use of prior probabilities. While it is certainly true that Bayesian methods make extensive use of priors, they aren‚Äôt the <em>point</em> of Bayesian methods. Having the ability to manipulate priors can be useful, but you should think of them as the price that Bayesians pay in order to enjoy principled access to the complete distribution of posterior probabilities (which we will get to soon) as opposed to the point estimates and confidence intervals that Frequentists use. So when thinking about what makes something Bayesian, don‚Äôt focus on the priors - they‚Äôre just the cost of entry, and you can develop models that minimize their influence anyhow. Instead, focus on the posteriors. Since you can‚Äôt really make much sense of posteriors before understanding the other components, we‚Äôll save them for last.</p>
<section id="prior-probability-or-priors" class="level4" data-number="17.8.2.1">
<h4 data-number="17.8.2.1" class="anchored" data-anchor-id="prior-probability-or-priors"><span class="header-section-number">17.8.2.1</span> Prior Probability, or ‚ÄúPriors‚Äù</h4>
<p>A ‚ÄòPrior Probability‚Äô is a probability that Bayesians place on any unobserved variable. In the strictest sense, Bayesian priors are intended to serve as a quantified representation of an individual‚Äôs ‚Äústate of belief‚Äù about a hypothesis under examination.</p>
<blockquote class="blockquote">
<p><strong>Further Reading</strong></p>
<p>‚ÄúBelief‚Äù is widely-used term here, but many Bayesians (myself included) think the term is a bit misleading while still being technically accurate. It‚Äôs probably at least partly responsible for the persistent but outdated and inaccurate characterization of Bayesian models as ‚Äúsubjective.‚Äù A better way of thinking about priors, which I encountered via <a href="https://statmodeling.stat.columbia.edu/2015/07/15/prior-information-not-prior-belief/">Andrew Gelman‚Äôs widely-read blog ‚ÄúStatistical Modeling, Causal Inference, and Social Science‚Äù</a>, is to think of priors as ‚Äúan expression of <em>information</em>‚Äù that is relevant to the modelling task. As far as I know, this is better represents how most statisticians and scientists who would call themselves Bayesian themselves think about the role of priors in modelling. When the word ‚Äúbelief‚Äù is thrown around in relation to Bayesian models, it does not refer to just any old opinion you might have, it‚Äôs a tentatively-held ‚Äúbelief‚Äù about what you think is going on in any given modelling context; it‚Äôs a hypothesis <em>grounded in relevant information</em>. While this <em>could</em> have a ‚Äúsubjective‚Äù source, it‚Äôs really a way of leveraging theory and other kinds of knowledge, such as from previous empirical research.</p>
</blockquote>
<p>Imagine you‚Äôve got some coins in an opaque jar. Some are fair coins, others are trick coins, weighted so that they tend to land heads up far more frequently than a fair coin (a fact that only becomes obvious once one starts flipping the trick coins). In this rather contrived scenario, you‚Äôre going to select a coin from the jar and make as good a guess as possible about the probability that the coin ‚Äì when flipped ‚Äì would land heads-up.</p>
<p>If you didn‚Äôt know there were some trick coins in the jar, then the best guess you could make is that any given coin has a 50% chance of landing heads up. Think of this as a hypothesis; we ‚Äúbelieve‚Äù that there is a 50% chance of getting heads when we flip this coin.</p>
<p><span class="math display">\[
P(\theta) = 0.5
\]</span></p>
<p>If you knew about those trick coins, however, you might have a good reason to adjust your prior upwards somewhat. You‚Äôd do this to account for the slim but nonzero chance that the coin you randomly grabbed from among all the coins in your jar would produce many more heads than tails. With that additional knowledge, maybe you hypothesize that the probability of getting heads is actually 0.65, for example.</p>
</section>
<section id="likelihood" class="level4" data-number="17.8.2.2">
<h4 data-number="17.8.2.2" class="anchored" data-anchor-id="likelihood"><span class="header-section-number">17.8.2.2</span> Likelihood</h4>
<p>Likelihood is, in many ways, the opposite of probability. For our purposes, Likelihood describes the relative plausibility of some data <em>if we assume a given hypothesis is true</em>. All of the Likelihoods we‚Äôre going to consider are going to be <em>conditional</em> upon a hypothesis, which as a brief reminder is ‚Äúthe probability of thing A in light of the fact that we know thing B has already occurred.‚Äù In this case, we‚Äôre not talking about conditioning on cards that we‚Äôve observed, we‚Äôre talking about conditioning data we‚Äôve observed upon a hypothesis. In Bayes‚Äô theorem, it‚Äôs this part:</p>
<p><span class="math display">\[
P(D|\theta)
\]</span></p>
<p>To briefly illustrate how likelihood operates, imagine we are testing the hypothesis that the coin we‚Äôre flipping is biased such that it produces heads 80% of the time; <em>if we assume that‚Äôs the case</em>, the likelihood of the coin landing heads is 0.8, and tails is 0.2.</p>
<p><span class="math display">\[
P(\text{D = Heads} | \theta \text{ = 0.8}) = 0.8
\]</span></p>
<p>and therefore</p>
<p><span class="math display">\[
P(\text{D = Tails} | \theta \text{ = 0.8}) = 0.2
\]</span></p>
<p>An important thing to keep in mind here is that Likelihoods are useful in that they let us compare the <em>plausibility</em> of data given a hypothesis <em>relative to the same data given other hypotheses</em>. Likelihood is <strong>not</strong>, however, equivalent to probability. There are many implications that stem from this distinction, but one of the more salient ones is that likelihoods do not need to sum (or integrate) to 1; an individual likelihood can, in fact, be greater than 1! Even when multiplied by a prior (which is a probability), a likelihood isn‚Äôt ready to be used as a probability just yet. For that, we need to add one more piece of the puzzle.</p>
</section>
<section id="the-normalizing-constant" class="level4" data-number="17.8.2.3">
<h4 data-number="17.8.2.3" class="anchored" data-anchor-id="the-normalizing-constant"><span class="header-section-number">17.8.2.3</span> The Normalizing Constant</h4>
<p>The normalizing constant is what converts the unstandardized ‚Äò<strong>Bayes numerator</strong>‚Äô (a term that refers to the product of the likelihood and the prior: <span class="math inline">\(P(D|\theta) \times P(\theta)\)</span>) back into a standardized probability. If you recall, all probabilities must sum to 1, and the product of the prior and the likelihood very rarely do. In Bayes‚Äô theorem, the normalizing constant is <span class="math inline">\(P(D)\)</span>, or <span class="math inline">\(P(E)\)</span>, and is often referred to as the ‚Äò<strong>Total Probability</strong>‚Äô.</p>
<p>The normalizing constant is interesting because it‚Äôs simultaneously the least important element in Bayes‚Äô Theorem and the most difficult to calculate. It‚Äôs the least important because Bayes‚Äô theorem is capable of working at nearly full power without it. <span class="math inline">\(P(D|\theta) \times P(\theta)\)</span> often won‚Äôt sum (or integrate) to 1, so it can‚Äôt be a probability, but it‚Äôll be exactly the same shape as the standardized posterior. From an inferential standpoint, they‚Äôre <em>almost</em> identical. The normalizing constant is the most difficult to calculate because it is often unclear what the marginal probability of any given data was. What‚Äôs more: even if one <em>knows</em> the marginal probability of the data, determining an exact analytical solution often involves performing some truly horrific multiple integrals, some of which have no closed-form solution (read: can‚Äôt be solved exactly, and must be estimated).</p>
<p>Nevertheless, one the great advantages offered by the Bayesian paradigm is the ability to take the result from one model (in the form of the Posterior probability) and use it as the prior for another model (a process called Bayesian updating, which we‚Äôll introduce later). In order to do that, we must make use of the normalizing constant.</p>
</section>
</section>
</section>
<section id="posterior-probability" class="level2" data-number="17.9">
<h2 data-number="17.9" class="anchored" data-anchor-id="posterior-probability"><span class="header-section-number">17.9</span> POSTERIOR PROBABILITY</h2>
<p>We‚Äôve saved the best for last. The whole point of <em>all</em> of this is to compute the <strong>posterior probability</strong>, which represents our ‚Äúbelief‚Äù in the hypothesis (<span class="math inline">\(\theta\)</span>) once we‚Äôve considered it in light of the empirical evidence, our data. It is typically depicted like so:</p>
<p><span class="math display">\[
P(\theta|D)
\]</span></p>
<p>In the outputs of many models ‚Äì including regression analysis ‚Äì Frequentists will typically report two or three statistics designed to give you a rough idea of how the variables in your model are related. You‚Äôll commonly find a coefficient, the standard error associated with that coefficient, and the significance of that coefficient in light of its standard error (the significance is typically displayed using one or more stars next to the coefficient).</p>
<p>Bayesians don‚Äôt skip immediately to summarizing model outputs like the Frequentists do; instead, Bayesian data analysis requires that you report the <em>entire posterior probability</em> of your model. In other words, we are not just interested in knowing what the ‚Äúbest‚Äù estimate is and a bit about how much certainty to place in that estimate. We want to know the relative plausability of <em>every</em> hypothesis in the form of a distribution. That‚Äôs what working with Bayes gives us.</p>
<p>Once you have the posterior probability, you can easily calculate statistics that mimic what the Frequentists report directly ‚Äì it‚Äôs generally simple to calculate the mean or median value of an effect size, its variance, credible intervals (the Bayesian equivalent of confidence intervals), and so on. The important thing here is that a Bayesian has delivered the fullest and most complete answer they can once they‚Äôve produced a posterior. Everything else is just designed to make the posterior easier to digest.</p>
</section>
<section id="conclusion" class="level2" data-number="17.10">
<h2 data-number="17.10" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">17.10</span> CONCLUSION</h2>
<section id="key-points" class="level3" data-number="17.10.1">
<h3 data-number="17.10.1" class="anchored" data-anchor-id="key-points"><span class="header-section-number">17.10.1</span> Key Points</h3>
<ul>
<li>Probability distributions are the cornerstone of most statistical analysis, and use Probability Mass Functions (for discrete distributions) or Probability Density Functions (for continuous functions) to describe how their probability is distributed</li>
<li>Joint and conditional probability are useful tools for describing how the probabilities of dependent random variables are related</li>
<li>Bayes theorem is a foundational concept in all statistics, but is most commonly associated with the Bayesian paradigm, where it allows for the development of a posterior based on a prior, likelihood, and description of total probability</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./16-prediction.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Prediction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./18-credibility.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Credibility</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>