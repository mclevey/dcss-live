[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Doing Computational Social ScienceThe Continuous Development Edition",
    "section": "",
    "text": "🏠",
    "crumbs": [
      "🏠"
    ]
  },
  {
    "objectID": "index.html#updates-to-the-book",
    "href": "index.html#updates-to-the-book",
    "title": "Doing Computational Social ScienceThe Continuous Development Edition",
    "section": "Updates to the Book",
    "text": "Updates to the Book\nI am thoroughly revising Doing Computational Social Science. Here’s an overview of the most important changes I’ve made, with a note about what to expect in fall 2024 and winter 2025:\n\nIn the print edition, I assumed that readers had already taken at least one course in quantitative research methods for the social sciences. In this edition, I’ve dropped that assumption. I now see the first half of this edition as an introduction to modern quantitative research methods, and the second half as an introduction to computational methods and models. Of course, it doesn’t really matter where “quantitative” methods end and “computational” methods begin; these are just labels and symbolic boundaries. The appendix Courses and chapter sequences outlines several proposed chapter sequences tailored for 12-week undergraduate and graduate-level courses in quantitative and/or computational methods, as well as for courses on specialized methods (like text or network analysis), substantive topics (e.g., culture, sociology of science), or short workshops.\nThis edition introduces research workflows much earlier and emphasizes a “generative” modeling approach throughout. The basics of this approach are covered in the early chapters and gradually developed as the book progresses, resulting in a more streamlined learning experience that unfolds more deliberately and with greater depth.\nComputational social science, quantitative social science, and data science have evolved rapidly since the print edition, even though it’s only been a couple of years. The most important shift is in natural language processing and text analysis, where transformers have completely revolutionized the game. In response, I’ve revised how machine learning and artificial intelligence are introduced, with significant changes to sections on text analysis and deep learning. Additionally, I’ve woven content throughout the book on the ethical use of large language models (LLMs) – such as OpenAI’s ChatGPT, Anthropic’s Claude, Meta’s LLaMA, Google’s Gemini, and various open-source models available via Ollama – in computational social science research.\nAll code examples have been updated to use the latest versions of popular packages, ensuring compatibility with current tools and best practices.\nFinally, I’ve made numerous minor revisions and corrections to improve clarity and fix small errors.\n\n\nPlanned Updates\nSeveral changes are still in progress, including new chapters that I’ll be working on through fall 2024 and early winter 2025. Pending updates are flagged throughout the book, but the impact on readers should be minimal. Stay tuned!",
    "crumbs": [
      "🏠"
    ]
  },
  {
    "objectID": "index.html#updates-to-the-computing-setup",
    "href": "index.html#updates-to-the-computing-setup",
    "title": "Doing Computational Social ScienceThe Continuous Development Edition",
    "section": "Updates to the Computing Setup",
    "text": "Updates to the Computing Setup\nMy recommendations for setting up your computing environment have changed to reflect recent innovations in open-source scientific computing. The new approach is outlined in Getting Started. In short, you’ll need to install Docker, VS Code, and git. Once you clone the course materials and open them in VS Code, you’ll be prompted to reopen the files in a “Container.” Simply say yes, and everything should work smoothly. Alternatively, you can skip local installation entirely and run everything in the cloud using GitHub Codespaces. Detailed instructions are provided in Getting Started.",
    "crumbs": [
      "🏠"
    ]
  },
  {
    "objectID": "index.html#updates-to-the-python-package",
    "href": "index.html#updates-to-the-python-package",
    "title": "Doing Computational Social ScienceThe Continuous Development Edition",
    "section": "Updates to the Python Package",
    "text": "Updates to the Python Package\nThe dcss Python package has been completely redesigned and refactored. I’ve simplified the software dependencies, added unit tests, implemented type hinting (which enhances modern editor features like code auto-completion), and improved the package documentation. It’s now easier to install, maintain, and update, and it’s more efficient and broadly applicable to research problems beyond those covered in the book.",
    "crumbs": [
      "🏠"
    ]
  },
  {
    "objectID": "index.html#the-supplementary-learning-materials",
    "href": "index.html#the-supplementary-learning-materials",
    "title": "Doing Computational Social ScienceThe Continuous Development Edition",
    "section": "The Supplementary Learning Materials",
    "text": "The Supplementary Learning Materials\nI’m in the process of redesigning and developing new supplementary materials, including problem sets and interactive learning tools like flashcards and quizzes for each chapter. I’ve updated the suggested chapter sequences for various university courses, and—most importantly—I’ve launched this “continuous development” edition. 🔥",
    "crumbs": [
      "🏠"
    ]
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "Doing Computational Social ScienceThe Continuous Development Edition",
    "section": "About the Author",
    "text": "About the Author\nJohn McLevey (he/him) is Professor and Chair of the Department of Sociology at Memorial University in St. John’s, NL, Canada (since 2024). From 2013–2024, he was an Associate Professor in Knowledge Integration, Faculty of Environment, and Sociology & Legal Studies at the University of Waterloo (Waterloo, ON, Canada). His current research focuses on disinformation, censorship, and political polarization across a wide range of national contexts and political regimes. This book is informed by his experiences as a researcher, advisor, and teacher in computational social science, data science, and quantitative research methods, working with students from diverse disciplinary backgrounds at both undergraduate and graduate levels.",
    "crumbs": [
      "🏠"
    ]
  },
  {
    "objectID": "index.html#dedication",
    "href": "index.html#dedication",
    "title": "Doing Computational Social ScienceThe Continuous Development Edition",
    "section": "Dedication",
    "text": "Dedication\nFor Quinn and Nora.",
    "crumbs": [
      "🏠"
    ]
  },
  {
    "objectID": "getting-started.html",
    "href": "getting-started.html",
    "title": "1  Getting Started",
    "section": "",
    "text": "1.1 Open Source Software\nI’ve developed a specialized computing environment that you can use for this book, and your computational social science research more generally. Like all good research computing setups, this one starts with a good text editor: VS Code. Whether you work in the cloud using GitHub Codespaces or locally on your own computer, you’ll be using VS Code.\nA good text editor is an essential part of your research toolkit. You’ll use it primarily for writing code, but it’s also great for writing academic articles, chapters, theses, books, and so on. These days, most text editors have similar capabilities. While they may differ in appearance and feel, they can be customized extensively to suit your needs. Picking an editor might seem overwhelming, but it doesn’t need to be. For instance, you can check out Wikipedia’s article on the Editor War between Emacs and Vi to see how debates over relatively minor differences can evolve into multi-generational rivalries. To quote Brian Ward (2021) from How Linux Works, “Most UNIX wizards are religious about their choice of editor, but don’t listen to them. Just choose yourself” (pp. 24). You should try out a few and stick with what works best for you.\nIn what follows, I’ll assume you’re using Visual Studio Code (VS Code). It’s free, open-source, runs on all operating systems, has excellent support for computing on remote machines, and has excellent community extensions for writing code, working with data, and even drafting technical documents. It’s especially useful for Python. VS Code also integrates some great features for collaborative coding and working in containers, making it a perfect fit for everything you’ll tackle in this book, and computational social science projects more generally.\nWe’ll be using VS Code configured to act like an Integrated Development Environment (IDE) specifically for data science and computational social science. This setup ensures that all the tools you need to work with data, write code, and create reports are integrated into a single environment. You’ll be writing and executing Python code, generating plots, and authoring documents—all within VS Code. Instead of Jupyter Notebooks, we’ll use Quarto, a powerful tool that enables you to combine code, text, and visuals in one place, much like Jupyter but with more flexibility, especially for creating publications.\nThe key benefit of this setup is that you’ll be working in a containerized environment. Think of a container as a pre-configured computer within your own, complete with all the software and tools you need for this course. Inside the container, you’ll have access to Python, data analysis libraries, and Quarto—so there’s no need to install or manage these tools on your personal machine. We’ve already taken care of that in the course’s Docker image.\nThere are two main ways to use this computing setup: in the cloud using GitHub Codespaces, or locally on your own machine. If you are entirely new to this stuff, I would recommend starting in the cloud and setting up your local environment later. But select whatever approach you prefer!",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "getting-started.html#open-source-software",
    "href": "getting-started.html#open-source-software",
    "title": "1  Getting Started",
    "section": "",
    "text": "1.1.1 Working in the Cloud with GitHub Codespaces\nIf you’re new to all of this and want to simplify the setup process, I recommend working in the cloud using GitHub Codespaces. First, you’ll need to sign up for a free GitHub account at github.com. GitHub Codespaces is a cloud-based development environment that allows you to run VS Code directly in your web browser, complete with all the tools and extensions you need. It’s especially convenient because it eliminates the need to install software on your own computer; everything runs on GitHub’s servers. As a student, you can access Codespaces for free. However, keep in mind that if your codespace remains inactive for a while, GitHub may prompt you to save your changes so that the codespace can be deleted to free up resources.\nTo get started, navigate to the supplementary materials repository for this book at https://github.com/UWNETLAB/dcss_supplementary. Once you’re there, you’ll see a green Code button near the top right corner of the repository page. Click on it, and in the dropdown menu, select Open with Codespaces. If you don’t see this option, it might be under Codespaces in the dropdown. This will launch a new Codespace instance in your browser, loading the repository’s contents and setting up the pre-configured development environment. Please note that the repository contents are being updated, so you might see new materials appear as they’re added.\nWhile working in the cloud with GitHub Codespaces is convenient, you might prefer to set up the environment on your own computer, especially if you want more control or if you anticipate working without a stable internet connection. In the next section, I’ll guide you through setting up the necessary software on your local machine so you can work offline and have all your tools directly at your fingertips.\n\n\n1.1.2 Working Locally on Your Own Computer\nIf you want to work locally on your own machine, instead of remotely in GitHub Codespaces, you’ll need to download and install Docker Desktop, Git, and VS Code on your computer. Once these are installed, you’ll be able to clone the course repository from GitHub, open it in VS Code, and start working in the container environment.\nTo get set up, install Docker Desktop and VS Code. Docker is the software that allows you to run the container, which contains the entire computing environment. To install Docker Desktop, go to the Docker Desktop page for your operating system. Download the installer and follow the instructions. Once Docker is installed, you’ll see its icon in your system’s toolbar or taskbar. Make sure it’s running before you proceed to the next steps.\nYou can download Visual Studio Code (VS Code) for free at code.visualstudio.com. After installing, open VS Code and head to the Extensions tab (on the left sidebar). Search for and install the following extensions:\n\nDev Containers (this lets you work in the container environment seamlessly)\nPython (for Python code development)\nQuarto (for authoring your Quarto files)\n\nThese extensions will make it easy to work with Python code and documents, as well as interact with the pre-configured container I’ve built for you.\nFinally, you’ll need Git, which will help you manage the course materials from GitHub. Install it from git-scm.com and follow the instructions for your operating system.\nOnce you have these tools installed, you’re ready to start working in the pre-configured environment. You don’t need to worry about installing Python, data science libraries, or Quarto—they’re all set up inside the container.\n\n\n1.1.3 Launching the Container and Getting Started\nNow that everything is installed, here’s how to set up the course materials and start working in the containerized environment:\n\n1.1.3.1 Clone the Course Repository\n\nOpen VS Code.\nClick on View in the top menu, then select Command Palette (or press Ctrl+Shift+P on Windows/Linux or Cmd+Shift+P on Mac).\nIn the command palette, type Git: Clone and select it.\nWhen prompted for the repository URL, enter https://github.com/UWNETLAB/dcss_supplementary.git. # update this\nChoose a local folder where you want to clone the repository.\n\n\n\n1.1.3.2 Open the Repository in VS Code\n\nAfter cloning, a prompt may appear asking if you want to open the cloned repository. Click Open.\nIf not, you can manually open it by going to File &gt; Open Folder and navigating to where you cloned the repository.\n\n\n\n1.1.3.3 Open in a Dev Container\n\nOnce the repository is open in VS Code, you might see a prompt at the bottom right corner asking if you want to Reopen in Container. Click on it.\nIf you don’t see the prompt, you can click on the green icon in the bottom-left corner of VS Code (the Dev Containers icon) and select Reopen in Container.\nVS Code will now build and open the container. This may take a few minutes the first time.\n\nThat’s it! You’re now set up to work within a complete computational social science environment, with everything you need installed and ready to go. In the rest of this chapter, I’ll introduce some basic knowledge and skills for command-line computing and using Git for version control. Then I’ll describe the differences between two types of virtualization used in this book: Docker containers and DevContainers on the one hand, and virtual environments managed by Poetry and/or Conda on the other. Feel free to skip this content and come back to it later if and when you need it.",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "getting-started.html#command-line-computing",
    "href": "getting-started.html#command-line-computing",
    "title": "1  Getting Started",
    "section": "1.2 Command-Line Computing",
    "text": "1.2 Command-Line Computing\nDoing computational social science or data science often requires interacting with computers using a Command-Line Interface (CLI). This may feel unfamiliar and inefficient in the age of mobile computing, beautifully designed Graphical User Interfaces (GUIs), and touch screens, but it is what enables us to do computational research in the first place. A bit of knowledge unlocks a vast world of high-quality open-source tools and enables us to benefit from the expertise and experience of many other researchers, scientists, and engineers around the world. Among a great many other things, command-line computing also enables us to work on remote computers; organize our software, data, and models according to best practices; efficiently manage and track our research projects; and make collaboration much easier, and transparency, accountability, and reproducibility possible. The command line is our common point of departure, regardless of which operating system you happen to be using. In what follows, I’ll introduce some essentials of command-line computing, starting with an introduction to “the shell.”\n\n1.2.1 The Shell\nWhen I talk about working “on the command line” or “in a terminal,” what I really mean is that we tell our computer’s operating system what we want it to do by interacting with a program called the shell. We interact with the shell by typing commands into a terminal emulator, or “terminal.” Linux, macOS, and Windows all come with pre-installed terminal emulators, but for the sake of convenience we’ll use the one that’s built into VS Code. You can open it by clicking on the ‘View’ and then ‘Terminal’ menu items, or by using a keyboard shortcut.\nWhat you see when you first open a terminal window will depend on your machine and whatever theme is active in VS Code. By default, most will likely display your username and the name of the computer you’re working on (e.g., user@computer$). My setup—which you can see in Figure 1—uses the Nord theme for VS Code and customizes the terminal prompt using a tool called Starship. As I’ll explain shortly, I’ve used Starship to configure the terminal prompt in the pre-built computing environment that we’ll use throughout this book.\n\n\n\n\n\n\nFigure 1.1: The terminal in VS Code\n\n\n\nTo interact with the shell—that is, to tell our computer what we want to do—we type commands on the command line and then hit the Return key (i.e., Enter). To see how this works, type cal on the command line, and then hit Return.\ncal\ncal is a command-line program installed on your computer; when you type the command cal, your computer executes that program and prints a calendar to the screen with the current day highlighted. Other commands we pass to the shell are also programs, and as we will see below, they tend to be most useful when we provide those programs with additional information about what we want. If you happen to issue a command that your computer does not recognize, nothing bad will happen. Your computer will just tell you that it doesn’t understand what you want it to do. Go ahead and type something like Hi, shell! and hit return. We can do almost anything we want on the command line, but we have to use commands that our computer knows about.\n\n\n1.2.2 The Structure of Shell Commands\nGenerally, the commands you execute will have the same basic structure, though the components of a command may be called different things by different people. The first word you type is the command itself, or the name of the program you are going to execute. Often, the default behavior can be modified by using options, which can be specified using a “short” version (a letter following a -, such as ls -l) or a long version (a word following --, such as ls --long). When using the short version, you can string multiple options together following a single -, such as ls -ltS. In addition to the command itself and the options that follow, commands may take arguments, such as the name of a file or directory that a command should act on. This may seem very abstract now, but if you follow along with the commands below in your terminal, it will become more concrete.\n[COMMAND] [OPTIONS] [ARGUMENTS]\nWhile there are different terminal applications available, they all do the same thing: interact with the shell. If you’re using Windows, you’re going to end up running into the limitations of your default terminal emulator pretty quickly. One solution to this problem is to get a proper terminal emulator and configure it to best meet your needs. However, in this case, it’s unnecessary because you’ll be using the terminal in VS Code to interact with the shell inside the course’s pre-built computing environment, which is Linux!\nNow that you’re set up with a good terminal emulator that runs in a Linux environment, let’s discuss a few very simple but essential actions for getting work done on the command line. We’ll focus on a small subset of commands for navigating the file system and performing various operations with directories and files.\n\n\n1.2.3 Getting Around the File System\nThe commands you will use most frequently are those that enable you to navigate a computer’s file system and perform basic operations on directories (i.e., “folders”) and files. Let’s focus on those commands first. I recommend that you work through this chapter with your terminal open. You are likely to learn faster and more deeply if you execute these commands as you read, and if you take little breaks along the way to practice what you’ve learned.\n\n1.2.3.1 Directories and Files\nAlthough there are some differences across operating systems—chiefly between Windows and the *nix systems Linux and macOS—these operating systems all organize directories (or “folders”) as hierarchical trees. The root directory sits at the top of that tree; all files and subdirectories are contained within it. We call it ‘root’, but in most operating systems, it’ll just look like a single forward slash (/). In what follows, we’ll use the pre-built computing environment, which, again, is Linux.\nMost users navigate through their computer’s directory structure using a GUI, such as Finder on macOS or File Explorer on Windows. This process works a little differently on the command line, of course, but ultimately we want to do the same thing: get “into” the directories where our files are, as well as the directories above and below them in the filesystem. The directory we are “in” at any given time is the current working directory. To see where we are in the filesystem, we can print the path to that directory by typing the command pwd (print working directory).\npwd\nTo change directories, we use the cd command (change directory). For example, to move to the home directory, you can type:\ncd ~\nOr to move up one level to the parent directory:\ncd ..\nThe command cd is usually followed by an argument, which provides more information to the shell about where you want to go. For example, you could provide an absolute path, which starts with the root directory and goes to whatever directory you want to be in. For example, if I wanted to cd into a directory where I keep various files related to graduate supervision, I could type the absolute path:\ncd /users/johnmclevey/Documents/supervision/grad_students/\nThe path is essentially an ordered list of the nested directories you would have clicked through if you were navigating the file system using a GUI like Finder or File Explorer. Each step in the path is separated by a /.\nBecause this path starts at the root directory, I can execute it regardless of where in the filesystem I currently am. Alternatively, I can provide cd with a relative path, which tells the shell where to go relative to the current working directory. For example, if my current working directory was /users/johnmclevey/Documents/ and I wanted to get to grad_students/, I could use the following relative path:\ncd supervision/grad_students/\nIn this case, the command worked because we were “in” the Documents directory. But if /users/johnmclevey/Dropbox/ was my current working directory and I typed the same command, the shell would tell me that there is no such file or directory. When using relative paths, you have to provide the path from the current working directory to the directory where you want to be.\nTo list the files in a directory, you can use the command ls. If we execute ls without any arguments, it will default to printing the files in the current working directory, but if provided with an absolute or relative path, it will list the contents of that directory instead. Below, for example, we list the contents of the current working directory’s parent directory.\nls ..\nWe can provide ls with a number of options that modify what the program prints to screen. For example, we can print some metadata about our files—such as their access permissions, the name of the user who owns the file, the file size, the last time they were modified, and so on—if we add the option -l, which is short for “long output.”\nls -l\nWe can string together a number of these short options to change the behavior of the command. For example, adding the option t to our command (ls -lt) changes the order of the files printed to screen such that the most recently modified files are at the top of the list, whereas ls -lS prints them with the largest files on top. Using ls -a will display “hidden” files, some of which we will discuss below.\n\n\n\n1.2.4 Creating Files and Directories\nIt is also possible to create new directories and files from the command line. For example, to make a new directory inside the current working directory, we can use the command mkdir followed by the name of the directory we want to create. Once created, we can move into it using cd.\nmkdir learning_shell\ncd learning_shell\nTo create a new file, we use the touch command followed by the name of the file we want to create. For example, we could create a simple text file called test.txt.\ntouch test.txt\ntest.txt is an empty file. If we wanted to quickly add text to it from the command line, we could use a built-in command-line text editor, the most minimal of which is called nano. We can edit the file by calling nano and providing the name of the file we want to edit as an argument.\nnano test.txt\n\n1.2.4.1 Getting Help\nThe commands we’ve just learned, summarized in Table 1, are the ones that you will use the most often when working on the command line and are worth committing to memory. You can build out your knowledge from here on an as-needed basis. One way to do this is to look up information about any given command and the options and arguments it takes by pulling up its manual page using the man command. For example, to learn more about ls, you could type:\nman ls\nYou can then page through the results using your spacebar, and return to the command line by pressing the letter q.\nTable 1: Essential commands for working from the command line\n\n\n\n\n\n\n\n\nCommand\nAction\nExample\n\n\n\n\npwd\nPrint current working directory\npwd\n\n\ncd\nChange directory\ncd ..\n\n\nls\nList directory contents\nls -ltS\n\n\nmkdir\nMake new directory\nmkdir figures tables\n\n\ntouch\nMake a new text file\ntouch search_log.txt\n\n\nrm\nRemove file/directory\nrm output\n\n\ncp\nCopy file/directory\ncp manuscript_draft submitted_manuscripts\n\n\nmv\nMove file/directory\nmv manuscript_draft submitted_manuscripts\n\n\ngrep\nSearch files for a pattern\ngrep 'pattern' file.txt\n\n\nopen\nOpen a file or directory in the default application\nopen search_log.txt\n\n\nhistory\nPrint the history of commands issued to the shell\nhistory\n\n\n\nIf you are looking to expand your general command-line toolkit even further, there is an enormous number of high-quality tutorials online. Now that you have a basic foundation, learning will be faster. However, as mentioned earlier, I recommend against taking a completionist approach. Instead of trying to learn everything at once, get comfortable with this foundation and expand your knowledge in a problem-driven way. That way you will have time to practice and make interacting with your computer this way feel more natural, fast, and automatic.\n\n\n\n1.2.5 Further Reading\nIn general, the best way to deepen your knowledge of open-source computing, for research or otherwise, is to learn more about Linux and command-line computing. You’ll find plenty of excellent free resources about Linux and open-source computing online, but if you’re looking for a more guided and scaffolded tour that doesn’t throw you right into the deep end, I would recommend William Shotts’ The Linux Command Line or Brian Ward’s How Linux Works.\nNow let’s learn how to use version control software, specifically Git, in order to do our work in more transparent, auditable, and reproducible ways. I’ll focus on using a visual interface for Git (a “GUI”) inside VS Code and then I’ll explain how to use Git on the command line. Use whichever approach you prefer.",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "getting-started.html#version-control-tools",
    "href": "getting-started.html#version-control-tools",
    "title": "1  Getting Started",
    "section": "1.3 Version Control Tools",
    "text": "1.3 Version Control Tools\nReal research projects can become complex messes very quickly. While there are organizational strategies you can adopt to keep the messiness to a minimum, a good version control system is essential. Version control systems watch your full project and record all the changes that happen everywhere, to every file (unless you tell it to ignore a file or subdirectory). It keeps logs of who did what, to what files, when, and if you use it properly, it can even keep log files and other notes associated with each change to the project. It eliminates the “need” for long descriptive file names with information about dates, authors’ comments, and revision stages, or emailing the “final version” of a .docx file back and forth between collaborators. At any given point, you can roll back in time to a previous state of the project. They also unlock enormous potential for collaboration—in fact, the version control system I recommend, Git, is used to manage what is arguably the largest and most complex software development project in the world, the Linux kernel, built by over 5,000 individual developers from over 400 organizations around the world.\nThe model for Git and other version control systems is that you store your project in a repository. Once your repository has been “initialized,” you work on your files as you normally would. From time to time—say each time you start and finish working on some piece of your project—you “add” or “stage” your changes and then “commit” them to the repository along with a brief log message about what you did and why. If your repository is linked to a remote server, such as GitHub, you can also “push” your changes to the remote repository.\nAs you do this, you build up a complete record of the history of your project, including who (your past self included) did what and why. You can roll back to any place in the history of your project, create experimental branches of the project to explore other lines of inquiry, and so on. Your collaborators can also have access to the remote repositories and can “clone” (i.e., download) the repository with the full history, make their own changes, add them, commit them, and push them back to the remote repository. You can get those changes yourself by “pulling” down new changes.\n\n1.3.1 Git from Inside VS Code\nUsing Git within VS Code provides a user-friendly way to manage version control without needing to remember all the command-line instructions. Here’s how you can use Git inside VS Code:\n\nAfter you’ve opened your project folder in VS Code, you’ll notice an icon on the left sidebar that looks like a branch with three nodes—that’s the Source Control tab. Click on it to access Git features.\nWhen you make changes to your files, VS Code will detect them and list them under Changes in the Source Control tab. You can click on each file to see what has changed.\nTo stage your changes, hover over the file and click on the + icon that appears, or right-click and select Stage Changes. Staged changes are ready to be committed.\nOnce you’ve staged the changes you want to commit, enter a commit message in the text box at the top (e.g., “Added data cleaning script”) and click the checkmark icon to commit them.\nIf your repository is linked to a remote repository on GitHub, you can push your commits by clicking on the ellipsis icon (...) in the Source Control tab and selecting Push. Similarly, you can pull changes from the remote repository by selecting Pull.\n\nUsing Git within VS Code makes version control more visual and can help you better understand what’s happening in your project. It also integrates well with other features of VS Code, such as the built-in terminal and extensions.\n\n\n1.3.2 Git from the Command Line\nIt’s also possible to use Git from the command line—in fact, it was designed for the command line! Let’s say you are starting a new web scraping project. You’ve written some code, all stored inside a directory called scraping_project. To use Git to manage your project, you would cd into the project directory and initialize Git. You only have to initialize Git once for a project.\ncd scraping_project\ngit init\nOnce you’ve made some changes to your project, you can “stage” the changes using git add. You can track individual file changes, but the easiest thing to do is to track any changes that have been made to the whole project directory. You can do this by specifying that you are adding changes for the full directory using the . (remember: . indicates the current directory).\ngit add .\nNext, you’ll want to write a commit message that briefly describes the changes you’ve made. For example, we might write:\ngit commit -m \"Drafted initial web scraping script\"\nGit provides a number of other useful commands that you’ll make frequent use of. For example, if you have an account on GitHub, GitLab, or some other service for managing Git repositories, you can push your changes to the remote version by using git push. Similarly, you could also “pull” down an up-to-date version of the project on another computer, once again making your work more portable. Other useful Git commands are provided in Table 2.\nTable 2: The subset of Git commands you need to do 99% of your work with\n\n\n\n\n\n\n\n\nCommand\nAction\nExample\n\n\n\n\ngit init\nInitialize a new Git repository\ngit init .\n\n\ngit add\nStage changes\ngit add ., git add article.md\n\n\ngit commit -m\nCommit changes with log message\ngit commit -m \"Drafted introduction\"\n\n\ngit push\nPush changes to remote repository\ngit push\n\n\ngit status\nCompare local repository with remote repository\ngit status\n\n\ngit pull\nUpdate your local repo with changes from remote\ngit pull\n\n\ngit clone\nClone a remote repository\ngit clone [URL]\n\n\n\nUsing version control software like Git has many benefits, not the least of which is that when you return to your work after a period of working on something else—say, when a chapter is being reviewed by your committee or a paper is under review—you will know exactly what you did and why later. When it comes to accountability, transparency, and reproducibility, this is perhaps the most important thing: you must always know what you did.\n\n\n1.3.3 Further Reading\nUsing tools like Git makes transparency, accountability, and reproducibility possible, but only if you use them properly. As you become more comfortable with these tools, you’ll want to seek out advice on how people use them for different types of projects. I recommend Patrick Ball’s (2016) “Principled Data Processing (PDP)” framework. Eric Ma’s (2021) Data Science Bootstrap: A Practical Guide to Getting Organized for Your Best Data Science Work also provides a lot of excellent advice for scientific computing more generally, and for setting up workflows for projects that don’t lend themselves well to the PDP framework.\nFinally, as a computational social scientist, you’ll need to understand and make use of virtualization tools. I’ve already set all of this up for you in our pre-built environment, so if all of this is new to you, it’s okay to skip over this next section and come back if and when you need it.",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "getting-started.html#virtualization-tools",
    "href": "getting-started.html#virtualization-tools",
    "title": "1  Getting Started",
    "section": "1.4 Virtualization Tools",
    "text": "1.4 Virtualization Tools\nAs a computational social scientist, you’ll need to understand and utilize virtualization tools to ensure your work is consistent, reproducible, and portable across different computing environments. While this might seem complex at first, virtualization allows you to create isolated environments that encapsulate all the dependencies your projects require. This ensures that your code runs exactly as intended, regardless of where or how it’s executed.\nWe’ll focus on two primary types of virtualization: containers and virtual environments. Both serve the purpose of isolating your project’s dependencies but operate at different levels.\n\n1.4.1 Containers\nContainers are a form of operating system virtualization that allows you to run applications in isolated environments. They package up software with all its necessary components—code, runtime, system tools, libraries, and settings—so it runs consistently across different computing environments.\nThe most popular tool for working with containers is Docker. Docker enables you to create, deploy, and run applications inside containers. In this book, we’ve provided a pre-built Docker image that contains all the software and libraries you’ll need for computational social science research.\nDevContainers are a feature of VS Code that leverages Docker to provide a seamless development experience. When you open a project configured with a DevContainer, VS Code automatically builds and starts the Docker container defined for the project. This means you don’t have to worry about installing the correct versions of Python or any libraries on your local machine—the container handles all of that for you.\nUsing Docker and DevContainers has several advantages:\n\nConsistency: Everyone working on the project uses the same environment, eliminating “it works on my machine” problems.\nIsolation: Your development environment is isolated from your local machine, preventing conflicts between project dependencies.\nReproducibility: Containers can be versioned and shared, ensuring that others can reproduce your computational environment exactly.\n\nTo get started with containers in this book, all you need is Docker Desktop and the Dev Containers extension in VS Code (which we’ve covered earlier). When you open the course repository in VS Code, it will automatically detect the DevContainer configuration and prompt you to reopen the project inside the container. Once you do, you’ll be working inside the pre-configured environment we’ve provided.\n\n\n1.4.2 Virtual Environments\nWhile containers isolate the entire operating system environment, virtual environments allow you to isolate Python dependencies at the project level. This is particularly useful when you have multiple projects requiring different versions of the same packages.\nIn this book, we’ll occasionally use virtual environments managed by Conda or Poetry. These tools help you create and manage project-specific environments, ensuring your Python packages and dependencies are neatly organized.\n\n1.4.2.1 Using Conda for Virtual Environments\nConda is an open-source package and environment management system that works across Windows, macOS, and Linux. It’s especially popular in the data science community. We can create and activate a Conda environment from the command line within VS Code:\nFirst, Open the Terminal in VS Code. Click on ‘Terminal’ in the top menu and select ‘New Terminal’, or use the shortcut Ctrl+Shift+ (Windows/Linux) or Cmd+Shift+ (macOS). Then Create a New Environment. Navigate (cd) to your project directory and create a new environment with\nconda create --name dcss\nwhere dcss is the name of the environment. (You can use whatever name you like.)\nOnce you’ve done that, you can Activate the Environment with\nconda activate dcss\nYou’ll notice your terminal prompt now starts with (dcss), indicating the environment is active. You can Install Packages (e.g., numpy and pandas) into your environment using conda install:\nconda install numpy pandas\n\n\n1.4.2.2 Using Poetry for Virtual Environments\nPoetry is a tool that manages Python dependencies and virtual environments, simplifying the process of setting up and maintaining your project. It’s installed in the Docker container we’re working in, and we’re using it to manage our dependencies right now! I won’t get into all the details of using Poetry here, but there are a couple of useful commands you should know about.\nFirst, to install Python packages (e.g., bambi) into the course environment, you can run\npoetry add bambi\nSecond, to install or update all packages, run\npoetry install\n\n\n\n\n\n\nPlanned Updates\n\n\n\nI am going to add a lot of content on Poetry at some point soon. Most likely in fall 2024.\n\n\n\n\n1.4.2.3 Activating Virtual Environments in VS Code\nVS Code is adept at recognizing and working with virtual environments. After creating and activating a virtual environment, you might need to tell VS Code to use it for running Python code:\n\nSelect the Python Interpreter: Press Ctrl+Shift+P (Windows/Linux) or Cmd+Shift+P (macOS) to open the Command Palette. Type Python: Select Interpreter and select it.\nChoose Your Environment: From the list of available interpreters, select the one corresponding to your virtual environment.\n\nThis ensures that when you run or debug your code in VS Code, it uses the correct environment.\n\n\n1.4.2.4 When to Use Virtual Environments\nWhile our pre-built Docker environment covers most of the computational needs for this book, there are instances—like when we explore generative models for networks—where you’ll need to create and manage virtual environments. Virtual environments are particularly useful when:\n\nWorking on Multiple Projects: Isolating dependencies prevents conflicts between projects.\nRequiring Specific Package Versions: Some projects might need specific versions of libraries that differ from others.\nCollaborating with Others: Sharing a pyproject.toml or environment.yml file allows collaborators to replicate your environment exactly.\n\nIf you’re new to this, don’t worry. You can rely on the container setup for now and revisit virtual environments when we reach those chapters.",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "getting-started.html#conclusion",
    "href": "getting-started.html#conclusion",
    "title": "1  Getting Started",
    "section": "1.5 Conclusion",
    "text": "1.5 Conclusion\nStarting with computational social science might seem overwhelming due to the array of tools and technologies involved. However, each tool serves a specific purpose in streamlining your workflow, enhancing collaboration, and ensuring reproducibility.\nWhile the initial setup requires effort, investing time in learning these tools will pay off immensely. You’ll find that they not only make your research more efficient but also open up possibilities for collaboration and innovation that aren’t feasible with traditional methods.\nRemember, it’s perfectly normal to feel a bit lost when diving into something new. Take it step by step, refer back to this chapter as needed, and don’t hesitate to seek out additional resources or ask questions. Soon enough, using these tools will become second nature, and you’ll be well-equipped to tackle any computational challenge in your research.",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "getting-started.html#key-points",
    "href": "getting-started.html#key-points",
    "title": "1  Getting Started",
    "section": "1.6 Key Points",
    "text": "1.6 Key Points\n\nCommand Line Proficiency: Understanding how to navigate and operate in the shell is foundational for computational social science.\nVS Code: A versatile text editor and IDE that integrates seamlessly with tools like Git, Docker, and virtual environments.\nGit for Version Control: Essential for tracking changes, collaborating with others, and ensuring reproducibility.\nContainers with Docker and DevContainers: Provide consistent and isolated environments, ensuring your code runs the same everywhere.\nVirtual Environments with Conda and Poetry: Allow for project-specific dependency management without interfering with system-wide settings.\nIntegration in VS Code: All these tools work together within VS Code, providing a unified and efficient workflow.\nReproducibility and Collaboration: Using these tools promotes transparency, accountability, and ease of collaboration in research.\nContinuous Learning: Embrace a problem-driven approach to expand your skills as you encounter new challenges.\n\n\n\n\n\n\nBall, Patrick. 2016. “Principled Data Processing.” Data & Society.\n\n\nMa, Eric. 2021. Data Science Bootstrap: A Practical Guide to Getting Organized for Your Best Data Science Work. LeanPub.\n\n\nWard, Brian. 2021. How Linux Works: What Every Superuser Should Know. no starch press.",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Started</span>"
    ]
  },
  {
    "objectID": "python-101.html#learning-python",
    "href": "python-101.html#learning-python",
    "title": "2  Python 101",
    "section": "2.1 Learning Python",
    "text": "2.1 Learning Python\nPython is designed to maximize human readability, and as a consequence, it’s common to feel that you have a good understanding of something because the code on the page makes sense and seems obvious, only to find yourself at a loss when you try to write the same code yourself. That’s totally normal. You must resist the temptation to copy and paste. You will understand more, identify what you don’t understand, and gain mastery faster if you actually type the code out yourself. Your future self will thank you.\nIf you are new to Python but have some experience doing scripted data analysis in another language, I suggest that you approach Python as if you were a beginner. It’s common to start working in Python by “translating” your R or Stata scripts into Python. While there are many generic programming concepts that are used in those languages and many others, efforts to “translate” your scripts will lead to writing poor Python code and will slow down your learning, causing problems later. While a certain amount of this is unavoidable, you will benefit from minimizing it. When you are working in Python, it is generally better to embrace doing things the “Pythonic” way.",
    "crumbs": [
      "**RESEARCH COMPUTING**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python 101</span>"
    ]
  },
  {
    "objectID": "python-101.html#python-foundations",
    "href": "python-101.html#python-foundations",
    "title": "2  Python 101",
    "section": "2.2 Python Foundations",
    "text": "2.2 Python Foundations\n\n2.2.1 Basic Data Types and Expressions\nEvery value in Python has a specific data type. The key data types to know are:\n\nIntegers (e.g., 42)\nFloats (e.g., 42.0)\nStrings (e.g., 'The Hitchhiker's Guide to the Galaxy' or 'cats are the best')\nBooleans (e.g., True or False)\n\nStrings are sequences of characters wrapped in single or double quotes. Both work the same way, but you must be consistent when starting and ending a string, and you can’t use quotes of one type inside quotes of the same type. Having the option for both enables us to include quotes and apostrophes within a string.\n\n\"That's correct.\"\n\n\"That's correct.\"\n\n\n\n'My teacher said, \"That is correct.\"'\n\n'My teacher said, \"That is correct.\"'\n\n\nAn expression consists of values joined by operators. The following examples use Python like a calculator.\n\n2 + 2  # Addition\n\n4\n\n\n\n2 * 9  # Multiplication\n\n18\n\n\n\n10 / 2  # Division\n\n5.0\n\n\n\n2 ** 6  # Exponentiation\n\n64\n\n\n\n2 + 9 * 7  # Python follows the order of operations\n\n65\n\n\nPython has mathematical operators for addition (+), subtraction (-), multiplication (*), division (/), floor division (//), exponentiation (**), and modulus/remainder (%) when working with numbers. Given expressions with multiple operators, Python follows the conventional mathematical order of operations.\nSome of these operators can also be used to perform operations on strings, but they represent different things. For example, when used in an expression with two strings, the + operator performs string concatenation by joining the two strings together. We demonstrate this below. The following expression is contained inside a print() function, which appropriately prints the result inline for notebooks or to the screen if you are executing scripts.\n\nprint('Miyoko is interested in ' + 'a career in data science.')\n\nMiyoko is interested in a career in data science.\n\n\nPython determines whether + should be addition or string concatenation based on context: whether both items being operated on are strings or numbers. However, Python will throw an error if you try to add, or concatenate, a number and a string. To demonstrate, create a new code cell and type 42 + 'is the answer'. You will see an error message—a “Traceback”—printed below the code cell. We will discuss errors and how to interpret them later. If you wrap the 42 in quotes and execute the cell again, you will see that Python now treats 42 as a string and performs concatenation.\n42 + ' is the answer'\n---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\n&lt;ipython-input-9-c30245934d2d&gt; in &lt;module&gt;\n----&gt; 1 42 + ' is the answer'\n\n\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n'42 ' + 'is the answer'\n\n'42 is the answer'\n\n\nAlternatively, we can convert numbers to strings with the str() function.\n\nstr(42) + ' is the answer'\n\n'42 is the answer'\n\n\nWe can also use the * operator with strings, where it becomes a string replicator, which requires a string and an integer. For example,\n\nprint('Sociology ' * 5)\n\nSociology Sociology Sociology Sociology Sociology \n\n\n\n\n2.2.2 Variables and Assignment\nWe can store data in variables by assignment, indicated by the = operator. We can call variables anything we want, provided:\n\nWe only use numbers, letters, and the underscore character (_).\nWe don’t start the name with a number.\nWe do not use any special words that are reserved for Python itself (e.g., class, def, if, etc.).\n\nVariable names are case-sensitive, so Variable, variable, and VARIABLE are all different. Use descriptive names for your variables (e.g., call the variable storing your last name as a string last_name, not ln). It makes your code much more readable and easier for you or your collaborators to understand after some time has passed.\nYou can think of a variable as a labeled container that stores specific information. In the example below, the container has a label called a_number and stores the integer value 16:\n\na_number = 16\nprint(a_number)\n\n16\n\n\nOnce you have created a variable, you can use it in expressions. For example:\n\na_number * a_number\n\n256\n\n\n\ncity = 'Cologne'\ncountry = 'Germany'\n\nprint(city + country)\n\nCologneGermany\n\n\n\nprint(city + ' is the fourth-most populous city in ' + country)\n\nCologne is the fourth-most populous city in Germany\n\n\nWe are not limited to printing the results of an expression. We can save our results in a new variable.\n\nsentence = city + ' is the fourth-most populous city in ' + country\nprint(sentence)\n\nCologne is the fourth-most populous city in Germany\n\n\n\n\n2.2.3 Objects and Methods, Illustrated with Strings\nPython is an object-oriented language, which means that it performs computations using “objects. You’ll learn more about object-oriented programming when we get to agent-based modelling, but for now it’s useful to understand a few simple concepts. Let’s use a simple analogy. Imagine two cats, Boo and Lando Catrissian, who can perform various actions like napping, drinking water, or playing. Boo and Lando are both specific instances of the general class of cats. As cats, they share common behaviors.\nIn Python, (almost) everything is an object of one kind or another. Like Boo and Lando, objects are specific instances of more general classes. Being specific instances, they typically have a name that differs from their class; the sentence object is an instance of the string class. Objects are capable of various actions—called methods—that they share with other objects of the same class. In Python, strings have methods for changing cases, checking for the presence of a value, replacing one substring with another, and many more. As computational social scientists, we frequently work with real-world text data; Python’s string methods are indispensable for doing so. To learn about a particular class and its methods, you can usually check online documentation, use Jupyter’s ? function (e.g., a_number?), or Python’s dir() function (e.g., dir(a_number)).\n\n2.2.3.1 Changing Case\nThe examples below illustrate some common string manipulation tasks. Any time we use a method, we provide the name of the object followed by a . and the name of the method. For example, to change the case of the characters in a string, we can use the .upper(), .lower(), and .title() methods. Let’s try on the city variable from earlier:\n\ncity.upper()\n\n'COLOGNE'\n\n\n\ncity.lower()\n\n'cologne'\n\n\n\ncity.title()\n\n'Cologne'\n\n\nTechnically, strings in Python are immutable, which means they cannot be changed after they are created. Methods like .upper() and .lower() do not actually change the original string; they create a new string. The code above printed those new strings, but Python did not change the string contained in city. To do that, we need to overwrite city with the new string.\n\nprint(city)\ncity = city.upper()\nprint(city)\n\nCologne\nCOLOGNE\n\n\n\n\n2.2.3.2 Checking for Substrings\nWe can check whether a string contains another string using the in operator. Python will return True if the substring is in the string or False if it is not.\n\n'GERMANY' in sentence\n\nFalse\n\n\nWe can also use the .index() method to return the starting index position where a substring appears in a string. If the substring is not in the string, this method will throw an error.\n\nsentence.index('Germany')\n\n44\n\n\nTo replace one substring with another substring, we can use the .replace() method. For example, to replace ‘COLOGNE’ with ‘KÖLN’:\n\nsentence = sentence.replace('Cologne', 'Köln')\nprint(sentence)\n\nKöln is the fourth-most populous city in Germany\n\n\n\n\n2.2.3.3 Joining and Splitting Strings\nWhen working with strings and other text data, you will often find yourself needing to split a string into multiple pieces or to join items together into a specific string. If we use the .split() method on a string with no arguments, it will split the string on whitespace and return a list.\n\nsent_split_1 = sentence.split()\nprint(sent_split_1)\n\n['Köln', 'is', 'the', 'fourth-most', 'populous', 'city', 'in', 'Germany']\n\n\nAlternatively, we can tell the .split() method to split a string at specific substrings.\n\nsent_split_2 = sentence.split('populous')\nprint(sent_split_2)\n\n['Köln is the fourth-most ', ' city in Germany']\n\n\n\nsent_split_3 = sentence.split('-')\nprint(sent_split_3)\n\n['Köln is the fourth', 'most populous city in Germany']\n\n\nTo join these items back into a single string, we use .join(). To use this method, we first provide the separator we want .join() to place between the items and then pass the items we want to reassemble into a string.\n\njoined = \" \".join(sent_split_1)\njoined\n\n'Köln is the fourth-most populous city in Germany'\n\n\n\nalso_joined = \"-\".join(sent_split_1)\nalso_joined\n\n'Köln-is-the-fourth-most-populous-city-in-Germany'\n\n\n\n\n2.2.3.4 Removing Whitespace\nStrings containing text data from the real world often have a lot of whitespace. To remove this whitespace, we can use the .strip(), .lstrip(), or .rstrip() methods to strip out extra whitespace from the whole string (not including the spaces between words) or from the beginning or end of the string.\n\nwith_whitespaces = '   This string has     extra whitespace. '\nprint(with_whitespaces)\n\n   This string has     extra whitespace. \n\n\n\nprint(with_whitespaces.strip())\n\nThis string has     extra whitespace.\n\n\n\nprint(with_whitespaces.lstrip())\n\nThis string has     extra whitespace. \n\n\n\nprint(with_whitespaces.rstrip())\n\n   This string has     extra whitespace.\n\n\n\n\n2.2.3.5 Formatting Strings with f-Strings\nWhile we can use + for string concatenation, it is usually better to use Python’s built-in tools for string formatting. One such tool is the f-string, which was introduced in Python 3.6 and is the recommended way to format strings. By putting an f before our string, we can include the name of the variable containing the relevant value inside each {}.\n\nprint(f\"{city.title()} is the fourth-most populous city in {country}.\")\n\nCologne is the fourth-most populous city in Germany.\n\n\nThis is more readable and efficient than older methods like str.format(). However, for completeness, here’s how you could use the format() method:\n\na_string = \"{} is the fourth-most populous city in {}.\"\nprint(a_string.format(city.title(), country))\n\nCologne is the fourth-most populous city in Germany.\n\n\n\n\n2.2.3.6 Multiline Strings\nIn Python, a string can be as short as zero characters (\"\" contains no characters but is a valid string) or arbitrarily long (provided it fits in your system’s memory). Sometimes, you’ll want to create or manipulate longer strings, such as the chapter of a book or the entirety of a congressional report. In such cases, it’s possible to preserve a long text’s layout using ‘newline’ characters (\\n) wherever there’s a paragraph break in the text. As you can imagine, however, this gets messy very quickly. Luckily, Python has a built-in syntax for representing multi-line strings: three single (''') or double (\"\"\") quotation marks in a row.\n\nmultiline_string = \"\"\"\nYou can work with strings longer than War and Peace, if you want.\n\nThe strings can contain line breaks.\n\"\"\"\n\nprint(multiline_string)\n\n\nYou can work with strings longer than War and Peace, if you want.\n\nThe strings can contain line breaks.\n\n\n\nLet’s set strings aside for now. We will return to them with some more advanced concepts later in the book.\n\n\n\n2.2.4 Comparison and Control Flow\nWe know how to tell our computer to execute individual instructions (e.g., evaluate 2 + 2). However, we often don’t want our computer to simply execute a series of individual instructions from top to bottom. Instead, we want to be able to tell our computer to execute code depending on one or more conditions. This is called control flow.\nControl flow statements include a condition that can be evaluated to either True or False (these are Boolean values, named after the mathematician George Boole), followed by a clause, which is an indented block of code to execute depending on whether the condition evaluates to True or False. In other words, we will execute the clause code if the condition returns True and skip over that code if the expression evaluates to False. Usually, the condition contains one or more of the comparison operators in Table 2.1, all of which resolve an expression to a Boolean value\n\n\n\nTable 2.1: Python Comparison Operators\n\n\n\n\n\n\n\n\n\nComparison\nOperator\n\n\n\n\nEqual to\n==\n\n\nNot equal to\n!=\n\n\nGreater than\n&gt;\n\n\nGreater than or equal to\n&gt;=\n\n\nLess than\n&lt;\n\n\nLess than or equal to\n&lt;=\n\n\n\n\n\n\n\ncountry == country.upper()\n\nFalse\n\n\n\ncountry != country.upper()\n\nTrue\n\n\n\ncountry == country\n\nTrue\n\n\n\n23 &lt; 33\n\nTrue\n\n\n\n33 &gt;= 33\n\nTrue\n\n\nWe can use these comparison operators to execute code conditionally. Let’s make this less abstract with a simple example that uses an if statement.\n\n2.2.4.1 If, Elif, Else\nAn if statement starts with if, followed by an expression that can be evaluated to True or False, and ends with a colon :. The expression is the condition, and the colon indicates that what follows will be the clause: the indented code to run if the condition is True.\nThe cell below illustrates control flow using a simple if statement. It executes the same if-statement on two variables, checking if a variable has the value 'COLOGNE' and printing a string if the condition evaluates to True.\n\nif city == 'COLOGNE':\n    print(\"city has the value: 'COLOGNE'\")\nelse:\n    print(\"city does not have the value: 'COLOGNE'\")\n\nif country == 'COLOGNE':\n    print(\"country has the value: 'COLOGNE'\")\nelse:\n    print(\"country does not have the value: 'COLOGNE'\")\n\ncity has the value: 'COLOGNE'\ncountry does not have the value: 'COLOGNE'\n\n\nNotice that we have included an else statement, which indicates that if the previous condition does not evaluate to True, Python should execute the indented clause code under else. In this case, we use the else statement to indicate when a variable did not match the condition.\nLet’s examine that == operator more closely.\n\nif 'doing computational social science' == 'Doing Computational Social Science':\n    print(\"The second two strings are equal to each other!\")\nif 'doing computational social science ' == 'doing computational social science':\n    print(\"The first two strings are equal to each other!\")\nif 'doing computational social science' == 'doing computational social science':\n    print(\"The third two strings are equal to each other!\")\n\nThe third two strings are equal to each other!\n\n\nThere are a few things to note here. First, Python cares about differences in capitalization (the first if-statement) and whitespace (the second if-statement) when comparing strings. 'doing', 'Doing', and 'Doing ' are all different strings.\nThis might seem frustrating at first, but Python’s capacity for specificity can pay dividends. We can also anticipate these issues and use some string manipulation methods to exert a bit more control over the comparison. For example, we can enforce title case and strip out whitespace using the string methods we just learned.\n\nif 'doing computational social science '.title().strip() == 'Doing Computational Social Science':\n    print(\"These two items are equal to one another!\")\nelse:\n    print(\"These two items are NOT equal to one another.\")\n\nThese two items are equal to one another!\n\n\nIn this example, there are only two options: if the strings are equal, do one thing; if not, do another. We can use elif, or else-if statements, to introduce code to execute under different conditions. Note that using an elif statement is functionally equivalent to nesting an if-statement within the clause of an else-statement. It will only run if the previous condition has evaluated to False.\n\nwe_do_sentence = \"We're learning control flow\"\nlearning_string = \"learning control flow\"\n\nif we_do_sentence == learning_string:\n    print(\"These two strings are equal to each other!\")\nelif learning_string in we_do_sentence:\n    print(\"The second string is in the first string, but they are not equal to each other.\")\nelse:\n    print(\"The two strings are NOT equal to each other, and the second string is NOT in the first string.\")\n\nThe second string is in the first string, but they are not equal to each other.\n\n\nWe can read this code as following this logic: if the first condition is True, then execute the first print statement. If the first condition is false, check if the second condition is True. If it is, execute the second print statement. If the preceding if- and elif-statements were all False, then execute the final print statement.\nAlso note that we used strings that contain ', such as We're. If we used single quotes to open and close those strings, Python would throw an error because it interprets the ' in 're as indicating the end of the string. If your string contains a ', then you need to open and close the string with double quotes \", and the reverse is true.\nVery importantly, the indentation in our code is meaningful. All indented code following a condition is a clause and will only be executed when the condition is met. Jupyter, VS Code, and other IDEs and text editors generally do a good job of managing indentation for you as you write your code, but you can still make mistakes. This is a double-edged sword: by making indentation syntax-relevant, Python has eliminated the need for much of the explicit statement formatting required by other languages and is thus far easier to read; in doing so, it demands that we be vigilant about finicky indentation levels, lest our code execute incorrectly.\n\n\n2.2.4.2 While Loops\nif statements are probably the most common type of statements used in control flow, but they are not the only ones. We can also use a while statement to conditionally execute code. A while statement starts with the word while and is followed by an expression that can be evaluated to True or False. You can read a while-loop as if it were saying “If condition is True, execute clause. Repeat this process until condition is False or told to stop.”\nIn the following example, we will use a while loop to print a string until we have reached the end of a course. In plain language, we say “while the current week is less than or equal to the last week, print ‘The course is still in progress’ and increase the week by 1.”\n\nweek = 1\n\nwhile week &lt;= 12:\n    print(f\"It's Week {week}. The course is still in progress.\")\n    week += 1  # equivalent to `week = week + 1`\n\nprint('\\nThe course is \"complete\". Congratulations!')\n\nIt's Week 1. The course is still in progress.\nIt's Week 2. The course is still in progress.\nIt's Week 3. The course is still in progress.\nIt's Week 4. The course is still in progress.\nIt's Week 5. The course is still in progress.\nIt's Week 6. The course is still in progress.\nIt's Week 7. The course is still in progress.\nIt's Week 8. The course is still in progress.\nIt's Week 9. The course is still in progress.\nIt's Week 10. The course is still in progress.\nIt's Week 11. The course is still in progress.\nIt's Week 12. The course is still in progress.\n\nThe course is \"complete\". Congratulations!\n\n\nRemember that the indentation is meaningful; because the last line is not indented, Python knows it is not part of the while loop and only runs it when the while loop has completed.\nAlso, note that line 5 contains a comment. Comments in Python start with #. Anything on the line after the # is not considered code and will not be executed. Comments can be very useful to include in your code. You should use comments to remind yourself what certain chunks of code do. However, as you become more comfortable with Python, you will want to avoid writing comments that translate the code into ordinary language and instead write comments that remind your future self—or inform a collaborator—about what you were trying to do, or what you were thinking at the time. These tend to be much more useful comments than descriptive translations, which become less helpful as your Python skills develop.\nA third thing to notice is that, unlike the if clause from above, this example is a loop. If the condition on Line 3 evaluates to True, the clause (the code block on Lines 4 and 5) will be executed over and over again until Line 3 evaluates to False, at which point Python exits the loop.\nYou may already be anticipating this, but it is possible to write your code in a way that leads Python into an infinite loop, in which it keeps executing the same code over and over again… forever, or until something unexpected interrupts it, like a power outage. Infinite loops are caused when the condition always evaluates to the same value. For example, if we forgot to include the + in the += on Line 5 of the example above, then Python would not add 1 to the variable week. Instead, it would simply keep re-assigning 1 to week, Line 3 will always evaluate to True, and Python is stuck in an infinite loop. To get out of an infinite loop, press Ctrl+C. If you are in a Jupyter Notebook, press the stop button to interrupt the kernel.\n\n\n2.2.4.3 Combining Comparisons with Logical Operators\nIt’s often useful to combine comparisons in a condition with logical operators such as and, or, and not to exert even more control over the code that is executed under various conditions. For example, to use the and operator in a conditional:\n\nbook = 'Doing Computational Social Science'\nif len(book) &gt; 30 and book == 'Doing Computational Social Science':\n    print(\"Correct! That's the name of this book\")\n\nCorrect! That's the name of this book\n\n\nThe conditional on Line 1 above uses and to chain together two comparisons. When using and, the line will evaluate to True if both comparisons are True. If one of them evaluates to False, then the line evaluates to False.\nAlternatively, we could use the or operator. Below, Line 1 evaluates to True if either of the two comparisons evaluates to True.\n\nif len(book) &gt; 30 or book == 'Doing Computational Social Science':\n    print(\"Correct! That's the name of this book\")\n\nCorrect! That's the name of this book\n\n\nIt might sound a bit strange, but we can also use not to make an expression return True (and therefore execute a clause) if it evaluates to False.\n\nif not book == 'Doing Computational Social Science':\n    print(\"Sorry, that's not the name of this book\")\nelse:\n    print(\"Yes, that's the name of this book!\")\n\nYes, that's the name of this book!\n\n\nIt’s possible to combine as many comparisons as you like, though this can make your code a little more difficult to read. The convention in Python is to wrap each comparison in (), which makes the code cleaner and easier to read.\n\nif (len(book) &gt; 30 and len(book) &lt; 100) or (book == 'Doing Computational Social Science'):\n    print(\"Correct! That's the name of this book\")\n\nCorrect! That's the name of this book\n\n\n\n\n\n2.2.5 Tracebacks\nSometimes, something goes wrong with our code. When errors occur, Python will provide a special report called a Traceback that identifies the nature of the problem and where it was encountered. Traceback reports can contain a lot of information and may be a bit overwhelming at first, but if you understand what Python is providing and why it is providing it, you can diagnose problems with your code and, over time, become a better Python programmer.\nIn general, you should read a Traceback from the bottom up. The final line of the Traceback tells you what kind of problem Python encountered, as well as an error message that helps you understand why the exception was raised. Lines that appear earlier in the Traceback show you where in your code the error occurred. Depending on your code, the Traceback may be short and concise or long and nested.\nAt this point, the exceptions that you are most likely to encounter are NameError (you use a variable that is not yet defined), TypeError (you perform an operation on an incorrect datatype), ValueError (you pass an argument of the right type but an inappropriate value), and SyntaxError (you broke Python’s grammar rules). We saw an example of a TypeError earlier when we tried to use + on a string and an integer.\n\n2.2.5.1 Try / Except\nWe can distinguish between at least two classes of errors. If code has broken one of the grammar rules of Python, like not properly closing brackets, you will get a syntax error. An exception occurs when syntactically correct code still manages to produce an error. While both are errors, syntax errors are detected while the code is being parsed, before the code is executed, resulting in it not running at all. Exceptions occur during execution and may not cause the execution to fail.\nWhen it encounters an exception of any type, Python’s default behavior is to halt execution of your code and provide you with a Traceback. While this is often a useful feature, it isn’t always what we want. Sometimes, we know that some of the code we’ve written is likely to encounter a very particular type of error, and we would rather Python handle the error in some way and continue executing the rest of our code. In these cases, we can use try and except statements. In general, you should not use try/except to handle syntax errors. It’s usually impossible because, as mentioned, the syntax error will halt execution before it even attempts to execute the code.\nThe try statement is used before an indented block of code, and it indicates that Python should attempt to execute the code in the indented block. When Python encounters an error whilst executing code contained within a try block, it doesn’t immediately halt execution: instead, it first checks all of the following except statements to see if the exception it has encountered is listed (e.g., except ValueError:). If so, Python then executes the code in the pertinent except block before carrying on as normal. Let’s consider an example.\nLet’s expand on the example we used for the while loop. Rather than assuming that users will always start from the first week, let’s pretend the code now allows users to input the week they wish to start from, so long as it’s one of the first three weeks. We have stored our hypothetical user’s input in the user_input variable. If its value is an integer equal to or less than 12, you’ll see one line of printout for each of the remaining weeks. What happens if the user had typed in “seven” instead of a number?\nprint(\"Please enter which week you will start in.\")\nuser_input = \"seven\"\nweek = int(user_input)\n\nwhile week &lt;= 12:\n    print(f\"It's Week {week}. The course is still in progress.\")\n    week += 1  # equivalent to `week = week + 1`\n\nprint('\\nThe course is \"complete\". Congratulations!')\nPlease enter which week you will start in.\n\n\n---------------------------------------------------------------------------\n\nValueError                                Traceback (most recent call last)\n\n&lt;ipython-input-53-9d4cb53a15de&gt; in &lt;module&gt;\n      1 print(\"Please enter which week you will start in.\")\n      2 user_input = \"seven\"\n----&gt; 3 week = int(user_input)\n      4\n      5 while week &lt;= 12:\n\n\nValueError: invalid literal for int() with base 10: 'seven'\nYou should see a ValueError exception. This makes sense. Python isn’t inherently aware of how to interpret the string \"seven\" as an integer. We can use try and except to catch some of the cases of the value error without sacrificing any of our original functionality. To do this, we’ll have to start by figuring out where the error is coming from. We can do this by using the Traceback Python printed when it encountered the exception: looks like line 3 is the culprit. Let’s try wrapping that line in a try block, and then using except ValueError plus some hand-written string comprehension to handle the error. Let’s assume that people can only start the course in the first three weeks, and account for those in our try block:\n\nprint(\"Please enter which of weeks.\")\nuser_input = \"three\"\ntry:\n    week = int(user_input)\n\nexcept ValueError:\n    if user_input.lower().strip() == \"one\":\n        week = 1\n    elif user_input.lower().strip() == \"two\":\n        week = 2\n    elif user_input.lower().strip() == \"three\":\n        week = 3\n    else:\n        raise ValueError(\"I don't recognize that as a valid number! Try again!\")\n\n\nwhile week &lt;= 12:\n    print(f\"It's Week {week}. The course is still in progress.\")\n    week += 1 # equivalent to `week = week + 1\n\nprint('\\nThe course is \"complete\". Congratulations!')\n\nPlease enter which of weeks.\nIt's Week 3. The course is still in progress.\nIt's Week 4. The course is still in progress.\nIt's Week 5. The course is still in progress.\nIt's Week 6. The course is still in progress.\nIt's Week 7. The course is still in progress.\nIt's Week 8. The course is still in progress.\nIt's Week 9. The course is still in progress.\nIt's Week 10. The course is still in progress.\nIt's Week 11. The course is still in progress.\nIt's Week 12. The course is still in progress.\n\nThe course is \"complete\". Congratulations!\n\n\nNotice that the else statement, if executed, re-raises ValueError (albeit with a different message). Rather than assume your workaround will work in every case, it’s good practice to manually raise an exception if all else fails; that way, Python will have a way of letting you know that your fix was a bust, and that it’s time to head back to the drawing board.\nWhen used judiciously, try and except are invaluable tools that will serve you well. That said, try and except – like many of the tools we will cover in this book – are prone to abuse. If you don’t specify an individual exception after your except statement, your try/except will cover all possible exceptions. When a deadline looms, the clock has struck 4am, and you’re at your wits’ end trying to hunt down the one error-throwing bug preventing your code from executing, the siren song of try and except may be very tempting. Simply wrap all of your code in a try statement, and provide a wildcard except at the end. Poof! No more errors! Problem solved, right?\nSadly no. In Python programming as in life, errors occur for a reason: something is wrong. If you don’t know what’s causing an error, you should not trust your code. Code that cannot be trusted is worthless at best, and potentially harmful. Avoid doing this at all costs.",
    "crumbs": [
      "**RESEARCH COMPUTING**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python 101</span>"
    ]
  },
  {
    "objectID": "python-101.html#conclusion",
    "href": "python-101.html#conclusion",
    "title": "2  Python 101",
    "section": "2.3 Conclusion",
    "text": "2.3 Conclusion\nIn this chapter, we’ve laid the foundation for your journey into Python programming. You learned about the basic data types in Python, including strings, integers, floats, and Booleans, and how to assign them to variables. We explored how to manipulate strings using methods like .upper(), .lower(), .strip(), and .replace(), which are essential when working with text data.\nWe delved into control flow, understanding how to use comparison operators and conditional statements like if, elif, and else to execute code based on certain conditions. You learned how to loop over code using while loops and the importance of careful indentation in Python.\nWe also discussed how to read and interpret Python’s Traceback messages when errors occur, which is crucial for debugging your code. By understanding the difference between syntax errors and exceptions, you can better diagnose and fix issues in your programs.\nFinally, we introduced try and except statements for handling exceptions gracefully. You saw how to use these constructs to catch specific exceptions, provide meaningful feedback, and ensure your programs can handle unexpected inputs without crashing.\nThese foundational skills are essential as you progress in computational social science. They enable you to write more robust, efficient, and user-friendly code. In the next chapter, we’ll build on this knowledge by exploring data structures, functions, and how to work with external files in Python.\n\n\n2.3.1 Key Points\n\nPython Fundamentals: You learned about basic data types (strings, integers, floats, Booleans) and how to assign them to variables.\nString Manipulation: Explored methods for manipulating strings, such as changing case, concatenation, and formatting.\nControl Flow: Gained an understanding of control flow using comparison operators and conditional statements (if, elif, else, while).\nError Handling: Learned how to read and interpret Python’s Traceback messages to diagnose errors.\nException Handling: Discovered how to handle exceptions using try and except blocks, enhancing the robustness of your code.\nBest Practices: Emphasized the importance of catching specific exceptions and avoiding the use of broad except clauses that can mask underlying issues.\nNext Steps: With these foundational skills, you’re well-prepared to delve deeper into Python programming in the next chapter.",
    "crumbs": [
      "**RESEARCH COMPUTING**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python 101</span>"
    ]
  },
  {
    "objectID": "python-102.html",
    "href": "python-102.html",
    "title": "3  Python 102",
    "section": "",
    "text": "3.1 Working with Python’s Data Structures",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python 102</span>"
    ]
  },
  {
    "objectID": "python-102.html#working-with-pythons-data-structures",
    "href": "python-102.html#working-with-pythons-data-structures",
    "title": "3  Python 102",
    "section": "",
    "text": "3.1.1 Working with Lists and Tuples\nIn Python, lists are ordered collections of any object, such as strings, integers, floats, or even other data structures like lists or dictionaries. The items in a list may be of the same type, but they don’t have to be. Python lists are very flexible—you can mix information of various kinds, add information to the list on the fly, and remove or change any of the information the list contains. This is not always the case in other languages.\nThe most basic list looks like this:\nmy_list = []\nThe above code produces an empty list; it contains no objects. You can also create an empty list by referring directly to the built-in list type:\nmy_list = list()\nAll lists begin and end with square brackets [], and elements are separated by commas. Below, we define two lists containing strings (megacities in one list and their countries in another) and one list containing numbers (city population in 2018). Note that we’re using _ as a thousands separator to make our code more readable. As far as Python is concerned, 37_468_000 and 37468000 are identical numbers, but the former is easier to read.\nmegacities = ['Tokyo', 'Delhi', 'Shanghai', 'São Paulo', 'Mexico City', 'Cairo', 'Dhaka', 'Mumbai', 'Beijing', 'Osaka']\ncountries = ['Japan', 'India', 'China', 'Brazil', 'Mexico', 'Egypt', 'Bangladesh', 'India', 'China', 'Japan']\npop2018 = [37_468_000, 28_514_000, 25_582_000, 21_650_000, 21_581_000, 20_076_000, 19_980_000, 19_618_000, 19_578_000, 19_281_000]\nEvery item in a list has an index based on its position in that list. Indices are integers, and in Python, indexing starts at 0, which means that the first item in any list starts at index 0. In the megacities list, the index for 'Tokyo' is 0, 'Delhi' is 1, 'Shanghai' is 2, and so on.\n\n\n\nIndexing in Python starts at 0, not 1.\n\n\nWe can use the index to select a specific item from a list by typing the name of the list and then the index number inside square brackets:\nmegacities[3]  # Returns 'São Paulo'\nWe can also access individual items by working from the end of the list using negative indices. In this case, -1 refers to the last item, -2 to the second-last item, and so on:\ncountries[-2]  # Returns 'China'\nWhen we access an individual item from a list, Python returns the item in its expected data type. For example, megacities[3] returns 'São Paulo' as a string, and pop2018[3] returns the integer 21650000. We can use any methods associated with that particular data type:\npop2018[3] * 3  # Multiplies the population by 3\nmegacities[3].upper()  # Converts 'São Paulo' to uppercase\nUsing square brackets to access an element in a list (or tuple, or dictionary) is called subscripting, and it can accept a wider variety of indices than a simple integer. One particularly useful way to subscript an object is to use slice notation, where two index positions are separated by a colon:\nmegacities[0:3]  # Returns the first three cities\nUsing a slice to subscript a list returns the item at the first index position and every item up to—but not including—the second index position. To retrieve the last three entries of our list, you would use:\ncountries[7:10]  # Returns the last three countries\nYou can also use slice notation with one integer missing to return all of the items in a list up to—or starting at—a particular index position. The following gives us the first three megacities:\nmegacities[:3]\nAnd this returns the last seven:\nmegacities[-7:]\n\n3.1.1.1 Looping Over Lists\nPython’s lists are iterable objects, which means that we can iterate (or loop) over the list’s elements to execute code for each one. This is commonly done with a for loop. Below, we iterate over the list megacities and print each item:\nfor city in megacities:\n    print(city)\nThis code creates a temporary variable called city that refers to the current element of megacities being iterated over. After the loop completes, city will have referred to each element in the list. The name for this variable should be descriptive and convey information about the elements of the list.\n\n\n3.1.1.2 Modifying Lists\nLists can be changed in a number of ways. We can modify the items in the list like we would other values, such as changing the string 'Mexico City' to 'Ciudad de México' using the value’s index:\nmegacities[4] = 'Ciudad de México'\nprint(megacities)\nWe often want to add or remove items from a list. Let’s add 'Karachi' to our three lists using the .append() method:\nmegacities.append('Karachi')\ncountries.append('Pakistan')\npop2018.append(16_093_786)\n\nprint(len(megacities), len(countries), len(pop2018))\nOur lists now contain 11 items each; our Karachi data was appended to the end of each list.\nYou’ll use .append() frequently. It’s a convenient way to dynamically build and modify a list. This book has many examples of creating an empty list that is populated using .append(). Let’s create a new list that will contain a formatted string for each city:\ncity_strings = []\n\nfor city in megacities:\n    city_string = f\"What's the population of {city}?\"\n    city_strings.append(city_string)\n\nfor city_string in city_strings:\n    print(city_string)\nRemoving items is just as straightforward. There are a few ways to do it, but .remove() is one of the more common ones:\nmegacities.remove('Karachi')\ncountries.remove('Pakistan')\npop2018.remove(16_093_786)\nSometimes we want to change the organization of a list. This usually involves sorting the list in some way (e.g., alphabetically, numerically). Below, we make a copy of megacities and sort it alphabetically. We don’t want to modify the original list, so we explicitly create a new copy using the .copy() method:\nmegacities_copy = megacities.copy()\nmegacities_copy.sort()\nprint(megacities_copy)\nNote that we do not use = when we call .sort(). This method modifies the list in place. Assigning megacities_copy = megacities_copy.sort() will actually set megacities_copy to None because the .sort() method returns None.\nIf we change the order of items in a list using the .sort() method, the original order is lost. We cannot ‘undo’ the sort unless we keep track of the original order. That’s why we started by making a copy. To temporarily sort our list without changing the order of items in the original list, use the sorted() function:\nsorted_megacities = sorted(megacities)\nprint(sorted_megacities)\nWhen applied to a list of numbers, .sort() will reorder the list from smallest to largest:\npop_copy = pop2018.copy()\npop_copy.sort()\nprint(pop_copy)\nTo sort a list in reverse alphabetical order, or numbers from largest to smallest, use the reverse=True argument:\npop_copy.sort(reverse=True)\nprint(pop_copy)\n\nmegacities_copy.sort(reverse=True)\nprint(megacities_copy)\nThe fact that lists are ordered makes them very useful. However, if you change the order of a list, you could easily introduce costly mistakes. For example, if we sorted our pop2018 list above without also sorting the megacities and countries lists in the same way, they would become misaligned. We would lose the ability to accurately pair cities with their populations:\nprint(f'The population of {megacities[4]} in 2018 was {pop2018[4]}')  # Might produce incorrect pairing\n\n\n3.1.1.3 Zipping and Unzipping Lists\nWhen you have data spread out over multiple lists, it can be useful to zip those lists together so that all the items with an index of 0 are associated with one another, all the items with an index of 1 are associated, and so on. The most straightforward way to do this is to use the zip() function, which is illustrated in Figure 3.1 and the code block below. Clever usage of zip() can accomplish a great deal using very few lines of code.\n\n\n\n\n\n\nFigure 3.1: cap\n\n\n\nfor paired in zip(megacities, countries, pop2018):\n    print(paired)\nThe actual object that the zip() function returns is a ‘zip object’, within which our data is stored as a series of tuples (discussed later). We can convert these zipped tuples to a list of tuples using the list() function:\nzipped_list = list(zip(megacities, countries, pop2018))\nprint(zipped_list)\nIt’s also possible to unzip a zipped list using the * operator and multiple assignment (also called ‘unpacking’), which allows us to assign multiple values to multiple variables in a single line:\ncity_unzip, country_unzip, pop_unzip = zip(*zipped_list)\nprint(city_unzip)\nprint(country_unzip)\nprint(pop_unzip)\n\n\n3.1.1.4 List Comprehensions\nEarlier, we created an empty list and populated it using .append() in a for loop. We can also use list comprehensions, which can produce the same result in a single line of code. To demonstrate, let’s try counting the number of characters in the name of each country in the countries list using a for loop and then with a list comprehension.\nUsing a for loop:\nlen_country_name = []\n\nfor country in countries:\n    n_chars = len(country)\n    len_country_name.append(n_chars)\n\nprint(len_country_name)\nUsing a list comprehension:\nlen_country_name = [len(country) for country in countries]\nprint(len_country_name)\nList comprehensions can be a little strange at first, but they become easier with practice. The key things to remember are that they will always include:\n\nThe expression itself, applied to each item in the original list.\nThe temporary variable name for the iterable.\nThe original iterable, which in this case is the list.\n\nAbove, the expression was len(country), country was the temporary variable name, and countries was the original iterable.\nWe often wish to add conditional logic to our for loops and list comprehensions. Let’s create a new list of cities with populations greater than 20,500,000 with the help of the zip() function:\nbiggest = [[city, population] for city, population in zip(megacities, pop2018) if population &gt; 20_500_000]\nprint(biggest)\nThe result—biggest—is a list of lists. We can work with nested data structures like this using the same tools we use for flat data structures. For example:\nfor city in biggest:\n    print(f'The population of {city[0]} in 2018 was {city[1]}')\nWhen should you use a for loop and when should you use list comprehension? In many cases, it’s a matter of personal preference. List comprehensions are more concise while still being readable with some Python experience. However, they become unreadable very quickly if you need to perform a lot of operations on each item or if you have complex conditional logic. In those cases, you should avoid list comprehensions to keep your code readable.\nList comprehensions are very popular in Python, so it’s important to know how to read them. Since for loops and list comprehensions do the same thing in slightly different ways, you should be able to convert one into the other and back again.\n\n\n3.1.1.5 Copying Lists\nEarlier, we copied a list using the .copy() method, which is helpful if we want to preserve our original list. Could we accomplish this using the familiar = operator?\ncountries_copy = countries\nprint(countries_copy)\nThis approach appears to create a copy of countries. However, when we copy a list using the = operator, we are not creating a new object. Instead, we have created a new variable name that points to the original object in memory. Any modifications made using countries_copy will change the same object in memory referenced by countries. If we append 'Karachi' to countries_copy and print countries, we would see 'Karachi' in both lists. If we want to preserve the original list and make modifications to the second, this will not do.\nInstead, we can use the .copy() method to create a shallow copy of the original list or use the copy module to create a deep copy. To understand the difference, compare a flat list (e.g., [1, 2, 3]) with a list of lists (e.g., [[1, 2, 3], [4, 5, 6]]). The list of lists is nested; it is deeper than the flat list. If we perform a shallow copy (i.e., .copy()) of the flat list, Python will create a new object that is independent of the original. But if we create a shallow copy of the nested list of lists, Python only makes a new object for the outer list; it’s only one level deep. The contents of the inner lists [1, 2, 3] and [4, 5, 6] are not copied; they are references to the original lists.\nWhen working with nested data structures, such as lists of lists, we need to use the deepcopy() function from the copy module if we want to create a new object that is fully independent of the original:\nimport copy\n\nnested_list = [[1, 2, 3], [4, 5, 6]]\nshallow_copy = nested_list.copy()\ndeep_copy = copy.deepcopy(nested_list)\nNow, modifications to deep_copy will not affect nested_list, even in the nested elements.\n\n3.1.1.5.1 In or Not In?\nLists used in research contexts are usually far larger than the examples in this chapter. They may have thousands or even millions of items. To find out if a list contains or does not contain a specific value, rather than manually searching a printed list, we can use the in and not in operators, which will evaluate to True or False:\n'Mexico' in countries\n'Mexico' not in countries\nThese operators can be very useful when using conditions. For example:\nto_check = 'Toronto'\n\nif to_check in megacities:\n    print(f'{to_check} was one of the 10 largest cities in the world in 2018.')\nelse:\n    print(f'{to_check} was not one of the 10 largest cities in the world in 2018.')\n\n\n\n3.1.1.6 Using enumerate\nIn some cases, we want to access both the item and its index position from a list at the same time. We can do this with the enumerate() function. Recall the three lists from the megacity example. Information about each megacity is spread out across three lists, but the indexes are shared across those lists. Below, we enumerate megacities, creating a temporary variable for the index position (i) and each item (city), and iterate over it. We use those values to print the name of the city and then access information about the country and city population using the index position. This works because the items in the lists are ordered and aligned:\nfor i, city in enumerate(megacities):\n    print(f'{city}, {countries[i]}, has {pop2018[i]} residents.')\nAs previously mentioned, we can include as many lines as we want in the indented code block of a for loop, which can help us avoid unnecessary iteration. If you have to perform multiple operations on items in a list, it’s best to iterate over the data structure once and perform all the necessary operations rather than iterate over the list multiple times.\n\n\n3.1.1.7 Tuples: When Your Lists Won’t Change\nIn Python, every object is either mutable or immutable. We’ve just shown many ways that lists are mutable: adding and removing items, sorting them, and so on. Any data type in Python that allows you to change something about its composition is mutable. Data types that do not permit changes after instantiation are immutable.\nGenerally speaking, computational social scientists prefer to work with mutable objects such as lists. The flexibility we gain by working with a mutable data type usually outweighs the advantages of working with immutable types—but not always. To illustrate the usefulness of immutable types, we’ll introduce the tuple.\nA tuple is an ordered, immutable series of objects. You can think of tuples as a special kind of list that can’t be modified once created. In terms of syntax, values in a tuple are stored inside parentheses () rather than square brackets []. We can instantiate an empty tuple in a similar fashion to lists:\nmy_empty_tuple = ()\nJust like with lists, we can instantiate our tuples with pre-loaded values:\na_useful_tuple = (2, 7, 4)\nWe can easily convert between tuples and lists using the tuple() and list() functions, respectively:\nprint(type(countries))\ncountries_tuple = tuple(countries)\nprint(type(countries_tuple))\nThere are many uses for tuples: if you absolutely must ensure that the order of a series of objects is preserved and unchangeable, use a tuple. To illustrate, let’s use the list method .sort() to change the order of items in our countries list. Note that we will use the .copy() method to preserve a record of the original order:\ncountries_sorted = countries.copy()\ncountries_sorted.sort()\ncountries_sorted\nNow, the countries are in alphabetical order. However, the countries_sorted list is out of order with the megacities and pop2018 lists. In a large project, accidentally sorting a list that shouldn’t have been sorted might create serious mismatches in your data. To prevent something like this from happening, consider storing your lists as tuples instead; that way, if you try to use the .sort() method on a tuple, Python will throw an error and prevent unintended changes.\nTuples have a few other advantages over lists:\n\nPerformance: Using tuples can speed up your code and reduce memory usage.\nHashability: Tuples can be used as keys in dictionaries because they are immutable, whereas lists cannot.\n\nFinally, even though lists are mutable and tuples are immutable, they have another feature in common: they are both iterable. Any of the forms of iteration that can be applied to lists can be applied to tuples, too.\n\n\n\n3.1.2 Dictionaries\nAnother Python data structure that you’ll frequently see and use is the dictionary. Unlike lists, dictionaries are designed to connect pieces of related information. Dictionaries offer a flexible approach to storing key-value pairs. Each key must be an immutable Python object, such as an integer, a float, a string, or a tuple, and there can’t be duplicate keys. Values can be any type of object. We can access values by specifying the relevant key.\nWhere lists use square brackets [], and tuples use parentheses (), Python’s dictionaries wrap key-value pairs in curly braces {}, where the keys and values are separated by a colon :, and each pair is separated by a comma. For example:\ntokyo = {\n    'country': 'Japan',\n    'pop2018': 37_468_000\n}\n\nprint(tokyo)\nWe can use as many keys as we like when we create a dictionary. To quickly access a list of all the keys in the dictionary, we can use the .keys() method:\nprint(tokyo.keys())\nTo access any given value in a dictionary, we provide the name of the dictionary object followed by the name of the key whose value we want to access inside square brackets and quotes:\ntokyo['pop2018']\nLike lists, but unlike tuples, dictionaries can be modified as we work. We can add a new key-value pair to tokyo—say, the population density of the Tokyo Metropolitan Area—using the same syntax we learned for referencing a key. Because the key we are referencing doesn’t exist in the dictionary, Python knows we are creating a new key-value pair:\ntokyo['density'] = 6_168\nprint(tokyo)\nWe could have also started with an empty dictionary and populated it with key-value pairs:\ndelhi = {}\n\ndelhi['country'] = 'India'\ndelhi['pop2018'] = 28_514_000\ndelhi['density'] = 11_312\n\nprint(delhi)\n\n3.1.2.1 Nested Data Structures\nLists, tuples, and dictionaries can all be nested in a variety of ways, including using dictionaries as items in a list, lists as items in dictionaries, and so on. Working with these nested structures is straightforward. Whatever value you are working with, no matter its position in the nested data structure, you can use the methods appropriate to that type.\nIf we have a dictionary that has lists as values, we can subscript the values after accessing them with the appropriate key:\njapan = {}\njapan['cities'] = ['Tokyo', 'Yokohama', 'Osaka', 'Nagoya', 'Sapporo', 'Kobe', 'Kyoto', 'Fukuoka', 'Kawasaki', 'Saitama']\njapan['populations'] = [37, 3.7, 2.7, 2.3, 1.9, 1.5, 1.5, 1.5, 1.5, 1.2]\n\nprint(japan)\nAccessing the fifth city:\njapan['cities'][4]\n\n\n3.1.2.2 Lists of Dictionaries\nWe can also store dictionaries as elements in a list. Earlier we created dictionaries tokyo and delhi. Both contain the same keys: 'country', 'pop2018', and 'density'. Adding them to a list is straightforward:\ntop_two = [tokyo, delhi]\n\nfor city in top_two:\n    print(city)\nWhile any number of arrangements is possible, things can quickly become complicated the more deeply data structures are nested. If you find yourself building complex nested data structures, take a moment to think about what problem you’re trying to solve and assess your approach. There is almost certainly a way to approach the problem that is cleaner and simpler, reducing the likelihood of making a difficult-to-detect mistake while also making your code more readable.",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python 102</span>"
    ]
  },
  {
    "objectID": "python-102.html#custom-functions",
    "href": "python-102.html#custom-functions",
    "title": "3  Python 102",
    "section": "3.2 Custom Functions",
    "text": "3.2 Custom Functions\nSo far, we’ve used a few functions that are built into Python, such as print() and len(). In these and other cases, built-in functions take an input, perform some operations, and then return an output. For example, if we pass the len() function a string, it computes the number of characters in that string and returns an integer:\nseoul = 'Seoul, South Korea'\nlen(seoul)\nWe could have computed the length of that string without using len(), for example:\nlength = 0\nfor character in seoul:\n    length += 1\nprint(length)\nBoth chunks of code compute the length of the string stored in seoul, but using len() avoids unnecessary work. We use functions to take advantage of abstraction: converting repeated tasks into condensed and easily reusable tools. Modern software is built on decades of abstraction. We don’t code in binary because we have abstracted that process, moving into higher-level languages. Functions save us time, space, and brainpower. This is what you should aim for when you write your own functions: identify small tasks or problems that you repeat often, and write a function that handles them so you can focus on more important problems.\nImagine a set of operations that we need to apply multiple times, each time with a different input. You start by picking one of those inputs and writing the code that produces the end result you want. Where do you go from here? One option, which I do not recommend, is to copy and paste that code for each of the inputs. Once your code is copied, you change the names of the inputs and outputs so that you get the desired output for each input.\nWhat happens if you discover a problem in the code or decide to improve it? You have to change the relevant parts of your code in multiple places, and each time you risk missing something or making a mistake. To make matters worse, the script is far longer than it needs to be, and the sequence of operations is harder to follow and evaluate.\nInstead, we can write our own functions that let us reuse chunks of code. If we discover a problem or something we want to change, then we only have to make the change in one place. When we execute our updated function, it will reliably produce the newly desired output. We can store our functions in a separate script and import them elsewhere, making those scripts and notebooks more concise and easier to understand. And if we use descriptive names for our functions, we can abstract away low-level details to focus on higher-level concepts. This is always a good idea, especially when working on large projects.\nWriting our own functions offers several benefits:\n\nReusability: Don’t do work that has already been done.\nAbstraction: Abstract away low-level details so you can focus on higher-level concepts and logic.\nError Reduction: If you find a mistake, you only need to fix it in one place.\nReadability: Shorter and more readable scripts are easier to understand and maintain.\n\n\n3.2.1 Writing Custom Functions\nTo define a function in Python, you start with the keyword def followed by the name of the function, parentheses containing any arguments the function will take, and then a colon :. All code that is executed when the function is called is contained in an indented block. Below, we define a function called welcome() that accepts a name and prints a greeting:\ndef welcome(name):\n    print(f'Hi, {name}! Good to see you.')\nCalling the function:\nwelcome('Miyoko')\nIn this case, the function prints a new string to the screen. While this can be useful, most of the time we want to actually do something to the input and then return a different output. If a function does not explicitly return an output, it will still return None.\nHere’s an example of a function that cleans a string:\ndef clean_string(some_string):\n    cleaned = some_string.strip().lower()\n    return cleaned\n\ncleaned_str = clean_string('   Hi, my name is John McLevey.   ')\nprint(cleaned_str)\nUser-defined functions can be as simple or as complex as we like, although you should strive to design functions that are as simple as possible. The first and most important step is to know what problem you’re trying to solve. If you understand a problem, you can often break it down into smaller sub-problems, some of which might be repeated many times. Rather than write an enormous block of code that handles everything, you can write several sub-functions that individually handle these smaller problems, letting you better organize your approach.\nYou can also define functions with default arguments, allowing you to call the function without specifying all parameters every time:\ndef greet(name, message='Good to see you'):\n    print(f'Hi, {name}! {message}.')\n\ngreet('Miyoko') \n\n# or override the default message\ngreet('Miyoko', 'How have you been')\nThere is plenty more that could be said about writing your own functions; however, we’ll set further discussions aside for now. Throughout the rest of this book, we’ll see many examples of custom functions, and you’ll have many opportunities to write and improve your own.",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python 102</span>"
    ]
  },
  {
    "objectID": "python-102.html#reading-and-writing-files",
    "href": "python-102.html#reading-and-writing-files",
    "title": "3  Python 102",
    "section": "3.3 Reading and Writing Files",
    "text": "3.3 Reading and Writing Files\nWhen working with data, you will routinely find yourself reading from and writing to files on disk. While we’ll delve into specifics like reading CSV files or interacting with databases in later chapters, it’s important to understand the basics of file I/O (Input/Output) in Python.\nTo interact with files in Python, you typically need to:\n\nOpen the file.\nPerform some operation (read, write, append, etc.).\nClose the file.\n\nHowever, manually opening and closing files can be error-prone if, for example, an exception occurs before you close the file. To handle this gracefully, Python provides the with statement, which ensures that the file is properly closed after its suite finishes, even if an exception is raised.\nHere’s how you can read from a text file using the with statement:\nfrom pathlib import Path\n\nfile_path = Path('data/some_text_file.txt')\n\nwith file_path.open(mode='r', encoding='utf-8') as file:\n    text_data = file.read()\n    print(len(text_data))\nIn this example:\n\nWe use the pathlib module to handle file paths in an object-oriented way, which is more robust and portable across different operating systems.\nThe file_path.open() method is used instead of the built-in open() function, which integrates nicely with pathlib.\nThe with statement ensures that the file is automatically closed after we’re done with it.\nThe mode='r' argument specifies that we are opening the file in read mode.\nThe encoding='utf-8' argument specifies the file encoding, which is important for correctly reading text files, especially those containing non-ASCII characters.\n\nTo read the file line by line into a list, you can use the .readlines() method:\nwith file_path.open(mode='r', encoding='utf-8') as file:\n    list_of_lines = file.readlines()\n    print(len(list_of_lines))\nIf you want to write data to a file, you can use mode='w' for write mode:\nwith file_path.open(mode='w', encoding='utf-8') as file:\n    file.write(\"Hello, World!\")\nThis will create a new file or overwrite an existing file with the text “Hello, World!”.\n\n3.3.1 Exception Handling with File Operations\nWhen working with files, it’s good practice to include exception handling to manage potential errors, such as the file not existing or permission issues.\ntry:\n    with file_path.open(mode='r', encoding='utf-8') as file:\n        text_data = file.read()\nexcept FileNotFoundError:\n    print(f\"The file {file_path} does not exist.\")\nexcept IOError:\n    print(f\"An error occurred while reading the file {file_path}.\")",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python 102</span>"
    ]
  },
  {
    "objectID": "python-102.html#pace-yourself",
    "href": "python-102.html#pace-yourself",
    "title": "3  Python 102",
    "section": "3.4 Pace Yourself",
    "text": "3.4 Pace Yourself\nThere’s much more to Python than what we’ve covered here, but it’s important to pace yourself. You don’t need to learn everything all at once before diving into your research projects. As you progress through the book, we’ll introduce additional Python concepts and techniques relevant to specific research contexts. This just-in-time learning approach allows you to build a solid foundation while acquiring new tools as you need them.\n\nFurther Reading\nThere is no shortage of excellent resources for learning Python. If you’re looking for more in-depth material, consider the following:\n\n“Python for Everybody” by Charles Severance: A great starting point for beginners, offering a comprehensive introduction to Python programming.\n“Automate the Boring Stuff with Python” by Al Sweigart: Focuses on practical programming skills for automating tasks.\n“Think Python” by Allen B. Downey: Offers a deeper dive into Python programming concepts.\n\nRemember, the best way to learn programming is by doing. Practice regularly, experiment with code, and don’t hesitate to revisit concepts that are challenging.",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python 102</span>"
    ]
  },
  {
    "objectID": "python-102.html#conclusion",
    "href": "python-102.html#conclusion",
    "title": "3  Python 102",
    "section": "3.5 Conclusion",
    "text": "3.5 Conclusion\nIn this chapter, we’ve expanded your Python programming skills by exploring essential data structures, functions, and file operations.\nYou learned about:\n\nLists, Tuples, and Dictionaries: How to create and manipulate these built-in data structures, including adding, removing, and accessing elements. You also learned about the importance of mutability and immutability in Python objects.\nIteration: How to loop over items in lists, tuples, and dictionaries using for loops and comprehensions. This included understanding the use of functions like zip(), enumerate(), and list comprehensions to perform efficient data processing.\nCustom Functions: The importance of writing your own functions to promote code reuse, abstraction, and readability. You saw how to define functions using def, pass arguments, and return values.\nReading and Writing Files: The fundamentals of file I/O in Python, including using the with statement for safe file handling, reading from and writing to text files, and working with common data formats like JSON and CSV.\nBest Practices: Introduced modern practices like using the pathlib module for file path management and the importance of exception handling when working with files.\n\nBy mastering these concepts, you’re building a strong foundation for more advanced topics in computational social science. You’ll find that these skills are indispensable as you tackle complex data analysis tasks, work with larger datasets, and develop more sophisticated programs.\n\n\n3.5.1 Key Points\n\nData Structures: Understanding and using Python’s built-in data structures—lists, tuples, and dictionaries—is crucial for effective data manipulation.\nIteration and Comprehensions: Leveraging loops and comprehensions allows for efficient processing of iterable objects.\nControl Flow Operators: Using in, not in, and isinstance enhances your ability to write conditional code.\nCustom Functions: Writing your own functions promotes code reuse and better organization.\nFile Operations: Mastering file I/O, including reading and writing different file types, is essential for handling data.\nModern Practices: Employing modules like pathlib and proper exception handling aligns your code with contemporary best practices.",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Python 102</span>"
    ]
  },
  {
    "objectID": "iterative-workflow.html",
    "href": "iterative-workflow.html",
    "title": "5  Iterative workflows",
    "section": "",
    "text": "In Progress\n\n\n\nThis is a new chapter! I’m currently revising an initial draft, which I will publish here in the coming weeks.",
    "crumbs": [
      "**THINKING THROUGH MODELS**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Iterative workflows</span>"
    ]
  },
  {
    "objectID": "sampling-and-survey-data.html",
    "href": "sampling-and-survey-data.html",
    "title": "6  Processing Structured Data Sampling and Survey Data",
    "section": "",
    "text": "7 Processing Structured Data",
    "crumbs": [
      "**OBTAINING DATA**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>~~Processing Structured Data~~ Sampling and Survey Data</span>"
    ]
  },
  {
    "objectID": "sampling-and-survey-data.html#imports",
    "href": "sampling-and-survey-data.html#imports",
    "title": "6  Processing Structured Data Sampling and Survey Data",
    "section": "7.1 Imports",
    "text": "7.1 Imports\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom dcss import download_dataset\nfrom dcss import set_style\nset_style()",
    "crumbs": [
      "**OBTAINING DATA**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>~~Processing Structured Data~~ Sampling and Survey Data</span>"
    ]
  },
  {
    "objectID": "sampling-and-survey-data.html#practical-pandas-first-steps",
    "href": "sampling-and-survey-data.html#practical-pandas-first-steps",
    "title": "6  Processing Structured Data Sampling and Survey Data",
    "section": "7.2 PRACTICAL PANDAS: FIRST STEPS",
    "text": "7.2 PRACTICAL PANDAS: FIRST STEPS\n\n7.2.1 Getting Data into Pandas\nThe Pandas package makes it easy to load data from an external file directly into a dataframe object. It uses one of many reader functions that are part of a suite of I/O (input / output, read / write) tools. I’ve listed some common examples in the table below. Information on these and other reader functions can be found in the Pandas documentation, which also provides useful information about the parameters for each method (e.g. how to specify what sheet you want from an Excel spreadsheet, or whether to write the index to a new CSV file).\n\nI/0 Methods for Pandas\n\n\nData Description\nReader\nWriter\n\n\n\n\nCSV\nread_csv()\nto_csv()\n\n\nJSON\nread_json()\nto_json()\n\n\nMS Excel and OpenDocument (ODF)\nread_excel()\nto_excel()\n\n\nStata\nread_stata()\nto_stata()\n\n\nSAS\nread_sas()\nNA\n\n\nSPSS\nread_spss()\nNA\n\n\n\nI will focus on the read_csv() function to demonstrate the general process. The only required argument is that we provide the path to the file location, but there are many useful arguments that you can pass, such as the file encoding. By default, Pandas assumes your data is encoded with UTF-8. If you see an encoding error or some strange characters in your data, you can try a different encoding, such as latin1.\nThis chapter will use data from the Varities of Democracy (VDEM) dataset. VDEM is an ongoing research project to measure the level of democracy in governments around the world and updated versions of the dataset are released on an ongoing basis. The research is led by a team of over 50 social scientists who coordinate the collection and analysis of expert assessments from over 3,200 historians and Country Experts (CEs). From these assessments, the VDEM project has created a remarkably complex array of indicators designed to align with five high-level facets of democracy: electoral, liberal, participatory, deliberative, and egalitarian. The dataset extends back to 1789 and is considered the gold standard of quantitative data about global democratic developments. You can find the full codebook online, and I strongly recommend that you download it and consult it as you work with this data. You can find the full dataset at (https://www.v-dem.net/en/data/data/v-dem-dataset-v11/) and the codebook here (https://www.v-dem.net/figures/filer_public/e0/7f/e07f672b-b91e-4e98-b9a3-78f8cd4de696/v-dem_codebook_v8.pdf). The filtered and subsetted version we will use in this book can be downloaded using the download_dataset() function. Note that this will also download additional VDEM materials, including the codebook.\nLet’s load the CSV file into a Pandas dataframe.\nvdem_data_url = \"https://www.dropbox.com/scl/fo/6ay4x2qo4svyo92wbvlxt/ACtUxCDoLYxLujkekHdXiJ4?rlkey=lhmhiasjkv3ndvyxjxapi24sk&st=2p76a0dw&dl=0\"\n\ndownload_dataset(\n    vdem_data_url,\n    save_path='data/vdem'\n)\ndf = pd.read_csv(\n    'data/vdem/V-Dem-CY-Full+Others-v10.csv',\n    low_memory=False\n)\nOnce you have your data loaded, one of the first things you will want to know is how many rows and columns there are. You can do this using the .shape attribute of the dataframe.\ndf.shape\nThis is a fairly large dataset. It has 27,013 observations and 4,108 variables! First, I will construct a new dataframe from this one that contains only the columns I want.\n\n\n7.2.2 What Do You Need? Selecting Columns\nI will create a list of the variable names I want to retain, and call the original dataframe followed by the name of the list in square brackets. In this case, I will retain the following variables:\n\nthe country name,\nthe country ID,\nthe geographic region,\nthe year,\nthe polyarchy index,\nthe liberal democracy index,\nthe participatory democracy index,\nthe deliberative democracy index, and\nthe egalitarian democracy index,\nwhether Internet users’ privacy and their data is legally protected,\nhow polarized the country is on political issues, and\nlevels of political violence.\nwhether or not the country is a democracy\n\nI will call the new dataframe sdf, for ‘subsetted dataframe.’ Of course, you can call it anything you like. If you are going to be working with multiple dataframes in the same script or notebook, then it’s a good idea to give them much more descriptive names. For now, I am only working with two, so I will use df for the full dataframe and sdf for the dataframe with a subset of the original variables. I will make careful note of any dataframes I add.\nsubset_vars = ['country_name', 'country_text_id', 'e_regiongeo', 'year', 'v2x_polyarchy', 'v2x_libdem', 'v2x_partipdem', 'v2x_delibdem', 'v2x_egaldem', 'v2smprivex', 'v2smpolsoc', 'v2caviol', 'e_boix_regime']\nsdf = df[subset_vars]\nsdf.shape\nWe’ve created a new dataframe called sdf. It still has 27,013 rows, but only 13 variables. We can print their names using the .columns attribute for the dataframe.\nlist(sdf.columns)\n\n7.2.2.1 What’s in Your dataframe?\nWe can use the .info() method to see: the total number of observations, the total number of columns, the names of the columns, the number of non-missing observations for each, the datatype for each variable, the number of variables that contain data of each type (e.g. integers and floats), and the total amount of memory used by the dataframe.\nsdf.info()\nThe datatypes in this dataframe are float64 (numbers with decimals), int64 (integers), and object. In Pandas, object refers to columns that contain strings, or mixed types, such as strings and integers (object encompasses many more things, too: it’s a catchall category). Pandas can also work with booleans (True or False), categorical variables, and some specialized datetime objects. Recall how we selected columns to make our dataset. In the code below, I use the same idea to show only a few variables, rather than all 35, to save space. We will explain this a little more later in the chapter.\nWe can also use the .describe() method to get summary information about the quantitative variables in our dataset, including the number of non-missing information, the mean and standard deviation, and a five number summary:\nsdf[['e_regiongeo', 'year', 'v2x_polyarchy']].describe()\n\n\n\n7.2.3 Heads, Tails, and Samples\nWe can also inspect the “head” or the “tail” of our dataframe using the .head() and .tail() methods, which default to the first or last 5 rows in a dataframe unless you provide a different number as an argument, such as .head(10).\nsdf[['country_name', 'year', 'v2x_libdem']].head()\nsdf[['country_name', 'year', 'v2x_libdem']].tail(3)\nIf you would prefer a random sample of rows, you can use the .sample() method, which requires you to specify the number of rows you want to sample.\nsdf[['country_name', 'year', 'v2x_libdem']].sample(15)\n\n\n7.2.4 What Do You Need? Filtering Rows\nWhen we executed the .describe() method earlier, you may have noticed that the range for the year variable is 1789-2019. Let’s say we have a good reason to focus on the years from 1900-2019. We will have to filter the data to have only the rows that meet my needs.\nThere are several ways to filter rows, including slices (e.g. all observations between index \\(i\\) and index \\(j\\)), or according to some sort of explicit condition, such as “rows where the year &gt;= 1900.” Note that when we filter or slice a dataframe, the new object is just a view of the original and still refers to the same data. Pandas will warn us if we try to modify the filtered object, so a lot of the time, things are smoother if we make a new copy.\nrowfilter = sdf['year'] &gt;= 1900\nfsdf = sdf[rowfilter].copy()\nfsdf.info()\nWe could also do this using the .query() method, which accepts a boolean expression as a string.\nalternate_fsdf = sdf.query('year &gt;= 1900').copy()\nalternate_fsdf.info()\nOur final dataframe – which I have called fsdf for filtered and subsetted dataframe – now has 13 columns (from 4,108) and 18,787 observations (from 27,013).\n\n\n7.2.5 Writing Data to Disk\nJust as I read our initial CSV file into Pandas using the read_csv() function, I can write this new dataframe to disk using the write_csv() function.\nfsdf.to_csv('data/vdem_filtered_subset.csv', index=False)",
    "crumbs": [
      "**OBTAINING DATA**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>~~Processing Structured Data~~ Sampling and Survey Data</span>"
    ]
  },
  {
    "objectID": "sampling-and-survey-data.html#understanding-pandas-data-structures",
    "href": "sampling-and-survey-data.html#understanding-pandas-data-structures",
    "title": "6  Processing Structured Data Sampling and Survey Data",
    "section": "7.3 UNDERSTANDING PANDAS DATA STRUCTURES",
    "text": "7.3 UNDERSTANDING PANDAS DATA STRUCTURES\nNow let’s discuss Pandas’ main data structures, Series and DataFrames, and how they relate to one another.\n\n7.3.1 The Series\nEach column in a dataframe is an object called a Series. A Series is a one-dimensional object (e.g. a vector of numbers) with an index, which is itself a vector, or array, of labels.\nFor example, the column v2x_delibdem in fsdf is a Series containing floats and the index label for each observation. Printing a sample of 15 observations gives me a numerical index for each observation on the left and the actual value on the right. The index values are ordered in the Series itself, but they are out of sequence here because we pulled a random sample. As this is for demonstration purposes, I’ve included a random_state value to ensure you get the same sample that I do if you re-run this block.\nfsdf['v2x_delibdem'].sample(15, random_state = 42)\nIn most cases, the default index for a Series or dataframe is an immutable vector of integers:\nfsdf.index\nWe can easily modify an index so that it is made of up some other type of vector instead, including a string. Surprisingly, index values do not need to be unique. This enables some powerful techniques, but most of the time, you should avoid manually changing indexes.\n\n7.3.1.1 Accessing a Specific Row by its Index\nWe can use the index to retrieve specific rows from a dataframe or specific values from a Series, much as we would if we were selecting an element from a list, tuple, or array. The easiest way to do this is to pass the index value (e.g. 202) to .loc[]. As you can see below, the result is the observation-specific value for each variable in the dataframe.\nfsdf.loc[202]\nfsdf['v2x_delibdem'].loc[202]\nfsdf['v2x_delibdem'].loc[20000]\nNote that .loc does not refer to the 202nd row of the dataframe. If you were looking closely at the .index command above, you might have noticed the dataframe only contains 18,787 rows but .loc can still return row 20,000 - the index didn’t change when you removed a bunch of rows from the dataframe. Think of .loc as accessing a dictionary of the index values - it will even give a KeyError if you ask for an element that doesn’t exist.\nInstead, if we want the access the n-th row of a dataframe, we can use .iloc[n]. Think of the index as a list and you’re referring to an element of that list by its list index. Let’s use .iloc to select the last element in the dataframe. Note that the index position for the last element will be 18,786 even though the dataframe length is 18,787, because Python data structures are almost always 0-indexed. Here you see the index of the row, which was formerly the row number, as the Name at the bottom.\nfsdf.iloc[18786]\nIf there isn’t a reason to retain the original indexing of the unfiltered dataframe, it’s usually a good idea to reset the index.\nfsdf.reset_index(inplace = True, drop = True)\nfsdf.loc[18786]\nAfterwards, .loc and .iloc become fairly interchangeable, with a few exceptions: .loc has dictionary-like capabilities whereas .iloc is more list-like. Now, let’s take a closer look at the dataframe.\n\n\n\n7.3.2 Dataframes\nDataframes in Pandas are really just collections of Series that are aligned on the same index values. In other words, the Series we worked with previously have their own indices when we work with them as standalone Series, but in the fsdf dataframe, they share an index.\nAs you’ve already seen, dataframes are organized with variables in the columns and observations in the rows, and you can grab a single Series from a dataframe using square brakets – let’s do that now, using the fsdf dataframe:\ndeliberative = fsdf['v2x_delibdem']\nNote that we can also use dot notation to select columns. fsdf.v2x_delibdem is functionally equivalent to fsdf['v2x_delibdem'], and may be used interchangeably.\nWe are not limited to selecting columns that already exist in our dataset. You can also create and add new ones. For example, you can create a new column called “21 Century” and assign Boolean value based on whether the observation is in the 2000s.\nfsdf['21 Century'] = fsdf['year'] &gt;= 2000\nfsdf[['21 Century']].value_counts().reset_index()\nSometimes, the new columns created are transformations of a Series that already exists in the dataframe. For example, you can create a new missing_political_violence_data column which will be True when the v2caviol Series (levels of political violence) is empty and False otherwise.\nfsdf['missing_political_violence_data'] = fsdf['v2caviol'].isna()\nfsdf['missing_political_violence_data'].value_counts().reset_index()\nAs you can see from executing value_counts(), there is missing data on levels of political violence for 6042 observations.\n\n\n7.3.3 Missing Data\nIt’s important to understand how missing data is handled. Missing data is common in real-world datasets, and it can be missing for multiple reasons! Generally, Pandas uses the np.nan value to represent missing data. NumPy’s np.nan value is a special case of a floating point number representing an unrepresentable value. These kinds of values are called NaNs (Not a Number).\nimport numpy as np\n\ntype(np.nan)\nnp.nan cannot be used in equality tests, since any comparison to a np.nan value will evaluate as False. This includes comparing np.nan to itself.\nn = np.nan\nn == n\nnp.nan values do not evaluate to False or None. This can make it difficult to distinguish missing values. You can use the np.isnan() function for this purpose, and it is especially useful in control flow.\nif np.nan is None:\n    print('NaN is None')\nif np.nan:\n    print('NaN evaluates to True in control flow')\nif np.isnan(np.nan):\n    print('NaN is considered a NaN value in NumPy')\nAdditionally, np.nan values are generally excluded from Pandas functions that perform calculations over dataframes, rows, or columns. For example, documentation often stipulates that a calculation is done over all values, excluding NaN or NULL values.\ntotal = len(fsdf['v2caviol'])\ncount = fsdf['v2caviol'].count()\nprint(f'Total: {total}')\nprint(f'Count: {count}')\nprint(f'Diff: {total-count}')\nThe total number of items in the v2caviol column (political violence) is much higher than the counts received from the count() function. If what we learned above is correct, this difference should be accounted for when we discover how many items in this column are NaNs.\nnans = fsdf['v2caviol'].isna().sum()\nprint(' NaNs: {}'.format(nans))\nAs you can probably tell, the .isna() method, which is similar to np.isnan() but covers additional cases, can be very useful in transforming and filtering data.",
    "crumbs": [
      "**OBTAINING DATA**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>~~Processing Structured Data~~ Sampling and Survey Data</span>"
    ]
  },
  {
    "objectID": "sampling-and-survey-data.html#aggregation-grouped-operations",
    "href": "sampling-and-survey-data.html#aggregation-grouped-operations",
    "title": "6  Processing Structured Data Sampling and Survey Data",
    "section": "7.4 AGGREGATION & GROUPED OPERATIONS",
    "text": "7.4 AGGREGATION & GROUPED OPERATIONS\nData analysis projects often involve aggregation or grouped operations. For example, we might want to compute and compare summary statistics for observations that take different values on a categorical variable. It can be helpful to be able to carve up the dataset itself, performing operations on different subsets of data. We’re going to do that using the .groupby() method, which partitions the dataframe into groups based on the values of a given variable. We can then perform operations on the resulting groups. Let’s group our countries into geographic regions using the e_regiongeo variable.\ngrouped = fsdf.groupby('e_regiongeo')\nThe above code returns a grouped object that we can work with. Let’s say we want to pull out a specific group, like South East Asia, which is represented in the data using the numerical ID 13. I know this because the relevant information is provided in the VDEM codebook, which I suggest you keep open whenever you are working with the VDEM data.\nWe can use the get_group() method to pull a group from the grouped object. (Note that the .get_group() code below is equivalent to fsdf[fsdf['e_regiongeo'] == 13].)\nsouth_east_asia = grouped.get_group(13)\nsouth_east_asia[['country_name', 'year', 'e_boix_regime']].head()\nThe data stored in south_east_asia are all of the observations of South East Asian countries in the VDEM data, stored now in their own dataframe. .get_group() is yet another way to extract a subset of a dataframe (by way of a groupby object), and is especially useful when the subset of data you want to work with is only observations with a particular value for a categorical variable in your data.\nGenerally speaking, when we group a dataset like this it’s because we want to compute something for a group within the dataset, or for multiple groups that we want to compare. We can do this by specifying the grouped object, the Series we want to perform an operation on, and finally the operation we want to perform. For example, let’s compute the median polyarchy score for countries in each of the regions in the dataset.\npoly = grouped['v2x_polyarchy'].median()\npoly.head()\nIt would be more useful to see the name of the region rather than its numeric label. We can do this by creating a dictionary that maps the numeric IDs to the region name, and then use the .map() method to tell Pandas were to lookup the values it needs to create a new column with the country names. First, the dictionary:\nregions = {\n    1:'Western Europe',\n    2:'Northern Europe',\n    3:'Southern Europe',\n    4:'Eastern Europe',\n    5:'Northern Africa',\n    6:'Western Africa',\n    7:'Middle Africa',\n    8:'Eastern Africa',\n    9:'Southern Africa',\n    10:'Western Asia',\n    11:'Central Asia',\n    12:'East Asia',\n    13:'South-East Asia',\n    14:'South Asia',\n    15:'Oceania', # (including Australia and the Pacific)\n    16:'North America',\n    17:'Central America',\n    18:'South America',\n    19:'Caribbean' # (including Belize Cuba Haiti Dominican Republic)\n}\nAnd now we can pass this dictionary into the .map() method applied to the fsdf['e_regiongeo'] Series, creating a new Series called fsdf['Region']\nfsdf['Region'] = fsdf['e_regiongeo'].map(regions)\nIt is also possible to group by multiple variables, such as geographic region and year, and then perform an operation on those slightly more fine-grained groups. This will result in 2,211 groups, so we will preview a random sample of 10.\ngrouped = fsdf.groupby(['Region', 'year'])\npoly = grouped['v2x_polyarchy'].median()\npoly.reset_index()\npd.DataFrame(poly).reset_index().sample(10)\nWe can perform other types of operations on the grouped object itself, such as computing the number of observations in each group (equivalent to value_counts()).\ngrouped.size().sort_values(ascending=False)\nFinally, we can perform multiple operations on a grouped object by using the agg() method. The agg() method will apply one or more aggregate functions to a grouped object, returning the results of each.\n#| warning: false\nwith_agg = grouped['v2x_polyarchy'].agg([min, np.median, 'max', 'count'])\nwith_agg.reset_index().sample(10)\nWe can even define our own function for agg() to use! If we’re willing to pass a dictionary, .agg() also lets us apply different functions to multiple variables at the same time! Instead of passing one list per function, you can use a dictionary where the column names are the keys and the functions are the values (you can also pass a list of functions) to perform some truly involved aggregration all in one line of code.",
    "crumbs": [
      "**OBTAINING DATA**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>~~Processing Structured Data~~ Sampling and Survey Data</span>"
    ]
  },
  {
    "objectID": "sampling-and-survey-data.html#working-with-time-series-data",
    "href": "sampling-and-survey-data.html#working-with-time-series-data",
    "title": "6  Processing Structured Data Sampling and Survey Data",
    "section": "7.5 WORKING WITH TIME SERIES DATA",
    "text": "7.5 WORKING WITH TIME SERIES DATA\nMany real world datasets include a temporal component. This is especially true if you are working with data that comes from the web, which may have precise timestamps for things like the time an email was sent, or a news story was published. Strings are often used to store dates and times, but this is not ideal because strings don’t take advantage of the unique properties of time. It is difficult to sort dates if they are stored in strings with strange formats, for example.\n\"Monday Mar 2, 1999\" &gt; \"Friday Feb 21, 2020\"\nExtracting features like day, month, or timezone from strings can be time-consuming an error-prone. This is why Pandas and Python have implemented special types for date/time objects, called [Timestamp](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html) and [Datetime](https://docs.python.org/2/library/datetime.html), respectively. These are essentially equivalent to one another.\nThe VDEM data contains an enormous amount of temporal data, but all at the level of the year. Let’s switch over to a different dataset that has more fine-grained temporal data, and more closely resembles data that you would obtain from the web. In this case, we are going to use some data on Russian information operations targeting the 2016 American Presidential Election. You can read a bit about this data on the FiveThirtyEight blogpost Why We’re Sharing 3 Million Russian Troll Tweets.\nUnlike the VDEM data, the Russian Troll Tweets come as a collection of csv files. We will use a clever little trick to load up all the data in a single dataframe. The code block below iterates over each file in the russian-troll-tweets/ subdirectory in the data directory. If the file extension is csv, is reads the csv into memory as a dataframe. All of the dataframes are then concatenated into a single dataframe containing data on ~ 3M tweets.\nrussian_troll_data_url = \"https://www.dropbox.com/scl/fo/a3uxioa2wd7k8x8nas0iy/AH5qjXAZvtFpZeIID0sZ1xA?rlkey=p1471igxmzgyu3lg2x93b3r1y&st=xvhtn8gi&dl=0\"\n\ndownload_dataset(\n    russian_troll_data_url,\n    save_path='data/russian_troll_tweets'\n)\ndata_dir = os.listdir(\"data/russian_troll_tweets/\")\n\nfiles = [f for f in data_dir if 'csv' in f]\n\ntweets_df = pd.concat((pd.read_csv(\n    f'{\"data/russian_troll_tweets/\"}/{f}',\n    encoding='utf-8', low_memory=False) for f in files), ignore_index=True)\n\ntweets_df.info()\nAs you can see, we have two datatypes in our dataframe: object and int64. Remember that Pandas uses object to refer to columns that contain strings, or which contain mixed types, such as strings and integers. In this case, they refer to strings.\nOne further thing to note about this dataset: each row is a tweet from a specific account, but some of the variables describe attributes of the tweeting accounts, not of the tweet itself. For example, followers describes the number of followers that the account had at the time it sent the tweet. This makes sense, because tweets don’t have followers, but accounts do. We need to keep this in mind when working with this dataset.\nWe can convert date strings from a column or Series into Timestamps using the to_datetime function. We will do that here, assigning the new datetime objects to new variables. Note that this code will take a bit of time to run when executed on all 3 million tweets (if your computer isn’t the strongest, you might want to consider first using tweets_df.sample() to reduce the size of the dataframe).\ntweets_df['dt_publish_date'] = pd.to_datetime(tweets_df['publish_date'])\ntweets_df['dt_harvested_date'] = pd.to_datetime(tweets_df['harvested_date'])\ntweets_df[['author', 'content', 'publish_date']].sample(5)\nIn order, the datetime object fields are as follows: year-month-day hour:minute:second:microsecond. To retrieve an integer corresponding to the month when the tweet was published:\ntweets_df['dt_publish_date'].dt.month\nWhen our date and time variables are stored as datetime objects, we can access many time-specific attributes using dot notation. The Pandas documentation includes many examples of the kinds of temporal units and other functionality.\nWe can also sort our dataframe based on publish_date because Pandas knows that it is working with datetime objects.\nsorted_df = tweets_df.sort_values(['dt_publish_date'])\nWe can also add and subtract datetime columns to create new columns.\ntweets_df['days_until_harvest'] = tweets_df['dt_harvested_date'] - tweets_df['dt_publish_date']\ntweets_df['days_until_harvest'].sample(10)\nLet’s create new variables for the Year, Month, and Day each tweet was created on. We can do this by using the year, month, and day attributes on the datetime object.\ntweets_df['Year'] = tweets_df['dt_publish_date'].dt.year\ntweets_df['Month'] = tweets_df['dt_publish_date'].dt.month\ntweets_df['Day'] = tweets_df['dt_publish_date'].dt.day\nPandas offers specialized tools for grouping data into various segments of time. This involves converting a time series at one level into another (e.g. from days to weeks), and is known as resampling. Within resampling broadly, upsampling aggregates dates / times and downsampling disaggregates dates / times. Let’s upsample our data to plot the number of Tweets per day.\nThe first thing we will do is use the datetime object dt_publish_date as an index. This will let us easily group observations by resampling dates.\ntweets_df = tweets_df.set_index('dt_publish_date')\nWe can now use the .resample() method with the argument D to specify that we want to group by day. The table below provides some other options you can use when resampling dates.\n\nUnits of time in Pandas. You can use any of these units to upsample or downsample temporal data.\n\n\nValue\nDescription\n\n\n\n\nB\nbusiness day frequency\n\n\nC\ncustom business day frequency (experimental)\n\n\nD\ncalendar day frequency\n\n\nW\nweekly frequency\n\n\nM\nmonth end frequency\n\n\nBM\nbusiness month end frequency\n\n\nCBM\ncustom business month end frequency\n\n\nMS\nmonth start frequency\n\n\nBMS\nbusiness month start frequency\n\n\nCBMS\ncustom business month start frequency\n\n\nQ\nquarter end frequency\n\n\nBQ\nbusiness quarter endfrequency\n\n\nQS\nquarter start frequency\n\n\nBQS\nbusiness quarter start frequency\n\n\nA\nyear end frequency\n\n\nBA\nbusiness year end frequency\n\n\nAS\nyear start frequency\n\n\nBAS\nbusiness year start frequency\n\n\nBH\nbusiness hour frequency\n\n\nH\nhourly frequency\n\n\nT\nminutely frequency\n\n\nS\nsecondly frequency\n\n\nL\nmilliseonds\n\n\nU\nmicroseconds\n\n\nN\nnanosecondsa\n\n\n\nWe will also use the .size() method to determine the number of tweets that were produced each day.\ngrouped_cal_day = tweets_df.resample('D').size()\ngrouped_cal_day\nAt this point, we are going to visualize the results of our work with a line plot. We are going to do this with the Seaborn and matplotlib packages, which we will discuss in the next chapter. For now, focus on the visualization and ignore the code. The code blocks below produces Figures Figure 8.1 and Figure 8.2.\n#| output: false\nsns.lineplot(data=grouped_cal_day, color='#32363A')\nsns.despine()\nplt.savefig('figures/06_01.png', dpi=300)\n\n\n\n\n\n\nFigure 7.1: caption…\n\n\n\nDays may not be the best unit of time to work with in this case. We can, of course, upsample from days to weeks instead, and produce the same plot.\nweekly = tweets_df.resample('W').size()\nweekly.head()\n#| output: false\nax = sns.lineplot(data=weekly, color='#32363A')\nax.set_xlabel('\\nWeekly observations')\nax.set_ylabel('Number of Tweets\\n')\nsns.despine()\nplt.savefig('figures/06_02.png', dpi=300)\n\n\n\n\n\n\nFigure 7.2: caption…\n\n\n\nThe plot is much cleaner when we count at the level of weeks rather than days.",
    "crumbs": [
      "**OBTAINING DATA**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>~~Processing Structured Data~~ Sampling and Survey Data</span>"
    ]
  },
  {
    "objectID": "sampling-and-survey-data.html#combining-dataframes",
    "href": "sampling-and-survey-data.html#combining-dataframes",
    "title": "6  Processing Structured Data Sampling and Survey Data",
    "section": "7.6 COMBINING DATAFRAMES",
    "text": "7.6 COMBINING DATAFRAMES\nCombining dataframes is a very common task. In fact, though it might not seem obvious, combining datasets is one of the most valuable skills you can have when doing computational social science. Here, we will consider some of the most common approaches: concatenating and merging, and we will briefly describe a more advanced set of methods commonly referred to as record linkage.\nConcatenating a dataframe is conceptually pretty simple - think of it like attaching the rows or columns of one dataframe below/to the right of the last row/column of another dataframe. For this to be useful, the two dataframes should have at least one row or column in common, but usually you would only concatenate if there were many such overlapping entries.\nfull_df =  pd.read_csv(\"data/vdem/filtered_subset.csv\")\ndf_australia = full_df.query(\"country_name == 'Australia'\")\nlen(df_australia)\ndf_sa = full_df.query(\"country_name == 'South Africa'\")\nlen(df_sa)\nThe default behaviour for pd.concat() is to perform a row-wise join, which it refers to as axis=0. We can override this default by specifying axis=1, which will produce a column-wise join:\nconcatenated = pd.concat([df_australia, df_sa], axis=1)\nlen(concatenated)\nWhen we concatenate the two dataframes the number of columns stays the same but the number of rows increases, accounting for the rows in both the original dataframes. Normally, this kind of concatenation would result in a different number of columns, but in this case, the two dataframes we joined had the exact same columns (which makes sense, given that they were both extracted from the same parent dataframe).\n\n7.6.1 Merging\nAn alternative way to combine datasets is to merge them. If you want to create a dataframe that contains columns from multiple datasets but is aligned on rows according to some column (or set of columns), you probably want to use the merge() function. To illustrate this, we will work with data from two different sources. The first is the VDEM data we used in first part of this chapter (fsdf). The second is a dataset from Freedom House on levels of internet freedom in 65 countries. More information is available at https://freedomhouse.org/countries/freedom-net/scores.\ndownload_dataset(\n    'https://www.dropbox.com/scl/fo/fnw5yrslxza9plhqnqhxr/AA7997oGIdd3k3EjluHyLBc?rlkey=hr93qtcdp6uh7d3lsfzbc6nr6&st=bz0xzw41&dl=0',\n    'data/freedom_house/'\n)\n\nfreedom_df = pd.read_csv( \"data/freedom_house/internet_freedoms_2020.csv\")\nTo merge these dataframes we need to find a column which can be used to match rows from one dataframe to the rows in the other. The columns don’t need to have the same name, just values that can be matched with one another. Whatever columns we choose will be called “keys” in our merge. In our case this will be the country name columns from each dataset.\nfsdf.columns\nfreedom_df.columns\nWe will use the merge function to combine these two dataframes using ‘country_name’ and ‘Country’. We’re going to do an inner merge, which is the default if the option isn’t set, and will keep only the keys (ie. countries) that appear in both dataframes.\nmerged = pd.merge(fsdf, freedom_df, how='inner', left_on='country_name', right_on='Country')\nprint('merged has {} rows and {} columns'.format(len(merged), len(merged.columns)))\nlen(fsdf) + len(freedom_df)\nYou should see 5 new columns in the merged dataframe compared to the fsdf one. Notice how many rows each of the dataframes have: many fewer rows than the original VDEM dataframe but many more than the Freedom House dataframe. So in our case, if a row’s country doesn’t appear in the other dataset, that row will not be included in the merged dataframe.\nThis can be adjusted using the how parameter. There are five ways of merging dataframes in Pandas: left, right, outer, inner, and cross. Check out the documentation to see how the other four methods work.\nThere are ways to improve the matching, either manual methods or semi-automated methods such as record linkage, described below. Let’s see which countries aren’t common between the dataframes, using a set operation ^ (XOR), which returns a set of elements from the combination of set1 and set2 that are either not in set1 or not in set2.\nfsdf_set = set(fsdf['country_name'].tolist())\nfreedom_set = set(freedom_df['Country'].tolist())\n\nunmatched = fsdf_set ^ freedom_set\n\nprint('Total countries: ' + str(len(fsdf_set) + len(freedom_set)))\nprint('Unmatched countries: ' + str(len(unmatched)))\nWe can then use the & set operator to see which of the missing countries are present in each of the country sets. If the data is small enough, we can print the two sets as sorted lists in a dataframe. The most obvious manual change we could do here is make “United States” and “United States of America” consistent but we would also expect Myanmar to be in the VDEM data. We could also make this change manually by knowing that Myanmar was referred to as Burma until 1989. However, it just so happens that at the top of the south_east_asia aggregated group dataframe from earlier, “Burma/Myanmar” was the name used, rather than Burma alone. For a more complex but automated solution to disambiguating different versions of country names, we would have to use some form of record linkage, discussed briefly below.\nfsdf_missing = list(fsdf_set & unmatched)\nfsdf_missing.sort()\nfreedom_missing = list(freedom_set & unmatched)\nfreedom_missing.sort()\npd.DataFrame({'VDEM': pd.Series(fsdf_missing), 'Freedom': pd.Series(freedom_missing)})\n\n\n7.6.2 Record Linkage\nThe merge function works great when you can make exact matches between columns. It also works really well because checking for exact matches has been optimized in Pandas. However, it’s often the case that we need to combine datasets which cannot be merged based on exact matches.\nInstead, we often have to use inexact matching (aka “fuzzy matching” or “approximate matching”) to combine datasets. Typically, this involves using some similarity metric to measure how close two keys are to one another. Then a match is made based on thresholds, rules, or a nearest-neighbour approach. However, naively calculating similarity between all possible key combinations results in incredibly lengthy compute times. Instead, there are ways to exclude some key pairs from the beginning. This allows you to drastically reduce the number of comparisons you need to make. Additionally, inexact matching can leverage machine learning techniques which uses human curated examples to learn to predict whether two rows should be matched with one another.\nIf this “more advanced” approach to combining datasets is of interest, I highly suggest looking into the recordlinkage Python package.\n\nFurther Reading\nMuch of what I introduce in this chapter is foundational; you’ll build on that foundation in later chapters. But if you are looking for a slower and more comprehensive introduction to Pandas and Numpy, then I would recommend VanderPlas’ (2016) Python Data Science Handbook.",
    "crumbs": [
      "**OBTAINING DATA**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>~~Processing Structured Data~~ Sampling and Survey Data</span>"
    ]
  },
  {
    "objectID": "sampling-and-survey-data.html#conclusion",
    "href": "sampling-and-survey-data.html#conclusion",
    "title": "6  Processing Structured Data Sampling and Survey Data",
    "section": "7.7 CONCLUSION",
    "text": "7.7 CONCLUSION",
    "crumbs": [
      "**OBTAINING DATA**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>~~Processing Structured Data~~ Sampling and Survey Data</span>"
    ]
  },
  {
    "objectID": "sampling-and-survey-data.html#key-points",
    "href": "sampling-and-survey-data.html#key-points",
    "title": "6  Processing Structured Data Sampling and Survey Data",
    "section": "7.8 Key Points",
    "text": "7.8 Key Points\n\nIn this chapter, we expanded into the world of processing structured data using Pandas; these are critical skills for computational social scientists\nWe covered the basic Pandas data structures, Series and dataframes, and the index and datetime objects\nWe discussed how to subset dataframes by selecting columns and filtering rows, followed by a discussion of how to do systematic comparisons by performing operations on grouped dataframes\nWe then discussed how to combine multiple dataframes using merge and concatenate and introduced the general idea of record linkage.",
    "crumbs": [
      "**OBTAINING DATA**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>~~Processing Structured Data~~ Sampling and Survey Data</span>"
    ]
  },
  {
    "objectID": "web-data-apis.html#application-programming-interfaces-apis",
    "href": "web-data-apis.html#application-programming-interfaces-apis",
    "title": "7  Web data (APIs)",
    "section": "",
    "text": "IP Addresses and Domain Names: Every device on the internet has a unique IP address. We use human-readable domain names (e.g., nytimes.com) which are translated to IP addresses by DNS servers.\nHTTP and HTTPS: These protocols govern how data is transferred between clients (like your computer) and servers. They use methods like GET, POST, PUT, and DELETE to request or send data.\nREST APIs: These are interfaces that allow programs to interact with web services over HTTP/HTTPS. They abstract away many network complexities, letting you focus on data exchange.\nClient-Server Model: Your program (the client) sends requests to a server, which processes them and sends back responses.\nData Packets: Information is broken into small packets for transmission, then reassembled at the destination.\n\n\n\n7.1.1 Making Requests with Python\nWe have just learned, at a relatively high-level, what happens when your computer (the “client”) makes a request for a webpage (the “resource”) from a server. In a similar fashion, you can write programs that make GET requests for resources from a server. These resources can be just about anything, but as researchers, we usually want to retrieve some sort of data or metadata from a web server. To do so, we send our requests to a specific “endpoint.”\nAn endpoint is a location on the Internet where you send a request for a specific resource. Recall that “resource” is an abstract term for just about anything that we want to retrieve from a web server. If you are working with a Twitter API, for example, you will see that there are endpoints for requesting data about tweets and an endpoint for requesting data about users. If you are working with the API for The Guardian or another news organization, you will see that there is an endpoint for “content,” among other things. To access the resources we want, we have to send our request to the correct endpoint. In nearly all cases, this will be in the form of a URL.\nThe URLs that we use to send requests to endpoints typically include several important pieces of information that enable us to specify what we want the API to return. For example, the URL may include information about our query, as well as some optional parameters and filters. Usually, this information is combined into a single URL.\nIf web servers receive too many requests at once – whether intentionally, in the form of a coordinated attack, or unintentionally, in the form of a poorly written or inconsiderate script – they can be overwhelmed. To prevent this from happening, most APIs use rate limiting to restrict the number of requests that a user can make within specific time frames. Ss of June 2020, The Guardian content endpoint limits users to making 12 requests per second and a maximum of 5,000 calls per day. Other APIs will have their own restrictions, which you can learn about in the API Terms of Service. Most of the time, APIs enforce rate limits by detecting overuse and then disconnecting or throttling users, while others use an honour system but heavily penalize users who are found to be violating those limits, such as blacklisting the user.\n\n\n7.1.2 API Keys, Tokens\nTo make requests to an API you will usually need an API Key, or API Token. You can think of these as a username and password that identify you to the API. Unlike usernames and passwords, you don’t set them yourself. Instead, the API provides them to you when you sign up for their service and agree to their Terms of Service. We will see several examples of obtaining and working with different API tokens in the examples that follow.\nIt is crucial that you do not share your API tokens. Do not write them directly into your scripts, or inadvertently commit them to a public repository (e.g., on GitLab or GitHub). Doing so is comparable to sharing usernames and passwords you use for any other web service and any misuse will be tied to you as the user. If someone obtained your API keys for The Guardian, they could access the service as if they were you, and you will suffer the consequences. If you believe your access token has been compromised for some reason, find an appropriate point of contact and inform them. They have an interest in ensuring that no one is using compromised tokens. Later in the chapter, we will learn how to use your keys while maintaining a high level of security.\n\n\n7.1.3 Responses\nWhen we make a GET request to an API, we get a response in return. These responses have numerical codes, such as the familiar 404 (Page Not Found) error you get when you follow a dead link. There are many possible response codes, most of which you don’t see when you’re just browsing the web. For example, if your request was successful, you will get a 200 (OK) response code. On the other hand, if there was some sort of problem with your request, you will likely get a response code such as 401 (unauthorized), 403 (forbidden), 500 (internal server error), or 503 (the server is unavailable). When you are developing scripts to programmatically collect data from APIs, it is always a good idea to check the status of any given request. Because there are many possible responses for errors, it is better to check for success (i.e. 200) than failure.\nTechnically, web-based APIs can return anything, but by far the most common way of providing data is json, which stands for JavaScript Object Notation. json is a nested data structure that looks a lot like a Python dictionary in that the data is stored using key-value pairs inside curly braces. For this reason, working with json is relatively painless in Python. If you import the standard library json, you can easily read and write json files, and when loaded in memory you can use dictionary methods to work with that data. Additionally, it is possible to use the Pandas package (discussed in later chapters) to read json directly into a dataframe using the .read_json() method. We will see many examples of working with json throughout the rest of this chapter.",
    "crumbs": [
      "**OBTAINING DATA**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Web data (APIs)</span>"
    ]
  },
  {
    "objectID": "web-data-apis.html#working-with-apis",
    "href": "web-data-apis.html#working-with-apis",
    "title": "7  Web data (APIs)",
    "section": "7.2 Working with APIs",
    "text": "7.2 Working with APIs\nNow that we have a conceptual understanding of what APIs are and how they work in general, we can get a bit more practical. In the rest of this chapter, we make these abstract concepts more concrete by comparing several practical examples of collecting data programmatically via web-based APIs. Remember, these examples are not intended to be comprehensive recipes for working with specific APIs. Instead, the idea is that you will deepen your general understanding of APIs by comparing multiple examples. When doing your own research, you will likely want to use some feature of an API that I haven’t covered, but you will have the foundational knowledge and understanding required to understand the documentation and solve the problem yourself.\n\n7.2.1 Setup\nimport requests\nimport json\n\nimport pandas as pd\nimport dcss.youtube as yt\nimport dcss.cleaners as clean\nimport dcss.utils as utils\n\n\n7.2.2 The Guardian\nMany major newspapers provide access to non-trivial amounts of data on their published articles via APIs and The Guardian is no exception. As of January 2021, it offers five endpoints:\n\nThe content endpoint provides the text and metadata for published articles. It is possible to query and filter results. This endpoint is likely the most useful for researchers.\nThe tags endpoint provides API tags for greater than 50,000 results, which can be used in other API queries.\nThe sections endpoint provides information on groupings of published articles into sections.\nThe editions endpoint provides content for the each of regional main pages: US, UK, Australia, and International.\nThe single items endpoint returns data for specific items, including content, tags, and sections.\n\nOften, the easiest way to work with an API is to use a “client.” Python clients for The Guardian API or other APIs are no different than any other Python package: they provide functions that abstract away some of the complexities of authenticating with, making requests to, and processing results from the API. You may want to use clients from time to time, such as when working with large and relatively complex APIs. Here, however, we will work directly with The Guardian API using a package called requests. This affords a bit more flexibility and freedom in how we interface with the API, and will help make some of the previously introduced concepts more concrete.\n\n7.2.2.0.1 Accessing The Guardian API\nAs with most other APIs, you need to register for an API key to access The Guardian API. This key enables them to monitor your access to their data and ensure you are following their terms of service. Once you have your API key, you can make 12 calls per second and up to 5,000 calls per day. You can access the article text (but not images, audio, or video) for millions of articles for free. As with many other APIs, it’s possible to unlock more content by paying for a commercial license.\nYou can obtain your API keys by registering on The Guardian’s website. The process is outlined on their developer page. We won’t review all the steps here, as they can easily change and result in confusion. However, the process is straightforward and well-explained.\nIn this case, your API key will be a single alphanumeric string. To store and use this key securely, open a new text file with the following one liner:\nGUARDIAN_KEY = 'paste_your_key_here'\nSave this file with the name .env and store it in the same directory as whatever notebook or script will contain the code you write to query the API. If you are using git for version control, don’t forget to add .env to your .gitignore file. Once you have saved .env, you can load your keys into your script and authenticate with the API.\nKEY_NAMES = ['GUARDIAN_KEY']\nAPI_KEYS = utils.load_api_key_list(KEY_NAMES)\nGUARDIAN_KEY = API_KEYS[0]\nWe are now ready to make requests to the API.\n\n\n7.2.2.0.2 Making Requests\nWe’ll use a package called requests to make our API requests. Once the package has been imported, we can do this by providing the .get() method with the base API url for the content endpoint. We will also create a dictionary called PARAMS, which will contain a key-value pair for our API key. Later, we will add more key-value pairs to this dictionary to change what the API returns.\nThe actual call to the API is made in line 7 of the code block below, where requests authenticates us with The Guardian’s servers by sending a GET request to the API with our API key. The API returns a response, including some json data that we store in the variable response_dict.\nAPI_ENDPOINT = 'http://content.guardianapis.com/search'\nPARAMS = {'api-key': GUARDIAN_KEY}\n\nresponse = requests.get(API_ENDPOINT, params=PARAMS)\nresponse_dict = response.json()['response']\nprint(len(response_dict))\nIf you print the response_dict, you will see there is quite a lot of information included here and we haven’t even provided any specific search criteria! Why is that?\nBy default, The Guardian is returning a sample of current news stories. Let’s start by digging into the fields contained in this response. Remember, since json is essentially identical to a Python dictionary, it’s natural to store it as a dictionary. We can get a list of the available fields by using the .keys() method.\nprint(response_dict.keys())\nThe most useful data is contained in the results field, which you can access with response_dict['results']. This is where the actual article context is stored.\nIf you look at the contents of response_dict['results'], you will find a list of 10 dictionaries, each corresponding to one of the 10 retrieved stories, which is the default number of stories returned. Each story has several key value pairs containing useful article metadata such as an ID, a type, section IDs, publication date, the title, the URL for the story and for further API access, and so on. The actual content of the publications is not included, though; we will learn how to retrieve it shortly.\n\n\n7.2.2.0.3 Filtering Results\nEarlier, I mentioned that we can use queries and filters to retrieve specific types of content from an API. You can use queries to find content just as you would if you were using a search engine and you can use filters to narrow the returned content on the basis of specific metadata. The API documentation provides information on what kinds of filters are available. For example, in the code block below, we can use filters to specify:\n\na specific date or range of dates when the articles were last published\nthe language\nthe production office\na term to search for\n\nPARAMS = {\n    'api-key': GUARDIAN_KEY,\n    'from-date': '2020-04-10',\n    'to-date': '2020-04-10',\n    'lang': 'en',\n    'production-office': 'uk',\n    'q': 'coronavirus'\n}\n\nresponse = requests.get(API_ENDPOINT, params=PARAMS)\nresponse_dict = response.json()['response']\nNotice that the resulting response_dict contains more information than our last set of results.\nprint(response_dict.keys())\nThere are several new fields here, but still no article content or bylines. To retrieve this and other data, we can specify it using the show-fields parameter. Let’s add it to our search.\nPARAMS = {\n    'api-key': GUARDIAN_KEY,\n    'from-date': '2020-04-10',\n    'to-date': '2020-04-10',\n    'lang': 'en',\n    'production-office': 'uk',\n    'q': 'coronavirus',\n    'show-fields': 'wordcount,body,byline'\n}\n\nresponse = requests.get(API_ENDPOINT, params=PARAMS)\nresponse_dict = response.json()['response']\nNow, when you print the content of response_dict, you will see we have the additional data we were looking for. I won’t print all of that here, but you can by executing the following code:\nfor response in response_dict['results']:\n    print(response['fields']['body'])\nNote that the text itself contains HTML tags – we will discuss these in the next chapter.\n\n\n7.2.2.0.4 Asking for More Data\nWe’ve now seen how to get useful data from article publications that meet our search criteria, but we still only have that data for 10 stories. To get more, we need to dig into some additional API concepts. Three of the keys of our response_dict describe the volume of data we receive: total, pages, and pageSize. They all work together, in a fashion similar to the results that would be returned by a search engine. Total is a count of the total number of stories available. These results can be broken up into multiple pages, again, like results returned from a search engine. The number of stories included in any given page is determined by the pageSize. If we want the API to return all available stories, we need to change these parameters when we make our request. We can do this by specifying how many stories should be returned in a single page, and then request stories from multiple pages, much as we might click to navigate to the second, third, nth page of results in a search.\nBelow, we update our search parameters with a new parameter specifying pageSize. We will increase it from 10 to 50.\nPARAMS = {\n    'api-key': GUARDIAN_KEY,\n    'from-date': '2020-04-10',\n    'to-date': '2020-04-10',\n    'lang': 'en',\n    'production-office': 'uk',\n    'q': 'coronavirus',\n    'show-fields': 'wordcount,body,byline',\n    'page-size': 50,\n}\n\nresponse = requests.get(API_ENDPOINT, params=PARAMS)\nresponse_dict = response.json()['response']\nIncreasing the number of stories on any given page is not actually necessary to obtain all of the data we want, since we can simply request more pages. However, we have to make a new API request for each page, which increases the load on The Guardian servers. Instead, we reduce the number of calls we need to make by increasing the amount of data returned in each individual call. You could probably get away with increasing this number, but there are a couple good reasons why you might want to keep it at a modest setting. First, many APIs have rate limits or maximum thresholds above which they’ll refuse to return any data; haphazardly increasing the amount of data you ask for in a single request might run you afowl of these limits. Second, it’s simply more considerate! Other people and organizations are likely trying to use the same API, and the API itself only has so much bandwidth (both literally and figuratively); just because you can push the limits doesn’t mean you should. Exercising temperence is part of being a good digital citizen.\nTo iterate through each page of results, we will use yet another parameter: page. However, unlike before, we will update this parameter dynamically, enabling us to make new requests for each page of available data until we have collected all results.\nThe dict PARAMS has been written and re-written several times now, but the most recent version contains our fully developed search, including the increased number of stories on each page. We will execute this search multiple times, each time retrieving data for a new page. Because we want to use the page parameter and to update it dynamically, we will use a while loop.\nall_results = []\ncur_page = 1\ntotal_pages = 1\n\nwhile (cur_page &lt;= total_pages) and (cur_page &lt; 10):  # with a fail safe\n    # Make a API request\n    PARAMS['page'] = cur_page\n    response = requests.get(API_ENDPOINT, params=PARAMS)\n    response_dict = response.json()['response']\n\n    # Update our master results list\n    all_results += (response_dict['results'])\n\n    # Update our loop variables\n    total_pages = response_dict['pages']\n    cur_page += 1\nlen(all_results)\nDon’t forget, we need to be very careful about rate limiting when we automate our API requests like this, to be mindful of The Guardian servers, and to prevent losing access. To ensure that you’re not over-taxing the API, consider adding in time.sleep() calls, which will have the effect of spacing out your requests. During testing, it’s also a good idea to keep your requests to an absolute minimum.\n\n\n7.2.2.0.5 Storing Your Data\nAt this point, we have learned how to use use the requests package and our API key to make GET requests to The Guardian’s API, and how to use parameters to query and filter results returned from the content endpoint. We also learned that the data we want is split up into multiple “pages” of results, and to retrieve everything, we have to make individual requests for each page. In doing so, we need to adopt an approach to automating API requests that ensures we stay within the rate limits and don’t violate any other terms of service.\nNow that we have our data, we need to write it to disk so that it is easily accessed, now or later, without making redundant calls to the API. This will also enable us to decouple the code used to collect this data from any code we develop to clean and analyze it. Remember, that kind of code separation is considered best practice in general, and is a key component of specific computational research workflows, such as the principled data processing framework.\nWe can use the json module to write this data to disk using the with open(x) as approach.\nFILE_PATH = 'data/guardian_api_results.json'\nwith open(FILE_PATH, 'w') as outfile:\n    json.dump(all_results, outfile)\nNow that we have a firm grasp on how to query data from The Guardian’s relatively simple REST API, you’re ready to move onto a more powerful and complex APIs.\n\n\n\n7.2.3 Working with the YouTube Data API\nThe YouTube API is well-documented, but it can be a challenging API to start with. The YouTube module in dcss will make things a little easier. If you are comfortable with the content I present here and want to learn more, I would encourage you to review the code I wrote for the package the dcss package.\n\n7.2.3.1 Get a YouTube API Key\nThe first thing you need to do is get yourself an API key. You can do that by following these steps, each described in more detail below.\n\nLog into / sign up for a Google Account\nGo to the Google Cloud Console website\nCreate a new project\nEnable the YouTube Data API v3\nCreate an API key for the YouTube Data API v3\nStore Your API Key Securely\nRestrict your API key\n\n(1) First, you’ll need to sign up for a Google account if you don’t already have one. (2) Next, open the Google Cloud Console webpage. You should see something like this:\n\n(3) Use the dropdown menu to the right of Google Cloud to create a New Project. If you already have a project setup, it may show the name of the current project. In my case, it shows INTRO-CSS. Give your project an informative name and leave the location field as is. Press Create.\n\n(4) Next, you’ll need to enable the YouTube Data API v3. Under “Quick Access”, click on APIs & Services and select Library. Type YouTube into the search bar and then select YouTube Data API v3. A new page will load with “1 result”. Click the button and then enable the YouTube API on the new page.\n\n(5) Now you can create an API key for the YouTube Data API v3. Select Credentials from the left-side navigation pane. When the page loads, click the Create Credentials button and select API Key.\n\nYou should see a popup that looks something like this:\n\n(6) Your API key is like a password; you should treat it as such. Copy your key, store it someplace secure,1, and then close the popup. You should see your new key listed under API Keys with an orange alert icon to the right of your key name. In my case, the newly created key is API key 2.\n\n(7) Finally, you’ll want to restrict your API Key. Click the three dots under Actions to Edit API Key. You should see something like this:\n\nUnder API Restrictions, select Restrict key and then select YouTube Data API v3 from the drop-down menu. Click Save.\n\nYour API key is now ready to use!\n\n\n7.2.3.2 Using Your API Key Securely\nLet’s add our API key(s) to the .env file created earlier. Remember that each line of the file should contain the name and the value of your API key (so just 1 line if you are using 1 API key). The name itself doesn’t matter, but it’s useful to give it a name that corresponds to the name of the project you created to get the API key. The example below is a randomly-generated fake API key assigned to the name GESIS.\nGESIS='GEzaLyB69Xh5yz3QRsdP-X8QeLMpgWuva-XmWKh'\nIf you have more than one API key (which can come in handy), make sure each key is on its own line. We can then load our API key(s) using the same process we did before.\nKEY_NAMES = [\"GESIS\", \"GESISPY\", \"INTRO_CSS\", \"INTROCSS2024\", \"YouTubeAPILecture\", \"metascience_golems\", \"McLevey\", \"MSGD\"]\n\nAPI_KEYS = utils.load_api_key_list(KEY_NAMES)\nYOUTUBE_API = yt.YouTubeAPI(API_KEYS)\n\n\n7.2.3.3 Understanding the YouTubeAPI Class\nThe YouTubeAPI class is designed to handle interactions with the YouTube API, specifically error handling and the task of using multiple API keys at once and an exponential back-off strategy to avoid rate limiting and handle errors automatically.\nAs you can see in the code block above, we initialize the YouTubeAPI class with a list of API keys. If you use the load_api_key_list() function and a config file, this is a simple process. Once you initialize it, the class:\n\nCreates a “service” object, which is the main interface to the YouTube API and allows us to send requests and receive responses. It does this using the googleapiclient.discovery.build() function and the build_service() method.\nAutomatically switches API keys if one API key hits a rate limit.\nExecutes requests using an execute_request() method. This method handles the actual sending of requests to the API and includes error handling to manage rate limits and retries. It also uses an exponential backoff strategy to avoid overwhelming the API with too many requests in a short time.\n\n\n\n7.2.3.4 Get the YouTube Channel IDs\nIn this example, we’re going to collect data from the talksatgoogle YouTube channel. To do that, we need to start by getting its YouTube channel ID. We can do this using the get_channel_id() function from the icsspy course package. This function is robust in handling different ways users might identify YouTube channels, making it easier to work with the API.\nget_channel_id() tries two methods to find the channel’s ID:\n\nCustom URL Search: It first checks if the provided channel argument is a custom URL. Many YouTube channels use custom URLs for easier access.\nUsername Search: If the custom URL search fails, it then tries to get the channel ID using the YouTube username.\n\nIf both methods fail, the function returns None.\nchannel = 'talksatgoogle'\n\nchannel_id = yt.get_channel_id(YOUTUBE_API, channel)\nprint(f'The YouTube Channel ID for {channel} is {channel_id}.')\n\n\n7.2.3.5 Use the Channel ID to Collect Video Data\nWith the channel ID in hand, we can retrieve a list of video IDs associated with the channel using the get_channel_video_ids() function, which sends a request to the YouTube API to get the channel’s uploads playlist ID and then iteratively fetches all the channel’s public video IDs.\nWe can pass the resulting lists of video IDs to the get_channel_video_data() function, which makes another API query to collect data such as the video’s title, description, statistics (like views and likes), and other metadata.\nvideo_ids = yt.get_channel_video_ids(YOUTUBE_API, channel_id)\nvideo_details = yt.get_channel_video_data(YOUTUBE_API, video_ids)\nprint(f\"Collected data on {len(video_details)} videos from {channel}.\")\n\nutils.save_json(video_details, 'data/videos.json')\nLike most modern APIs, the YouTube API returns data in JSON format. We’ll store this data by writing the JSON to disk, which will allow us to easily reload the data later without needing to re-query the YouTube API. For example, we can load the JSON data – from disk or memory – directly into a Pandas dataframe.\nvideos = pd.json_normalize(video_details)\nvideos.to_csv('data/videos.csv', index=False)\n\nvideos.info()\nNow that we have data on {python} len(video_details) videos, let’s query the YouTube API to collect data on the comments on these videos.\n\n\n7.2.3.6 Process Channel Data and Prepare to Collect Video Comments\nYou’ll likely end up running the code in this notebook several times (or more). Each time you’ll query the YouTube API, potentially re-collecting data you’ve already collected. Since collecting comments can involve a very large number of API calls, and API calls are expensive in terms of quota, we’ll do some prep work to minimize our API calls and the risk of hitting the rate limit.\nvideos[\"statistics.commentCount\"] = pd.to_numeric(\n  videos[\"statistics.commentCount\"], errors='coerce'\n)\n\nprobably_no_public_comments = videos[videos[\"statistics.commentCount\"].isna()][\"id\"].tolist()\nno_public_comments = videos[videos[\"statistics.commentCount\"] == 0][\"id\"].tolist()\nhas_public_comments = videos[videos[\"statistics.commentCount\"] &gt; 0][\"id\"].tolist()\nWe’ll check for the file data/talks_at_google_video_comments.csv, which is created a little later in this tutorial; if it already exists, we’ll get the IDs for videos we’ve already downloaded and skip their collection. If the file hasn’t been created yet (i.e., this is the first time you’re running this code), then it will collect everything.\nno_redownloading = True\n\nif no_redownloading is True:\n    try:\n        already_downloaded = pd.read_csv(\"data/comments.csv\")\n        already_downloaded = already_downloaded[\"video_id\"].unique().tolist()\n        has_public_comments = [\n            video\n            for video in has_public_comments\n            if video not in set(already_downloaded)\n        ]\n    except (FileNotFoundError, pd.errors.EmptyDataError):\n        already_downloaded = []\nWe’ll also skip the videos in “probably_no_public_comments,” but if you want to try collecting them, just uncomment the second line below.\ncollect = has_public_comments\n# collect = has_public_comments + probably_no_public_comments\n\n# set this to true the first time you run it; then false\noverwrite = True\n# overwrite = False\nWith this prep work done, we can collect comment data using the collect_comments_for_videos() function, which iterates over a list of video IDs and collects comments for each video. It starts by opening a CSV file to store the comments. If overwrite is True (see the code block above), it will create a new file; otherwise, it appends to an existing file. For each video, it calls the get_video_comments() function, which fetches the comments using the YouTube API. The comments are then written to the CSV file in real-time.\ncollect_comments_for_videos() includes error handling for cases where comments might be disabled or where rate limits are exceeded, which means the function can handle issues that come up without crashing. This makes it useful for collecting large amounts of data.\nThere are a lot of comments to collect, so this code will take a while to run. There’s a progress bar to let you know what to expect. Once it’s up and running, you’ll want to leave it for a bit and come back.\nall_comments = yt.collect_comments_for_videos(\n    YOUTUBE_API, collect, \"data/comments.csv\", overwrite=overwrite\n)\nFinally, after collecting the comments, we can load them into a dataframe and take a look. We’ll also write the data a local file so we can re-load and analyze it later.\nall_comments = pd.read_csv('data/comments.csv')\nall_comments.info()\nall_comments.head()",
    "crumbs": [
      "**OBTAINING DATA**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Web data (APIs)</span>"
    ]
  },
  {
    "objectID": "web-data-apis.html#conclusion",
    "href": "web-data-apis.html#conclusion",
    "title": "7  Web data (APIs)",
    "section": "7.3 Conclusion",
    "text": "7.3 Conclusion\nIn this chapter, you learned how to collect data programmatically from web-based APIs. You developed a mental model of how APIs work, including authentication, making requests, handling responses, and managing rate limits.\nBy working with The Guardian API and the YouTube Data API, you gained hands-on experience in:\n\nAuthenticating and Making Requests: Using API keys and the requests library to interact with APIs.\nFiltering and Paginating Results: Using parameters and loops to retrieve all relevant data.\nHandling JSON Data: Parsing JSON responses and storing data in Python data structures.\nStoring Data: Saving data to JSON and CSV files for later analysis.\nError Handling and Rate Limiting: Implementing strategies to handle API limitations gracefully.\n\nAPIs are powerful tools for data collection in computational social science. They provide structured and accessible data, enabling you to perform various analyses. However, it’s important to use them responsibly, adhering to terms of service and ethical considerations.\n\n\n7.3.1 Key Points\n\nAPIs Simplify Data Access: APIs abstract away the complexities of interacting with web services, providing a simple interface for data access.\nUnderstanding RESTful APIs: REST APIs use standard HTTP methods and are stateless, resource-based interfaces.\nSecure API Authentication: Always protect your API keys and tokens by storing them securely and not exposing them in your code.\nMaking Efficient Requests: Use parameters and filters to retrieve only the data you need, and handle pagination to access large datasets.\nHandling Responses and Errors: Check status codes, parse JSON data, and implement error handling to create robust data collection scripts.\nManaging Rate Limits: Be mindful of API rate limits and implement strategies like exponential backoff and API key rotation.\nData Storage Best Practices: Save collected data to files to avoid redundant API calls and to facilitate data analysis.\n\nIn the next chapter, we’ll explore web scraping, another method for collecting data from the web when APIs are unavailable or insufficient for your research needs. You’ll learn how to extract data from web pages using tools like BeautifulSoup and handle challenges like navigating HTML structures and handling dynamic content.",
    "crumbs": [
      "**OBTAINING DATA**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Web data (APIs)</span>"
    ]
  },
  {
    "objectID": "web-data-apis.html#footnotes",
    "href": "web-data-apis.html#footnotes",
    "title": "7  Web data (APIs)",
    "section": "",
    "text": "I recommend using Bitwarden or another password manager.↩︎",
    "crumbs": [
      "**OBTAINING DATA**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Web data (APIs)</span>"
    ]
  },
  {
    "objectID": "web-data-scraping.html",
    "href": "web-data-scraping.html",
    "title": "8  Web data (Scraping)",
    "section": "",
    "text": "8.1 AN HTML AND CSS PRIMER FOR WEB SCRAPERS\nThis chapter builds on the previous chapter’s foundations; I will begin by describing how the content on webpages is structured using HTML and CSS, and then explain how to write web scrapers that take advantage of these types of markup to programmatically extract data from web pages. As you learn this material, keep in mind that it takes far less knowledge to scrape a website than it does to develop it in the first place. A little knowledge goes a long way.\nAs someone who frequently reads and writes “documents” (news stories, blog posts, journal articles, Tweets, etc.), you are already familiar with the basics of structuring and organizing documents using headings, subheadings, and so on. This chapter, for example, has all those features and more. As humans, we parse these organizational features of documents visually.\nIf you create a document using a WYSIWYG (“what you see is what you get”) word processor like Open Office, Microsoft Word, or Google Docs, you apply different styles to parts of the text to indicate whether something is a title, a heading, a paragraph, a list, etc. HTML documents also have these organizational features but use special ‘markup’ to describe structural features of the document (HTML) as well as how things should appear (Cascading Style Sheets, or ‘CSS’).\nHTML consists of ‘elements’ (e.g. paragraphs) with opening and closing tags. You can think of these tags as containers. The tags tell your browser about the text that sits between the opening and closing tags (or “inside the container”). Here’s an example:\nIn our example above, the paragraph element opens a paragraph with &lt;p&gt; and closes it with &lt;/p&gt;. The actual text – what you see in your browser – lives between those tags. We can see examples of them on lines 7 and 9 in the HTML code above.\nThe outermost element in any HTML document is the html element. Your browser knows that anything between &lt;html&gt; and &lt;/html&gt; tags should be processed as HTML markup. Most of the time, the next element in an HTML page will be a head element. The text inside the &lt;head&gt; and &lt;/head&gt; tags will not actually be rendered by your browser. Instead, it contains metadata about the page itself. This is where the page title is contained, which is displayed on the tab in your browser.\nInside the HTML tags, you’ll also find a body element. Anything inside the &lt;body&gt; and &lt;/body&gt; tags will be displayed in the main browser window (e.g. the text of a news story). Inside the body tags, you will typically find elements for headings (e.g. &lt;h1&gt; and &lt;/h1&gt;, &lt;h2&gt; and &lt;/h2&gt;, and so on), paragraphs (&lt;p&gt; and &lt;/p&gt;), bold text (&lt;strong&gt; and &lt;/strong&gt; or &lt;b&gt; and &lt;/b&gt;), italicized text &lt;i&gt; and &lt;/i&gt; or &lt;em&gt; and &lt;/em&gt;), as well as ordered and unordered lists, tables, images, and links.\nSometimes elements include ‘attributes,’ which provide more information about the content of the text. For example, a paragraph element may specify that the text contained within its tags is American English. This information is contained inside the opening bracket. &lt;p lang=\"en-us\"&gt;American English sentence here...&lt;/p&gt;. As you will soon learn, attributes can be extremely useful when scraping the web.\nBefore moving on, it’s important to understand one final type of HTML element you’ll frequently encounter when developing web scrapers: the division tag div. This is simply a generic container that splits a website into smaller sections. Developers often use them to apply a particular style (e.g. switch to a monospaced font to display code) to some chunk of text in the HTML document, using CSS. Splitting webpages into these smaller pieces using div tags makes websites easier for developers to maintain and modify. They also make it easier for us web scrapers to drill down and grab the information we need. You’ll see this in action in the examples to follow.\nWhen scraping the web you will also encounter CSS, which I previously mentioned is used to style websites. To properly understand how CSS works, remember that the vast majority of modern websites are designed to separate content (e.g. actual words that mean things to humans) from structure and style. HTML markup tells your browser what some piece of text is (e.g. a heading, a list item, a row in a table, a paragraph) and CSS tells your browser what it should look like when rendered in your browser (e.g. what font to use for subheadings, how big to make the text, what colour to make the text, and so on). If there is no CSS, then your browser will use an extremely minimal default style to render the text in your browser. In most cases, developing a good web scraper will require a deeper understanding of HTML than CSS, so we will set aside discussion of CSS for now, but will return later when knowledge of CSS can help us develop a better scraper.\nA full inventory of HTML and CSS elements is, of course, beyond the scope of this book. The good news is that you don’t need exhaustive knowledge of either to write a good web scraper. You need to have a basic understanding of the key concepts and you need to know what the most common tags mean, but more than anything else you need to be willing to spend time investigating the source code for websites you want to scrape, attempt to solve problems creatively, and work interactively.",
    "crumbs": [
      "**OBTAINING DATA**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Web data (Scraping)</span>"
    ]
  },
  {
    "objectID": "web-data-scraping.html#an-html-and-css-primer-for-web-scrapers",
    "href": "web-data-scraping.html#an-html-and-css-primer-for-web-scrapers",
    "title": "8  Web data (Scraping)",
    "section": "",
    "text": "&lt;html&gt;\n    &lt;head&gt;\n        &lt;title&gt;This is a minimal example&lt;/title&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;h1&gt;This is a first-level heading&lt;/h1&gt;\n        &lt;p&gt;A paragraph with some &lt;emph&gt;italicized&lt;/emph&gt; text.&lt;/p&gt;\n        &lt;img src=\"image.png\" alt=\"This is an image\"&gt;\n        &lt;p&gt;A paragraph with some &lt;strong&gt;bold&lt;/strong&gt; text.&lt;/p&gt;\n        &lt;h2&gt;This is a second-level heading&lt;/h2&gt;\n        &lt;ul&gt;\n            &lt;li&gt;first list item&lt;/li&gt;\n            &lt;li&gt;second list item&lt;/li&gt;\n        &lt;/ul&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\n\n\n\n\n\n\n\n\nFurther Reading\nWith this foundational knowledge, you’ll be able fill gaps in your knowledge of HTML and CSS with web searches as you develop scrapers to collect data for a research project. Still, I recommend setting aside a bit of time to browse some basic tutorials. Better yet, spend some time browsing Jon Duckett’s (2011) beautiful resource book HTML & CSS: Design and Build Websites, which is an excellent resource for learning the basics.\n\n\n8.1.1 DEVELOPING YOUR FIRST WEB SCRAPER\nNow that you have a baseline understanding of how your computer retrieves and renders websites, and the role that HTML and CSS play in that process, we can demystify web scraping even further. First, we know that the information we want to retrieve from a website will be sent to our computer from a remote web server following a GET request. Webpages are provided as source code, which is parsed and rendered by our web browser of choice.\nBefore jumping into coding a scraper, you need to familiarize yourself with the sources you plan to gather data from – this is good advice to apply to any data you plan to use for research purposes. The best way to study the source code of a website is to use the developer tools built into browsers like Firefox, Chrome, or Brave. Here, and throughout the rest of this book, I will use Firefox, but the developer tools we use here are available in the other browsers as well. We’ll start by learning how to study the source code of a webpage we want to scrape, and then move into writing the actual code for the scraper.\n\n8.1.1.1 Studying Website Source Code with Developer Tools\nFirst, navigate to a website in Firefox. I’ve selected a story from the front page of The Guardian on August 2nd 2019, “Charming but dishonest and duplicitous: Europe’s verdict on Boris Johnson.” The specific story you select doesn’t really matter, however. If you are writing and executing code along with me as you read this book, and for some reason this story is no longer available, you could select another story from The Guardian instead and follow along just fine.\nOnce the story is loaded in Firefox, I can right-click anywhere on the page and select “Inspect Element” to open a pane of developer tools. (You can also open this pane by selecting “Toggle Tools” from the “Web Developer” section of the “Tools” menu in the toolbar.) We can use these tools to study our target webpage interactively, viewing the rendered content and the raw source code of the webpage simultaneously.\nOne especially useful strategy is to highlight some information of interest on the webpage and then right-click and select ‘Inspect Source’ (or ‘View Selection Source’ or ‘Inspect Element’ or similar). This will jump to that specific highlighted information in the HTML code, making it much easier to quickly find what tags the information you need is stored in. From here, we can strategize how best to retrieve the data we want.\nFigure 8.1 is a screenshot of the developer tools pane open in a Firefox tab for the story about Boris Johnson. Unfortunately it might be a bit difficult to fully read on this printed page. If that’s the case, you can consult the high resolution screenshot available in the online materials. The text “As the Brexit deadline looms, Europe remains wary of the poker player behind the clown mask” is highlighted and revealed in the inspector tool at the bottom of the page. We can see in the developer tools that the text we’ve highlighted is enclosed in simple paragraph tags (&lt;p&gt;As the Brexit deadline looms, Europe remains wary of the poker player behind the clown mask&lt;/p&gt;). That’s precisely the kind of information we can exploit when developing a web scraper. Let’s see how exactly it’s done.\n\n\n\n\n\n\nFigure 8.1: A screenshot of the Firefox developer tools pane open for the story “Charming but dishonest: Europe’s verdict on Boris Johnson,” published in The Guardian on August 2, 2019.\n\n\n\nAs we develop our web scraper, we progressively narrow down to the information we need, clean it by stripping out unwanted information (e.g. white spaces, new line characters), and then write it to some sort of dataset for later use. Next, we’ll cover the steps one might take to develop a functional scraper from the ground up.\nOne very useful way to extract the data we want is to make use of Cascading Style Sheet (CSS) selectors. Many of the HTML elements on a website have class attributes, which allow web developers and designers to style specific chunks of content using styles defined in CSS. In addition to class attributes, we may encounter an id attribute for some elements. Unlike the class attributes, the id in an element is unique. It refers to that element, and that element only. If you’re looking for a way to grab multiple elements that are some subset of all the elements of that type, then you want to use class attributes. But if you want a single element, and that element has an id, then use the id!\nOne final thing that’s very helpful to know here is that almost all web pages use a ‘Document Object Model’ (DOM) to organize the elements they display. The DOM is a hierarchical structure resembling a tree, the trunk of which is the web page itself. In this model, all of the elements can be thought of as branches of the trunk, or branches of those branches, and so on. Many sources use language borrowed from family trees to describe elements’ relationships to one another, as most elements in the page will have other elements nested within them. These nested elements are ‘children’ of the larger ‘parent’ element they are nested within. If we follow through with the metaphor, the nested elements can be thought of as each others’ ‘siblings’.\nThis family tree structure of the DOM is useful to understand, especially in cases when you need to grab data from an element that doesn’t have an id and also doesn’t have a unique class attribute, such as what you often find for the headlines of news stories. In such cases, we can exploit the nested, hierarchical structure of the DOM to find the information we want: all we need to do is locate the element’s parent, at which point we can get information about all of its children and extract the data we need.\nIf you find that the website design is consistent across the pages you want to scrape, you could determine whether the element you want is always nested at the same level. If it is, you could provide a full path to the data you want to scrape, even when given the vaguest of elements. This might mean that you want to always grab the text located at &lt;body&gt;&lt;div&gt;&lt;div&gt;&lt;article&gt;&lt;div&gt;&lt;h1&gt;. If you need to access the second of two &lt;div&gt; elements that are at the same depth (and, thus, a sibling of the first), it can be referred to in the same way you would access an element in aPython list, by &lt;div[1]&gt;.\n\n\n8.1.1.2 Coding a Web Scraper for a Single Page\nIn order to get our scraper operational, we’ll need a way to actually get data from the web pages into Python. Normally, the process of requesting and rendering a page is handled by our browser, but as you learned in the previous chapter, this isn’t the only way to request HTML documents from a web server. We can also connect to a web server from a Python script using a package such as requests, and load the HTML provided by the web server into our computer’s memory. Once we have this HTML in memory (rather than rendered in a browser), we can move onto the next step, which is to start parsing the HTML and extracting the information we want.\nWhen we load an HTML document in Python, we’re looking at the raw markup, not the rendered version we see when we load that file in a browser. If we’re lucky, the information we want will be consistently stored in elements that are easy to isolate and don’t contain a lot of irrelevant information. In order to get that information, we need to parse the HTML file, which can be done using a Python package called BeautifulSoup. Note that BeautifulSoup’s naming conventions are a little confusing. The package is called BeautifulSoup but you have to install it using beautifulsoup4 and import it into Python using bs4. Clear as mud. Let’s make all of this a little less abstract by working through a specific example.\nTo get started, let’s grab the title and body text of the article on Boris Johnson mentioned previously. We will (1) request the HTML document from The Guardian’s web server using the requests package, (2) feed that HTML data into BeautifulSoup to construct a soup object that we can parse, and then (3) extract the article title and text, then store them in a couple of lists.\nIn the code block below, we import the three packages we will use, get the HTML, construct the soup object using an lxml parser, and then – just because we can – print the raw HTML DOM to our screen.\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\nurl = 'https://www.theguardian.com/politics/2019/aug/02/europes-view-on-boris-johnson'\n\nr = requests.get(url)\nsoup = BeautifulSoup(r.content, 'lxml')\nTo save space, I will not actually reproduce the DOM here, but you can do so by running:\nprint(soup.prettify())\nNow we need to get the title. I know that the article title is stored inside a &lt;title&gt; element. I use the findAll method from BeautifulSoup to retrieve that part of the text, which BeautifulSoup returns in the form of a list with one item. To get the string, I simply select the first item in the list using its index ([0]) and add .text to strip away the markup. Finally, although it is not strictly necessary at this point, I strip out any invisible new line characters by ending the line with .replace('\\\\n', '').\narticle_title = soup.findAll('title')[0].text.replace('\\n', '')\nprint(article_title)\nGetting the body text is even easier, as all body text is contained inside &lt;p&gt; elements. We can construct a list of paragraphs with the findAll method.\nparagraphs = soup.findAll('p')\nThe ninth paragraph in the list is:\nparagraphs[8].text\nSometimes it’s useful to combine all the text from an article into one long string (as we will discuss in the chapters on text analysis). We can do this by joining the items in the list, separated by white space.\nall_text = \" \".join(para.text for para in paragraphs)\nAnd with that, we have written our first scraper! It was a relatively simple one, in that our goal was simply to pull out a title and body text for an article in a newspaper. We could have collected a bit of other data if we wanted, such as the author of the page and the date it was published. However, one nice thing about our rather minimal scraper is that we can use it to grab text from other stories posted by The Guardian as well. In other words, simple web scrapers can be used in a broader variety of contexts, because they are not overly tailored to the content of any one specific page. The main takeaway here is that you should keep your web scrapers as simple and portable as possible. Avoid adding complexity unless it’s necessary to retrieve the data you need.\nLet’s wrap these steps up in a simple function, grab some text from a few more news stories, and then construct a Pandas dataframe with article titles in one column and article text in another. We will provide a much deeper explanation of the Pandas package and dataframes in the next chapter.\ndef scrape_guardian_stories(url):\n    soup = BeautifulSoup(requests.get(url).content, 'lxml')\n    article_title = soup.find('title').text.replace('\\n', '')\n    paras = \" \".join(para.text.replace('\\n', '') for para in soup.findAll('p'))\n    return [article_title, paras]\nThe function we just defined follows the same process we just used to scrape the text of the first story on Boris Johnson, but this time wraps the code up in a single function that returns a list containing two items: the title of a story and the main body text. To produce the dataframe, we will provide a list of urls and apply our function to each individual story. This will return a list of lists as a result, which we can then convert into a dataframe.\nIn the code block below, I read in a text file called guardian_story_links.txt. This file containing four URLs, each saved on it’s own line. When I read those lines in, each URL becomes an element in a list. I can then use list comprehension to iterate over the URLs and scrape their content.\nwith open('data/guardian_story_links.txt') as f:\n    stories = [line.rstrip() for line in f]\n\nscraped = [scrape_guardian_stories(s) for s in stories]\ndf_scraped = pd.DataFrame(scraped, columns=['Title', 'Article Text'])\nprint(df_scraped.info())\nWe’ll use a dataframe to summarize the result; dataframes are a form of structured data that we will be using frequently throughout the book; in the next chapter, we’ll go over them in detail. In this dataframe, the titles appear in one column and the article text in another.\nObviously, a simple script like this would not be especially helpful to us if we were only analyzing the text of three or four articles. But social scientists are almost never concerned with the content of just a few articles. With very little code, we can collect a lot of text very efficiently and store it in a dataset that we can analyze using a variety of methods, from traditional content analysis methods to applied natural language processing. All you need is a list of urls, which you can construct manually (although this is not the best approach), by scraping links from some sort of index page (or possibly the front page), or by writing a more complex web crawler (which is beyond the scope of this chapter but you can read about it in Ryan Mitchell’s (2018) excellent book Web Scraping with Python).\n\n\n8.1.1.3 Working with Many Webpages\nIn many cases, the data you want to scrape isn’t neatly packaged for you on a single webpage but is instead spread out across several different pages on a single website. In most cases, there will be some kind of ordering principle that undergirds each of those webpages’ URLs; we can take advantage of this to greatly simplify and expedite the scraping process. Just as we studied the source code for a webpage to determine how best to scrape it, we have to study how the website handles URLs to determine how best to get the data we want from multiple pages.\nLet’s talk first about detecting patterns when you browse through pages on a website. This is as simple as navigating though your page, from some starting point (say the front page of nytimes.com), and then clicking through to different sections and news stories. As you do this, you’ll notice that some parts of the URL change and other parts stay the same. For example, you might see that a new chunk of string is added to the URL when you navigate to parts of the website that cover international news, and a different string appears when you’re reading op-eds or the lifestyle section.\nAs you click through pages, make notes on how the URL changes. Then start simply entering new values into parts of the URL (such as different numbers for pages) and see what the results are. Does it return a valid page or not? Did it return what you expected? This is just one way of doing the necessary detective work – both in terms of the webpage source code and how the website handles URLs – to get the content we want from those pages. Figure 8.2 below presents an overview of these two interconnected processes.\n\n\n\n\n\n\nFigure 8.2: An overview of the front end investigatory work that goes into web scraping. You must spend time studying URLs and the source code of the pages you want to scrape.\n\n\n\nLet’s make this all a little less abstract by looking at a specific example of programmatically iterating over URLs to scrape a page.\nConsider the United Nations’ Sustainable Development Partnership Platform. This website contains a directory of sustainable development projects that the UN’s Department of Economic and Social Affairs is supporting. Here’s an example of one such project’s page on the website: https://sustainabledevelopment.un.org/partnership/?p=35711. Here, we can see that the ‘Future Rangers Program’ is an anti-poaching initiative designed to train a new generation of wildlife conservationists.\nLet’s say you wanted to scrape information about 30 projects approved by this initiative. These project pages are informative but it will take a fair bit of time to find URLs for each of those 30 projects and then put them into a list (as we did with The Guardian in the examples above). Luckily for us, there’s an easy way to iterate through the project pages on this website: the key can be found in the ?p=35711 at the trailing end of the URL we examined. Each of the projects listed on the website has a project number and each project’s page can be accessed by replacing the number at the end of our example URL with the project number you want to access. Give it a try!\nAfter plugging in a few random numbers, you’ll discover that not every 5-digit number has a corresponding project (you can use 35553, if you’d like to see one that works), and that continuing to use hand-typed numbers until we reach our desired threshold of 30 projects will take a very long time. Let’s write some code to move things along:\ndef scrape_UNSD_project(url):\n    result = requests.get(url)\n    if result.ok:\n        soup = BeautifulSoup(result.content, 'lxml')\n        headline = soup.find(id='headline').getText()\n        intro = \" \".join(\n            [segment for segment in soup.find(id='intro').stripped_strings])\n        return [headline, intro]\n    else:\n        return None\nIn the above code block, we define a function that takes one parameter (a URL) and retrieves textual data about a project from its page on the UN Sustainable Development website (defined by the URL). This function does things a little differently than in previous examples, so it’s worth going through it line by line.\nThe first thing the function does is pass the URL it was supplied to the requests.get function, which returns a result. Not all results are useful, though, and as you may have discovered while entering random numbers into the UNSD website, most project IDs don’t have a publicly-visible project associated with them. Whenever an HTTP GET requests a page that the server can’t find, it returns a 404 code, indicating that it couldn’t locate what we were asking for. When it can find what we’re looking for, the server will usually return a 200 code, indicating that everything is okay. There are a variety of HTTP Status Codes that a server can return and each of them carry a specific meaning (visit https://www.restapitutorial.com/httpstatuscodes.html for a list of what each code means). Generally speaking, codes from 0 to 399 indicate a successful GET request, whereas anything 400 or above indicates that something went wrong.\nLuckily for us, the Requests package was designed with ease-of-use in mind, and provides a convenient way of checking if our GET request was successful: ok. The ok attribute is False if something went wrong, and True in all other cases. As such, we can use result.ok to provide a boolean operator to an if-else statement; we’ll cover how this fits into the larger picture a few paragraphs from now. If the result is ok, the function then uses BeautifulSoup to parse it. We’ll use find to isolate the text we’re interested in, but this time we’ll use the named ids ‘headline’ and ‘intro’ to retrieve it.\nThe next block of code simply sets the starting parameters for our scrape - we’ll use them later on. In this case, we’ve used three variables to indicate to our scraper which URL we want it to start at (base_url and starting_number), and how many pages we want to collect (target_records).\nbase_url = \"https://sustainabledevelopment.un.org/partnership/?p={}\"\nstarting_number = 30000\ntarget_records = 30\nWe’re going to get Python to repeatedly replace those curly braces ({}) in our URL with different numbers, corresponding to the project IDs we want to gather information about.\nThe final code block of this section puts the pieces together, starting by defining a list, which we’ll populate using our scraper. Then, it uses a while statement with the condition that the number of scraped documents contained in scraped is smaller than target_records; this means that the code inside the while block will repeat until the condition is no longer true.\nscraped = []\n\ncurrent_number = starting_number\n\nwhile len(scraped) &lt; target_records:\n    url = base_url.format(current_number)\n    try:\n        output = scrape_UNSD_project(url)\n        if output is not None:\n            print(f\"scraping {current_number}\")\n            scraped.append(output)\n    except AttributeError:\n        pass\n    current_number += 1\n\ndf_scraped = pd.DataFrame(scraped, columns=['Headline', 'Introduction'])\n\nprint(df_scraped.info())\nWhen using while blocks, exercise caution! If you use a while loop with an end-state that isn’t guaranteed to be met (or might not be met in a reasonable time-frame), your computer will keep executing the same code over and over until the end of time (or Windows forces your computer to update – sorry, I couldn’t resist). In our case, we’ve used a condition that will eventually be broken out of by the code inside the while block. Here’s how:\n\nFirst, it uses format to replace the curly braces inside base_url with the current value of starting_number, giving us our url.\nSecond, it attempts to retrieve text data using the url and our scrape_UNSD_project function, storing the result in output.\nAfter checking to see if output contains anything (it can sometimes be empty, which we don’t want), our code appends output to our scraped list.\nFinally, it increments starting_number by one and if the number of successful scrapes is fewer than 30, it uses the new starting_number to begin another scrape on the next project.\n\nSo, now we have a dataframe containing text describing 30 different projects from the UNSD website and we’ve accomplished this without having to navigate the site using links or redirects – we just changed one number! While such an approach is extremely useful, it isn’t compatible with all websites. If the approaches we’ve covered thus far won’t work (which can happen when a website is dynamically generated or interactive, for instance), then we’ll have to call in the cavalry. In this case, the cavalry is a Python package called selenium. Since my publishers have unreasonably insisted that this book should be carryable without the assistance of a hydraulic lift, we’re not going to have room to cover selenium in-text. If you want to read more about how to scrape the interactive web, we’ve prepared an online supplement that will guide you through the process.\n\n\n\n8.1.2 ETHICAL AND LEGAL ISSUES IN WEB SCRAPING\nWhile it’s theoretically possible to systematically step through every page and element of a website and store it (sometimes called a site-dump), most websites would prefer you didn’t, may forbid it in their Terms of Service, or in some cases, may seek legal recourse if you were to use that data anywhere. Thankfully, academic researchers are under greater scrutiny over the ethics of their work than your average web-hacker trying to harvest content, but you may not have your research plan vetted by people who understand the legal and ethical issues involved in web scraping, and even if you do, you shouldn’t rely on them to tell you whether your plan is going to violate the Terms of Service for a website. You most definitely want to avoid a threat of legal action when web scraping, so be sure you are always checking the Terms of Service for any website you scrape.\nA general rule to follow here is that if a website wants to provide users access to data at scale, they will setup an API to provide it. If they haven’t, err on the side of not collecting data at scale, and limit your data collection efforts to what you need rather than gathering all of it and filtering later. You may recall that websites will sometimes deny service after receiving too many requests. Sometimes, their infrastructure can struggle to even send out the needed denials of service, and will crash. It’s very unlikely that you could cause this type of problem, which is usually performed by many computers at once in a distributed denial of service (DDoS) attack that is either orchestrated or the product of computer viruses. That said, it is likely you will run across denial of service errors, perhaps when doing a more extensive URL iteration like in the example above, and may want to think about implementing a modest rate limit in your script.\nFinally, there is not yet a widely-held standard, in academia or elsewhere, for the boundaries between ethical and unethical web scraping. The most widely-held position seems to be that public content online should be treated with the same standards that one would apply when observing people in public settings, as ethnographers do. If this is the position you take, you may sometimes need to think very carefully about what types of online content is “reasonably public.” If you were scraping some personal blog platform, for example, and found that you could iterate through URLs to access pages that aren’t linked to anywhere, this likely would not be considered “reasonably public” because it’s possible the blog owner thought they were no longer accessible. This ongoing debate will inevitably include web-scraping practices, so should be an important one to research and keep track of. We’ll discuss these issues in more depth in Chapter 19.\n\nBox. To further develop your web scraping skills, I strongly recommend Mitchell’s (2018) Web Scraping with Python. It covers a broader range of practical problems than I cover in this chapter, including parsing documents such as PDFs.",
    "crumbs": [
      "**OBTAINING DATA**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Web data (Scraping)</span>"
    ]
  },
  {
    "objectID": "web-data-scraping.html#conclusion",
    "href": "web-data-scraping.html#conclusion",
    "title": "8  Web data (Scraping)",
    "section": "8.2 CONCLUSION",
    "text": "8.2 CONCLUSION\n\n\n8.2.1 Key Points\n\nWeb scraping is a powerful approach to collecting data from the web, useful for when data is not available in an API, but could still be obtained ethically and legally\nBeautifulSoup is a very useful tool for processing HTML from websites, but cannot obviate the need to understand a web page’s Document Object Model (DOM)\nThe “iron rule of web scraping”: you must put in the proper time and energy to investigating the source code for the pages you want to scrape\nThe only true limits on the data you can collect from scraping the web are ethical and legal\n\n\n\n\n\n\nDuckett, Jon. 2011. HTML & CSS: Design and Build Websites. Vol. 15. Wiley Indianapolis, IN.\n\n\nMitchell, Ryan. 2018. Web Scraping with Python: Collecting More Data from the Modern Web. \"O’Reilly\".",
    "crumbs": [
      "**OBTAINING DATA**",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Web data (Scraping)</span>"
    ]
  },
  {
    "objectID": "audio-files-and-documents.html",
    "href": "audio-files-and-documents.html",
    "title": "9  Audio files and documents",
    "section": "",
    "text": "Coming Soon!\n\n\n\nThis is a new chapter. I am actively working on it in fall 2024 and intend to publish a working draft in early winter 2025. It will include using some deep learning models for the audio and document data, so I need to think about how best to integrate the chapter this early in the book, long before deep learning is introduced. Stay tuned! 😎",
    "crumbs": [
      "**OBTAINING DATA**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Audio files and documents</span>"
    ]
  },
  {
    "objectID": "exploratory-data-analysis.html",
    "href": "exploratory-data-analysis.html",
    "title": "10  Exploring with purpose",
    "section": "",
    "text": "10.1 ITERATIVE RESEARCH WORKFLOWS: EDA AND BOX’S LOOP\nThe term exploratory data analysis (EDA) has generally come to refer to the work you do to explore, examine, clean, and make sense of a dataset prior to any formal statistical modelling (confirmatory data analysis). The idea is that one should carefully examine their data (i.e., exploration) before getting into hypothesis testing (i.e., confirmation). However, it is important to understand that the relationship between “exploratory” and “confirmatory” analysis is rarely clear cut. Some prominent statisticians and social scientists have proposed approaches that intentionally combine the two. Andrew Gelman (2004) has pointed out that we can think of both approaches to analysis as sharing the same underlying logic: comparing what we observe relative to an implicit or explicit statistical model. In between these two views – EDA as first steps, EDA as a kind of implicit modelling – is the idea that EDA is an important part of iteratively developing almost any model, simple or complex. Let’s briefly consider this framework for thinking about the role of EDA in quantitative and computational research. We will do so using an idealized framework known as Box’s loop, first proposed by the statistician George Box.\nIn our idealized framework, the first thing we do is build a model. Note that data hasn’t entered the picture yet. In our ideal world, we have a question that we want to answer. Perhaps we want to know “How does education relate to yearly income in Canada?” To answer that question, we create a model that captures the salient features of our question. “I think education is positively associated with income, but I also think income is related to age, and education is probably related to age.” This model should tell us what kind of data we will need, which is why data enters in the second step: we should be collecting or using data with our model in mind first. One of the things that makes data “good” is its appropriateness for the model you are using. It doesn’t matter how accurate your data on house fly reproduction cycles are if you want to answer a question about dragonfly eating habits. In step 2, we analyze our data to answer our question. Oftentimes, we get a much better idea about how good our data really is after we analyze it. Technically, we should now have an answer to our question. However, in step three, we consider the process and results of our analysis. Does the model make sense? Is there anything surprising? If there is, does that mean there’s an unforseen issue with the data? Is this the best answer to my question that I can get? Science is messy, and it’s a rare person indeed who never makes a mistake or knows exactly how a research project will proceed. So, rather than accept the first answer we find, we find things that worked, things that didn’t, and we criticize our model. In step 4, we take our critique from step 3 and use that to revise our model, which takes us back to step 1. We continue this process until we are satisfied with our model and data, at which point we can break the loop and apply the model.\nReturning to exploratory data analysis, let’s pretend to be Gelman and consider why we visualize data. We want to answer a question, even if the question is something as simple as “how is height distributed in Canada?” To answer that question, we (1) build a model. Perhaps our model is “Height is measured in centimeters, which is a continuous variable. I can make a histogram that will tell me how it is distributed.” Then, we (2) analyze our data. We make our histogram. Then, we consider how good our visualization is at answering our question; we criticize our model by considering what worked, what didn’t. Perhaps we noticed that height is actually a bimodal distribution after visualizing it. Then we (4) use that critique to revise our model. Perhaps we decide to plot height as a histogram, but conditional on sex. We repeat this process until we are satisfied and (5) apply our model, actually drawing conclusions about the distribution of height in Canada.\nWhen we get to statistical models later, you may discover that this is a doubly-nested process. You will build a statistical model, analyze data, criticize the model using EDA (start a sub-loop here), revise the statistical model, and repeat the main loop until you’re satisfied or give up. This process is summarized in Figure 11.2. In later chapters, you’ll see variations on this basic loop.\nFor our purposes we will work on developing your EDA skills in the more classical sense: the first steps you take with a new dataset. However, I will continue to emphasize the iterative development process of visualizations, in part to help prepare you; once you have those skills, you can and should use them in the broader context of statistical model criticism and iterative development. You will continue to develop these skills as you work through the book, and you may find yourself further appreciating Gelman’s argument that even this EDA work is a kind of modelling, which further tightens the links between the kind of work we will consider here and the more complex statistical and machine learning models we will focus on in later chapters.\nEDA is often broken down into graphical and non-graphical methods for analyzing single variables (i.e., univariate graphical and univariate non-graphical) and graphical and non-graphical methods for analyzing two or more variables (i.e., multivariate graphical and multivariate non-graphical). We will discuss methods within these four categories that can be used for categorical and quantitative data. First, let’s first discuss visualization in general - specifically what differentiates good plots from bad plots, and ways to make creating good plots a bit easier.",
    "crumbs": [
      "**DISCOVERY**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploring with purpose</span>"
    ]
  },
  {
    "objectID": "exploratory-data-analysis.html#iterative-research-workflows-eda-and-boxs-loop",
    "href": "exploratory-data-analysis.html#iterative-research-workflows-eda-and-boxs-loop",
    "title": "10  Exploring with purpose",
    "section": "",
    "text": "Figure 10.1: Cap",
    "crumbs": [
      "**DISCOVERY**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploring with purpose</span>"
    ]
  },
  {
    "objectID": "exploratory-data-analysis.html#effective-visualization",
    "href": "exploratory-data-analysis.html#effective-visualization",
    "title": "10  Exploring with purpose",
    "section": "10.2 EFFECTIVE VISUALIZATION",
    "text": "10.2 EFFECTIVE VISUALIZATION\n\n10.2.1 Guidelines for Effective Visualization\nI’ll admit, I’m fastidious when it comes to the aesthetics of data visualization. But when I talk about good visualizations versus bad ones, I’m not talking about the fine distinctions between colors or typefaces. You can create good visualizations while being clueless about those things. No, I’m talking about more fundamental things like knowing what you want to learn or communicate.\nCreating graphs to visualize distributions, relationships, and so on, is relatively straightforward in Python. Creating good graphs has little to do with Python and everything to do with the decisions you make about what to show and how to show it. Some of those decisions are high level, like the kind of graph and how it should be structured. Other decisions are low-level, like selecting colors and shapes. What differentiates a really good visualization from a really bad one is rarely taste. It’s almost always the amount of thought behind those fundamental decisions.\nAs Kieran Healy points out, “bad graphs” tend to be bad for one of three reasons:\n\nAesthetics: there are a unnecessary details or modifications (e.g. unnecessary 3D effects, shadows, rotated axes). These “infographic” visualizations may be memorable but are difficult to interpret correctly. Avoid creating these at all costs. If a collaborator asks you to make one, politely decline.\nSubstantive data problems: the graph looks good and follows best practices, but the data themselves are bad or have been incorrectly processed, sending a misleading message. Avoid this by doing extensive EDA.\nBeing inattentive to the realities of human perception, and therefore making poor visualization choices that may mislead you or your readers.\n\nAs I have stressed, effective data visualization requires adopting an iterative approach. Start by thinking carefully about what you want to learn (if you’re doing exploratory data analysis), or what you want to communicate (if you’re producing a graph for publication). Try sketching the plot with pen and paper. It refines your thinking before you start coding and helps clarify what kind of plot you need.\nOnce you know what you want and need, then you start coding. Again: iterate. Start with the default settings for the kind of plot you want, such as a scatterplot. Then gradually modify the defaults, and add and remove elements, until you have a plot that clearly shows what you want it to show. Work slowly: change one thing at a time.\nIf you think of visualization as a communication problem, you need to consider the realities of perception and the human vision system. Perception is more complicated than simply creating a 1:1 mental representation of what we are looking at. Think of the many examples of visual effects and optical illusions.\nThere is a sizable empirical literature on perception and statistical visualization, largely built on Bill Cleveland’s work in the 1980s and 1990s. Psychologists, statisticians, and applied data analysts have documented many specific factors that affect the probability of drawing an incorrect conclusion from a graph. These include:\n\nSelecting the wrong type of colour palette increases the chance of mis-perception (e.g. it’s diverging when it should be qualitative, it’s not colour-blind friendly).\nUsing area or angles to represent important properties increases the chance of mis-perception because humans are inherently bad at comparing similar angles and areas.\nUsing length and position increase the chance of correct perceptions because humans are good at comparing differences in length and position. Think of these first.\n\nThis leads to some useful guidelines. One of the most important, and very easy to implement, comes from knowing we have an easier time perceiving some colours, shapes, and relationships than others. Visualizations that require comparing angles and areas – such as pie charts, bubble charts, and stacked bar charts – are non-starters. Follow that basic rule. You’ll avoid plenty of bad and misleading visualizations.\nPie charts are easy to criticize, but are stacked bar charts and bubble charts really all that bad? Yes. With stacked bar charts, each block has a different baseline / starting point in each bar. Given how bad we are at comparing areas, your reader will be more likely to interpret your graph incorrectly. Bubble charts are slightly more complex. Sometimes, it can be effective to change the size of points in a scatterplot, but be careful, and you shouldn’t expect a reader to perceive small differences between points.\nDistances are very important and meaningful in almost all data visualizations. Most of the time, bar graphs and dot plots should have a \\(y\\)-axis that starts at 0, but contrary to common wisdom, there are some rare cases when this is not the best choice. Generally, default to axes that start at 0, but don’t be dogmatic. Again, know exactly what you want to show and why. If you want to be instantly discredited by a quantitatively-literate reader, exaggerate differences by manipulating the range of the y-axis. Otherwise, don’t do this!\nHumans also tend to see patterns in data, even when those patterns are not actually meaningful. As such, it’s usually a mistake to use visualizations without doing some statistical modelling, just as it is a mistake to model your data without visualizing it. Recall Gelman’s argument; all EDA is done in reference to an implicit or explicit model. While statistical modelling will come later, I want to emphasize that you should pursue these two types of analysis simultaneously. In this chapter, we pair visualization with summary statistics that are commonly used in EDA.\nIn addition, here are some straightforward rules to reduce the chances of creating bad visualizations:\n\nIf you can show what you need to using a graphic that is widely-used and well-understood, do so. Don’t invent new graphics for the sake of it. Make it easy for you and your readers to interpret your graphs.\nLess ink is better than more ink. Simplify as much as possible, but no more. Find the balance of information density and visual minimalism by working on your visualizations iteratively.\nDon’t create 3D visualizations of any plot. If you have to add a third dimension, consider using either colour or shape, but not both.\nThe dimensions of a line plot have a huge effect on our perception of slope / rate of change. Exercise extreme care when selecting the dimensions. Do not intentionally mislead your readers.\nDo not vary colour or shape unless you are encoding important information in colours and shapes. A multi-coloured bar plot might look nicer than a solid grey, but if the colours aren’t meaningful, it dramatically increases the chances of mis-perception.\n\nFinally, ask others to critique your visualizations and practice! Learn any new skill takes time and effort.\nNow, let’s get into the code!\n\nFurther Reading\nIn addition to what’s provided here, I suggest reading Healy and Moody (2014) on the state of data visualization in sociology and other social sciences. And though it uses R rather than Python, Healy (2018) is an outstanding introduction to good data visualization practice.",
    "crumbs": [
      "**DISCOVERY**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploring with purpose</span>"
    ]
  },
  {
    "objectID": "exploratory-data-analysis.html#univariate-eda-describing-and-visualizing-distributions",
    "href": "exploratory-data-analysis.html#univariate-eda-describing-and-visualizing-distributions",
    "title": "10  Exploring with purpose",
    "section": "10.3 UNIVARIATE EDA: DESCRIBING AND VISUALIZING DISTRIBUTIONS",
    "text": "10.3 UNIVARIATE EDA: DESCRIBING AND VISUALIZING DISTRIBUTIONS\nWe’re going to make use of the VDEM data we used in the previous chapter as the basis for our exploratory data analysis. You should refresh yourself on the data and variables description before proceeding. We will start by reading in the filtered and subsetted dataframe fsdf that we produced earlier and saved as a CSV and produce get the mean, median, and standard deviation from some of its variables.\n\n10.3.1 Imports\nimport os\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom dcss import set_style\nset_style()\n# data was downloaded in chapter 6\nfsdf = pd.read_csv('data/vdem/filtered_subset.csv') \nfsdf.shape\negal = fsdf['v2x_egaldem']\nprint(f'Median Egalitarian Democracy Score: {egal.median()}')\nprint(f'Mean Egalitarian Democracy Score: {egal.mean()}')\nprint(f'Standard Deviation: {egal.std()}')\nSince the values returned from operations on Series are essentially equivalent to a NumPy array, we can use NumPy methods on quantitative Series. For example, here you can use the round() method to round these descriptives to a few decimal points.\nprint(f'Median Egalitarian Democracy Score: {round(egal.median(),3)}')\nprint(f'Mean Egalitarian Democracy Score: {round(egal.mean(), 3)}')\nprint(f'Standard Deviation: {round(egal.std(), 3)}')\nIf the Series is categorical, we can also easily compute useful information such as the number of unique categories, the size of each category, and so on. For example, you can use the .unique() method to get a list of the unique countries from the country_name Series. Here these values are cast as a list and sliced to display the first 10 elements.\nlist(fsdf['country_name'].unique())[0:10]\nWith a categorical variable like this, you can also use the value_counts() method to see how many observations you have for country_name in the dataset. Since there are 73, don’t print everything to screen, just peek at the top and the bottom 10 rows.\nfsdf['country_name'].value_counts().head(10)\nfsdf['country_name'].value_counts().tail(10)\n\n\n10.3.2 Visualizing Marginal Distributions\nYou should visualize distributions for individual variables (i.e. “marginal distributions”) before any more complex analyses. As with numerical summaries, graphical approaches to examining distributions will typically answer questions about the range of values, their central tendency, whether they are highly skewed, whether there are potentially influential outliers, and so on.\n\n10.3.2.1 Count Plots and Frequency Tables for Categorical Variables\nFor a single categorical variable, we are limited to examining frequencies, percentages, and proportions. For example, you can visualize the number of countries in each geographical region defined by the United Nations (the e_regiongeo categorical variable) using the .countplot() function. You want the bars in the countplot to be horizontal rather than vertical, so use the argument y='e_regiongeo' instead of x='e_regiongeo'. The code below produces Figure 10.2.\n#| output: false\n#| warning: false\nax = sns.countplot(data=fsdf, y='e_regiongeo', color='darkgray')\nsns.despine()\nplt.savefig('figures/07-01.png', dpi=300)\n\n\n\n\n\n\nFigure 10.2: caption…\n\n\n\nThis graph could use some improvements. Let’s iterate! First, it would be better if the data were in descending order by counts. Second, it would be better it had region names rather than number IDs. Third, a few small aesthetic adjustments that would improve the overall look of the graph, like removing the black line on the left side of the graph.\nRemember, address these issues one at a time. First deal with the order issue, which can be solved by using the order argument for .countplot(). Tell it to order the bars by sorting the fsdf['e_regiongeo'] Series. The code below produces Figure 10.3.\n#| output: false\n#| warning: false\nax = sns.countplot(\n    data=fsdf, \n    y='e_regiongeo', \n    color='darkgray',\n    order = fsdf['e_regiongeo'].value_counts().index\n) \n\nsns.despine()\nax.set(xlabel='Number of Observations', ylabel='Geographic Region')\nplt.savefig('figures/07-02.png', dpi=300)\n\n\n\n\n\n\nFigure 10.3: caption…\n\n\n\nLet’s replace the numerical region IDs with a string ID. The dictionary below is the mapping between IDs (keys) and the region strings (values) provided in the VDEM codebook.\nregion_strings = {\n    1: \"Western Europe\",\n    2: \"Northern Europe\",\n    3: \"Southern Europe\",\n    4: \"Eastern Europe\",\n    5: \"Northern Africa\",\n    6: \"Western Africa\",\n    7: \"Middle Africa\",\n    8: \"Eastern Africa\",\n    9: \"Southern Africa\",\n    10: \"Western Asia\",\n    11: \"Central Asia\",\n    12: \"East Asia\",\n    13: \"South-East Asia\",\n    14: \"South Asia\",\n    15: \"Oceania\", # (including Australia and the Pacific)\n    16: \"North America\",\n    17: \"Central America\",\n    18: \"South America\",\n    19: \"Caribbean\" # (including Belize, Cuba, Haiti, Dominican Republic and Guyana)\n}\nYou can now use a Pandas method called .replace() to replace each numerical ID with the string representation.\nfsdf.replace({'e_regiongeo': region_strings}, inplace=True)\ninplace=True changes the values in the original dataframe. If you create the same countplot again, the region names will be used as the labels on the y-axis. The code below produces Figure 10.4.\n#| output: false\n#| warning: false\nax = sns.countplot(\n    data=fsdf, \n    y='e_regiongeo', \n    color='darkgray',\n    order = fsdf['e_regiongeo'].value_counts().index\n)\n\nsns.despine(left=True)\nax.set(xlabel='Number of Observations', ylabel='')\nax.xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}')) # comma formats x-axis\nplt.savefig('figures/07-03.png', dpi=300)\n\n\n\n\n\n\nFigure 10.4: caption…\n\n\n\nWe can easily produce frequency tables with Pandas; in fact, we have already done this using .value_counts() at the country-level. The code below creates a frequency table for the same data you just plotted.\nfsdf['e_regiongeo'].value_counts()\n\n\n10.3.2.2 Univariate Histograms and Density Estimation\nOne of the first things you can do in EDA is create a histogram for each of the variables you are interested in. A histogram visualizes “where” your data is clustered. It’s a bar chart, where the x-axis contains the range of all values for a particular variable and the y-axis indicates a count of the number of observations. The height of each bar represents the number of observations of x values between the left and right boundaries of each bar. Histograms are a more easily digestible summary of a variable than the .describe() method used earlier.\nLet’s use Seaborn’s histplot() to create a histogram of the egalitarian democracy index variable from the fsdf dataframe. The code below produces Figure 10.5.\n#| output: false\n#| warning: false\nax = sns.histplot(data=fsdf, x='v2x_egaldem')\nsns.despine(left=True, right=True, top=True)\nax.set(xlabel='Egalitarian Democracy Index', ylabel='Count')\nax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\nplt.savefig('figures/07-04.png', dpi=300)\n\n\n\n\n\n\nFigure 10.5: caption…\n\n\n\nThe histogram clearly shows that most of the v2x_egaldem values in the dataset can be found at the lower end of the range, quickly sloping down and then evening out with a few gentle peaks. Note that the function has made an important decision implicitly - you did’ntt specify the number of bins, so it used a built-in method that provides generally good defaults. However, you should always double-check default parameters. Overly wide bins can “hide” peaks or troughs in the distribution if they fit entirely inside a bin. Overly narrow bins can produce visualizations that are especially sensitive to “noise”. Narrower bins will tend to result in graphs with sharper spikes as small clusters and gaps get magnified.\nYou can manually check for these cases by providing explicit values to the bins or binwidth parameters. Below, I provide extreme examples of overly wide and narrow bins to highlight the issues with both. The code below produces Figure 10.6.\n#| output: false\n#| warning: false\nax = sns.histplot(data=fsdf, x='v2x_egaldem', bins=3)\nsns.despine(left=True, right=True, top=True)\nax.set(xlabel='Egalitarian Democracy Index', ylabel='Count')\nax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\nplt.savefig('figures/07-05.png', dpi=300)\n\n\n\n\n\n\nFigure 10.6: caption…\n\n\n\nThe code below produces Figure 10.7.\n#| output: false\n#| warning: false\nax = sns.histplot(data=fsdf, x='v2x_egaldem', binwidth = 0.001)\nsns.despine(left=True, right=True, top=True)\nax.set(xlabel='Egalitarian Democracy Index', ylabel='Count')\nax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\nplt.savefig('figures/07-06.png', dpi=300)\n\n\n\n\n\n\nFigure 10.7: caption…\n\n\n\nWe can also use kernel density estimation (KDE) to visualize a distribution. KDE is a technique for estimating the probability density function of a random variable. Rather than visualizing the raw counts in combined bins, it estimates the probability of every possible value using a smooth function. KDE attempts to reduce the random noise in the data, smoothing out the spikes.\nYou can add a KDE line to histograms by providing the parameter kde = True. The code below produces Figure 10.8.\n#| output: false\n#| warning: false\nax = sns.histplot(data=fsdf, x='v2x_egaldem', kde=True)\nsns.despine(left=True, right=True, top=True)\nax.set(xlabel='Egalitarian Democracy Index', ylabel='Count')\nax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\nplt.savefig('figures/07-07.png', dpi=300)\n\n\n\n\n\n\nFigure 10.8: caption…\n\n\n\nSeaborn provides a large number of ways to customize and refine your visualizations. However, with great power comes great responsibility. In this case, you are responsible for creating visualizations that make sense, because the package may not stop you from making mistakes. Consider the previous histogram with binwidth=1. The entire range of v2x_egaldem is less than 1. The visualization is technically possible, though certainly useless. I left the KDE line to help keep track of where the data actually is. The code below produces Figure 10.9.\n#| output: false\n#| warning: false\nax = sns.histplot(fsdf['v2x_egaldem'], kde=True, binwidth=1)\nsns.despine(left=True, right=True, top=True)\nax.set(xlabel='Egalitarian Democracy Index', ylabel='Count')\nax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\nplt.savefig('figures/07-08.png', dpi=300)\n\n\n\n\n\n\nFigure 10.9: caption…\n\n\n\n\n\n10.3.2.3 Marginal Empirical Cumulative Distributions\nEmpirical Cumulative Distributions (ECDs) combine several of the advantages of the previously described visualizations. Unlike histograms, you do not need to bin your data because every data point is represented. This makes it easy to assess the shape of the distribution. For example, one can easily visually identify the five-number summary. Finally, they make it much easier to compare multiple distributions.\n\n\n10.3.2.4 Plotting Empirical Cumulative Distributions\nTo visualize an ECD, set one axis to be the variable of interest. The second axis represents the proportion of observations that have a value equal to or lower than a given cutoff point. Consider a five-number summary. The five numbers are the minimum value, the first, second, and third quartiles, and the maximum value. If a point on an ECD represents a value of our variable and the proportion of observations with equal or lower value, the minimum value must be given by the very first point: the lowest, and leftmost point on the plot. The first quartile is necessarily found at the point where proportion equals 0.25. The second quartile is where proportion equals 0.50, and so on for the third quartile, and the maximum value.\nSince the proportion axis captures each observation and all lower-valued ones, proportion can never decrease as the variable increases. Unfortunately, ECDs are less common than point or box-based visualization techniques, so we have less experience reading them. Like all skills though, it can be developed with time and practice. Most importantly, you can think of the slope of the line as telling you where data is clustered. A steep slope indicates a large portion of the data is clustered around those values. Consider the ECD of v2x_egaldem. The code below produces Figure 10.10.\n#| output: false\n#| warning: false\nax = sns.displot(fsdf, x=\"v2x_egaldem\", kind=\"ecdf\", color='darkgray')\nsns.despine()\nax.set(xlabel='Egalitarian Democracy Index')\nplt.xlim(0, 1) \nplt.savefig('figures/07-09.png', dpi=300)\n\n\n\n\n\n\nFigure 10.10: caption…\n\n\n\nThe slope shows us that proportion rises very rapidly between v2x_egaldem values of 0.0 and 0.2. Much of the data is clustered in that area, and the slope slowly tapers off afterwards, telling us the same thing we saw in the histogram: the number of observations decreases as v2x_egaldem increases, with a slight blip around 0.75.\nHistograms, kernel density estimation, and empirical cumulative distributions have different strengths and weaknesses; each makes some things easier to see and others harder. I advise you to use all three types when exploring your data.",
    "crumbs": [
      "**DISCOVERY**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploring with purpose</span>"
    ]
  },
  {
    "objectID": "exploratory-data-analysis.html#multivariate-eda",
    "href": "exploratory-data-analysis.html#multivariate-eda",
    "title": "10  Exploring with purpose",
    "section": "10.4 MULTIVARIATE EDA",
    "text": "10.4 MULTIVARIATE EDA\n\n10.4.1 Visualizing Conditional Distributions\nSometimes we might want to visualize a variable of interest conditioned on another variable. Perhaps we want to visualize only those instances of a variable \\(X\\) that are associated with a particular value of \\(Y\\).\n\n10.4.1.1 Conditional Histograms\nLet’s visualize the egalitarian index of countries, conditioned on whether the country is a democracy or not. A conditional distribution will exclude any observations that do not meet the condition, so for completeness, we can visualize both cases separately, but side-by-side.\nThe displot function allows us to visualize univariate (a single variable) or bivariate (two variables) data distributions across multiple subplots. Below, we will create a visualization that displays a v2x_egaldem histogram for each value of e_boix_regime, a binary variable. Thus, when we give the column parameter col the values for e_boix_regime, we should get 2 columns, one for each value it can take. The code below produces Figure 10.11.\n#| output: false\n#| warning: false\nax = sns.displot(fsdf, x=\"v2x_egaldem\", col=\"e_boix_regime\", multiple=\"dodge\")\nax.set(xlabel='Egalitarian Democracy Index')\nplt.savefig('figures/07-10.png', dpi=300)\n\n\n\n\n\n\nFigure 10.11: caption…\n\n\n\nThe result we get is the two conditional distribution histograms above. If you were to add the two together, you would get the original v2x_egaldem marginal distribution histogram we produced earlier. While this method makes very clean graphs, it can be hard to compare the graphs, especially if you want to graph many different conditions. One way around this would be to plot both conditional distributions on the same plot, but colour them differently and make them slightly transparent. We can do this by passing e_boix_regime to the hue parameter, rather than col. The code below produces Figure 10.12.\n#| output: false\n#| warning: false\ngrayscale_cmap = sns.cubehelix_palette(\n    50, \n    hue=0.05, \n    rot=0, \n    light=0.9, \n    dark=0, \n    as_cmap=True\n)\n\ngrayscale_palette = sns.color_palette(\"cubehelix\", 50)\n\nax = sns.displot(\n    fsdf, \n    x=\"v2x_egaldem\", \n    hue=\"e_boix_regime\", \n    palette=grayscale_palette\n)\n\nax.set(xlabel='Egalitarian Democracy Index')\nplt.savefig('figures/07-11.png', dpi=300)\n\n\n\n\n\n\nFigure 10.12: caption…\n\n\n\nThis plot makes it much easier to compare the two distributions, and we can clearly see where they overlap and don’t. However, it adds visual clutter and can quickly become hard to read as more conditions are added.\n\n\n10.4.1.2 Conditional KDE\nAs before, we can add KDEs to existing histograms by passing kde = True, or we may visualize just the KDE to reduce visual clutter by passing kind = \"kde\". The code below produces Figure 10.13.\n#| output: false\n#| warning: false\nax = sns.displot(fsdf, x=\"v2x_egaldem\", hue=\"e_boix_regime\", kde = True, palette=grayscale_palette)\nax.set(xlabel='Egalitarian Democracy Index')\nplt.savefig('figures/07-12.png', dpi=300)\n\n\n\n\n\n\nFigure 10.13: caption…\n\n\n\nThe code below produces Figure 10.14.\n#| output: false\n#| warning: false\nax = sns.displot(\n    fsdf, \n    x=\"v2x_egaldem\", \n    hue=\"e_boix_regime\", \n    kind = \"kde\", \n)\n\nax.set(xlabel='Egalitarian Democracy Index')\nplt.savefig('figures/07-13.png', dpi=300)\n\n\n\n\n\n\nFigure 10.14: caption…\n\n\n\nIn all of these plots, countries with a value of 1.0 for e_boix_regime are electoral democracies. Those with 0.0 are not. The comparative plots we have been creating show, unsurprisingly, that non-democratic countries tend to have lower scores on the Egalitarian Democracy Index.\n\n\n10.4.1.3 Conditional ECDs\nExtending an ECD to conditional distributions is as easy as passing e_boix_regime to the hue parameter. Consider the previous example of plotting the histograms of v2x_egaldem conditioned on e_boix_regime. Histograms involve a lot of “ink”, resulting in a lot of visual clutter compared to ECDs, and get hard to read very quickly as the number of conditions increases. The code below produces Figure 10.15.\n#| output: false\n#| warning: false\nax = sns.displot(fsdf, x=\"v2x_egaldem\", kind = \"ecdf\", \n                 hue=\"e_boix_regime\") # palette=grayscale_palette\nsns.despine()\nax.set(xlabel='Egalitarian Democracy Index')\nplt.xlim(0, 1) \nplt.savefig('figures/07-14.png', dpi=300)\n\n\n\n\n\n\nFigure 10.15: caption…\n\n\n\nWe can see from slope for e_boix_regime = 0.0 (non-democratic countries) that observations are very clustered towards the lower values of v2x_egaldem, and they very rapidly taper off. The maximum value can be found around v2x_egaldem = 0.5. Conversely, the slope for e_boix_regime = 1.0 (democratic countries) is more gradual, telling us that the distribution is much more evenly distributed across all values, with a few spikes where the slope increases around 0.55 and 0.75. If we look back at the histograms, we can confirm that the visualizations agree with one another.\n\n\n\n10.4.2 VISUALIZING JOINT DISTRIBUTIONS\nWhile the marginal and conditional distributions of the earlier section are useful for understanding the shape of our data, we often want to examine the relationships between variables. We can visually represent these relationships by visualizing joint distributions. Let’s first look at two of the most common ways of visualizing joint distributions: cross tables for categorical variables and scatter plots for continuous variables.\n\n10.4.2.1 Cross Tables\nWhen we want to visualize the joint distribution of two categorical variables we can produce a cross-table: an extension of a frequency table. A cross table, sometimes shortened to crosstab, shows a grid, with the possible values of one categorical variable along one axis and the same for the other variable along the other axis. Each cell in the grid shows the number of observations that have both (hence joint) categorical variable values corresponding to its row and column.\nLet’s create a cross table using the .crosstab() function for the categorical variables e_regiongeo and e_boix_regime. The resulting table will tell us how many observations (country-year combinations) there are of each regime type for each geographic region.\nct = pd.crosstab(fsdf.e_regiongeo, fsdf.e_boix_regime)\nct.columns = ['Not democracies', 'Democracies']\nct\nIf we want to know how many non-democratic countries there were in Western Europe across all years in the VDEM dataset, we would look at the corresponding row value for the “Not democracies” column, which shows 110. The number of democratic observations, 703, is shown in the “Democracies” column.\n\n\n10.4.2.2 Scatter Plots\nA scatter plot shows the relationship between two variables by plotting each observation on a graph, where the x-axis represents the value of one variable, while the y-axis represents the value of the other. Let’s use the scatterplot function to plot the relationship between the egalitarian democracy and polyarchy indices. The code below produces Figure 10.16.\n#| output: false\n#| warning: false\nax = sns.scatterplot(data = fsdf, x=\"v2x_egaldem\", y=\"v2x_polyarchy\")\nsns.despine()\nax.set(xlabel='Egalitarian Democracy Index', ylabel='Polyarchy Index')\nplt.xlim(0, 1) \nplt.ylim(0, 1) \nplt.savefig('figures/07-15.png', dpi=300)\n\n\n\n\n\n\nFigure 10.16: caption…\n\n\n\nAt first glance, this seems to suggest a non-linear, but definitely positive relationship between the two variables. If an observation has a high value for one of the two variables, the other is also high. At lower values, the relationship is a bit less tight where we can see the points are less densely clustered, but the overall pattern remains visible. We can use different alpha (transparency) values to see where overlapping points are obscuring underlying points. The code below produces Figure 10.17.\n#| output: false\n#| warning: false\nax = sns.scatterplot(data = fsdf, x=\"v2x_egaldem\", y=\"v2x_polyarchy\", alpha = 0.1)\nsns.despine()\nax.set(xlabel='Egalitarian Democracy Index', ylabel='Polyarchy Index')\nplt.xlim(0, 1) \nplt.ylim(0, 1) \nplt.savefig('figures/07-16.png', dpi=300)\n\n\n\n\n\n\nFigure 10.17: caption…\n\n\n\nThe code below produces Figure 10.18.\n#| output: false\n#| warning: false\nax = sns.scatterplot(data = fsdf, x=\"v2x_egaldem\", y=\"v2x_polyarchy\", alpha = 0.01)\nsns.despine()\nax.set(xlabel='Egalitarian Democracy Index', ylabel='Polyarchy Index')\nplt.xlim(0, 1) \nplt.ylim(0, 1) \nplt.savefig('figures/07-17.png', dpi=300)\n\n\n\n\n\n\nFigure 10.18: caption…\n\n\n\n\n\n10.4.2.3 Bivariate Histograms\nWe can extend the idea of histograms to bivariate visualizations. To do so, we divide both the x and y-axes into bins, producing square bins. The square bins are coloured based on the number of observations within each box. Since our y-axis is now being used to define another dimension of the box, we use colour instead of bar height to indicate how densely observations are clustered within a bin. The code below produces Figure 10.19.\n#| output: false\n#| warning: false\nax = sns.displot(fsdf, x=\"v2x_egaldem\", y=\"v2x_polyarchy\")\nsns.despine()\nax.set(xlabel='Egalitarian Democracy Index', ylabel='Polyarchy Index')\nplt.xlim(0, 1) \nplt.ylim(0, 1) \nplt.savefig('figures/07-18.png', dpi=300)\n\n\n\n\n\n\nFigure 10.19: caption…\n\n\n\nUpon inspection, the majority of observations can be found clustering around low values of v2x_polyarchy and v2x_egaldem. There appears to be another, less dense cluster at the top right portion of the distribution as well.\nLike univariate histograms, we can refine our visualization by explicitly setting parameter values, like binwidth. We can even provide a further layer of information by including a rug plot, which acts like a one-dimensional scatterplot for each axis. For visualizations with lots of data, use of alpha values to avoid solid black bars like below. The code below produces Figure 10.20.\n#| output: false\n#| warning: false\nax = sns.displot(fsdf, x=\"v2x_egaldem\", y=\"v2x_polyarchy\", binwidth = 0.01, rug=True)\nsns.despine()\nax.set(xlabel='Egalitarian Democracy Index', ylabel='Polyarchy Index')\nplt.xlim(0, 1) \nplt.ylim(0, 1) \nplt.savefig('figures/07-19.png', dpi=300)\n\n\n\n\n\n\nFigure 10.20: caption…\n\n\n\n\n\n10.4.2.4 Bivariate Kernel Density Estimation\nLike histograms, we can also use kernel density estimation (KDE) to smooth the bins to give an idea of the underlying probability distribution. In this case, it takes the form of a 2D contour plot. Let’s also improve the appearance of the rug plot by providing rug_kws with a dictionary describing values for its parameters: in this case setting an alpha of 0.01. The code below produces Figure 10.21.\n#| output: false\n#| warning: false\nax = sns.displot(fsdf, x=\"v2x_egaldem\", y=\"v2x_polyarchy\", kind=\"kde\", rug = True, rug_kws = {\"alpha\": 0.01})\nsns.despine()\nax.set(xlabel='Egalitarian Democracy Index', ylabel='Polyarchy Index')\nplt.savefig('figures/07-20.png', dpi=300)\n\n\n\n\n\n\nFigure 10.21: caption…\n\n\n\nIf you are familiar with reading contour maps, many closely spaced lines indicate areas of rapid change in elevation, or in this case, density. The slopes of our ECD plots are very analogous to the slopes described by contour plots. Looking at the above visualization, we can see two major clusters: one at the lower left and a smaller, but still significant one in the upper right.\n\n\n10.4.2.5 Line of Best Fit\nThe visualizations in this section try to strike a balance between showing as much of the data as possible while conveying the relationship between the two variables as simply as possible. These two goals are in conflict with each other. The more data you visualize, the more complicated your plots become. If we want to display the relationship between two variables as simply as possible, we can produce a line of best fit. A line of best fit is a straight line that tries to minimize the distance between itself and each observation.\nThe concept is very closely tied to regression techniques, which we will discuss in later chapters. For now, we can take for granted that a line of best fit is a way of mathematically describing a linear relationship between two variables. Consider the earlier scatter plot. While we can see some kind of postive relationship between the two variables, a scatter plot only provides a qualitative description of that relationship. A line, on the other hand can be described exactly by a formula:\n\\[\ny = mx + b\n\\]\nIt is a common practice to include a line of best fit on a scatter plot. Scatter plots display the data in great detail, while the line of best fit describes the relationship within the data very concisely. Together, the two provide a lot of information about the data. Let’s reproduce the earlier scatter plot with a line of best fit. To do this, we will use the regplot function. The code below produces Figure 10.22.\n#| output: false\n#| warning: false\nax = sns.regplot(data = fsdf, x = \"v2x_egaldem\", y = \"v2x_polyarchy\", color='darkgray', scatter_kws = {\"alpha\": 0.05}, line_kws={\"color\": \"black\"})\nsns.despine()\nax.set(xlabel='Egalitarian Democracy Index', ylabel='Polyarchy Index')\nplt.savefig('figures/07-21.png', dpi=300)\n\n\n\n\n\n\nFigure 10.22: caption…\n\n\n\n\n\n\n10.4.3 Correlation\nYou likely have an idea of what correlation is about, but it’s important to get these things right. Firstly and very generally, correlation is a measure of how closely random variables are related to one another, or how linearly dependent they are. What does that mean?\nIf a variable is closely related to another (there is a high degree of dependence between them) we can use an observation of one to predict the other. For example, a person’s weight in kilograms is perfectly correlated/dependent on their weight in pounds. If you know one, you also know the other. The fuel economy of a car (usually expressed in L/100km or mpg) and the distance it traveled on a full tank are strongly correlated variables, though not perfect. Other factors can influence the distance traveled, but knowing the fuel economy of a car lets you make an accurate guess.\nMore rigorously, Correlation describes the standardized direction of a linear relationship between variables as well as its strength. Correlation values, or correlation coefficients, range between -1 and +1. A coefficient of 1 represents a perfectly linearly dependent relationship for any two variables. A coefficient of -1 also represents a perfectly linearly dependent relationship, but in the opposite direction. For example, the number of eggs in a standard 12-pack egg carton is linearly dependent on the number of eggs I removed from it. The difference is that, with weight in lbs and weight in kilograms, when one goes up, the other also goes up. Hence, a positive linear relationship. With eggs in the carton and eggs removed from the carton, when one goes up, the other necessarily goes down: a negative linear relationship. In both cases, if you know one value, you also know the other.\nLet’s calculate some correlations. To do so, we call .corr() on a variable and pass another of equal length as an argument.\ncorr_libdem_partipdem = fsdf.v2x_libdem.corr(fsdf.v2x_partipdem)\ncorr_libdem_year = fsdf.v2x_libdem.corr(fsdf.year)\n\nprint(f'Correlation of v2x_libdem and v2x_partipdem: {corr_libdem_partipdem}')\nprint(f'Correlation of v2x_libdem and year: {corr_libdem_year}')\nNote that here we access dataframe columns by name using ‘dot notation’ rather than the square brackets we used earlier. Both methods see frequent use, so it’s a good idea to get used to seeing them as largely interchangeable.\nWhile the math behind correlation coefficients is beyond the scope of this chapter, it’s useful to have an idea of what I mean when I say that it is standardized and what a linear relationship actually means. To demonstrate, let’s create a new variable that is just year multiplied by 100 and correlate that with v2x_libdem like we did in the previous cell.\ndf_new = fsdf.copy()\ndf_new['year_x100'] = fsdf['year'].apply(lambda x: x*100)\n\nnew_corr_libdem_partipdem = df_new.v2x_libdem.corr(df_new.v2x_partipdem)\nnew_corr_libdem_year = df_new.v2x_libdem.corr(df_new.year_x100)\n\nprint(f'Correlation of v2x_libdem and v2x_partipdem: {new_corr_libdem_partipdem}')\nprint(f'Correlation of v2x_libdem and year*100: {new_corr_libdem_year}')\nThe correlation remains the same, despite multiplying one of the variables by 100. This is because the correlation coefficient is defined so that its value will always be between -1 and 1: it is actually another measure (covariance, which we will discuss in another chapter) that has been standardized to those values. As for linear relationships, recall the line of best fit in a scatter plot. If we plot observations of two variables, the strength of the linear relationship between the two is how closely those points lie on a straight line. If you can draw a straight line through every single point, the two variables have a perfect linear relationship. When we multiplied year by 100, all we did was change the angle of the line; we did not change the direction of the relationship, or the strength of it, so the coefficient remains the same.\n\n10.4.3.1 Correlation Coefficient: Pearson and Spearman\nJust as there are different kinds of data, there are different ways of calculating correlation coefficients. We have been using the default Pearson correlation. A full discussion of different correlation coefficients is beyond our scope, so I will briefly mention one of the most common alternatives to Pearson correlation. Spearman correlation is intended for use with rank data, where a variable has some order, but the distance between values is not necessarily consistent. Consider the placings of runners in a race. We know that the first-place runner finished before second-place, but we don’t know by how much. The distance between first and second could be very different than the distance between second and third place. To calculate the Spearman rank order correlation coefficient in Pandas, we simply provide the .corr() function with the appropriate method parameter.\n\n\n10.4.3.2 Correlation Matrices and Heatmaps\nSometimes we want to explore our data to see if there are any strong or weak relationships that we might not expect. We could calculate a correlation coefficient for each pair of variables in our data, but it’s more efficient to create a correlation matrix and visualize the results. A correlation matrix shows the correlation coefficient for every pairing of variables in a given data set. Since every variable is being compared to every other variable, we will get an \\(n \\times n\\) table, where \\(n\\) is the number of variables.\nTo create a correlation matrix, we need to select only the appropriate columns. Since we’re using Pearson correlation, that will be continuous variables. Previously, we used the .corr() method to calculate the correlation between two columns. If we provide it a dataframe without specifying any columns, it will produce a correlation matrix with all possible pairings.\nfsdf_corr = fsdf[['v2x_polyarchy', 'v2x_libdem', 'v2x_partipdem', 'v2x_delibdem', 'v2x_egaldem']].corr()\n\nfsdf_corr\nNote the diagonal line of 1s arranged from top left to bottom right. Any variable correlated with itself should provide a coefficient of 1: a variable will always be perfectly associated with itself. Secondly, the coefficients are mirrored across the diagonal. The coefficient for the v2x_polyarchy row and the v2x_libdem column is the same as the one for the v2x_libdem row and the v2x_polyarchy column. Recall that Pearson correlation is commutative, so \\(corr(X,Y) = corr(Y,X)\\).\nWe can also create a heatmap to help us scan the data quickly for correlations that especially stand out. The code below produces Figure 10.23. Unlike me, you have more color options. I would recommend using a color palette other than grayscale.\n#| output: false\n#| warning: false\nax = sns.heatmap(\n    data = fsdf_corr, vmin = 0.9, vmax = 1 #, cmap=grayscale_cmap\n)\n\nplt.savefig('figures/07-22.png', dpi=300)\n\n\n\n\n\n\nFigure 10.23: caption…\n\n\n\nFirst, note the scale. I have disregarded my own guideline that scales should start at 0. This is because the values are highly clustered at top of the range, and what I want to learn is which correlations stand out from the others. If I included the whole scale, each box would be a nearly identical colour. Guidelines can help you make decisions about visualizations, but they can’t replace thinking about what you’re doing and why.\nTo create a heatmap that doesn’t duplicate data and is less cluttered, we can use a “mask” to cover up the upper triangle. The code below produces Figure 10.24.\n#| output: false\n#| warning: false\nmask = np.triu(np.ones_like(fsdf_corr, dtype = bool))\nax = sns.heatmap(fsdf_corr, mask = mask, vmin = 0.9, \n                 vmax = 1) # cmap=grayscale_cmap\nplt.savefig('figures/07-23.png', dpi=300)\n\n\n\n\n\n\nFigure 10.24: caption…",
    "crumbs": [
      "**DISCOVERY**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploring with purpose</span>"
    ]
  },
  {
    "objectID": "exploratory-data-analysis.html#visualization-with-more-informational-density",
    "href": "exploratory-data-analysis.html#visualization-with-more-informational-density",
    "title": "10  Exploring with purpose",
    "section": "10.5 Visualization with More Informational Density",
    "text": "10.5 Visualization with More Informational Density\nI’ve mentioned that good visualization practice involves thinking about what to display. That requires exercising restraint regarding what you add to a graph. Less is usually, but not always, more. We often find ourselves in situations where the best option is to layer several simple visualizations together in ways that substantially increase the overall information density of a visualization while still keeping the parts and the whole as simple as possible. This is often preferable to, for example, generating multiple simple graphs that you, or a reader would have to consult simultaneously to learn from. Again, think about what exactly you are trying to learn or show, and construct a visualization for that purpose and no other.\nThe two primary examples of more informationally-dense visualizations we will consider here, and which we will use in various parts of the books, are graphs that combine marginal and joint distributions, and small multiples and pair plots. We will only briefly introduce these visualizations here. In later parts of the book, you will see many examples of using and refining them in specific data analysis contexts.\n\n10.5.1 Layering Marginal and Joint Distributions\nPreviously, we have occasionally combined different distributions or visualizations in the same plot. We plotted conditional histograms together, histograms with KDEs, scatter plots with lines of best fit, and heat maps and contour maps with rug plots. Lets focus on supplementing our scatterplot with line of best fit. Rather than the rug plots we used earlier, let’s include a histogram with a KDE to easily visualize the marginal distributions of each variable to complement the joint distribution. The code below produces Figure 10.25.\n#| output: false\n#| warning: false\nax = sns.jointplot(\n    data=fsdf, \n    x=\"v2x_polyarchy\", \n    y=\"v2x_egaldem\", \n    kind=\"reg\", \n    color='darkgray',\n    joint_kws={\n        'line_kws':{'color':'black'},\n        'scatter_kws':{'alpha':0.03}\n    }\n)\n\nplt.savefig('figures/07-24.png', dpi=300)\n\n\n\n\n\n\nFigure 10.25: caption…\n\n\n\nThe above visualization provides us with a wealth of information, letting us see individual points as well as the relationship between the two variables with the joint distribution. The marginal distributions show us the observed distributions and an estimate of the underlying probability distribution.\n\n\n10.5.2 Quick Comparisons with Pair Plots\nWhen we get an entirely new data set, we often want to perform some exploratory analysis to get a sense of the data and potential relationships that we may not even be aware of. Rather than choosing which variables to plot against each other, we can cover all of our bases and use the pairplot function to plot all variables against each other. This will produce a grid of scatter plots, pairing every variable against every other variable. Where a variable would be paired with itself (the diagonal of the grid), we instead get a histogram of the marginal distribution of that variable. A pairplot is a great way to get a sense of your data before letting theory and research questions guide more focused analysis and visualization. The code below produces Figure 10.26.\n#| output: false\n#| warning: false\nhigh_level_indexes = ['v2x_polyarchy', 'v2x_libdem', 'v2x_partipdem', 'v2x_delibdem', 'v2x_egaldem']\nax = sns.pairplot(fsdf[high_level_indexes])\nplt.savefig('figures/07-25.png', dpi=300)\n\n\n\n\n\n\nFigure 10.26: caption…\n\n\n\nIn later chapters, we will often produce “small multiples” that have a structured subplot design like the pairplots. We will set that aside until later, however.\n\nFurther Reading\nIn addition to being a good introduction to Pandas and Numpy, VanderPlas’ (2016) Python Data Science Handbook contains a good introduction to matplotlib, which Seaborn is built on top of. Regrettably I don’t have space to get into matplotlib in this book, but it’s very powerful package. We use little bits of matplotlib code to supplement Seaborn in this book. If you want to take more control over the aesthetics of your plots, it’s well worth learning more about matplotlib.",
    "crumbs": [
      "**DISCOVERY**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploring with purpose</span>"
    ]
  },
  {
    "objectID": "exploratory-data-analysis.html#conclusion",
    "href": "exploratory-data-analysis.html#conclusion",
    "title": "10  Exploring with purpose",
    "section": "10.6 CONCLUSION",
    "text": "10.6 CONCLUSION",
    "crumbs": [
      "**DISCOVERY**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploring with purpose</span>"
    ]
  },
  {
    "objectID": "exploratory-data-analysis.html#key-points",
    "href": "exploratory-data-analysis.html#key-points",
    "title": "10  Exploring with purpose",
    "section": "10.7 Key Points",
    "text": "10.7 Key Points\n\nThis chapter introduced the concept of Exploratory Data Analysis alongside Box’s Loop and Iterative research workflows\nWe explored some basic guidelines for creating effective visualizations\nWe then applied those guidelines to univariate and multivariate data drawn from a (very large!) real-world dataset\n\n\n\n\n\n\nGelman, Andrew. 2004. “Exploratory Data Analysis for Complex Models.” Journal of Computational and Graphical Statistics 13 (4): 755–79.\n\n\nHealy, Kieran. 2018. Data Visualization: A Practical Introduction. Princeton University Press.\n\n\nHealy, Kieran, and James Moody. 2014. “Data Visualization in Sociology.” Annual Review of Sociology 40: 105–28.\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential Tools for Working with Data. \" O’Reilly Media, Inc.\".",
    "crumbs": [
      "**DISCOVERY**",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploring with purpose</span>"
    ]
  },
  {
    "objectID": "association-and-latent-variables.html",
    "href": "association-and-latent-variables.html",
    "title": "11  Latent factors",
    "section": "",
    "text": "11.1 Imports and Data Preparation\nWe will make extensive use of the Sklearn package in this chapter. Sklearn is very important and widely-used in machine learning, and it features heavily in the rest of this book. Some of the methods we introduce here are also implemented in other Python packages, including statsmodels, which implements a wide-variety of statistical models. If it suits your needs better, you should feel free to use statsmodels instead.\nWe will continue working with the VDEM data in this chapter, filtered to contain observations from 2019 only.\nNow that we have the VDEM data from 2019 loaded up, we can select the columns we will use in our analyses. In this case, we want the country name as well as a series of variables related to political deliberation, civil society, media and internet, private and political liberties, and the executive. The specific variables we will use in each of these categories are given in Table 11.1. Given that I don’t have space to discuss each variable (there are 35 of them in total), I recommend that you consult the VDEM codebook to ensure you know what each represents.\nWe will create a list of these indicator variables names that we can use to subset the larger dataframe.\nWe can now subset the original dataframe so that it includes only these variables, and then use the country names as the dataframe index.\nThe resulting dataframe has 179 observations (each one a country in 2019) and our 35 variables. Before moving on, we can do a quick check to see whether we have any problems with missing data. The code below counts the number of variables with missing (1) and non-missing (0) data. None of our variables have missing data.\nAll we need to finish preparing our data is to get our indicator variables into a Numpy array. We will do some additional cleaning a bit later in the chapter.",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Latent factors</span>"
    ]
  },
  {
    "objectID": "association-and-latent-variables.html#imports-and-data-preparation",
    "href": "association-and-latent-variables.html#imports-and-data-preparation",
    "title": "11  Latent factors",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nimport random\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, silhouette_samples\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom dcss import set_style\nset_style()\n\n# Data downloaded in Chapter 6\ndf = pd.read_csv(\n    'data/vdem/V-Dem-CY-Full+Others-v10.csv', low_memory=False\n) \n\ndf = df.query('year == 2019').reset_index()\ndf.shape\n\n\n\n\nTable 11.1: Table: VDEM variables used in this chapter.\n\n\n\n\n\n\n\n\n\n\n\n\nDeliberation\nCivil Society\nMedia & Internet\nPrivate & Political Liberties\nThe Executive\n\n\n\n\nv2dlreason\nv2cseeorgs\nv2mecenefm\nv2cldiscm\nv2exrescon\n\n\nv2dlcommon\nv2csreprss\nv2mecenefi\nv2cldiscw\nv2exbribe\n\n\nv2dlcountr\nv2cscnsult\nv2mecenefibin\nv2clacfree\nv2exembez\n\n\nv2dlconslt\nv2csprtcpt\nv2mecrit\nv2clrelig\nv2excrptps\n\n\nv2dlengage\nv2csgender\nv2merange\nv2clfmove\nv2exthftps\n\n\nv2dlencmps\nv2csantimv\nv2mefemjrn\n\n\n\n\nv2dlunivl\nv2csrlgrep\nv2meharjrn\n\n\n\n\n\nv2csrlgcon\nv2meslfcen\n\n\n\n\n\n\nv2mebias\n\n\n\n\n\n\nv2mecorrpt\n\n\n\n\n\n\n\n\n\nindicators = [\n    'v2dlreason', \n    'v2dlcommon', \n    'v2dlcountr', \n    'v2dlconslt', \n    'v2dlengage',\n    'v2dlencmps', \n    'v2dlunivl', \n    'v2cseeorgs', \n    'v2csreprss', \n    'v2cscnsult',\n    'v2csprtcpt', \n    'v2csgender', \n    'v2csantimv', \n    'v2csrlgrep', \n    'v2csrlgcon',\n    'v2mecenefm', \n    'v2mecenefi', \n    'v2mecenefibin', \n    'v2mecrit', \n    'v2merange',\n    'v2mefemjrn', \n    'v2meharjrn', \n    'v2meslfcen', \n    'v2mebias', \n    'v2mecorrpt',\n    'v2exrescon', \n    'v2exbribe', \n    'v2exembez', \n    'v2excrptps', \n    'v2exthftps',\n    'v2cldiscm', \n    'v2cldiscw', \n    'v2clacfree', \n    'v2clrelig', \n    'v2clfmove'\n]\n\ncountries = df['country_name'].tolist()\ndf = df.set_index('country_name')[indicators]\ndf.shape\n\ndf.isna().sum().value_counts()\n\nX = df.to_numpy()",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Latent factors</span>"
    ]
  },
  {
    "objectID": "association-and-latent-variables.html#latent-variables-and-the-curse-of-dimensionality",
    "href": "association-and-latent-variables.html#latent-variables-and-the-curse-of-dimensionality",
    "title": "11  Latent factors",
    "section": "11.2 LATENT VARIABLES AND THE CURSE OF DIMENSIONALITY",
    "text": "11.2 LATENT VARIABLES AND THE CURSE OF DIMENSIONALITY\nBefore getting into “latent variables,” let’s clear up a bit of terminology. In this chapter, and many that follow, we’ll talk about the “dimensions” and “dimensionality” of dataframes and matrices. All of this talk of “dimensions” is really just about the number of variables we are using. If we have 10 variables, we have 10 dimensions; 147,002 variables, 147,002 dimensions. When we have a lot of variables, we often describe our dataset as “high-dimensional.”\nHigh-dimensional datasets pose all sorts of problems for statistical and machine learning models that low-dimensional datasets do not. That’s why we refer to this situation as the curse of dimensionality even if it might seem like an embarrassment of riches. Typically, we reduce the number of variables we are working with by manually selecting the variables of interest, or by performing some sort of dimensionality reduction on the dataset that mitigates problems associated with the curse of dimensionality. Below, you will learn about two different but related approaches to dimensionality reduction, one driven by theory and measurement, the other by data-driven induction.\n\n11.2.1 Theory First: Measuring Latent Variables with Exploratory Factor Analysis\nWe social scientists spend a huge amount of time trying to measure things that we can’t directly observe. If you take a moment, you can probably list dozens of such things: political ideology, religiosity, well-being, job satisfaction, social capital, opportunity costs, social anxiety, confirmation bias, populism, introversion, personality types, emotional labour, resource mobilization, and so on. In earlier chapters of this book we spent a fair amount of time working with five variables that measure things we can’t directly observe:\n\na country-level measure of the principle of electoral democracy (polyarchy),\na country-level measure of the principle of liberal democracy,\na country-level measure of the principle of participatory democracy,\na country-level measure of the principle of egalitarian democracy, and\na country-level measure of the principle of deliberative democracy.\n\nWhen I say that none of these five “principles” of democracy can be directly observed, I mean that quite literally: you can’t look at a country and eyeball the amount of deliberative democracy you see, as if it had material form. That’s because deliberative democracy is not a material thing in the world, it’s an abstract concept developed by social scientists and political philosophers in the context of theoretical debate and applied research. Because these are unobservable abstract concepts, we call the variables that represent them latent or hidden.\nWhen we develop theoretical concepts, we can’t assume other researchers share our meanings, or that our meaning is somehow self-evident. Instead, we put in quite a lot of work to ensure conceptual clarity, as that is the only way to advance the state of our collective knowledge. These abstract concepts are essential in the quest to advance collective knowledge, but to treat them as if they were “real” material things, and not theoretical constructs, is a mistake called reification.\nAbstract concepts, like the principles of democracy above, are meaningful both in our everyday lives and in relation to broader theories and paradigms, but because they can’t be directly observed, they are difficult to measure empirically. This requires us to adopt measurement strategies that combine careful reasoning and logic with measurement models that we carefully and transparently validate. The first step in this process is specification, which involves developing conceptual and operational definitions of concepts. Specification is essential because it helps ensure we are talking about the same thing; in social science, as in everyday life, you can’t get very far just making up your own definitions of things, however good those definitions might be.\nA conceptual definition involves defining an abstract concept in relation to other concepts whose meaning is more widely shared, usually by breaking it down into more concrete aspects or dimensions. For example, my friend and colleague Igor Grossmann conducts fascinating psychological research on wisdom. What exactly, is “wisdom,” and how does one study it scientifically? Even something as grand and abstract as “wisdom” can be specified in terms of concrete dimensions that, together, speak to the more general concept. For example, as Brienza et al. (2018) propose, wisdom can be represented by the extent to which people demonstrate intellectual humility, recognition of uncertainty and change, consideration of the broader context at hand and perspectives of others, and the integration of these perspectives/compromise in specific situations.\nThe various dimensions that are specified in the conceptual definition of wisdom are, of course, other concepts. The difference between an operational definition and a conceptual one is that an operational definition describes the specific operations that one would have to perform to generate empirical observations (i.e., data) for each of the various dimensions. The variables that contain data on these specific dimensions are typically called indicator variables. Operational definitions involve specifying things like the level at which to measure something, the type of variables to use (e.g. ordinal, interval, ratio, categorical), the range of variation those variables should have, and so on. A good operational definition of a concept enables one to measure the concept by measuring the concept’s dimensions with a high degree of reliability and validity, and then aggregating the measures of specific dimensions into a measure of the abstract concept that also has high reliability and validity. In the case of measuring wisdom, for example, Brienza et al. (2018) outline an explicit measurement strategy that attempts to mitigate social desirability biases (which inevitably come into play when you ask people about the wisdom of their reasoning) by assessing how people respond to specific scenarios. They provide an online supplement that includes the specific survey instruments used to collect the data according to the operational definitions laid out in their paper.\nLet’s return to our abstract “principles of democracy.” The principle of electoral democracy, for example, is represented by the five dimensions listed below. The set of accompanying questions come straight from the VDEM codebook (Coppedge et al. 2020).\n\nFreedom of expression: “To what extent does government respect press and media freedom, the freedom of ordinary people to discuss political matters at home and in the public sphere, as well as the freedom of academic and cultural expression?” (variable: v2x_freexp_altinf)\nFreedom of association: “To what extent are parties, including opposition parties, allowed to form and to participate in elections, and to what extent are civil society organizations able to form and to operate freely?” (variable: v2x_frassoc_thick)\nShare of adult citizens with suffrage: “What share of adult citizens (as defined by statute) has the legal right to vote in national elections?” (variable: v2x_suffr)\nFree and fair elections: “To what extent are elections free and fair?” (variable: v2xel_frefair)\nOfficials are elected: “Is the chief executive and legislature appointed through popular elections?” (variable: v2x_elecoff)\n\nEach dimension is a bit more concrete than “electoral democracy,” but for the most part, we still can’t directly observe these dimensions. Perhaps you noticed that some contain multiple questions! The first dimension, for example, contains several questions about freedom of the press and media, freedom of ordinary people, and freedom of academic and cultural expression. In this case, each of the five dimensions that make up the higher-level measure of electoral democracy are called indices, which is a type of measure that is constructed by combining the values of lower-level indicator variables.\nFor example, the freedom of expression dimension represented by the index variable v2x_freexp_altinf is constructed from the values of the variables government censorship effort (v2mecenefm), harassment of journalists (v2meharjrn), media self-censorship (v2meslfcen), freedom of discussion (v2xcl_disc), freedom of academic and cultural expression (v2clacfree), levels of media bias (v2mebias), how critical the media is (v2mecrit), and the diversity of perspectives promoted by the media (v2merange). These lower-level indicators are easier to observe than the higher level index variables above them, or the even higher still indices representing types of democracies. If you want to learn more about the conceptual and operational definitions of these principles of democracy, as well as the specific measurement models used, you can consult Coppedge et al. (2020 CITE).\nThe difference between these indices and indicator variables maps directly back to the process of specification; the variables we use to record observations about the specific dimensions of concepts are indicator variables because they indicate part of the concept, and the overall concept is measures by combining the values for those indicators into an index. Indices are composite measures because they are created by systematically and transparently combining multiple indicators.\nWhen we want (or need) to measure something really big and abstract like a concept that is part of a larger theory (e.g. the amount of deliberative democracy that we see in a given country at some point in time), we break the big abstract concept down into various different dimensions, and sometimes we break those dimensions down into even smaller ones. The measures for the higher-level concepts are indices constructed by combining the values of lower-level indicator variables.\nThis general idea is sketched out in Figure 11.1 below, with example indicator variables on the top feeding into mid-level index measures for latent concepts (in gray), which in turn feed in to the high-level index measures of the latent concept of the principle of electoral democracy, or “polyarchy” (also in gray). The ...s are meant to emphasize that there are other indicators that feed into the mid-level indices in addition to those shown here.\n\n\n\n\n\n\nFigure 11.1: cap\n\n\n\nWhen trying to measure latent variables, degree of electoral democracy or freedom of association, we typically perform some sort of factor analysis that tells us whether the indicators we observed and measured (e.g. the share of adult citizens with voting rights and the power of the Head of State relative to the Head of Government) are indeed likely to reflect some underlying “factor.”\nA “factor” is simply a subset of highly correlated variables that have been combined into a single composite variable. If v2x_freexp_altinf, v2x_frassoc_thick, v2x_suffr, v2xel_frefair, and v2x_elecoff are all highly correlated with one another (and not strongly correlated with other variables), it might be because they are all indicating different dimensions of the same underlying latent construct: electoral democracy. The factor analysis lets us take a larger set of variables, of which some are highly correlated with one another, and reduce them to a smaller subset of explanatory factors. Depending on the type of factor analysis you conduct, those factors may or may not be correlated with one another, but usually they are not.\nWhen we conduct a factor analysis, we also compute factor loadings that clarify the relationship between each of the original variables in our analysis and the underlying factors extracted in the analysis. Variables that are strongly associated with the latent factor contribute more to that factor, and hence have higher loading. The specific factor loadings we can compute vary a bit depending on how we are approaching things. If we assume that the latent variables might be at least somewhat correlated with one another (which is a very reasonable assumption!), then we compute two sets of factor loadings, one being the Pearson correlation coefficients between the variables and the latent factors (a “structure matrix”) and one being coefficients from a linear regression (a “pattern matrix”).\nIf we assume that the latent variables are not correlated with one another (rarely a reasonable assumption, but it has its place), then there is only one set of factor loadings (either the correlation coefficients or the regression coefficients, which in this scenario would be the same). These loading scores are often “rotated” to help make them more substantively interpretable. Though we won’t discuss them here, the type of rotation you perform depends on whether you think the factors are correlated with one another. If you suspect they are at least somewhat correlated with one another, then you would use an oblique rotation, and if you suspect they aren’t, you whould choose an orthogonal rotation.\nFactor loadings describe how specific variables (e.g., government intimidation of the opposition) contribute to a latent factor. Factor scores, on the other hand, tell us how specific observations (e.g., the United States of America in 2020) score on a given latent factor (e.g., electoral democracy). You can probably see where I’m going with this: if the latent factors represent meaningful variables that we want to measure but can’t observe, then the factor scores that describe how an observation is related to that latent factor is the measurement of that observation for that latent factor. For example, on the egalitarian democracy measurement variable in 2019, Namibia scored 0.453, Egypt scored 0.118, France scored 0.773, North Korea scored 0.096, Vanuatu scored 0.566, Senegal scored 0.517, Canada scored 0.776, and Ukraine scored 0.316. Where did these numbers come from? The egalitarian democracy variable is a latent index variable constructed from several other indices, which are in turn constructed from more concrete low-level indicators. The latent variables and the individual country scores are mathematically constructed using factor analysis.\nIn the interest of space, we will not actually conduct a theory-oriented factor analysis in this chapter. Instead, we will focus on a different approach that is more inductive and data-driven: Principal Components Analysis (PCA).\n\nFurther Reading\nChapter 13 from Barbara Tabachinick and Linda Fidell’s (2007) Using Multivariate Statistics and Chapter 17 of Field, Miles, and Field (2012) Discovering Statistics Using R both provide a good introduction to exploratory factor analysis and PCA as widely-practices in the social and cognitive sciences. Chapter 8 of G{'e}ron’s (2019) Hands-on Machine Learning and Chapter 3 of M{\"u}ller and Guido’s (2016) Introduction to Machine Learning with Python provide a good introduction to “dimensionality reduction” in machine learning.",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Latent factors</span>"
    ]
  },
  {
    "objectID": "association-and-latent-variables.html#conducting-a-principal-component-analysis-in-sklearn",
    "href": "association-and-latent-variables.html#conducting-a-principal-component-analysis-in-sklearn",
    "title": "11  Latent factors",
    "section": "11.3 CONDUCTING A PRINCIPAL COMPONENT ANALYSIS IN SKLEARN",
    "text": "11.3 CONDUCTING A PRINCIPAL COMPONENT ANALYSIS IN SKLEARN\n\n11.3.1 Standardization\nWe did most of the necessary pre-processing at the start of the chapter when we imported our data, filtered the rows, selected the relevant columns, and then converted the data to a Numpy ndarray, which is a nice way of storing matrix data. There is, however, one very important piece of pre-processing that we need to do before we conduct a PCA: scaling our variables via z-score normalization, or “standardization.”\nRemember, PCA reduces the dimensionality of a dataset by constructing “principle components” from highly correlated features. If the variance contained in any one component differs from the variance contained in another because of the scales for the features that contribute to it, then PCA will make consequential mistakes. In short, PCA is heavily impacted by feature scaling. To prevent any such issues, we can use Sklearn’s StandardScaler(), which performs z-score normalization on each feature. The z-score normalization ensures we are comparing things on the same scales.\nX = StandardScaler().fit_transform(X) \nMany statistical and machine learning models require standardization. If you need a refresher, you can consult the sub-section below. Otherwise you are free to skip over it. I recommend consulting the documentation whenever you use a model with which you are unfamiliar.\n\n11.3.1.1 A Brief Refresher on Variance, Standard Deviation, and Z-score Normalization\nFirst, let’s very briefly revisit the concepts of variance and standard deviation. Variance is a statistical measure of how spread out or clustered the values in a data set are. More specifically, it’s a measure of how far each value is from the mean. Variance is usually represented with the symbol \\(\\sigma^2\\). A larger variance means that the values are more spread out, while a smaller variance means that they are more clustered around the mean. Let’s use some very simple examples to see how this works.\nABCD = {\n    'A': [1, 1, 1, 1, 1], # no variance...\n    'B': [1, 2, 3, 4, 5], # some variance...\n    'C': [-1, 1, 3, 5, 7], # a bit more variance...\n    'D': [-10, -9, 3, 4, 4] # still more variance...\n}\n\nfor k, v in ABCD.items():\n    print(f'{k} has a variance of {np.round(np.var(v), 3)}.')\nThe standard deviation of a data set is the square root of the variance (\\(\\sigma^2\\)), and is therefore represented with the symbol \\(\\sigma\\).\nfor k, v in ABCD.items():\n    print(f'{k} has a standard deviation of {np.round(np.std(v), 3)}.')\nA z-score is a measure of how far an observation’s value (\\(x\\)) is from the mean (\\(\\mu\\)), standardized by dividing by the standard deviation (\\(\\sigma\\)). Thus, an observation \\(x\\) has a z-score:\n\\[\nz = \\frac{x - \\mu}{\\sigma}\n\\]\nWhile there are other ways of standardizing data, usually when we are standardizing our data, we are converting each observed value into a z-score. Below, we use the zscore() function from the stats module of a package called scipy. Note that the values in A all return nan because they have a standard deviation of 0, which means there is no variance.\nfor k, v in ABCD.items():\n    print(f'The values in {k} have the following Z-scores: {np.round(zscore(v), 3)}.')\n\n\n\n11.3.2 Back to PCA!\nNow that our data has been standardized, we can conduct the PCA. When we initialize the model object with PCA(), we have the option of telling sklean to compute a specific number of components (e.g. pass the number 15 to get back the 15 principal components that account for the most variance) or a float specifying the amount of variance we want to be accounted for by the PCA (e.g. pass the number .9 to produce a solution that accounts for 90% of the variance). In this example, we will not specify either.\nOnce we initialize the model object, we can use the .fit_transform() method on our standardized array \\(X\\).\npca = PCA()\npca_results = pca.fit_transform(X)\nLet’s take a look at the results!\nBelow, we create a dataframe representation of the Numpy matrix returned by pca.fit_transform(X) because it’s a bit easier to read. As you can see, each country is represented as a row and each principal component as a column. The cells indicate the association between each country and component pairing. These scores don’t have any meaning to us just yet, but they will become more clear shortly.\nres = pd.DataFrame(pca_results, index=countries)\nres.columns=[f'PC {i}' for i in res.columns]\n\nres['PC 0'].head()\nEach of the 35 principal components we have constructed accounts for some amount of variance in the dataset. The components are ordered such that the first component accounts for the most variance, followed by the second, third, fourth, and so on. The amount of variance that each individual component accounts for is stored in the pca model object as an attribute (explained_variance_ratio_), which means we can access it using dot notation. Because we used the default parameters, rather than specifying the n_components parameter, the explained variance ratio scores will sum to 1, which means that together the principal components account for 100% of the variance in the data.\nevr = pca.explained_variance_ratio_\nevr\nThe first value in the evr array above is roughly .6, which means that the first principle component contains roughly 60% of the variance in the dataset. You can interpret the rest of the numbers the same way: the second component contains roughly 9% of the variance, the third roughly 4% of the variance, and so on. In this particular example, a quick glance at this array alone suggests that the first component accounts for substantially more variance than any of the others.\nprint(f'The sum of the array is: {np.round(np.sum(evr), 2)}')\nUsually, we want to see how much cumulative variance is accounted for by some subset of principal components, starting with the first component. In other words, how much variance is accounted for by each component and those before it. The cumulative variance of the first three components, for example, is:\nnp.sum(evr[:3]) \nKnowing how the explained cumulative variance changes with each additional principal component is useful because we typically want to work with some subset of the components rather than the entire set of variables. That is, after all, generally the point of using a data-driven dimensionality reduction method like PCA. If you are going to work with a subset, you should know how much information you kept and how much you threw away.\nLet’s create a Series containing information on the cumulative explained variance for the components in our PCA. We can do this by passing the array of explained variance ratios (evr) to Numpy’s cumsum() function, which is short for cumulative sum. The Series tells us how much variance is accounted for by each component and those preceding it (remember, the index starts with 0, so the 0th element of the series represents the first component).\ncve = pd.Series(np.cumsum(evr))\ncve[:12]\nIn this case, a simple preview of the cumulative explained variance tells us that the first two components alone account for 68% of the variance in the dataset, which is a very substantial amount. Similarly, we can see that the first 12 components still account for 90% of the variance – pretty good considering we started with 35 indicator variables!\nThis is only part of the picture, though. Let’s plot the proportion of cumulative explained variance for each successive principal component. Notice that, by default, PCA will construct a number of components equal to the number of original variables in the dataset. You should be able to see the diminishing returns, here, even if they set in rather smoothly. The code below produces Figure 11.2.\nfig, ax = plt.subplots()\nsns.lineplot(x=cve.index, y=cve)\nplt.scatter(x=cve.index, y=cve)\nax.set(xlabel='Principal component ID',\n       ylabel='Proportion of explained variance (cumulative)')\nax.set(ylim=(0, 1.1))\nsns.despine()\nplt.savefig('figures/08-01.png', dpi=300)\n\n\n\n\n\n\nFigure 11.2: png\n\n\n\n\n\n11.3.3 Matrix Decomposition: Eigenvalues, Eigenvectors, and Extracting Components\nWhen you conduct a principal component analysis, you only want to keep some components. You’re trying to reduce dimensionality while preserving as much information (in the form of variance) possible. So, which components do you extract?\nThe above plot of cumulative variance accounted for can be helpful if you want to preserve a certain amount of variance in the data. An alternative approach is to construct a scree plot to determine which components are substantively important enough to use. To understand how to construct and interpret a scree plot, you need to know a little bit more about how PCA works, and more specifically what role eigenvalues and eigenvectors play in a PCA, and how those roles differ from loadings in factor analysis.\nAs discussed earlier, PCA creates a covariance matrix of standardized variables (or sometimes a correlation matrix). We can understand the structure and other properties of these matrices mathematically using eigen-decomposition. We won’t get into the linear algebra here, but here’s the basic idea: We can decompose the matrix into two parts. The first are the principal components themselves, which are directions of axes where there is the most variance (eigenvectors). The second part is the amount of variance that is accounted for by the principal components (eigenvalues). Every eigenvector has an eigenvalue and there is an eigenvector (principal component) for every variable in the original data. A very important feature of PCA is that the first principal component accounts for the greatest possible variance in the data set. The second principal component is calculated in the same way as the first, except that it cannot be correlated with the first. This continues for each principal component until you have as many as you do variables. It is important to remember that all of the information about scale (the amount of variance explained) is contained in the eigenvalues. The eigenvectors in a PCA do not tell us anything about the magnitude of explained variance.\nScree plots graph the eigenvalues for each component in the PCA, which you now know represents the amount of variance that each component accounts for. The higher the eigenvalue, the more important the component. Because eigenvalues represent the amount of explained variance, Sklearn helpfully stores them in the explained_variance_ attribute of the PCA model object.1 This also makes it very straightforward to create a scree plot. The code below produces Figure 11.3.\neigenvalues = pd.Series(pca.explained_variance_)\n\nfig, ax = plt.subplots()\nsns.lineplot(x=eigenvalues.index, y=eigenvalues)\nplt.scatter(x=eigenvalues.index, y=eigenvalues)\nax.set(xlabel='Principal component ID', ylabel='Eigenvalue')\nsns.despine()\nplt.savefig('figures/08-03.png', dpi=300)\n\n\n\n\n\n\nFigure 11.3: png\n\n\n\nThis figure should be straightforward to understand. The first few components are more important than the others. In a scree plot, you are usually looking for an inflection point, i.e., a point where the slope of the line changes rather abruptly. Usually, that point is clear, but we can also inspect the eigenvalues themselves if we want a bit more precision, just remember that the eigenvalues are 0-indexed, so 0 represents the first component, 1 represents the second component, and so on.\neigenvalues.head(10)\nWe might use the fact that dropping from 3.14 to 1.48 (a decrease of more than 50%) is significantly greater than the drop from 1.48 to 1.12, and from 1.12 to 1.01, to identify an inflection point at 1.48. The general rule is that you extract the components to the left of the inflection point, excluding the component at the inflection point itself. However, there are debates about whether it is best to keep all components with eigenvalues higher than some threshold, such as 1, the idea being that this is still quite a lot of variation even if less than the other components. In this example, cutting at the inflection point would be the third component, which means we would extract the first two. On the other hand, if we go with a threshold of 1, then we would take the first 5.\nWhen different rules suggest different courses of action, the best solution is the one most aligned with your goals. One reason why researchers perform PCA is because they want to do some sort of regression analysis but have a bad multi-collinearity problem. In that case, keep lots of components! It is better to keep information than throw it away unless you really need to throw some away. If, conversely, you are trying to visualize a high-dimensional dataset by collapsing it down to two significant dimensions, then you should only extract those two components provided they contain a lot of variance.\nHere, we will extract the first two because they preserve a lot of variance, and because the next thing I want to do is create a simple visualization of where the countries in our analysis are positioned in terms of these latent dimensions, and creating informative visualizations in three or more dimensions is a fool’s errand.\ncomponent_1 = pca_results[:, 0]\ncomponent_2 = pca_results[:, 1]\n\nPC12 = pd.DataFrame(zip(component_1, component_2), columns=['PC1', 'PC2'])\nWe can now easily visualize how the countries in our dataset are positioned in relation to these two principal components. Let’s grab the country names from our metadata variables to use in the visualization, which will be a simple density plot with country names indicating where each country is given these two components.\nPC12['Country'] = countries\nax = sns.kdeplot(data=PC12, x='PC1', y='PC2', alpha=.8, fill=True)\nfor i, country in enumerate(PC12['Country']):\n    ax.text(PC12['PC1'][i],\n            PC12['PC2'][i],\n            country,\n            horizontalalignment='left',\n            size=3,\n            color='black',\n            weight='normal')\nax.set(xticklabels=[], yticklabels=[])\nax.set(\n    xlabel=\n    f'$\\longleftarrow$ PC1 (eigenvalue: {np.round(eigenvalues.loc[0], 2)}) $\\longrightarrow$',\n    ylabel=\n    f'$\\longleftarrow$ PC2 (eigenvalue: {np.round(eigenvalues.loc[1], 2)}) $\\longrightarrow$'\n)\n\nplt.savefig('figures/08-04.png', dpi=300)\n\n\n\n\n\n\nFigure 11.4: png\n\n\n\nWhile the text is dense in Figure 11.4 (a high resolution version is available in the online supplement), careful inspection should lead to noticing several patterns. The first principal component is defined by the opposition between countries like Norway, Denmark, Switzerland, Luxembourg, and Germany on the one hand, and by Burundi, Tukmenistan, Syria, Eritrea, and North Korea on the other. The second principal component is defined by the opposition of countries like Haiti, Dominican Republic, Nigeria, Gabon, and Honduras on the one hand, and Laos, Eritrea, United Arab Emirates, China, and Singapore on the other. The eigenvalue is much higher for the first principal component, suggesting that the interpretation of the differences between countries on the left and the right of the graph is most important.\nThis is not a factor analysis. We have not guided the PCA towards this solution. Instead, we have obtained these two latent dimensions mathematically, through matrix decomposition, and projected the countries onto that latent space. These two dimensions only represent 68% of the variance in the dataset, but when you think about it, that’s a lot of information for just two variables. The challenge, given that this is computationally inductive, is to do the qualitative and historical work necessary to interpret this representation of the latent structure in the data. However, don’t forget that the only information the PCA has to work with comes from our original variables, so those variables are a great place to start.",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Latent factors</span>"
    ]
  },
  {
    "objectID": "association-and-latent-variables.html#conclusion",
    "href": "association-and-latent-variables.html#conclusion",
    "title": "11  Latent factors",
    "section": "11.4 CONCLUSION",
    "text": "11.4 CONCLUSION",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Latent factors</span>"
    ]
  },
  {
    "objectID": "association-and-latent-variables.html#key-points",
    "href": "association-and-latent-variables.html#key-points",
    "title": "11  Latent factors",
    "section": "11.5 Key Points",
    "text": "11.5 Key Points\n\nWe learned about latent variables and the differences between theory-driven and data-driven dimensionality reduction\nDiscussed distinctions between Factor Analysis and Principal Component Analysis\nConducted a PCA\n\n\n\n\n\n\nBrienza, Justin, Franki Kung, Henri Santos, Ramona Bobocel, and Igor Grossmann. 2018. “Wisdom, Bias, and Balance: Toward a Process-Sensitive Measurement of Wisdom-Related Cognition.” Journal of Personality and Social Psychology 115 (6): 1093.\n\n\nCoppedge, Michael, John Gerring, Adam Glynn, Carl Henrik Knutsen, Staffan Lindberg, Daniel Pemstein, Brigitte Seim, Svend-Erik Skaaning, and Jan Teorell. 2020. Varieties of Democracy: Measuring Two Centuries of Political Change. Cambridge University Press.\n\n\nField, Andy, Jeremy Miles, and Zoë Field. 2012. Discovering Statistics Using r. Sage publications.\n\n\nGéron, Aurélien. 2019. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O’Reilly Media.\n\n\nMüller, Andreas, and Sarah Guido. 2016. Introduction to Machine Learning with Python: A Guide for Data Scientists. \" O’Reilly Media, Inc.\".\n\n\nTabachnick, Barbara G, Linda S Fidell, and Jodie B Ullman. 2007. Using Multivariate Statistics. Vol. 5. Pearson Boston, MA.",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Latent factors</span>"
    ]
  },
  {
    "objectID": "association-and-latent-variables.html#footnotes",
    "href": "association-and-latent-variables.html#footnotes",
    "title": "11  Latent factors",
    "section": "",
    "text": "If you are looking for the eigenvectors, Sklearn stores them in the .components_ attribute.↩︎",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Latent factors</span>"
    ]
  },
  {
    "objectID": "text-as-data.html",
    "href": "text-as-data.html",
    "title": "12  Text as Data",
    "section": "",
    "text": "13 Iterative text analysis\nThe previous chapter introduced some basic methods for processing natural language data stored as unstructured text. Typically, these methods are part of a much larger project; we are preparing text data for some other downstream analysis. Before we get there, this chapter offers a bigger picture view of generic text processing pipelines and workflows. The goal is to understand how the various text analytic methods that are introduced in this book fit together, and to highlight a few core challenges in text analysis.\nBefore we get started, I want to clarify exactly what I mean by “pipelines” in this chapter. As a reminder, we briefly discussed SpaCy’s text processing pipeline in the previous chapter. In this chapter, I am using “pipelines” to refer to the same general idea; it’s the sequence of operations that we are pushing our data through a series of steps, transforming the data and fitting various kinds of models along the way. However, we are focusing on an idealized text analysis pipeline for an entire project.",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Text as Data</span>"
    ]
  },
  {
    "objectID": "text-as-data.html#exploration-in-context-text-analysis-pipelines",
    "href": "text-as-data.html#exploration-in-context-text-analysis-pipelines",
    "title": "12  Text as Data",
    "section": "13.1 EXPLORATION IN CONTEXT: TEXT ANALYSIS PIPELINES",
    "text": "13.1 EXPLORATION IN CONTEXT: TEXT ANALYSIS PIPELINES\nThe methods introduced in the previous chapter are rarely used on their own. Instead, they are paired with other methods and models in larger text analysis pipelines. Let’s start by discussing these larger pipelines to provide context for what you’ve already learned and what is still to come. This chapter will focus on summarizing and describing the content of many different documents. We will consider other possible goals later in the book.\nFigure 14.1 is a high-level overview of a typical computational text analysis pipeline focused on describing the content of many documents in a corpus. Keep in mind that this is a typical project pipeline and the details may differ in any specific project. At the top left of the figure is the “original data;” let’s use a data set consisting of 236,074 speeches made by UK MPs between 2016 and 2019 as an example to make this more concrete. Working with this full dataset is going to be fairly slow on most machines, and at this stage, we don’t want to be sitting around waiting for our code to execute. Instead, we want to enable quick, iterative, and multi-method analyses, so we draw a sample to work with instead.\nOnce we have our sample, we perform some initial processing, or preprocessing, which usually involves a combination of cleaning and pre-screening text that we want to analyze. The cleaning tasks vary by project, but may include converting characters to lowercase, removing punctuation, and normalization via lemmatization. I think of pre-screening as the selection of relevant text, rather than filtering of unwanted text, because we are not modifying the original data; our research workflows are always non-destructive.\nThe methods introduced conceptually in this chapter and concretely in the next are represented in the next stage of the pipeline, which is the construction of a feature matrix. There are two main ways to do this: by extracting features from the text itself, or by “coding” the data (also known as labelling and annotation). This is a somewhat controversial stage in the process, as researchers and methodologists disagree about the “best” way to accomplish this task. Both approaches have their merits and demerits, and you should select the approach that will best enable you to answer your research questions.\nThe next step in the pipeline is exploratory analysis, the focus of the next chapter. The main purpose of these exploratory methods is to develop a deeper understanding of both the manifest and latent content in a corpus. Manifest content is plainly communicated, whereas latent content is “below the surface” of the text and therefore requires more intrepretation from the researcher. I want to emphasize that this interpretive work is done iteratively, by going back and forth between data-driven exploration of the kind introduced here, formal modelling (discussed in later chapters), and careful close readings of individual documents. Recall the discussion of Box’s loop from Chapter 8. Exploratory text analysis serves the same purpose as the techniques from Chapter 8: better understanding our data and analysis so we can iteratively critique, revise, and improve our models.\n\n\n13.1.1 Counting, Coding, Reading\nSocial scientists have been answering questions about our social, political, psychological, and economic lives by systematically collecting, interpreting, and drawing inferences from text data for over a hundred years, long before anyone had the kind of computational powerful at their fingertips that we do now. (Humanists have been doing it even longer, of course.) Formal content analysis techniques have been a core part of the social sciences’ methodological toolkits since shortly after the First World War, when researchers such as Lasswell (1927) started developing methods for analyzing propaganda and political discourse in newspapers (Krippendorff 2019). It should hardly come as a surprise that the explosion of possibilities afforded by computation and large-scale textual data is viewed in part through the lens of this long history, much of which has revolved around competing ideas about the best way to analyze latent content.\nFor many years these differences divided text analysts, with some being more oriented toward scientific approaches and others toward the humanities. These divisions are not so clear-cut in practice, and they involve far more rigor and depth than their oversimplified names suggest. The methods used by these groups are sometimes summarized as counting (identifying patterns in the manifest content of text), coding (identifying latent content through careful specification of concepts), and reading (of the painstaking variety practiced by our friends in humanities departments).\nRather than rehashing comparisons of specific approaches (see Ignatow and Mihalcea 2016), we will focus on understanding why manual coding and the role of interpretation has been so divisive, and how these debates have informed multiple scientific approaches to content analysis, be they quantitative, qualitative, computational, or hybrid (Krippendorff 2019; see also Neuendorf 2016).\nThe distinction between manifest and latent content played an important role in the early development of mainstream quantitative approaches to content analysis (Krippendorff 2019; Neuendorf 2016; Berelson 1952). Focusing on manifest content is often considered more objective because it’s measures are “closer” to its observations (eg. there is little theory and interpretation distance between observing words on the page and counting the number of times two words co-occur). With manifest content, meanings are unambiguous and sit at the surface level of text. Analyzing latent content, however, is a little too close to subjective judgement for some. The distance from words on the page to the latent meanings and messages behind them requires a greater leap of interpretation. Any analysis of latent content necessarily requires us to use our human brains – wired as they are with preconceived notions, theories, cultural schemas, and prone to cognitive biases like conformation bias and motivated reasoning – to interpret ambiguous meanings. This is unfortunate, as latent content tends to be much more interesting than manifest content. To be clear, counting techniques are in no way free of subjectivity; the main goal of the “counting” is feature extraction under different constraints (eg. count occurrences, count co-occurrences), which can then be modeled. No serious social scientist should be satisfied with a table of word co-occurrences and no further interpretation. The major difference is where the interpretations take place, and how accessible and transparent they are.\n\nFurther Reading\nEvans and Aceves (2016) provide a great review of the intersection of natural language processing and social scientific content analysis. If you want to learn more about the general methodological foundations of quantitative content analysis in the social sciences, Krippendorff (2019) and Neuendorf (2016) are widely-used sources. Ignatow and Mihalcea (2016) provide a broader methodological discussion that includes high-level discussions of text analysis methods from the social sciences and humanities as well as computer science.\n\nDifferences in interpretations of latent content are bound to arise. For a very long time, the mainstream solution for dealing with this problem has been specification, which we’ve already discussed in the context of working with latent factors (Chapter 30), and manual coding. Researchers specify precise operational definitions that indicate what concepts mean, and what types of things would constitute an observation of that concept in a document. Once defined, researchers manually construct the quantitative representation of their text data by coding each document.\nIn this context, “coding” is the process of transforming unstructured documents into structured datasets by manually labeling data according to some set of variables that are coupled to theoretical concepts via the specification process. While there are different coding styles, they tend to generally follow a similar pattern. First, you have a research question you want to answer. Usually you also have some idea of what you expect, grounded in some larger theory (i.e., a hypothesis). If you want to compare the tone and argumentative style of letters to the editor addressing local or non-local issues (e.g. Perrin and Vaisey 2008), you would first decide what types of tones and argumentative styles are relevant, and then you would carefully operationalize those tones and styles based, at least in part, on theory. Then you would read each text and assign codes based on the presence or absence of specific tones and argumentative styles. If resources allow, you would have multiple trained researchers (including yourself) code the documents. This makes it possible to compare the codes assigned to documents by different researchers and compute an inter-coder reliability rate (Krippendorff 2019). Codes with a reliability rate above a given threshold (e.g. 90% agreement between coders) are retained, shifting the coding process from one based on subjective interpretation to inter-subjective agreement. In short, the coding approach is one that hinges on good specification.\nThough widely practiced, and despite plenty to love, there are some valid concerns about manual coding that go beyond the time (and money) it requires. The difference between approaches that “code” and those that count and map was the subject of an animated debate in the American Journal of Cultural Sociology following the publication of Monica Lee and John Levi Martin’s (2015a) “Coding, Counting, and Cultural Cartography.” (I’ve provided the references for this debate in the “Where to Go Next” section at the end of the chapter.) Lee and Martin start by engaging with an argument made by Richard Biernacki (2012, 2009) that manual coding just makes things worse. Biernacki thinks that any content analysis requires the kind of careful interpretation that our colleagues in the humanities practice. From his perspective, manual coding both lowers the quality of the interpretation (by virtue of being coupled to theoretical concepts and hypotheses) and obscures it.\nConsider an example. If I were to code the presence or absence of different types of political arguments in a collection of news stories about immigration reform, I would start specifying the types of political arguments I think are relevant and likely to be found. I would have to be explicit about what constitutes an observation of one type of political argument versus another (i.e., operationalization). Researchers who question the validity of the coding approach would likely point out that my (or any) choice of coding scheme would invariably misrepresent the texts themselves. As a result, my codes could be contested by researchers who see the same text differently, and any results I obtained from analyzing the final dataset would likely not be replicated by another researcher. Their second objection would be that this potential interpretive chaos is hidden away behind the codes, where other researchers and readers can’t see it.\nBiernacki’s (2015) solution is to reject coding altogether, and to replace it with humanistic approaches to interpretation. Somewhat surprisingly, he argues that this approach is actually more scientific because it “better engages standards for validity, transparency, producing competing hypotheses, generalizing and hypothesis-testing by recalcitrant detail” (page 313). Lee and Martin (2015a, 2015b) accept Bernacki’s critique that manual coding hides the essential, but messy, work of interpretation rather than eliminates it, but they disagree that turning to humanistic approaches is the only, or the best, response to the problem. Instead, they propose a refinement of the “counting” methods that begins by representing original texts in a simplified form, like a map represents terrain in simplified form. To be a good “map,” these simplified representations need to remove a lot of information from the texts while still faithfully representing the core features of the original texts. Lee and Martin offer semantic networks (discussed in later chapters) as an approach, which work by exploiting the low-level relationships between words within semantic units like sentences and paragraphs.\nLee and Martin’s goal is not to eliminate interpretation, but rather to move it out into the open where it can be seen, evaluated, and potentially contested. The idea is that this becomes possible if we have formal procedures for producing map representations from text. This leaves the researcher to openly and transparently interpret the map rather than hiding interpretive judgements behind codes, and then analyzing relationships among the codes as if no really challenging interpretation had taken place at all.\nThis debate boils down to whether, and how, to make complex interpretive research, which is absolutely unavoidable, more open and transparent. The debate between coding and count-based approaches is largely a debate about where the inevitable interpretation should happen, and who should be able to see and assess it. Those who code and those who count both break with Bernacki, and personally I think that’s a good thing because the approach he recommends – close reading – is not an alternative to counting or coding. Coding and counting both have many strengths, but should always be paired with close reading of a subset of documents. In other words, Bernacki is right that close reading and interpretation are essential, but it doesn’t follow that manual coding has no place in text analysis, or in social science more broadly. For the same reason, Lee and Martin are right to shift interpretation out into the open, but their critique of manual coding is also overly dismissive and “maps” don’t just magically reveal their unambiguous meanings to us. We should not abandon manual coding in favour of an exclusive commitment to humanistic interpretation or formalism; we should combine close reading, manual coding, formal approaches, and other methods.\nIn the rest of this chapter, and in subsequent chapters focused on text data, I will assume the following:\n\nclose reading is not an alternative to any other method, it must be paired with other methods;\n“coding” and “counting” approaches need not be pitted against each other either, as they can be used together to mitigate the limitations of employing either approach in a vacuum; and\nany computational approach to text analysis benefits from combining all of these approaches in some way.\n\nIn the rest of this chapter, we will introduce some important count-based feature extraction methods for constructing quantitative representations of text, and we will see how to use these representations to compare high-level differences in manifest language use and to explore the latent dimensions of text data. Like the methods you learned in the previous chapter, the methods you learn here are useful regardless of whether you want to interpret a “map” or model your data a bit further downstream. In later chapters, we will discuss several ways of doing this using different types of machine learning. We will also return to the idea of close reading, and how to integrate it into larger text analysis workflows.",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Text as Data</span>"
    ]
  },
  {
    "objectID": "text-as-data.html#count-based-feature-extraction-from-strings-to-a-bag-of-words",
    "href": "text-as-data.html#count-based-feature-extraction-from-strings-to-a-bag-of-words",
    "title": "12  Text as Data",
    "section": "13.2 COUNT-BASED FEATURE EXTRACTION: FROM STRINGS TO A BAG OF WORDS",
    "text": "13.2 COUNT-BASED FEATURE EXTRACTION: FROM STRINGS TO A BAG OF WORDS\nAny quantitative or computational text analysis requires some sort of quantitative representation of the text to operate on. Once you’ve constructed that representation, the analysis typically involves going back-and-forth between algorithmic manipulations and modelling of the quantitative representation on the one hand and careful interpretation of the textual representation on the other hand. For that reason, it is very useful to have the following four things accessible to you at any point in the analysis process:\n\nthe original texts\nany relevant metadata about the texts, such as who produced them\nthe pre-processed versions of the texts\na quantitative representation of the texts\n\nThere are two main types of quantitative representations of text that you will learn in this book: (i) long sparse vectors and (ii) short dense vectors. The long and sparse vector representation is usually referred to as a bag-of-words, and the most widely-used data structure is the Document-Term Matrix (DTM). The short dense vector representations have come to be know as embeddings. Alternative ways of representing texts quantitatively, such as networks, can easily be interpreted as variations on these two types of representation. We will set embeddings aside for now and focus on Document-Term Matrices.\n\n13.2.1 Long and Sparse Representations with Document-Term Matrices (DTMs)\nThe first step in constructing a quantitative representation of text is to learn the “vocabulary,” which is the set of unique terms (i.e., words and short phrases) that are used across the entire corpus. In our example of political speeches by UK MPs between 2016 and 2019, for example, the corpus would consists of the full text across all speeches by all political parties in our sampled dataset.\nNote that the vocabulary depends on how we define the corpus. If we define it as the original speech data, then the vocabulary will consist of every unique token used across all speeches. If we define it as our pre-processed speech data, then the vocabulary will consist of all the unique words that make it through our pre-processing step in the text analysis pipeline. This process of defining the corpus, learning the vocabulary, and constructing the DTM is an example of automated count-based feature extraction.\nWhen we create a DTM representation of our text data, each unique term in the corpus vocabulary will become an individual feature (i.e., column) unless we specifically set some sort of condition that filters terms out (e.g., must appear in a minimum of 5 documents).\nThe cells in a DTM typically represent one of three things:\n\nthe presence or absence of a token in the relevant document (0 or 1),\na count of the number of times a token appears in the relevant document (integers), or\nsome measure of word importance or relevance, such as TF-IDF (floats), which we will discuss below.\n\nThe DTM shape will always be equal to the number of unique tokens in the vocabulary (minus any that we screen out in the process of constructing the DTM) and the number of documents (i.e., rows). Table 13.1 is a hypothetical example of a DTM with term counts in each cell.\n\n\n\nTable 13.1: Table: A hypothetical Document-Term Matrix\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocuments\nToken 1\nToken 2\nToken 3\nToken 4\nToken …\nToken \\(n\\)\n\n\n\n\nDocument 1\n0\n0\n3\n0\n2\n8\n\n\nDocument 2\n2\n0\n1\n1\n0\n0\n\n\nDocument 3\n1\n0\n0\n0\n1\n4\n\n\nDocument 4\n0\n2\n1\n0\n1\n3\n\n\nDocument …\n0\n0\n0\n1\n2\n1\n\n\nDocument \\(n\\)\n1\n0\n0\n1\n5\n1\n\n\n\n\n\n\nIn this case, each row of the matrix is a vector representation for a document and each column is a vector representation for a token in the vocabulary. The long sparse vector representation for Document 1, then, would be all of the numbers in the first row of the table (Document 1: [0,0,3,0,2,8]) and the long sparse vector representation for Token 1 would be all of the numbers in the column Token 1 ([0,2,1,0,0,1]).\nWhen we describe vectors as “long and sparse” we are typically referring to the document vectors, which are long because each element in the vector (i.e., feature in the matrix) represents a unique term in the vocabulary. Vocabularies are always large, and most words in the vocabulary do not appear in most documents. As a result, these vector representations are mostly full of 0s; hence sparse.\n\n\n13.2.2 Weighting Words with Term Frequency Inverse Document Frequency (TF-IDF)\nIn many approaches to computational text analysis, working with simple count data is rarely ideal because the words that occur the most frequently are function words (e.g., ‘the’, ‘and’, ‘of’) that carry very little information about the actual content of a document. Extremely rare words are also generally uninformative. What we really want are the words that are somewhere in between those two extreme ends of the frequency distribution. This is generally done by computing some sort of word weight, and the most common by far is a measure called TF-IDF.\nTF-IDF stands for “Term-Frequency Inverse Document Frequency,” and it is intended to measure the usefulness of any given token for helping reveal what a document is about relative to other documents in a corpus. It weights words rather than counts them, and the weights are lower for words that are either too common or too rare. To understand how it works, let’s break it down and look at term frequency and inverse document frequency separately, and then the full measure. To do so, we will use a hypothetical example of a dataset of 150 journal article abstracts.\nAs you might expect, Term Frequency is a measure of how common a word is in some document. Rather than using a straight count (which would be biased towards longer documents), we multiply the number of times the word appears in a document by the inverse ratio of the number of documents that have the term compared to the total number of documents in the corpus. Let’s say, for example, that the word “environment” appears four times in a 200 word abstract for a journal article about environmental activism. The term frequency \\(TF_{i,j}\\) for “environment” in this specific document would be 0.02.\n\\[\nTF_{environment} = 4/200 = 0.02\n\\]\nNow let’s say there are a total of 150 abstracts in our dataset and the word “environment” appears 42 times in the full dataset. We want to know how important the word “environment” is across the whole collection, so we calculate the inverse document frequency, IDF, using the following equation:\n\\[\nIDF = \\log\\Big(\\frac{N}{DF_i}\\Big)\n\\]\nWhere \\(N\\) is the total number of documents in the dataset, and \\(DF_i\\) is the number of documents that the word \\(i\\) appears in. The IDF score for “environment” is the log of this value, which is 0.55.\n\\[\nIDF_{environment} = \\log\\Big(\\frac{150}{42}\\Big)\n\\]\nTo compute the TF-IDF weight for any word in any document in a corpus, we multiply \\(TF\\) with \\(IDF\\).\n\\[\nW_{i,j} = TF_{i,j} \\times \\log\\Big(\\frac{N}{DF_i}\\Big)\n\\]\nPutting it all together, TF-IDF is as its name suggests: Term Frequency times Inverse Document Frequency. The TF-IDF weight of a word in a document increases the more frequently it appears in that document but decreases if it also appears across many other documents. Rare, but not too rare, words are weighted more than words that show up across many documents. The result is a set of words that, while not the most common, tell us a lot about the content of any one document relative to other documents in the collection. This measure is far more useful than raw counts when we are attempting to find meaningful words. In the next chapter, we will further clarify TF-IDF by comparing word weights with their frequencies in our political speech dataset.",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Text as Data</span>"
    ]
  },
  {
    "objectID": "text-as-data.html#close-reading",
    "href": "text-as-data.html#close-reading",
    "title": "12  Text as Data",
    "section": "13.3 CLOSE READING?",
    "text": "13.3 CLOSE READING?\nSo far, I’ve sketched out a pretty high-level and idealized pipeline that explains how different types of text processing, analysis, and modelling fit together. I’ve also explained the challenges involved in one crucial step: the approach used to represent text quantitatively. This has led to some disagreements over the various approaches to this problem, with some arguing in favour of coding over counting, others counting over coding, and others for throwing the baby out with the bathwater. Now let’s turn our attention to another issue, which is the role of close reading in a computational text analysis. The idea, illustrated in the pipeline, is that you engage in deep reading as you iteratively explore your data and develop models.\nIn computational text analysis, methodologists are beginning to think through ways of systematically combining various inductive and deductive approaches to computational text analysis with good old fashioned reading. Why? Because:\n\nmixed-methods research (Small 2011) is especially valuable when one of the methodologies is less familiar to the scientific community (as computational text analysis often is), and / or when it pulls the researcher further away from the original data than more familiar methods (validation and triangulation);\nthere is a lot to be gained by thoughtfully combining induction and deduction; and\nmachines and humans are good at different types of things, and we want to use both our brains and our computers for the things they are best at.\n\nMost computational text analyses involve machine learning of one kind or another, and the impressive results that these models produce, combined with the use of metaphors like “reading” and “learning,” can make it easy to forget, at least temporarily, that computers are not actually reading; they don’t understand words, sentences, or meaning (manifest or latent) in the same way that humans do. When computers “read,” they are applying mathematical operations to internal representations of data. More inductive computational models, such as probabilistic topic models (introduced in Chapter 30) identify patterns in documents that, hopefully, correspond to what we humans recognize as reasonably coherent themes and “topics.” Despite finding the pattern, the computer doesn’t know what a topic is, or what a word is for that matter. Behind the scenes, it’s all probability distributions. To really know, understand, and assess the validity of the computational analysis, we humans need to read things carefully. Systematic comparisons of manual and computational text analysis support this combination (L. Nelson et al. 2018). There is no way around it; whatever our specific interests or text analysis methodology, we have to read carefully. That’s a good thing.\nHumans with domain knowledge should do the things that humans are good at and computers are bad at (e.g. interpretation, critical thinking), and that computers should do the things that computers are good at but humans are comparably bad at (e.g. computing the similarity of two massive vectors of numbers); In the next section, we explore one practical implementation of human-computer division of labour: computational grounded theory.\n\n13.3.1 “Computational Grounded Theory”\nOne of the most exciting and promising examples of a mixed-approach framework is Laura Nelson’s (2017) “computational grounded theory.” It is, to date, the most systematic and sophisticated approach to combining machine learning and computation more generally with deep reading and interpretation by humans. As the name of the framework suggests, Nelson’s approach builds on the qualitative foundations of grounded theory (Glaser and Strauss 1999; Charmaz 2006), which is (somewhat confusingly) both a process and a product. To risk oversimplifying things, the process involves inductively identifying, integrating, and refining categories of meaning in text. This is accomplished through a variety of specific procedures, the most common of which is the method of “constant comparison” of cases. The product is a set of relatively abstract concepts and statements (i.e. theory) that are “grounded” in the data.\nNelson builds on this methodological foundation because it is well-established, unapologetically inductive, and emphasizes the interpretive work that is unavoidable in text analysis. But, as she points out, grounded theory does not scale well to large datasets, and the results can be difficult to validate and replicate. Her computational framework is designed to address these problems while retaining the good parts of the original approach.\nComputational grounded theory involves three basic steps. The first is pattern detection using exploratory and computationally inductive methods – such as those introduced in the next chapter, as well as Chapters 30 and 33 – to discover latent themes and topics in a corpus. This is a shift in the logic of the grounded theory method. In classic grounded theory, the researcher is doing interpretive work to develop and refine categories of meaning. In computational grounded theory, the computer identifies potential categories of meaning (i.e. topics) using unsupervised methods that can be replicated; the researcher interprets and evaluates those categories.\nThis is the starting point for the second step – “guided deep reading” – in which the researcher makes informed decisions about specific texts to read and interpret. The guided part is key here, because it allows the researcher to select texts that are representative of some larger theme or topic, not an unusual outlier. This helps mitigate the effects of confirmation bias and other cognitive biases that can affect the judgements of even the most well-intentioned researcher. It also makes the interpretive part of the analysis easier to validate and replicate. Think of it as the difference between exploring an unfamiliar city with and without a map. Without a map, you may end up seeing the same amount of the city, but if you have a map you can make more informed decisions about where to go and you will have a better sense of what you did and did not see. You can also trace your route on the map, making it easier for someone else to understand where you went and potentially to go there themselves.\nTo summarize: we use computationally inductive methods to discover some potential themes and estimate how they are distributed within and across texts in our corpus. We then use the results of that analysis to select a sample of texts that are representative of specific themes and, through a process of “deep reading,” use our human brains to develop a better and more sophisticated understanding of what those themes are. This enables us to come to an understanding of the text that is better than any one method could have produced on its own.\nThe third and final step of the computational grounded theory framework is pattern confirmation. For Nelson, this step forces the researcher to operationalize concepts and ideas discovered in the first two steps, and then check to see how common they are across the corpus. One way to do this is to go through the supervised learning process covered in Chapters 21 and 22, but we will set further discussion of supervised learning methods aside for now.\nThe full process is summarized in Figure 14.2, which is based on a figure from Nelson’s (2017) article. I encourage you to read her article carefully, in part because she thoroughly illustrates each step with examples from her work on the political logics underlying the women’s movement in New York and Chicago from 1865 to 1975 (L. Nelson 2015). It’s an excellent article with fascinating examples.\n\n\n\n\n\n\nFigure 13.1\n\n\n\n\nFurther Reading\nIf you are interested in the debate over coding and counting that was discussed in this chapter, I would recommend reading the original articles by Lee and Martin (2015a), Biernacki (2015), Reed (2015), Spillman (2015), and Lee and Martin (2015b).\nIn addition, I recommend reading Laura Nelson’s (2017) original article on computational grounded theory. You can also learn more about the original grounded theory method by consulting and Glaser and Strauss (1999) or Charmaz (2006). Finally, Small (2011) offers a great overview of various different ways of doing mixed methods research.",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Text as Data</span>"
    ]
  },
  {
    "objectID": "text-as-data.html#conclusion",
    "href": "text-as-data.html#conclusion",
    "title": "12  Text as Data",
    "section": "13.4 CONCLUSION",
    "text": "13.4 CONCLUSION\n\n\n13.4.1 Key Points\n\nOutlined a generic text analysis pipeline that starts with sampling and preprocessing text, constructing quantitative representations using manual coding and/or automated count-based feature extraction\nDemonstrated how to perform exploratory analysis of the manifest and latent content of those texts, combined with guided close reading and model development\nDiscussed the challenge of transparently interpreting latent content and the tensions between the coding, counting, and close reading approaches\nHighlighted Laura Nelson’s computational grounded theory framework as an exemplar of the foregoing",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Text as Data</span>"
    ]
  },
  {
    "objectID": "text-as-data.html#package-imports",
    "href": "text-as-data.html#package-imports",
    "title": "12  Text as Data",
    "section": "14.1 Package Imports",
    "text": "14.1 Package Imports\nimport pickle\nfrom pprint import pprint\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy\nimport spacy\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import Normalizer\n\nfrom dcss import set_style, download_dataset\nfrom dcss.text import bigram_process, bow_to_df, preprocess\nfrom dcss.utils import sparse_groupby\n\nset_style()\n\nnlp = spacy.load('en_core_web_sm')",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Text as Data</span>"
    ]
  },
  {
    "objectID": "text-as-data.html#scaling-up-processing-political-speeches",
    "href": "text-as-data.html#scaling-up-processing-political-speeches",
    "title": "12  Text as Data",
    "section": "14.2 SCALING UP: PROCESSING POLITICAL SPEECHES",
    "text": "14.2 SCALING UP: PROCESSING POLITICAL SPEECHES\nIn this chapter, we’re going to work with text data from speeches made by British Members of Parliament (MPs) between 2016 and 2020, available in full from the British Hansards dataset. We will drop any observations that are missing values from the party, speakername, or speech columns.\nyears = [2016, 2017, 2018, 2019, 2020]\ncolumns = [\n    'speech', \n    'speakername', \n    'party', \n    'constituency', \n    'year'\n]\ndfs = []\n\nfor year in years:\n    # download the data using Dropbox share link\n    download_dataset(\n        data_url=f'https://www.dropbox.com/scl/fi/c9d1aqzrage1juf276nvd/british_hansard_{year}.csv?rlkey=ilyn06y4hw4jocr4fhq6w6olk&st=ioslogzq&dl=1',\n        save_path=f'data/british_hansard/bh{year}.csv'\n    )\n\n    # load the data\n    df = pd.read_csv(\n        f'data/british_hansard/bh{year}.csv', low_memory=False,\n        usecols=columns\n    )\n\n    dfs.append(df)\n\nuk_df = pd.concat(dfs)\nuk_df.dropna(\n    subset=['party', 'speakername', 'speech'], inplace=True\n)\n\nuk_df.reset_index()\nuk_df.info()\nuk_df['party'].value_counts()\nThe Conservative Party has made far more speeches than other parties within this time frame due to the fact that they were the governing party for that entire window, first under Theresa May (2016-2019), later under Boris Johnson (2019-2022).\nWe will also ignore speeches made by the Speaker of the House and Independents. We will focus only on parties whose MPs collectively made more than 400 speeches within our four year window.\nparties_keep = [\n    'Conservative', \n    'Labour', \n    'Scottish National Party', \n    'Labour (Co-op)',\n    'Liberal Democrat',\n    'Democratic Unionist Party',\n    'Plaid Cymru',\n    'Green Party'\n]\n\nparty_subset = uk_df[uk_df['party'].isin(parties_keep)].copy()\nparty_subset.reset_index(drop=True, inplace=True)\n\ntotal_speech_counts = party_subset['party'].value_counts()\ntotal_speech_counts\nThis leaves us with 224,016 speeches.\nSo far, all of the text processing we have done has been on a very small amount of text. When scaled up to data of this size, things inevitably take a lot longer. Powerful computers help a lot, of course, but even then you can spend a lot of time just waiting around for code to finish running, and that’s not ideal when you are rapidly iterating over many different analyses. Instead, it can be helpful to work with a smaller representative sample of the full dataset – you can always execute your code against the full dataset when your code is developed. The best way to do this is by drawing a random sample, of course.\nWe will draw a stratified random sample where the strata are political parties. In the code block below, we do this by grouping the dataframe by political party and then drawing a random sample of 30% from each strata. This is done without replacement; once a speech has been sampled, it can’t be sampled again. We set the random_state to ensure that your sample matches mine.\nsampled_speeches = party_subset.groupby('party')\n\nsampled_speeches = sampled_speeches.sample(\n    replace=False,\n    frac=.3,\n    random_state=23\n)\n\nlen(sampled_speeches)\nsampled_speeches.to_csv(\n    'data/sampled_british_hansard_speeches.csv', index=False\n)\nsampled_speech_counts = sampled_speeches['party'].value_counts()\n\nsample_sizes = pd.DataFrame(\n    zip(total_speech_counts, sampled_speech_counts),\n    columns=['Total', 'Sample'],\n    index=parties_keep)\nThere are now 67,204 speeches in our dataset, sampled from 8 political parties (if we treat Labour Co-op as if it were a separate party, which it sort of is) proportional to the number of speeches each made within our 4 year window.\nLet’s start by quickly taking a look at the length of speeches by politicians from each party. We will do so by computing the length of each string (i.e., the number of tokens in each speech).\nsampled_speeches['speech_len'] = sampled_speeches['speech'].apply(lambda x: len(x.split(\" \")))\nNow we can group by political party, extract each group from the grouped object, and plot the kernel density estimate for our new speech length variable. We will put each plot side by side, as small multiples, to facilitate comparisons. Note that in the graph below, the kernel density estimates shows the density for speeches within each party, not across parties.\nWe will define a function called party_subplot() to avoid needlessly repeating code. The result is shown in Figure 14.1.\n#| warning: false\n\ndef party_subplot(subgroup, title, position):\n    sns.kdeplot(\n        ax=position, \n        data=subgroup, \n        x='speech_len',\n        log_scale=True, \n        fill=False, \n        alpha=1, \n        linewidth=3, \n        color='C0'\n    )\n    \n    position.set(\n        xlabel='Number of tokens (log scale)', \n        title=title\n    )\n\n\nparties = sampled_speeches.groupby('party')\n\nfig, ax = plt.subplots(\n    2, 4, \n    sharex=True, sharey=True, \n    figsize=(20, 6)\n)\n\nparty_subplot(\n    parties.get_group('Conservative'), \n    'Conservative', \n    ax[0, 0]\n)\n\nparty_subplot(\n    parties.get_group('Labour'), \n    'Labour', \n    ax[0, 1]\n)\n\nparty_subplot(\n    parties.get_group('Scottish National Party'), \n    'Scottish National Party', \n    ax[0, 2]\n)\n\nparty_subplot(\n    parties.get_group('Labour (Co-op)'), \n    'Labour (Co-op)', \n    ax[0, 3]\n)\n\nparty_subplot(\n    parties.get_group('Liberal Democrat'), \n    'Liberal Democrat', \n    ax[1, 0]\n)\n\nparty_subplot(\n    parties.get_group('Democratic Unionist Party'), \n    'Democratic Unionist Party', \n    ax[1, 1]\n)\n\nparty_subplot(\n    parties.get_group('Plaid Cymru'), \n    'Plaid Cymru',\n    ax[1, 2]\n)\n\nparty_subplot(\n    parties.get_group('Green Party'), \n    'Green Party', \n    ax[1, 3]\n)\n\nplt.tight_layout()\nplt.savefig('figures/speech_length_by_party.png', dpi=300)\n\n\n\n\n\n\nFigure 14.1: png\n\n\n\nparties['speech_len'].median()\nWe can see that the distributions for each party follow roughly the same pattern of proportions. The distribution of speech lengths is strongly skewed, with the median length generally being in the ballpark of 70-90 terms for all parties.\n\n14.2.1 From Rule-Based Chunks and Triplets to Statistically Dependant n-grams\nPreviously, you learned how to extract phrases contained in spaCy docs by accessing the noun chunks attribute (.noun_chunks), and you saw how to leverage the syntactic dependency labels assigned to each token to extract information such as verb-object pairs. You also saw how to generalize that knowledge to semantic triplets, also known as SVOs or subject-verb-object triplets. Although the results of an automated SVO extraction often involve a lot of noise, there is a fair amount we can do to improve the results by customizing them to our research contexts (e.g., changing how we walk through the dependency trees when working with social media data).\nEach of those methods are especially helpful when we are exploring text data or trying to extract specific pieces of information. Often, however, we want to identify n-grams, which are phrases that denote some sort of concept that we want to treat as if they were a single token. The n in n-gram refers to the number of tokens in the phrase. For example, bigrams are two tokens that make up a phrase that ostensibly has a different meaning than the two constituent tokens. For example, climate and change tokens could be transformed into a single climate_change token. Don’t forget, your computer has no idea what the tokens “climate,” “change,” or “climate change” mean, so it can only estimate when two tokens co-occur frequently enough to be considered a phrase, rather then simply being adjacent tokens from time to time. It’s important to keep this in mind at all times when working with advanced computational techniques.\nThe Phrases model class in gensim is widely-used for this task and is the recommended go-to complimentary n-gram package for spaCy. It’s a well-optimized way to detect bigrams in a corpus without a lot of effort or processing time. Ultimately though, it’s a statistical model that calculates maximum likelihood estimates for token co-occurrences (pairs of tokens that co-occur too frequently to be random). In other words, it scores tokens that appear next to each other based on their statistical dependencies rather than their syntactic dependencies (i.e., not based on linguistic rules and domain expertise).\ngensim’s Phrases includes two scoring functions for the likelihood estimation, Pointwise Mutual Information (PMI) and Normalized Pointwise Mutual Information (NPMI). Neither scoring method is inherently better or worse, and the choice between them depends on your objective. NPMI is generally better at prioritizing frequent co-occurrences, while the PMI scorer tends to give high probabilities to less frequent cases. As you may have guessed from the name, NPMI scores modify PMI ones by normalizing them to a scale from -1 to 1, where a score of 1 would mean that the two tokens only ever appear together and negative scores indicate that they appear together less than expected by chance. The normalized values are also easier to interpret in comparison to each other and as you will see, the trained Phraser model class, which is a leaner form of the Phrases class when you no longer need to update the model, can return a dictionary of all bigrams and their associated scores. This can be helpful to get a better sense of the parameters that result in higher scores for the bigrams that you expect.\nIn this example, I’ll use the npmi scorer because we will be training the model on a very specific domain (political speeches), so we can reasonably expect that meaningful bigrams in that context will be repeated frequently. With that said, it’s always worth comparing the results of the various options and configuration parameters. There are ways to quantitatively evaluate the model, but often it’s enough to look at the text itself with the merged tokens because the poor results tend to be noticeable right away if the parameters weren’t set to capture the results you want, or if the input data wasn’t pre-processed correctly.\nAs of 2021, gensim is transitioning to a major new version, and some of the planned changes impact the Phraser class implementation. Rather than publish gensim code that will soon be out of date, I’ve included the relevant code in the dcss package (enabling it to be updated as appropriate) in the form of two functions, bigram_process() and preprocess(). The former is simply a few lines of code that passes our text into Phrases in the form gensim expects and returns the exact same text but with pairs of words detected as bigrams joined (ie. word1_word2).\nI’ve set the scoring threshold pretty high: 0.75 out of a maximum of 1.0. Sometimes it’s preferable to process the text with a strict threshold like this and miss some bigrams rather than worry about handling too many nonsense results from a relaxed score minimum.\nThe preprocess() function also removes stop words, which are words that are important in communication but do not convey content, such as function words (e.g., and, the).Stop words can be a bit tricky because of socio-linguistic variation within and across cultural groups. The idea here is that different cultural groups, large or small, tend to have their own culture-specific stop words that we want to disregard in any text analyses that are focused on content. “Social” might be a stop word in a dataset of documents produced by sociologists, but not for chemists, classicists, or East Anglian dwile flonkers. In a domain or culture-specific application, we want to be able to identify words like this and exclude them along with more language-specific stop words (e.g., English, Spanish, Korean).\nWe won’t actually call the bigram_process() function directly. Instead, we will call the preprocess() function from the dcss package that includes the bigramming process as an option alongside other pre-processing steps. All of those steps are things you’ve learned how to do in this chapter. Below, we call the function using the speeches from all the selected parties, rather than a random sample. Fair warning, this is gonna take a while. We’ve got a lot of text to process.\nbigram_model, preprocessed = preprocess(\n    sampled_speeches['speech'], \n    nlp=nlp, \n    bigrams=True, \n    detokenize = True, \n    n_process=4\n)\n\nlen(preprocessed)\nSome time later, you’ll be left with a list of ~67,000 speeches that have been thoroughly prepared for downstream analysis. spaCy is ridiculously fast relative to comparable packages. This much text will still take time to analyze. That’s why we worked with a stratified random sample earlier and why you’ll want to while prototyping.\nWhen your code finishes running, you’ll want to save the results to disk so they can easily be re-loaded later. Below, we do this with pickle, which stores Python data structures in binary format, which is OK for data generated within a larger pipeline, and which could easily be re-generated if necessary. The pickle package is remarkably adaptable and can safely interact with most python objects, but it’s important not to rely on it unless you’ve thoroughly tested whether or not what you want to save can be converted to and from binary without ill effect. We already know that this is going to work out just fine. We can save and load our preprocessed and bigram_model objects to and from memory, respectively, using the dump() and load() functions from the pickle package:\nwith open(\n    'data/british_hansard_processed_sample.pkl', 'wb') as fp:\n    pickle.dump(preprocessed, fp)\n\n    \nwith open('data/british_hansard_processed_sample_bigram_model.pkl', 'wb') as fp:\n    pickle.dump(bigram_model, fp)\nwith open ('data/british_hansard_processed_sample.pkl', 'rb') as fp:\n    preprocessed = pickle.load(fp)\nTo briefly recap, we’ve just used a function called preprocess() that applied a series of operations to a sample of political speeches. Specifically, it\n\ndetected bigrams using gensim’s Phraser class and merged them into single tokens;\nfiltered out English-language stopwords and tokens containing fewer than 2 characters;\nfrom the remaining tokens, selected nouns, proper nouns, and adjectives; and\nreplaced each selected token with it’s lemma.\n\nIn the rest of this chapter, we will primarily work with the data that resulted from that process. We can re-access that data anytime by loading the pickle we created, which is very handy because you don’t want to be sitting around needlessly re-preprocessing your data all the time.\nIt’s generally a good idea to do your text analysis in a non-destructive way, and to always have on hand:\n\nThe original text data, in full;\nAny relevant metadata, such as who created the text data;\nThe preprocessed text data, pre-transformation into a feature matrix or other quantitative representation; and\nThe feature matrix itself (created later in this chapter).\n\nLet’s add the pre-processed speech data to our sampled_speeches dataframe, to help keep everything together. As you can see, it will contains two Series with text data, one with the original full speech text, such as this remark from Theresa May:\nsampled_speeches.iloc[700]['speech']\nand another with the version that was produced by our pre-processing function:\nsampled_speeches['preprocessed'] = preprocessed\nsampled_speeches.iloc[700]['preprocessed']\nAs you can see, our preprocessing has removed a lot of information. When working with small data sets or individual documents, this would make little sense. But, when you are trying to understand the content of a large collection of documents, it’s enormously helpful. It helps us understand the forest for the trees.\nNow that our data is ready, let’s move to the next step in our pipeline. If you recall from the previous chapter, our next task is to construct a quantitative representation of our text data. We’re going to use feature extraction methods in Sklearn. We’ll start with simple term counts.",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Text as Data</span>"
    ]
  },
  {
    "objectID": "text-as-data.html#creating-dtms-with-sklearn",
    "href": "text-as-data.html#creating-dtms-with-sklearn",
    "title": "12  Text as Data",
    "section": "14.3 CREATING DTMS WITH SKLEARN",
    "text": "14.3 CREATING DTMS WITH SKLEARN\nIn Sklearn, we can construct DTMs with Boolean or count data using CountVectorizer() and with TF-IDF weights using TfidfVectorizer(). The process of learning the vocabulary is a method of the vectorizer itself, so the first thing we will do is make a decision about which vectorizer to use and how to tune it. Let’s start with the CountVectorizer.\nOnce we initialize a vectorizer object, Sklearn learns the vocabulary in our corpus using the fit() method. It can then transform our raw unstructured text data into a DTM using the transform() method. In the resulting DTM, each document is a row and each token (i.e. word) in our corpus vocabulary is a column.\nAs always, the quality of any machine learning analyses depends in large part on the quality of the data we provide. In the context of feature extraction methods such as the construction of a DTM from text data, we can control this by (a) pre-processing our data and / or (b) customizing the feature extraction process itself by changing specific parameters in our vectorizer. You’ve already learned how to do the first part. We can use our preprocessed list from earlier in the vectorization process below.\n\n14.3.1 Count Vectorization\nSklearn’s CountVectorizer has a number of parameters that we can tune. For a simple example: we often want to avoid words that are too generic to the corpus, so we can use the max_df parameter to specify that we don’t want to keep tokens that appear more than \\(n\\) times, or in more than \\(n\\)% of the documents in our collection. This can be especially helpful when working with text datasets that include a lot of specialist language. Similarly, we can use the min_df parameter to specify that we do not want to keep tokens that appear in fewer than 3 documents in our collection. While some parameters might be useful, others will be irrelevant to your task. I encourage you to read the documentation to get an better idea of what you can do with Sklearn.\nWhich parameters should you use? These decisions are part of a large and complex literature on “feature selection,” and there is no one rule you can follow that will get the best results every time. The best advice I can give you is to keep things as simple as you can and align your decisions with your research needs. If it makes sense to do something given the question you are trying to answer, then do it and report the decision when you report on the rest of your methodological decisions. If it doesn’t, don’t do it just because you can. In this case, our spaCy pre-processing and bigram detection with gensim took care of most of what we would want to do. However, given the volume of data we are working with, we will also:\n\nignore tokens that appear very frequently and very infrequently,\nstrip accents from characters.\n\nMake note of the parameters we are using here; consider the the effects they will have, given the data.\ncount_vectorizer = CountVectorizer(\n    max_df=.1,\n    min_df=3,\n    strip_accents='ascii'\n)\nOnce we have instantiated our CountVectorizer with the relevant arguments, we want to learn the vocabulary and construct the DTM. We can use the fit_transform() method to do this, which simply combines the fit() and transform() methods. Below, we do this for preprocessed texts.\ncount_matrix = count_vectorizer.fit_transform(preprocessed)\nvocabulary = count_vectorizer.get_feature_names_out()\n\ncount_matrix.shape\nLet’s pickle both of these objects for future use.\nwith open('data/british_hansard_sample_dtm.pkl', 'wb') as fp:\n    pickle.dump(count_matrix, fp)\n    \nwith open('data/british_hansard_sample_vocabulary.pkl', 'wb') as fp:\n    pickle.dump(vocabulary, fp)\nOur vectorizer has produced a DTM with 16,428 unique tokens (all of which met the criteria specified in the arguments passed to CountVectorizer()) from 67,204 documents (i.e., speeches). We can also use the ngram_range argument to return ngrams up to three tokens long if we’re using the default word analyzer, or a chosen number of letters if we’re using the character analyze. Our “vocabulary” would then include these ngrams. Again, we’ve already done this using the statistical model in gensim but there are times when you just want to stick to one library for easier interopability between its functions and object types, so it is convenient to have so many options available. There are two versions of the character n-gram analyzer: char_wb will respect token boundaries while char could result in a trigram whith the last letter of one token, a space, and the first letter of the next token.\n\n14.3.1.1 Comparing Token Frequencies and Proportions\nWe can start discovering some very high-level patterns in our text data just by working with these simple frequencies, akin to doing exploratory data analysis prior to modelling. For example, we can convert the count_matrix to a dataframe and add a column indicating the party of the speaker, group the dataframe by party, and then compare some simple aggregate patterns in word usage across each political party. We’ll start by creating the dataframe, which with data this size will require staying within the sparse matrix framework unless you’re working with a system that has a great deal of memory resources. This is made quite clear below, where 67K speeches is not a particularly huge text dataset by modern standards, but keeping track of 26K features for each of those speeches becomes a huge memory burden when most of the values for those features are zeroes.\ncount_data = pd.DataFrame.sparse.from_spmatrix(count_matrix)\ncount_data.columns = vocabulary\n\ncount_data.index = sampled_speeches['party']\ncount_data.shape \nThe sparse form of the count vectorizer data uses only about 21MB of memory, because the density is around 0.001 - only 0.1% of the values are non-zero and sparse matrices don’t actually store the zero or np.nan values. In fact, you are able to select whatever value you like to “fill” the empty areas of the matrix.\nprint('sparse size: ' + str(count_data.memory_usage().sum()/1048576) + \"MB\")\nprint('sparse density : ' + str(count_data.sparse.density))\nThe dense version, on the other hand, occupies up a straight-up remarkable 8400MB of memory! The code block below will turn a sparse matrix into a dense one then calculate the size. You probably won’t want to run it yourself!\ncount_data_d = count_data.sparse.to_dense()\nprint('dense size: ' + str(count_data_d.memory_usage().sum()/1048576) + \"MB\")\nThe next step is to group the dataframe by the subset of parties, aggregate the token frequencies, and calculate their proportions within each party. We will use some full matrix manipulations for this, storing the percentages in the results dataframe and then transposing it so that each row is a token (indexed by the token string itself) and each column contains the token proportions for each party. With sparse matrix handling in the current version of pandas, aggregation with a groupby operation is unfortunately extremely slow. The function sparse_groupby from dcss.utils is a handy trick that at least works for doing a sum aggregation, and is very fast.\nNow we can create the dataframe, transpose it, and look at a random sample of word proportions.\nparty_counts = sparse_groupby(\n    sampled_speeches['party'], count_matrix, vocabulary\n)\n\nresults = party_counts.div(party_counts.sum(axis=1), axis=0)\nresults_t = results.T\n\nresults_t.sample(20, random_state=10061986)\nWith this dataframe, we can easily retrieve (and compare) the proportions for any given token across each of our parties. For example, if we search for scotland, we find that the Scottish National Party comes out on top. Note how small the differences in scores are across Plaid Cymru, Labour (Co-op), Labour, Conservative, and SNP.\nsearch_term = 'scotland'\nresults_t.loc[search_term].sort_values(ascending=False)\nWhile it is useful to compare the proportion of specific tokens of interest across each group, we can also compare parties by inspecting the top \\(n\\) tokens for each.\nn_top_words = 5\ntop_words_per_party = {}\n\nfor party in results_t.columns:\n    top = results_t[party].nlargest(n_top_words)\n    top_words_per_party[party] = list(zip(top.index, top))\n     \nfor k, v in top_words_per_party.items():\n    print(k.upper())\n    for each in v:\n        print(each)\n    print('\\n')\n\nFinally, we can compute the difference of proportions between any given pair of document groups. This will result in a single vector of positive and negative numbers, where tokens with the largest positive values are associated with the first group and not the second, and tokens with the largest negative values are associated with the second group but not the first.\ndiff_con_snp = results_t['Conservative'] - results_t['Scottish National Party']\ndiff_con_snp.sort_values(ascending=False, inplace=True)\ncon_not_snp = diff_con_snp.head(20) # Conservatives but not SNP\ncon_not_snp\n\nlab_not_snp = diff_con_snp.tail(20) # SNP but not Conservatives\nlab_not_snp\n\nWe can concatenate these two series to more easily visualize their differences. The results are show in Figure 14.2.\ndop = pd.concat([con_not_snp, lab_not_snp])\nfig, ax = plt.subplots(figsize=(6, 6))\nsns.swarmplot(x=dop, y=dop.index, color='black', size=4)\nax.axvline(0) \nplt.grid() \nax.set(\n    xlabel=r'($\\longleftarrow$ Scottish National Party)        (Conservative Party $\\longrightarrow$)',\n    ylabel='',\n    title='Difference of Proportions'\n)\nplt.tight_layout()\nplt.show('figures/british_hansard_difference-of-proportions.png')\n\n\n\n\n\n\nFigure 14.2: png\n\n\n\nAs you can see, simple token frequencies and proportions can be very useful when we are starting to explore our text data. Before moving on to the larger problem of modelling latent topics, let’s discuss an alternative way of scoring tokens in a DTM. In the next chapter we will take a look at Term Frequency-Inverse Document Frequency (TF-IDF) weights.\n\nFurther Reading\nThe count-based methods we discussed in this chapter are the foundation of “Dictionary-based” approaches that are widely-used in the literature. For example, Bonikowski and Gidron (2016) uses count-based dictionary methods to study populist claimsmaking in the 2016 American general election. L. K. Nelson et al. (2021) discusses dictionary-based methods alongside machine learning methods that we will cover later in the book.",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Text as Data</span>"
    ]
  },
  {
    "objectID": "text-as-data.html#conclusion-1",
    "href": "text-as-data.html#conclusion-1",
    "title": "12  Text as Data",
    "section": "14.4 CONCLUSION",
    "text": "14.4 CONCLUSION\n–\n\n14.4.1 Key Points\n\nLearned about chunks, triplets, bi-grams, and n-grams\nUsed Gensim’s Phraser with SpaCy to detect n-grams\nUsed Sklearn to create a Document Term Matrix (DTM)\nDiscussed differences between using token counts vs proportions\n\n–\n\n\n\n\nBerelson, Bernard. 1952. “Content Analysis in Communication Research.”\n\n\nBiernacki, Richard. 2009. “After Quantitative Cultural Sociology: Interpretive Science as a Calling.” In Meaning and Method, 125–213. Routledge.\n\n\n———. 2012. Reinventing Evidence in Social Inquiry: Decoding Facts and Variables. Springer.\n\n\n———. 2015. “How to Do Things with Historical Texts.” American Journal of Cultural Sociology 3 (3): 311–52.\n\n\nBonikowski, Bart, and Noam Gidron. 2016. “The Populist Style in American Politics: Presidential Campaign Discourse, 1952–1996.” Social Forces 94 (4): 1593–1621.\n\n\nCharmaz, Kathy. 2006. Constructing Grounded Theory: A Practical Guide Through Qualitative Analysis. Sage.\n\n\nEvans, James A, and Pedro Aceves. 2016. “Machine Translation: Mining Text for Social Theory.” Annual Review of Sociology 42: 21–50.\n\n\nGlaser, Barney, and Anselm Strauss. 1999. Discovery of Grounded Theory: Strategies for Qualitative Research. Aldine Transaction.\n\n\nIgnatow, Gabe, and Rada Mihalcea. 2016. Text Mining: A Guidebook for the Social Sciences. Sage Publications.\n\n\nKrippendorff, Klaus. 2019. Content Analysis: An Introduction to Its Methodology. Sage.\n\n\nLasswell, Harold. 1927. Propaganda Technique in the World War. Ravenio Books.\n\n\nLee, Monica, and John Levi Martin. 2015a. “Coding, Counting and Cultural Cartography.” American Journal of Cultural Sociology 3 (1): 1–33.\n\n\n———. 2015b. “Response to Biernacki, Reed, and Spillman.” American Journal of Cultural Sociology 3 (3): 380–415.\n\n\nNelson, Laura. 2015. “Political Logics as Cultural Memory: Cognitive Structures, Local Continuities, and Women’s Organizations in Chicago and New York City.” Working Paper.\n\n\n———. 2017. “Computational Grounded Theory: A Methodological Framework.” Sociological Methods & Research, 1–40.\n\n\nNelson, Laura K, Derek Burk, Marcel Knudsen, and Leslie McCall. 2021. “The Future of Coding: A Comparison of Hand-Coding and Three Types of Computer-Assisted Text Analysis Methods.” Sociological Methods & Research 50 (1): 202–37.\n\n\nNelson, Laura, Derek Burk, Marcel Knudsen, and Leslie McCall. 2018. “The Future of Coding: A Comparison of Hand-Coding and Three Types of Computer-Assisted Text Analysis Methods.” Sociological Methods & Research, 0049124118769114.\n\n\nNeuendorf, Kimberly A. 2016. The Content Analysis Guidebook. Sage.\n\n\nPerrin, Andrew J, and Stephen Vaisey. 2008. “Parallel Public Spheres: Distance and Discourse in Letters to the Editor.” American Journal of Sociology 114 (3): 781–810.\n\n\nReed, Isaac Ariail. 2015. “Counting, Interpreting and Their Potential Interrelation in the Human Sciences.” American Journal of Cultural Sociology 3 (3): 353–64.\n\n\nSmall, Mario Luis. 2011. “How to Conduct a Mixed Methods Study: Recent Trends in a Rapidly Growing Literature.” Annual Review of Sociology 37: 57–86.\n\n\nSpillman, Lyn. 2015. “Ghosts of Straw Men: A Reply to Lee and Martin.” American Journal of Cultural Sociology 3 (3): 365–79.",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Text as Data</span>"
    ]
  },
  {
    "objectID": "mapping-text.html",
    "href": "mapping-text.html",
    "title": "13  Text similarity and latent semantic space",
    "section": "",
    "text": "13.1 Package Imports",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Text similarity and latent semantic space</span>"
    ]
  },
  {
    "objectID": "mapping-text.html#package-imports",
    "href": "mapping-text.html#package-imports",
    "title": "13  Text similarity and latent semantic space",
    "section": "",
    "text": "import pickle\nfrom pprint import pprint\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport spacy\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import Normalizer\n\nfrom dcss import set_style\nfrom dcss.text import bigram_process, bow_to_df, get_topic_word_scores, preprocess\nfrom dcss.utils import sparse_groupby\n\nset_style()\n\nnlp = spacy.load('en_core_web_sm', disable=['ner'])",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Text similarity and latent semantic space</span>"
    ]
  },
  {
    "objectID": "mapping-text.html#tf-idf-vectorization",
    "href": "mapping-text.html#tf-idf-vectorization",
    "title": "13  Text similarity and latent semantic space",
    "section": "13.2 TF-IDF Vectorization",
    "text": "13.2 TF-IDF Vectorization\nWhen analyzing content, we are rarely interested in the most and least frequent words, as the former tend to be domain- or group-specific stop words and the latter are too rare. As discussed in Chapter 11, the main benefit of using TF-IDF is that it preserves all tokens (words) in the corpus, but decreases the weights of tokens that are at the exremes of the frequency distribution.\nWhen we call TfidfVectorizer instead of CountVectorizer, the values assigned to each token (i.e. features) are TF-IDF scores rather than binary presence / absence or frequency counts. Similar to the example in Chapter 12, we can use this vectorizer to produce a DTM. (Alternatively, we could use sklearn’s TfidfTransformer() to convert the count-based DTM from Chapter 12 to TF-IDF.) The shape of the resulting matrix would be identical to before, but only because we are passing the exact same arguments to the vectorizer, which is deterministic. If parameters were different, we would obtain different results.\nwith open ('data/british_hansard_processed_sample.pkl', 'rb') as fp:\n    preprocessed = pickle.load(fp)\ntfidf_vectorizer = TfidfVectorizer(\n    max_df=.1,\n    min_df=3,\n    strip_accents='ascii'\n)\n\ntfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed) \ntfidf_matrix.shape\nTo help clarify the differences between the count data and the TF-IDF scores, let’s construct a dataframe with the counts from the previous chapter and the above TF-IDF scores for each token across all documents. Given that the TF-IDF vectorized documents here have the same matrix shape as the count vectorized ones – so, num_documents x num_features – it follows that we have the same documents and features here. This means the vocabulary is identical and we can use the same one for both matrixes!\nwith open ('data/british_hansard_sample_dtm.pkl', 'rb') as fp:\n    count_matrix = pickle.load(fp)\n\ntfidf_scores = np.ravel(tfidf_matrix.sum(0))\ntfidf_scores = tfidf_scores/np.linalg.norm(tfidf_scores)\nterm_counts = np.ravel(count_matrix.sum(0))\nterm_counts = term_counts/np.linalg.norm(term_counts)\nvocabulary = tfidf_vectorizer.get_feature_names_out()\n\n\ndf = pd.DataFrame({'Term': vocabulary, 'TFIDF': tfidf_scores, 'Count': term_counts})\ndf.sort_values(by='TFIDF', ascending=False, inplace=True)\nThe code below creates a scatterplot showing each token in the corpus by count and TF-IDF. The result is Figure 13.1.\nsns.jointplot(data=df.head(5000), x='Count', y='TFIDF', kind='hist')\nplt.savefig('figures/12_01.png', dpi=300)\n\n\n\n\n\n\nFigure 13.1: png\n\n\n\nWhen you inspect this plot, you should notice that:\n\nmost tokens in the vocabulary are used very rarely, and so the marginal distribution of counts is skewed towards low values,\nmost tokens in the vocabulary have relatively low TF-IDF scores,\nthe tokens with high count values almost always have low TF-IDF values, and\nthe tokens with high TF-IDF scores tend to have lower counts.\n\nIf you understand the TF-IDF formula, this should make intuitive sense. If it doesn’t, I recommend reviewing the formula, as you don’t want to proceed with a text analysis that relies on a token scoring method that you don’t understand.\nTo visualize the relationship between counts and TF-IDF weights, we used two matrices (count_matrix and tfidf_matrix) with the same shape. The reason why those two matrices have the same shape is because we passed the same arguments to both vectorizers. But actually, we should not really be using the min_df and max_df arguments with TfidfVectorizer. The reason is because TF-IDF assigns very low scores to the tokens at the top and bottom of the frequency distribution, so removing them is unhelpful and can change the actual scores that are computed. Before continuing to analyze our dataset with tokens weighted by TF-IDF, let’s construct a final TF-IDF DTM without the min_df and max_df arguments.\ntfidf_vectorizer = TfidfVectorizer(strip_accents='ascii', sublinear_tf=True)\n\ntfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed)\ntfidf_matrix.shape",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Text similarity and latent semantic space</span>"
    ]
  },
  {
    "objectID": "mapping-text.html#computing-semantic-similarity-and-clustering-documents",
    "href": "mapping-text.html#computing-semantic-similarity-and-clustering-documents",
    "title": "13  Text similarity and latent semantic space",
    "section": "13.3 Computing Semantic Similarity and Clustering Documents",
    "text": "13.3 Computing Semantic Similarity and Clustering Documents\nIn the previous chapter, I mentioned that each feature and each document has an associated vector of numbers. The documents, or row vectors, assign a specific value to the document for each feature in the DTM. The features, or column vectors, tell you how a specific feature is distributed across documents. Because each document is represented by a vector, this approach is also known as a vector space model; the documents are represented by ‘long and sparse’ vectors that position them in relation to one another in multi-dimensional vector space. This makes it relatively simple to assess the semantic similarity between documents using measures of the distance or similarity between their vectors representations.\nPerhaps the most basic of these measures is Euclidean distance, which is a flat measure of the distance between two points in a Euclidean space like you find in classical geometry. This measure works fine when you just want to compare the literal text contained in documents, as in plagiarism detection software. For that purpose, the importance of individual tokens in a document matters less than the degree to which two documents have similar tokens that appear with similar frequency. This is a pretty reliable measure of how similar the text strings are between documents.\nEuclidean distance has some limitations when it comes to measuring semantic similarity, however, and especially when we are working with term counts rather than TF-IDF weights. This is because Euclidean distance tends to overestimate the importance of tokens that appear frequently, as they increase the magnitude of the vectors in space. For example, imagine two documents about natural language processing in some larger corpus. If the term “language_processing” appears 100 times in the first document about that topic but only once in the other, then there’s a good chance the the Euclidean distance between these two documents will be large, suggesting (incorrectly) that they are about totally different topics! This overestimation is most pronounced when comparing texts of different lengths, as longer documents will of course have higher token counts. One benefit of using similarity and distance-based measures to compare vectors of TF-IDF scores is that the vectors themselves take these document length differences into account.\nUnlike Euclidean distance, cosine similarity compares the angle between two vectors; whereas Euclidean distance measures how far the vector has extended into space in a given direction, cosine distance considers only the direction the vector is headed in. To paraphrase Gregory Bateson, it’s the difference that makes a difference. The result is a document similarity score that better compares the two documents in terms of their conceptual/abstract similarity, rather than their physical make-up. Let’s see what this looks like by comparing the cosine similarity between speeches by MPs from different parties using the tfidf_matrix produced above.\nWe’ll use the same sparse_groupby function we used in the previous chapter to aggregate the TF-IDF scores into a dataframe where each row is a vector for the entire party.\n# with open ('data/british_hansard_processed_sample.pkl', 'rb') as fp:\n#     speech_df = pickle.load(fp)\n    \nspeech_df = pd.read_csv('data/sampled_british_hansard_speeches.csv')\n\nparty_names = speech_df['party']\ntfidf_vocabulary = tfidf_vectorizer.get_feature_names_out()\nparty_scores = sparse_groupby(party_names, tfidf_matrix, tfidf_vocabulary)\nlen(party_names)\nBecause we’ve aggregated the TF-IDF scores by summing them, we should normalize them again to unit norm. We can use the Normalizer() preprocessing utility from sklearn to handle the math for us here. The main benefit of doing it this way is that the sklearn code is highly optimized (it’s actually running C code in the background, which is super efficient) and operates on the whole matrix at once.\nnormalize = Normalizer()\nparty_scores_n = normalize.fit_transform(party_scores)\nNow that we’ve normalized the matrix, we’ll compute the cosine similarity between each pair of vectors. The maths are beyond the scope of this chapter, but what you need to do to compute the cosine similarity between political parties here is to compute the product of our rectangular party-by-feature and a transpose of that same matrix. The result with be a square “self-to-self” cosine similarity matrix. In the code below, the @ symbol is used to compute the product of two matrices.\nsim_matrix = party_scores_n @ party_scores_n.T\nsim_df = pd.DataFrame.sparse.from_spmatrix(sim_matrix).sparse.to_dense()\nThe top-left to bottom-right diagonal will always be 1 in a self-to-self cosine similarity matrix because the diagonal reports how similar each entity (in this case, political party) is to itself. Perfect similarity every time! You might also notice that the data below the diagonal is mirrored above the diagonal. We can use Numpy to clean it up a bit for us by filling the diagonal and one of the triangles (above or below the diagonal, it doesn’t matter which) with np.nan. If we use the .values attribute for Pandas dataframes, we can use Numpy array functions directly without doing any conversions from Pandas to Numpy.\nnp.fill_diagonal(sim_df.values, np.nan)\nsim_df.values[np.tril_indices(sim_df.shape[0], -1)] = np.nan\nNow let’s add in the party names as the index and column names for our fresh, shiny, new cosine similarity matrix.\nsim_df.index = party_scores.index\nsim_df.columns = party_scores.index\nWith a matrix this size, it’s possible to eyeball what’s going on, but when you have a lot of comparisons to make it can be handy to write a bit of code to show you the highlights. For example, we might want to print the 3 most similar and the 3 least similar party pairings for each party. We can do this by using Pandas’ .stack() method to flatten the dataframe dimensions so that .nlargest() and .nsmallest() return results for the entire matrix rather than row by row.\nprint(sim_df.stack().nlargest(3))\nprint(sim_df.stack().nsmallest(3))\nWe can see that Labour and Labour (Co-op) have very high similarity, and that both have similarities with the Liberal Democrats (who from time to time have had pacts with Labour). All three of these parties are considered left-of-centre. On the other hand, the Green Party and Plaid Cymru are also considered left-leaning, but Plaid Cymru is a Welsh nationalist party seeking independence from the UK, so we should expect to see that they differ from the other parties despite being social democratic. The Democratic Unionist Party is a right-leaning socially conservative party in Ireland, so their lower similarity to the other two parties also makes some sense.\nWe know that there are similarities between the content of what Labour and Lib Dem MPs have focused on in their speeches, and that Plaid Cymru and the Democratic Unionist Party differ from the others. One way to gain a bit of insight into these comparisons is to look at the tokens that are most strongly associated with each party. Below, we’ll print the 10 most associated tokens for each of the four parties. Note that these will differ a bit from the scores we previously computed because we are working with TF-IDF scores, not counts.\nparty_scores_df = pd.DataFrame.sparse.from_spmatrix(party_scores_n)\nparty_scores_df.index = party_scores.index\nparty_scores_df.columns = tfidf_vectorizer.get_feature_names_out()\n\nfor party in ['Labour','Liberal Democrat', 'Democratic Unionist Party', 'Plaid Cymru']:\n    print(party + '\\n')\n    print(party_scores_df.loc[party].nlargest(10))\n    print('\\n')\nThe highest scoring tokens for Labour and the Liberal Democrats are not particularly noteworthy in this case. The Irish and Welsh parties, on the other hand, have very high scores for the terms that refer to their respective countries. Remember that TF-IDF scores terms highly if they appear frequently in a given document but don’t appear in many documents in the corpus. The high scores in this case may indicate that these parties often refer to their home countries in parliament or that the rest of the parties don’t talk about them much - it’s likely some combination of these two factors.\nWhile cosine similarity performed on token count (e.g., count__matrix from the CountVectorizer()) and TF-IDF weights (e.g., tfidf_matrix from the TfidfVectorizer()) does a better job of measuring meaningful similarity between documents, it still relies on exact term matches to calculate the spatial distances. This is a significant limitation of using long sparse vector representations. In later chapters, we will discuss short dense vector representations called word embeddings that allow us to go beyond identical token matches to compare the semantic similarity of tokens and documents that are conceptually close. Using these short and dense word embeddings in similarity analysis means that seemingly identical words with different meanings can be differentiated based on their usage contexts, while other words that are not identical can be considered conceptually close. For now, we’ll move onto another set of exploratory text analysis methods: Latent Semantic Analysis, or LSA.",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Text similarity and latent semantic space</span>"
    ]
  },
  {
    "objectID": "mapping-text.html#exploring-latent-semantic-space-with-matrix-decomposition",
    "href": "mapping-text.html#exploring-latent-semantic-space-with-matrix-decomposition",
    "title": "13  Text similarity and latent semantic space",
    "section": "13.4 EXPLORING LATENT SEMANTIC SPACE WITH MATRIX DECOMPOSITION",
    "text": "13.4 EXPLORING LATENT SEMANTIC SPACE WITH MATRIX DECOMPOSITION\nSo far, we’ve discussed how to represent unstructured text data as long and sparse vectors. These vectors are stored as structured matrices, where the rows are documents, the columns are tokens, and the cells are either Boolean (a token is present or absent), frequencies, or TF-IDF weights. You saw how we can use token frequencies and proportions to do some preliminary comparisons of language-use across document collections, and how to perform some simple semantic similarity comparisons. While useful, there are some limitations in using these methods to learn about the actual topical content of the documents in our corpus.\nRemember, our DTMs are made up of long and sparse vectors. One of the first substantial breakthroughs in contemporary topic modelling was the realization that one could use matrix decomposition methods (i.e., dimensionality reduction) to construct a set of latent variables that could be used to position documents in relation to one another in latent semantic space. These latent variables could be interpreted as latent topics or concepts that are more abstract than the individual tokens that make them up.\nThese are the same type of latent variable and dimensionality reduction methods that you learned about in Chapter 9, only applied to matrices that represent the text content of a document collection. Remember that latent variables are variables that exist but which cannot be directly observed or measured. Instead, we use dimensionality reduction methods to infer them from the lower-level features that can be observed and measured. In the case of text analysis, the observed and measured low-level features are specific tokens. We can interpret the latent variables several ways, but generally, we refer to them as latent topics.\nIn Chapter 9 you learned about dimensionality reduction methods with an emphasis on Principal Component Analysis (PCA). As a brief reminder, PCA and other dimensionality reduction methods reduce the number of features in a dataset by combining highly correlated, or covarying, features into principal components. These principal components represent latent dimensions of a dataset, always at a higher level of abstraction than the original features. When we are performing dimensionality reduction on text data, we often used a method called truncated Singular Value Decomposition (SVD) rather than PCA. SVD and PCA are very similar, but SVD is an extension that can be used for non-square matrices (recall that PCA starts by converting data to a square matrix, e.g., correlation) and it handles sparse matrices efficiently. When SVD is used in the context of text analysis and the vector space model, it is called Latent Semantic Analysis (LSA).\n\n13.4.1 Latent Semantic Analysis (LSA) with Singular Value Decomposition (SVD)\nSingular Value Decomposition is a dimensionality reduction method comparable to PCA introduced in Chapter 9. Other than relatively small differences in how PCA and SVD decompose matrices, the salient difference between the latent variable analyses here and from Chapter 9 is interpretation. When the original features are individual tokens from the vocabulary, the latent components that are produced via the linear combination of highly-correlated features are interpreted as topics. This is just another latent variable problem where we attempt to learn about the latent variables by decomposing matrices using methods from linear algebra.\nWe won’t get deep into the mathematics here, but it important that you have at least a conceptual understanding of how SVD works. It all starts with a feature matrix, which in a text analysis like this will typically be a DTM. In this example, we will use our tfidf_matrix DTM.\nSVD decomposes the DTM into three smaller matrices, each of which contains essential information that can be used to reconstruct the original matrix:\n\n\\(U\\), which is a matrix with documents in the rows and latent topics in the columns,\n\\(S\\), which is a diagonal matrix of singular values indicating the importance of each topic, and\n\\(V\\), which is a matrix with latent topics in the rows and tokens from the vocabulary in the columns.\n\nThe columns in matrix \\(U\\) are orthogonal to one another, and the rows in matrix \\(V\\) are orthogonal to one another. When you multiply these three matrices, you get a matrix that is extremely close, or approximately equivalent, to the original matrix (i.e., our DTM). This is represented in the equation:\n\\[\n\\text{DTM} \\thickapprox U \\cdot S \\cdot V\n\\]\nFigure 13.2 below further clarifies the relationships between these three matrices.\n\n\n\n\n\n\nFigure 13.2\n\n\n\nWe can use these three matrices to interpret the latent topics in our dataset. We can use \\(V\\) to learn about the tokens most strongly associated with each latent topic, enabling us to interpret what that latent topic represents. We can use \\(S\\) to learn roughly how important the topic is relative to the other topics. Finally, we can use \\(U\\) to better understand how the discovered topics are distributed across the documents in our corpus.\nAs with PCA, when we perform an SVD we will get back a number of latent components equal to the number of features in the original matrix, and those components will be sorted such that the ones explaining the most variance come first, and those explaining the least amount of variance come last. We are almost never interested in using all of the topics, so we usually select some subset of the latent components to interpret. This is called truncated SVD, and is the approach implemented in sklearn. This means we have to tell sklearn in advance how many components we want.\n\n13.4.1.1 LSA via SVD in sklearn\nTo conduct an LSA analysis with sklearn, we first initialize a TruncatedSVD() object and indicate the number of latent topics we want by using the n_components argument. In this case, we will set the number of components to work with to 20.\nlsa = TruncatedSVD(n_components=100, n_iter=6, random_state=12)\nNow we can fit it to our tfidf_matrix (or count_matrix, for that matter) to actually execute the LSA.\nlsa = lsa.fit(tfidf_matrix)\nAs previously mentioned, the singular values (the diagonal matrix \\(S\\)) summarize the relative importance of each of the latent dimensions. We can access these values from the .singular_values_ attribute of the fitted lsa model object. Plotting them gives us a quick view of how important each latent dimension is. Let’s look at the top 20 singular values.\nsvs = lsa.singular_values_[:20]\nsvs\nEach dimensions contains a little bit of pretty much every term in the vocabulary. When you are interpreting the meaning of the dimensions, what you want to look for is the terms that have the highest values.\nTo make this a bit easier, we can transpose the lsa.components_ matrix (V Figure 13.2) to create a dataframe where the rows are terms in the vocabulary and each column represents one of the latent dimensions. The score in each cell tells you how strongly associated that word is for the given topic.\n# transpose the dataframe so WORDS are in the rows\nword_topics = pd.DataFrame(lsa.components_).T \ncolumn_names = [f'Topic {c}' for c in np.arange(1,101,1)]\nword_topics.columns = column_names\n\nword_topics.shape\nLet’s get a list of the tokens in the vocabulary and use them as an index for our dataframe.\nterms = tfidf_vectorizer.get_feature_names_out()\nword_topics.index = terms\n\nword_topics.sort_values(by='Topic 2', ascending = False)['Topic 2'].head(20)\nNow we can easily use .loc[] on our dataframe to get the scores for any specific word across all latent topics. To get the topic scores for England, we would pull the row vector for the row indexed with england. Since the result is simply a Pandas Series, we can sort it to print the topics in order of most to least strongly associated. Note that we have to make-do with wale rather than wales because the word has been lemmatized during pre-processing. Using named entity recognition, which you will learn about later chapters, you can ensure that this doesn’t happen during pre-processing.\ncompare_df = pd.DataFrame()\n\ncompare_terms = ['england', 'scotland', 'wale', 'ireland']\n\nfor i, term in enumerate(compare_terms):\n    scores = word_topics.loc[term].sort_values(ascending=False)\n    compare_df[i] = scores.index\n    compare_df[term] = scores.values\ncompare_df.head()\nNote that for many terms (including england and ireland) the requested terms are not strongly loaded for any particular theme. This is different for scotland and wale, however. This suggests that there may be a topic here that is focused on issues related to Ireland, but perhaps not for Scotland and Wales. Now, it turns out this is a bit tricky, so let’s think things through for a moment. Perhaps most importantly, we need to understand what these loadings (weights) tell us. When looking at a given topic, the loading of a word between -1 and 1 is the contribution it makes to the composition of that latent topic. A score closer to 1 means the presence of that word contributes, while a score closer to -1 means the absence of that word contributes to the definition of the topic. Scores around 0 have very little impact. In LSA, words are considered in relation to the words they appear with, so a focal word might only indicate a certain topic if some other word isn’t in the document.\nAn example of an ideal outcome can be helpful here: if your focal word was escape and it appeared in the same document as backslash, you could assume the topic of the document was related to computers or programming. If instead the word prison appeared in the document, it would suggest the topic of the document was a prison escape. So for the latent topic of “computing”, the word prison could end up fairly negatively loaded, as it distinguishes between the two different uses of escape. LSA is capable of distinguishing between different uses of the same word, but it’s important to put some thought into what the negative loadings mean in relation to the positive ones.\nIn both of these cases, we can find out what topics a given word is most associated with, but since there is no guarantee that the word we are interested in is actually important (or even really relevant) to the topic, this is not an especially helpful way to discover what the topic is about. Instead, if we want to know what words are most strongly associated with each topic, we can pull the top (and bottom!) scoring words for each.\nTo do so, we can use the utility function get_topic_word_scores() from the dcss package. One of the arguments is all_topics. By default, this argument is set to False, which means the function will return data on the top \\(n\\) words and their scores for the specified topic. If changed to True, the function returns a full dataframe with all the other topics alongside the topic of interest. The word scores for these other topics tell you the relationship that the top words for the topic of interest have across the other topics, so it is important to interpret this properly. Let’s explore the topic that’s most relevant to “scotland”.\nword_topics.loc['scotland'].sort_values(ascending=False)\nget_topic_word_scores(word_topics, 10, 'Topic 8')    \nSo with LSA, we can have a topic that is significantly “latent” that we have to decipher. A key topic related to Scotland is about business and school but is also distinguished by not being about crime and police. It’s important to know that computational methods are improving all the time, so earlier ones may have revealed very solid topics but without telling us what they are. In this case, we might have to examine why Scotland, business, police, and school would be a topic of British parliamentary debate. Thankfully, new methods are being developed all of the time, for market goals rather than social science ones, so we’ll explore those in later chapters. Before moving on, let’s briefly take stock of what we’ve done here. First, we constructed a document term-matrix using sklearn’s TfidfVectorizer(). Then we decomposed the DTM with truncated SVD, which produced a matrix with the component coefficients for each of the 67,204 sampled political speeches on 100 latent dimensions, which we can interpret as representing latent topics. The final step is to interpret the results by inspecting the terms that contribute the most to each latent dimension.\n\nFurther Reading\nIf you want to deepen your understanding of latent semantic analysis, and what it was originally developed to do, I would suggest reading papers by some of the major contributors to the methodology. I recommend Dumais (2004) and Deerwester et al. (1990). This work is an important foundation for some of the machine learning models used for text analysis that we discuss later in the book.",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Text similarity and latent semantic space</span>"
    ]
  },
  {
    "objectID": "mapping-text.html#conclusion",
    "href": "mapping-text.html#conclusion",
    "title": "13  Text similarity and latent semantic space",
    "section": "13.5 CONCLUSION",
    "text": "13.5 CONCLUSION\n\n\n13.5.1 Key Points\n\nUsed SpaCy’s TfidfVectorizer to compute TF-IDF scores for weighting tokens\nVector space models represent documents using vectors that are long (many features) and sparse (few non-zero values)\nLearned about semantic similarity measures including Euclidean Distance and Cosine similarity\nConducted a Latent Semantic Analysis (LSA) using Singular Value Decomposition (SVD)\n\n\n\n\n\n\nDeerwester, Scott, Susan Dumais, George Furnas, Thomas Landauer, and Richard Harshman. 1990. “Indexing by Latent Semantic Analysis.” Journal of the American Society for Information Science 41 (6): 391–407.\n\n\nDumais, Susan. 2004. “Latent Semantic Analysis.” Annual Review of Information Science and Technology 38 (1): 188–230.",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Text similarity and latent semantic space</span>"
    ]
  },
  {
    "objectID": "networks-as-not-data.html",
    "href": "networks-as-not-data.html",
    "title": "11  Networks and relational thinking",
    "section": "",
    "text": "11.1 WHAT ARE SOCIAL NETWORKS?",
    "crumbs": [
      "**DATA, NOT DATA**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Networks and relational thinking</span>"
    ]
  },
  {
    "objectID": "networks-as-not-data.html#what-are-social-networks",
    "href": "networks-as-not-data.html#what-are-social-networks",
    "title": "11  Networks and relational thinking",
    "section": "",
    "text": "11.1.1 From Independent Individuals to Networks\nLin Freeman’s (2004) history of social network analysis starts with a colorful (and slightly unsettling) quote from an American sociologist in the late-1960s that gets straight to the core of the difference between network analysis and the traditional quantitative social science of his day:\n\nFor the last thirty years, empirical social research has been dominated by the sample survey. But as usually practiced, using random sampling of individuals, the survey is a sociological meatgrinder, tearing the individual from his (sic.) social context and guaranteeing that nobody in the study interacts with anyone else in it. It is a little like a biologist putting his (sic.) experimental animals through a hamburger machine and looking at every hundredth cell through a microscope; anatomy and physiology get lost, structure and function disappear, and one is left with cell biology… If our aim to is to understand people’s behaviour rather than simply to record it, we want to know about primary groups, neighbourhoods, organizations, social circles, and communities; about interaction, communication, role expectations, and social control.\nAllen Barton (1968)\n\nWhile many people still practice this kind of “meatgrinder” research, Barton’s distinction is much less salient now than it used to be. Mainstream quantitative social science has changed a lot since he wrote that in 1968, and again since he was quoted by Freeman in 2004. For one thing, network analysis is no longer just an obscure undertaking of mathematically-inclined sociologists, social psychologists, and other social scientists; it’s well within the mainstream of applied quantitative science across dozens of disciplines and is an important research area in contemporary statistics.\n\nTODO: Cite the new edition of the handbook here.\n\nNetwork analysis is one of several major developments in quantitative data analysis that attempts to model interdependent relationships and institutional contexts. Another, multilevel analysis (or hierarchical modelling), will be covered in Chapter 29. Both seek to explicitly model the complex interdependencies between entities (e.g., people) by emphasizing their shared contexts, relationships, and interactions. However, in network analysis, an entity’s context is typically their connections to other entities and their structural position in a network (a concept we will discuss briefly here and again in later chapters). In multilevel analysis, an entity’s context is typically some sort of institutional environment that is shared with other entities, such as classrooms in a school, teams in a league, provinces or states in a country, or countries in the world polity (see Buhari-Gulmez 2010; Meyer, Krücken, and Drori 2009 on world polity theory and Stanford school institutionalism). In a multilevel network analysis (see Lazega and Snijders 2015), the context would be the entities’ concrete connections with one another nested in one of many possible shared institutional contexts, such as networks of informal relations between employees nested in the specific firms they work for (e.g., Brailly et al. 2016).\nWe care about these network connections and shared institutional contexts for many reasons. Perhaps the most obvious is that we think complex interdependencies have important effects on specific outcomes that we care about. For example, we might hypothesize that whether someone believes misinformation that vaccines cause autism depends in part on the beliefs of the people they interact with frequently or whom they trust the most. Or we might hypothesize that one’s overall health and wellbeing depends in part on the health and wellbeing of the people they are connected to. The logic is similar for multilevel analysis, but what follows “depends in part on” would refer to some sort of shared institutional context or environment rather than a complex network of concrete relationships and interactions.\nNot all hypotheses about how networks influence individual outcomes are based on direct connections, however. My friends influence me, but their friends (including those whom I am not friends with) influence them, and my friends’ friends’ friends influence them, and so on. Each step out in such a friendship network usually brings new, but diminished, influences. Networks are complex systems; what happens in one region of the network can affect disparate regions, and seemingly small differences in micro-level interaction processes (e.g., norms around who you interact with, how you interact with them, and what they mean to you) can have dramatic macro-level outcomes. For example, we might design a network study to better understand how micro-level social norms generate macro-level structures that shape disease transmission dynamics (Bearman, Moody, and Stovel 2004), or how network structures differently impact the spread of an infectious disease through a population and the adoption of health behaviours necessary to mitigate the spread of that disease (Centola 2018).\nWe also care about networks and institutional contexts because, as social scientists, we want to understand networks and institutions for their own sakes, inclusive of the social and political processes that generate different kinds of structural configurations. This might be because we are interested in advancing scientific knowledge by doing rigorous theory-driven research, or because we want to leverage that knowledge for some applied reason, such as intervening in a network to mitigate the effects of misinformation and disinformation, to disrupt the diffusion of a violent political ideology, or to improve health and wellbeing.\nThis is what makes networks so interesting and beautiful: we are all linked together in a vast and dense web of intersecting, meaningful, and mutually-influential relationships. But this complexity can quickly get out of hand. There’s a reason why old fashioned quantitative social science worked like a meatgrinder: it was extremely difficult to do much of anything else. Consequently, the history of network analysis is full of fascinating stories of intrepid sociologists, social psychologists, anthropologists, and other social scientists coming up with clever new mathematical models to describe and analyze interdependent relationships, and developing research software to use those models in applied research. Now that network analysis is being practiced across so many scientific fields, methods and models are improving at breakneck speed.\nJust about anything that a social scientist might be interested in can be usefully described in network terms, and just about any research question you might ask could be cast as a question about networks, where the network or some network-based variable might be:\n\npart of an explanation for something else, such as why some people practice safe sex while others don’t, or\nthe thing we are trying to explain, such as why some schools have racially segregated friendship networks while others do not.\n\nDoes this mean you should model everything as a network and pose every question as a question about networks? No (though as a network scientist and sociologist I’m tempted to say yes). Recall Box’s loop: your model should be whatever will best answer your question.\nFreeman didn’t include it in his excerpt of Barton’s 1968 article, but just a wee bit further down the page, Barton poses an important question: “what do we want?” Different things, of course. Network analysis is diverse and interdisciplinary, and you can find meaningful divisions between different groups of network analysts that use different tools to answer different questions. But what unites network scientists of virtually every social science, now and in the past, is a paradigmatic preference for holistic research strategies that focus on people and groups embedded in complex interdependent relationships. Let’s turn towards some of the various ways network analysts have conceptualized those relationships.\n\n\n11.1.2 What is a Network?\nThe spread of network analysis to and from so many scientific fields, often with developments occurring independently of one another, has produced a variety of synonymous terms in network analysis. These interchangeable terms can be confusing when you first run into them. For clarity, we will say that networks consist of a set of entities, which we will usually call nodes (also called vertices or actors), and the relationships between those entities, edges (also called ties or arcs). In theory, a node can be any kind of entity and an edge can be any kind of relationship or interaction between such entities. In the social sciences, we typically work with social networks where the nodes have some sort of agency, such as individual people or groups of people (e.g., an organization), and the edges represent some sort of meaningful relationship between them. Following Kitts (2014) and Kitts and Quintane (2020), we can categorize these common types of edges as defined by:\n\npatterns of sentiment, such as who likes or dislikes whom;\nsocially-constructed role relations, such as friendships, research collaborations, romantic partnerships, doctoral student and supervisor relationships, family;\nbehavioural interactions and contact over time, such as who messages whom;\nproviding access to resources, support, information, and opportunities, such as who contacts whom for advice in a personal crisis, to discuss a problem at work, or to pick up your groceries while quarantining in a global pandemic.\n\nThese four types of edges provide us with a high-level framework for talking about types of networks based on the relational information they encode and, importantly, the questions we want to answer and the theories we are using. In many respects, the type of relation that defines the edges in a network is the most important thing in determining what type of network you have, and what you can reasonably do with it. I emphasize that one of the most common mistakes that novices make is trying to answer a research question, or apply a theory, that is a poor match for the type of network, like trying to answer a question about power and inequality with data on a collaboration network. It can work with the right data and a bit of mental gymnastics, but it shouldn’t have to. As with any other type of research, this is a match that you want to ensure you get right. This is, once again, a matter of good research design.\nGraphically, we can represent nodes as points and edges as lines that connect those nodes. Figure 11.1 is a hypothetical network with five nodes (Chen, Nate, Anika, Anvita, and Patrick) and the relationships between them. When two nodes are connected, such as Chen and Anvita, we say they are adjacent. If we choose to focus on a specific node, we refer to it as the ego, and the nodes ego is directly connected to can be referred to as alters. Together, an ego’s alters represent a neighbourhood. For example, if we are focused on Patrick, Patrick is “ego,” and their neighbourhood would consist of the alters Anvita, Chen, and Anika.\nWe’ll come up with a fictitious story about information sharing in this network later in the chapter. For now, let’s just focus on learning some technical vocabulary and understanding how we can represent social networks with relational data structures.\n\n\n\n\n\n\nFigure 11.1: Cap\n\n\n\nThe edges have arrows because the network is directed as opposed to undirected. For example, there is an arrow pointing from Anika to Chen because Anika sends something to Chen, or initiates something with Chen, that Chen may not reciprocate. Many interactions and relationships can be represented this way. Email and SMS communication can be modelled as directed interactions: “Anika emails Chen” becomes an edge from Anika to Chen. Requests for support or advice can be modelled as coming from one person (“help, please!”) and the support or advice being sent back in return (“Here you go!”). In a network defined by patterns of sentiment, one person may like another who may or may not like them back. Other types of relationship don’t make sense to represent as directed. While one node might nominate another node as a “friend” and not be nominated in return (tragic!), this really shouldn’t be the case in coauthorship. If Anika wrote a book with Chen, Chen must also have written a book with Anika.\nNetworks are simply relational mathematical objects and have no inherent visual form. The image above is just a common and convenient way of representing relational data for small networks. However, just as a scatterplot is a representation of data, rather than the data itself, so too is the above image. With networks, as with text, it’s matrices all the way down. In this case, it’s a square adjacency matrix. Consider Figure 11.2, which shows how the graphical network representations of the network (directed on the left, undirected on the right) align with two different adjacency matrices.\n\n\n\n\n\n\nFigure 11.2: Cap\n\n\n\nFirst, for both the directed network (left) and the undirected network (right), many cells are empty, which represents 0, or the absence of an edge. The diagonals are highlighted in gray, but this is just to emphasize them. The diagonals are 0 because the nodes in these networks are not permitted to connect to themselves, which means that there are no self-loops. In other types of networks, such as email communication networks, self-loops might be possible, and a self-loop could be created by an action such as a node emailing themselves.\nIf you look closely at the undirected network’s adjacency matrix, you will notice that the data above and below the diagonal are mirror images of one another, but not in the directed network. That’s because you can have relational asymmetry in a directed network (Anika can send a tie to Chen that Chen does not reciprocate) but not in an undirected network.\nBoth of these networks are binary; the adjacency matrices contain 0s (not shown) to represent the absence of an edge and 1s to represent their presence. However, we can populate these cells with plenty of other information, often interpreted as some sort of edge weight. For example, in an interaction network we might populate cells with count data representing frequency of interaction within a given time frame. In a sentiment network, we might populate the cells with numbers that indicate different types or levels of sentiment, such as a Likert scale from 1 - 5 or a set of distinctions such as “strong” and “weak.” There is a lot of flexibility in how these data are collected, and it is largely up to you are to make decisions that make sense in the context of your research project.\nTraditionally, researchers have focused on positive ties like friendship, support and sharing, or collaboration and collective action. But as Harrigan, Labianca, and Agneessens (2020) and others have pointed out, “some relationships harm. Others are characterised by avoidance, dislike, or conflict” (page 1). These negative ties are (hopefully) less common, but are disproportionately impactful in our lives. They also operate in ways in that are fundamentally different than positive ties. Networks that incorporate data on the positive and negative ties are called signed graphs, and are a major area of theoretical and methodological development in contemporary network analysis.\nLet’s make two final distinctions. First, the network we are working with here is unipartite, which means there is only one type of node (people) and the matrix storing the data is square, with the same set of nodes in the rows and the columns. However, it is also possible to consider networks with two types of nodes, such as between people and organizational affiliations, or between people and events. This kind of network is bipartite, because there are two types of nodes, and the underlying matrix is a rectangular incidence matrix (or affiliation matrix) with one node type represented on the rows and the other in the columns. There are fascinating theoretical and methodological literatures on bipartite networks (for some foundational ideas, see Breiger 1974; Mützel and Breiger 2020), but regrettably we don’t have the space to discuss bipartite networks here.\nFinally, the example we are working with is a whole network, in contrast to an ego network. As I mentioned, we can think of each node in a network as “ego” with a neighbourhood composed of their direct connections (alters). If the network data is collected to capture all of the relevant relationships within some network boundary (e.g. all students in a classroom), then we are working with a whole network, and the main data collection tasks include specifying the boundaries of the network (e.g., the classroom) within which we want to record relationships. If, however, we collect some sample of people and then collect data on their individual relationships, then we are working with a collection of ego networks, one for each node in the study. Ego networks, fascinating though they are, are also out of scope for this book. If you are interested in learning more about ego network analysis, I recommend Crossley et al. (2015) and Small et al. (2021), as well as the 2020 special issue of Network Science on ego network analysis edited by Perry et al. (2020).\n\nFurther Reading\nThere is no shortage of outstanding conceptual introductions to network analysis. Crossley et al. (2015) provide a great introduction to ego-network analysis, which regrettably is not covered in this book. Christina Prell’s (2012) Social Network Analysis, John Scott’s (2017) Social Network Analysis, Garry Robins (2015) Doing Social Networks Research, and Borgatti, Everett, and Johnson’s (2018) Analyzing Social Networks are all great general introductions to network analysis. If you want to know more about data collection in network analysis, I recommend jimi adam’s (2020) Gathering Social Network Data.",
    "crumbs": [
      "**DATA, NOT DATA**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Networks and relational thinking</span>"
    ]
  },
  {
    "objectID": "networks-as-not-data.html#working-with-relational-data",
    "href": "networks-as-not-data.html#working-with-relational-data",
    "title": "11  Networks and relational thinking",
    "section": "11.2 WORKING WITH RELATIONAL DATA",
    "text": "11.2 WORKING WITH RELATIONAL DATA\nWith these generalities out of the way, let’s start getting into the details of working with relational data.\n\n11.2.1 Edgelists and Nodelists\nAs I mentioned earlier, matrices are the heart of network analysis, but they are not ideal ways to enter, store, or manage network data for anything other than small networks. In contemporary practice, most network data is stored in the form of edgelists and nodelists. In Figure 11.3, the same relational data that is represented in the graph, and encoded in the adjacency matrix above, is encoded as an edgelist. The first two columns of the edgelist is where the relational data itself is stored. The columns are labelled source and target because the network is directed; the node in the source column sends an edge to the node in the target column.\n\n\n\n\n\n\nFigure 11.3: Cap\n\n\n\nWhile an edgelist only requires pairs of nodes, we can also include additional columns that provide data about the relationship. There is nothing special or unique about data that describes these edges except for the fact that they describe characteristics of the relationship between two entities rather than characteristics of the entities themselves. Just as we carefully specify variables for describing the attributes of entities, we can carefully specify variables for describing the attributes of relationships between entities. For example, we might have a variable that categorizes edges by the type of relationship (e.g., family, friend, foe, professional, romantic, people who dance together, people who do intravenous drugs with one another) or by it’s sentiment (positive, neural, negative), to suggest just a couple of possibilities. Just as we can with an adjacency matrix, we can record edge weight (such as interaction frequency) as a variable in the edgelist. In longitudinal or dynamic networks (discussed in later chapters), we might also record the wave that a relationship was observed in, or perhaps a timestamp of when the interaction occurred (e.g., when Chen sent an email to Anvita). It may be a simple point, but it’s very important to understand: we can record any empirical observations of the attributes of relationships. The same considerations about what to observe and record apply for edges as with nodes. There is nothing special about edge data except that it describes edges.\n\n\n11.2.2 Graph Objects from Edgelists (and Matrices and More)\nLet’s examine some actual data. In the code block below, we will import a couple of packages and load up some relational data collected from a group of French high school students in 2013. This dataset is one of several collected by the SocioPatterns research team (with collaborators the ISI Foundation in Turin, the Centre de Physique Théorique in Marseille, and Bitmanufactory in Cambridge). The particular dataset we will use describes a network of face-to-face contacts between high school students in Marseille over a five-day period in December 2013. This data was collected via contact diaries, in which students recorded who they came into contact with (restricted to other students in the study) and for how long. Similar data was also collected with wearable sensors, but we will just focus on the contact diaries for now.\nThe edge data is provided in a CSV file with three columns: i, j, and weight. Physical co-presence is, of course, naturally undirected. It is not possible to be physically co-present with someone who is not also physically co-present with you. Therefore, the edgelist names the columns with i and j instead of source and target. This also means that a tie from i to j is the same as a tie from j to i. Finally, edge weight data is stored in the weight column and is coded as follows:\n\nweight = 1 if i and j were co-present for at most 5 minutes\nweight = 2 if i and j were co-present for 5-15 minutes\nweight = 3 if i and j were co-present for 15-60 minutes\nweight = 4 if i and j were co-present for more than 60 minutes\n\nWe can load this edge data in a Pandas dataframe and perform any necessary cleaning before reading the edge data into the NetworkX package to create a graph object that we can analyze using network methods.\n\nTODO: Swap this content out with the introductory graph-tool content from the FCIT 607 tutorial?\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport pandas as pd\nimport seaborn as sns\n\nfrom dcss import set_style, download_dataset\n\nset_style()\ndownload_dataset(\n    \"https://www.dropbox.com/scl/fo/wbj6l2tyoc67o3vlonxp5/AERxFPRIhfgWG_MaVK_rjM4?rlkey=48x2taz2t5mru1j2ucjf670a8&st=e5qs5gw8&dl=0\",\n    save_path=\"data/SocioPatterns/\"\n)\ncontact_diaries = pd.read_csv(\"data/SocioPatterns/Contact-diaries-network_data_2013.csv\", sep=' ')\ncontact_diaries.head()\ncontact_diaries.info()\nAll three columns in this data are numeric: the nodes in columns i and j are represented by numerical IDs rather than the names of the participants in the study. There are 502 rows in this edgelist, which means there are 502 observed edges.\nWe can import this weighted edgelist into the Python package NetworkX, which will transform our edge data into a graph object that we can analyze using methods and models from network analysis. NetworkX provides a number of useful functions for doing this. We’ll use from_pandas_edgelist() because our data is stored in an edgelist format in a pandas dataframe. When we construct the network G, we’ll provide NetworkX with the names of the columns that contain the IDs for each node in an edge. Any other columns in the dataframe will be treated as edge attribute data. Finally, we will also tell NetworkX that this is an undirected graph by passing the argument create_using=nx.Graph(). This tells NetworkX that, when it is creating the network, an edge from i to j is the same as an edge from j to i. If we were working with a directed network, we could pass the argument create_using=nx.DiGraph() instead.\nG = nx.from_pandas_edgelist(contact_diaries, 'i', 'j', create_using=nx.Graph())\nG.name = 'Reported Contacts (Diary Data)'\nprint(G)\nYou might be wondering why the number of rows in the edgelist differs from the number of edges in the network object G. The reason is because different students report the same relation. i might say they spent between 15 and 30 minutes with j and j might later report the same contact with i. However, it seems that not all students reported all interactions (if they had, we would expect there to be 502 reports of 251 edges). Because we believe that students are more likely to forget to record an interaction than they are to fabricate one in their contact diary, we symmetrize the relationship, making it undirected (if i spent time with j, then the reverse must necessarily be true). We have informed NetworkX that this network should use undirected edges by specifying a Graph object rather than a DiGraph (directed graph).\nFinally, before moving on, let’s create a quick visualization of this network (Figure 11.4). This is an inherently challenging task as networks are very high-dimensional objects and we are limited to 2 dimensions. It’s best not to rely on visualizations such as these for any serious analytic work, but for relatively small networks they can still be informative.\n\nTODO: Critique the visualization; hint that we will do better in a couple of chapters.\n\nlayout = nx.nx_pydot.graphviz_layout(G)\nfig, ax = plt.subplots(figsize=(12, 12))\nnx.draw(G,\n        pos=layout,\n        node_color='gray',\n        edge_color='lightgray',\n        node_size=10,\n        width=.5)\nplt.savefig('figures/13_04.png', dpi=300)\n\n\n\n\n\n\nFigure 11.4: png\n\n\n\nYou’ll see similar visualizations one in the coming chapters. Each time you’ll develop a deeper understanding of what to look for, such as clusters of densely connected nodes, or pendants hanging off a dense cluster of nodes at the core of a network. For now, let’s keep moving.\nNetworkX has a variety of functions for reading network data stored in other formats as well, and you can find the one that works with the your data’s format by checking the documentation, which you can find at https://networkx.org/documentation/latest/. Some of the formats supported at the time of writing include edgelists and matrices stored in Python’s built in data structures (e.g., dictionaries) as well as numpy arrays and matrices, scipy matrices, and pandas dataframes. In addition to these edgelist and matrix data structures, NetworkX can also read and write network objects using file formats commonly used by other network analysis software, including graphml, GEXF, JSON, and Pajek files. Unless you are have a specific need for them, my recommendation is that you store your network data in the form of an edgelist and a nodelist in separate plain text CSV files.\nNow it’s time to turn to some important building blocks of network analysis. We will start by briefly discussing the “walk-structure” of a network and the notion of network flow. This is intended to get you to start thinking about networks in a particular way in preparation for content that appears in the next chapter. We will then turn to some of the basic micro-level building blocks of network structure, dyads and triads. Together, these two final sections provide a foundation for much of what follows in later chapters.",
    "crumbs": [
      "**DATA, NOT DATA**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Networks and relational thinking</span>"
    ]
  },
  {
    "objectID": "networks-as-not-data.html#walk-structure-and-network-flow",
    "href": "networks-as-not-data.html#walk-structure-and-network-flow",
    "title": "11  Networks and relational thinking",
    "section": "11.3 WALK-STRUCTURE AND NETWORK FLOW",
    "text": "11.3 WALK-STRUCTURE AND NETWORK FLOW\nOne of the most foundational ideas in social network analysis is that your position in a network affects your ability to access or shape the flow of things – such as resources, information, or knowledge – through that network. Those things are often called contagions, or social contagions, even when they are not literally contagious in the way an infectious disease is (and most contagions that social scientists are interested in do not spread like infectious diseases, as you will learn in Chapter 18). Depending on where a node is positioned in the network, they can control or otherwise manipulate how a contagion is flowing through a network. For example, they may prevent someone from learning about some important information by feeding them incorrect information or by betraying their trust and spilling their secrets to the rest of the office.\nThis notion of contagions flowing along the structure of a network enables disparate regions of that network to influence one another. In a friendship network, for example, we are influenced by our friends, by our friends’ friends, by our friends’ friends’ friends, and so on. But exactly how does that happen, and to what degree? Which of the many possible paths do contagions flow on? All of them at once? Some subset of paths? If so, which ones? Perhaps a single most optimal one?\n\n11.3.1 Walks, Trails, Paths, and Cycles\nEarlier I mentioned that we would spin up a fictitious story about our five person network (Chen, Patrick, Anika, Anvita, and Nate). We’ll do that now to help clarify some of the essential terminology, but we’ll start with a slightly smaller version – just Anika, Chen, and Patrick – and then add the others in.\nImagine a research lab with an upcoming team meeting with an ominous sounding item on the agenda. One member of the team, Chen, knows something about the context for this item and shares it with another member of the team, Patrick. In this scenario, Chen sends the information to Patrick (remember, this makes them adjacent).\nAs we’ve discussed, nodes are also linked to other nodes indirectly. Let’s say Chen’s information came from another team member, Anika. Anika and Patrick are not adjacent, but they are indirectly connected via Chen. How can we describe this relationship? And why does it matter whether we describe it at all?\nIn network analysis, we are often interested in whether something (e.g., information about the ominous sounding agenda item) can travel from node \\(i\\) (e.g., Anika) to node \\(j\\) (e.g., Patrick). If it is indeed possible for that to happen, how many people would it have to go through to get there? And is there more than one way it might get there? If so, are some ways more efficient than others? If so, which ones? We can answer questions such as these about any nodes in a network by invoking the concept of a walk, which also provides both general and specific terminology for describing a wide variety of indirect relationships (Borgatti and Everett 2020). Consider, the hypothetical network in Figure 11.5, which we just described with words.\n\n\n\n\n\n\nFigure 11.5: Cap\n\n\n\nA walk is simply any sequence of adjacent nodes and edges that start with some node and end with a node. They can even start and end with the same node. In fact, the same node can appear in a walk more than once, and so can the same edges! In short, a walk is a very general way of describing any way that you can go from one node to another by “walking” along the edges, even if what you want to do is get back to where you started. There are no restrictions provided the edges to walk on actually exist (or rather, are observed). This opens all kinds of useful ways of thinking about the distances between nodes, operationalized in terms of lengths of walks, which is defined in terms of the number of edges contained in the walk. In the above network, the walk from Anika to Patrick passes through 1 node, Chen, but has a length of 2 because it consists of the relationship between Anika and Chen, and between Chen and Patrick (2 edges).\nLet’s complicate this just a wee bit by introducing a few additional team members, our fictitious friends Anvita and Nate. Chen, especially anxious about the ominous agenda item, shares the information with Anvita and Nate. Anvita doesn’t know Patrick already knows, so shares the information with Patrick. Patrick doesn’t know the information ultimately came from Anika, so sends the information back to Anika. Nate prefers not to pass the information along because they aren’t sure how credible it really is. Figure 11.6 shows the structure of this network with the new information sharing relations. Note that you can’t read this representation left to right! The information flow process started with Anika in this hypothetical example.\n\n\n\n\n\n\nFigure 11.6: Cap\n\n\n\nOur initial walk from Anika to Patrick still exists, of course, but now we also have the possibility of many other walks. Anika to Chen to Nate is a walk. Anika to Chen to Anvita to Patrick and back to Anika is a walk. More specifically it is a closed walk because it starts and ends with the same node: Anika.\nIn empirical networks, the number of possible walks between any pair of nodes can be vast, but we can impose some order by grouping them into different kinds of walks, such as trails, paths, and cycles. A trail is type of walk where edges are not allowed to repeat themselves. For example, Anika to Chen to Anvita to Patrick to Anika is a trail but the exact same walk would not be a trail if we included another step to Chen (as that would be repeating an edge). The length of a trail is equal to the number of edges contained in the trail, which in the example above would be 4. A path is a type of walk where nodes are not allowed to be repeated. That means that the trail from Anika to Chen to Anvita to Patrick to Anika is not a path, because Anika is repeated twice, but Anika to Chen to Anvita to Patrick is. As with trails, the length of a path is equal to the number of edges it contains. Finally, cycles are types of closed walks that (a) involve a minimum of three nodes where the only node that is repeated is the node that starts and ends the walk, and (b) no edges are repeated. There are many other specific types of walks that we will not discuss here. For directed networks, the three-node minimum may be relaxed if you have two nodes that send an edge to each other. This is sometimes called a trivial cycle and its status as a cycle or not should depend on why you care about cycles.\nAll of these examples are walks. Some of those walks are trails, and some of those trails are paths and others are cycles. If there is a path between two nodes, say between Anika and Nate, then we say that those two are reachable.\nIn connected networks, there are typically many possible paths that connect any given pair of nodes in a network, but they are not all equally efficient. While information and other resources can certainly travel through a network via inefficient routes, the likelihood of actually going the distance is much greater when traveling on efficient paths. For that reason, we are commonly interested in focusing on the shortest paths between nodes. We will spend a good amount of time in the next chapter discussing shortest paths, followed by a discussion of some alternative assumptions about how contagions flow in a network.\nWe will leave our discussion of walk-structures for now. The key thing to emphasize right now is the general logic of traversing a social network this way, and to understand that the structure of the network affects the flow of contagions through it, meaning people in the network will be differentially exposed to those contagions, good or bad. We will return to this issue in the next chapter. For now, let’s turn to the some micro building blocks of network structure.",
    "crumbs": [
      "**DATA, NOT DATA**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Networks and relational thinking</span>"
    ]
  },
  {
    "objectID": "networks-as-not-data.html#conclusion",
    "href": "networks-as-not-data.html#conclusion",
    "title": "11  Networks and relational thinking",
    "section": "11.4 CONCLUSION",
    "text": "11.4 CONCLUSION\n\n\n11.4.1 Key Points\n\nRelational thinking provides new, different, and valuable ways of approaching social science\nDifferent types of ties change how we should think about a network\nLearned how to work with network files and datatypes in NetworkX\nDiscussed walks, paths, cycles, trails: ways of describing how things can move or traverse through a network\n\n\n\n\n\n\nadams, jimi. 2020. Gathering Social Network Data. SAGE Publications Incorporated.\n\n\nBarton, Allen. 1968. “Bringing Society Back in Survey Research and Macro-Methodology.” The American Behavioral Scientist 12 (2): 1.\n\n\nBearman, Peter, James Moody, and Katherine Stovel. 2004. “Chains of Affection: The Structure of Adolescent Romantic and Sexual Networks.” American Journal of Sociology 110 (1): 44–91.\n\n\nBorgatti, Stephen, and Martin Everett. 2020. Three Perspectives on Centrality. Edited by Ryan Light and James Moody. Oxford University Press.\n\n\nBorgatti, Stephen, Martin Everett, and Jeffrey Johnson. 2018. Analyzing Social Networks. Sage.\n\n\nBrailly, Julien, Guillaume Favre, Josiane Chatellet, and Emmanuel Lazega. 2016. “Embeddedness as a Multilevel Problem: A Case Study in Economic Sociology.” Social Networks 44: 319–33.\n\n\nBreiger, Ronald. 1974. “The Duality of Persons and Groups.” Social Forces 53 (2): 181–90.\n\n\nBuhari-Gulmez, Didem. 2010. “Stanford School on Sociological Institutionalism: A Global Cultural Approach.” International Political Sociology 4 (3): 253–70.\n\n\nCentola, Damon. 2018. How Behavior Spreads: The Science of Complex Contagions. Princeton University Press Princeton, NJ.\n\n\nCrossley, Nick, Elisa Bellotti, Gemma Edwards, Martin G Everett, Johan Koskinen, and Mark Tranmer. 2015. Social Network Analysis for Ego-Nets: Social Network Analysis for Actor-Centred Networks. Sage.\n\n\nFreeman, Linton. 2004. “The Development of Social Network Analysis.” A Study in the Sociology of Science 1 (687): 159–67.\n\n\nHarrigan, Nicholas, Giuseppe (Joe) Labianca, and Filip Agneessens. 2020. “Negative Ties and Signed Graphs Research: Stimulating Research on Dissociative Forces in Social Networks.” Social Networks 60: 1–10.\n\n\nKitts, James. 2014. “Beyond Networks in Structural Theories of Exchange: Promises from Computational Social Science.” In Advances in Group Processes. Emerald Group Publishing Limited.\n\n\nKitts, James, and Eric Quintane. 2020. “Rethinking Social Networks in the Era of Computational Social Science.” The Oxford Handbook of Social Networks, 71.\n\n\nLazega, Emmanuel, and Tom Snijders. 2015. Multilevel Network Analysis for the Social Sciences: Theory, Methods and Applications. Vol. 12. Springer.\n\n\nMeyer, John W, Georg Krücken, and Gili Drori. 2009. World Society: The Writings of John w. Meyer. Oxford University Press.\n\n\nMützel, Sophie, and Ronald Breiger. 2020. “Duality Beyond Persons and Groups.” The Oxford Handbook of Social Networks, 392.\n\n\nPerry, Brea, Bernice Pescosolido, Mario Small, and Ann McCranie. 2020. “Introduction to the Special Issue on Ego Networks.” Network Science 8 (2): 137–41.\n\n\nPrell, Christina. 2012. Social Network Analysis: History, Theory and Methodology. Sage.\n\n\nRobins, Garry. 2015. Doing Social Network Research: Network-Based Research Design for Social Scientists. Sage.\n\n\nScott, John. 2017. Social Network Analysis. Sage.\n\n\nSmall, Mario, Brea Perry, Bernice Pescosolido, and Ned Smith, eds. 2021. Personal Networks: Classic Readings and New Directions in Ego-Centric Analysis. Cambridge University Press.",
    "crumbs": [
      "**DATA, NOT DATA**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Networks and relational thinking</span>"
    ]
  },
  {
    "objectID": "centrality.html",
    "href": "centrality.html",
    "title": "15  Centrality",
    "section": "",
    "text": "15.1 Package Imports",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Centrality</span>"
    ]
  },
  {
    "objectID": "centrality.html#package-imports",
    "href": "centrality.html#package-imports",
    "title": "15  Centrality",
    "section": "",
    "text": "import collections\nimport random\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom matplotlib.colors import Normalize\nfrom matplotlib.cm import ScalarMappable\nimport networkx as nx\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom dcss import set_style\nfrom dcss.networks import *\n\nset_style()",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Centrality</span>"
    ]
  },
  {
    "objectID": "centrality.html#data",
    "href": "centrality.html#data",
    "title": "15  Centrality",
    "section": "15.2 Data",
    "text": "15.2 Data\nWe will continue to work with data from the SocioPatterns project – specifically, the relational data reported by students in their contact diaries.\ncontact_diaries = pd.read_csv(\"data/SocioPatterns/Contact-diaries-network_data_2013.csv\", sep=' ')\n\nG = nx.from_pandas_edgelist(contact_diaries, 'i', 'j', create_using=nx.Graph())\nG.name = 'Reported Contacts (Diary Data)'\nprint(G)\nRecall from the previous chapter that create_using=nx.Graph() means we are creating an undirected network. The output of the info() function confirms this by telling us we have a Graph object, not a DiGraph object (directed).",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Centrality</span>"
    ]
  },
  {
    "objectID": "centrality.html#centrality-measures-the-big-picture",
    "href": "centrality.html#centrality-measures-the-big-picture",
    "title": "15  Centrality",
    "section": "15.3 CENTRALITY MEASURES: THE BIG PICTURE",
    "text": "15.3 CENTRALITY MEASURES: THE BIG PICTURE\nA node’s location in a social network matters. It determines what they are exposed to; the support, opportunities, and resources they have access to; their social status and prestige; and their ability to exercise power and influence over others. In network science, centrality measures are one of the main ways in which abstract concepts such as these are mapped onto concrete measurable variables that can be computed from relational data.\nConsider power. Like many other social scientific approaches to conceptualizing power, the network perspective is relational. It emphasizes relationships and situations where one person is dependent upon another, with the assumption that these dependency relations determine consequential things in our lives, both good and bad. We are embedded in a complex intersection of many such relationships, regardless of how aware we are of the dependency dynamics. To name just a few examples:\n\na child or young adult who depends upon a parent for everything;\na graduate student who depends upon a supervisor’s expertise, support, and connections;\nan academic department chair whose approval or disapproval partially determines a faculty member’s performance evaluations and salary increases (in a non-unionized context); or\na friend who depends upon another friend for support in a crisis.\n\nThese dependency relations can, of course, be exploited. Parents can be unloving, and physically or emotionally abusive. Supervisors can be take credit for work. Department chairs can be jealous, or petty, and favour others. Friends can take advantage of us.\nIn these cases, one person has asymmetric control and power in the relationship. They can impose their will and make life better or worse for the dependent individual. The point is outcomes for ego, such as health and wellbeing, are at least partially in the hands of alter. But it’s not just about one-to-one relationships, and this is one of the critical reasons for thinking about these dependency relations in the context of larger networks of relations, not just dyads.\nIf a student can jump ship and find a better supervisor, if a friend can turn to another friend, if a faculty member can be evaluated by committee rather than by a department chair, then the dependency is lessened and power is less concentrated. Children and adults in toxic and abusive relationships are more obviously constrained in their ability to reduce their dependencies. Children don’t get to choose their parents and there are most certainly specific reasons why people are prevented from leaving toxic and abusive relationships. The structure of social networks can constrain people in comparable ways, making it difficult to break free from the control of others. This is further complicated by the fact that most of us don’t really know all that much about the structure of the networks that we live our lives in. As Hanneman and Riddle (2005) put it so pithily, “ego’s power is alter’s dependence.”\nThis insight has profound implications. The structural properties of a social network determine the amount of power available in general, its distribution among nodes, and therefore the extent to which some nodes can influence others. In some networks, power may be relatively evenly distributed and opportunities for control and domination are rare. In others, it may be centralized around a small subset of nodes. In other words, the structure of our social networks determine the extent to which we are able to influence and control one another, with some network structures enabling more influence and control and others enabling less. We can change lives dramatically, for better or for worse, by changing the structure of dependency relations.\nCentrality analysis provides tools we can use to examine power empirically via dependency relations, but this is only one possibility. Centrality analysis can be used to assess the opportunities and constraints for any node, given the specific ways they connect to other nodes in a network, which we can refer to as a node’s “position.”1 Some positions may be more or less likely to result in exposure to novel information and ideas, or they may afford easy access to elites or resources that are more difficult for others in less advantageous positions to access.\nThere are various centrality measures designed to capture these differences and others. At this point it’s important to note:\n\nBeing central is not inherently good or desirable. The same types of central positions that give one early access to useful information can also mean early exposure to infectious diseases.\n\nBeing central is not a guarantee that a node will experience the hypothesized effects of their position, good or bad. Having the opportunity to access elites is not the same thing as seizing that opportunity.\n\nThere are usually specific types of structural positions that you expect to be important given your research question. As always, theory is really important here. Rather than computing every centrality score, you should select those that correspond to your research question or operationalize a relevant concept. You may want to identify information brokers in a collaboration network. This could help you study whether people who are in a position to influence the flow of information over walks (a concept we will turn to momentarily) in the collaboration network have different career outcomes than those who are not in such positions, or perhaps find key individuals in a needle sharing network for a public health intervention. In that case, you could use betweenness centrality, which we discuss shortly. While you certainly should analyze multiple centralities, you need to think deeply about the question you are trying to answer, what these specific centralities mean and how they work, and how well those align.",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Centrality</span>"
    ]
  },
  {
    "objectID": "centrality.html#shortest-paths-and-network-flow",
    "href": "centrality.html#shortest-paths-and-network-flow",
    "title": "15  Centrality",
    "section": "15.4 SHORTEST PATHS AND NETWORK FLOW",
    "text": "15.4 SHORTEST PATHS AND NETWORK FLOW\n\n15.4.1 Shortest Paths / Geodesics\nThe first framework for thinking about centrality is interested in the access that a node has to the rest of the network. It’s probably intuitive that the most efficient way to exchange something (information, resources, power) between two points is to find the shortest distance between them (i.e. the path between two nodes that involves the smallest number of nodes and edges). Those “shortest paths” are also called geodesics.\nWe can identify the shortest path between any two nodes using the shortest_path() function in NetworkX. If we use the function without specifying a specific pair of nodes, it will compute all of the shortest paths between every possible start and end point between every pair of nodes in a network. Even for small networks, that’s a lot of paths.\nTo help build some intuition about paths, we will define a couple of functions that will let us quickly query the paths between any pair of nodes, and then highlight those paths in simple network visualizations. Note that we are constructing our visualizations a little differently than we did in the last chapter.\nOur plot_path() function requires a layout for the network visualization. We will use the kamada_kawai_layout() function to compute the layout for a visualization of the contact diary network data from SocioPatterns, which we used to construct the network object G at the start of this chapter.\nlayout = nx.kamada_kawai_layout(G)\nThe nodes in the SocioPatterns data are, of course, anonymized. The research team assigned each node an integer ID. You can see those IDs with G.nodes().\nNow that we have our functions defined, and we know what the IDs are for the nodes in our network, let’s look at a few examples. We will provide our get_shortest_paths() function with the integer IDs for our source and target nodes. For example, let’s find the shortest path between node 173 and node 48.\npath_a, es_a = get_shortest_paths(G, 173, 48) \nprint(path_a)\nIn order for some information that 173 has to reach 48 along the shortest path, it would have to first go through nodes 295, 954, and so on. What does this path look like? Let’s visualize it. In Figure 15.1, the shortest path between 173 and 48 will be highlighted.\nplot_path(G, layout, path_a, es_a)\nplt.savefig('figures/15_01.png', dpi=300)\n\n\n\n\n\n\nFigure 15.1: png\n\n\n\n\n15.4.1.1 Multiple Shortest Paths\nIf there are more than one shortest paths between nodes, then get_shortest_paths() picks one at random to return. We can get the full list of shortest paths using the all_shortest_paths() function. Let’s see the other paths between 173 and 48.\nsps = [path for path in nx.all_shortest_paths(G, source=173, target=48)]\npath_nodes = set([item for sublist in sps for item in sublist])\n\nfor path in sps:\n    print(path)\nNotice that in these shortest paths that start with 173 and end with 48, there are some nodes that appear on all the shortest paths, such as 295 and 954. This enables us to count the number of shortest paths that involve any given node. Nodes that are involved in a larger number of shortest paths may be considered more central, as being involved in more shortest paths offers some distinct advantages for power and influence.\nLet’s plot all of these paths (Figure 15.2).\nfig, ax = plt.subplots(figsize=(12, 8))\n\nnx.draw_networkx_nodes(\n    G, \n    pos=layout, \n    node_size=200, \n    node_color='#32363A'\n)\n\nnx.draw_networkx_edges(\n    G,\n    pos=layout,\n    edge_color='darkgray',\n    width=1\n)\n\n## THE PATHS!\n\nnx.draw_networkx_nodes(\n    G,\n    pos=layout,\n    node_size=200,\n    node_color='crimson',\n    nodelist=path_nodes\n)\n\nfor p in sps:\n    edges = set(zip(p, p[1:]))\n    nx.draw_networkx_edges(G,\n                           pos=layout,\n                           edgelist=edges,\n                           edge_color='crimson',\n                           width=4)\n\nplt.axis('off')\nplt.savefig('figures/15_02.png', dpi=300)\n\n\n\n\n\n\nFigure 15.2: png\n\n\n\nShortest path lengths can also be computed, which can be useful when the distance between two nodes matters more than the specific paths connecting them. We can do this with the shortest_path_length() function. Recall from earlier that we are counting edges on a path between a source and target node, so the length will always be equal to 1 - the number of nodes in the path itself. This can be useful to know because information and influence usually degrade over longer distances. That’s why shorter paths are likely to be more important than longer ones. The shortest_path_length() function tells us that, regardless of the specific path, the closest that 173 and 48 can be is 10 steps.\nSometimes, depending on the network and the data, edges might vary in length, which can be encoded as edge weights. In these cases, the number of edges between two nodes is no longer the best measure of distance. Thankfully, many algorithms, like shortest paths, can take these weights into consideration. We will see an example of an algorithm that accounts for edge weight shortly when we discuss current flow betweenness centrality.\nnx.shortest_path_length(G, source=173, target=48) \nFinally, we can also compute the average length shortest paths in a connected network using the average_shortest_paths() function. This is an average across all pairs of \\(i,j\\) nodes in the full network.\nnp.round(nx.average_shortest_path_length(G), 2)\nNote that the path lengths between 173 and 48 are higher than the average path lengths in the network.",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Centrality</span>"
    ]
  },
  {
    "objectID": "centrality.html#betweenness-centrality-two-ways",
    "href": "centrality.html#betweenness-centrality-two-ways",
    "title": "15  Centrality",
    "section": "15.5 BETWEENNESS CENTRALITY, TWO WAYS",
    "text": "15.5 BETWEENNESS CENTRALITY, TWO WAYS\nThis discussion of paths and network walk-structure has been building a foundation for our discussion of betweenness centrality. In brief, the idea is that nodes that lie between nodes, or better yet, between groups of densely connected nodes (a concept we will turn to in the next chapter), often have advantages and opportunities that other nodes do not have. For example, information tends to be more homogenous within clusters of densely connected nodes and more diverse across such clusters. As a result, nodes that are embedded within a cluster tend not to have information that is unknown to their adjacent peers, but nodes that lie between clusters get access to diverse sources of information. In some cases, these nodes may be able to influence and perhaps even control the flow of information through the network. For example, they may filter and frame information in ways that increase their own power over others who depend on them for access to that information. In other words, they are in potential brokerage positions. Consider the hypothetical network in Figure 15.3. Which node is the in-between broker? If we consider the shortest paths in this network, which nodes will be on more shortest paths than others?\n\nFurther Reading\nIf you want to deepen your understanding of brokerage dynamics in social networks, I recommend Katherine Stovel and Lynette Shaw’s (2012) review article “brokerage,” which touches on many interesting theoretical ideas related to centrality.\n\nnx.draw(nx.barbell_graph(5,1), node_size=300, node_color='#32363A')\nplt.savefig('figures/15_03.png', dpi=300)\n\n\n\n\n\n\nFigure 15.3: png\n\n\n\nThere are two main ways of computing betweenness centrality: shortest path and current flow. As you might expect, shortest path betweenness is computed based on shortest paths, which are central to the any process where a contagion (e.g. information) spreads through a network.\nTo compute shortest path betweenness for any given node, we first determine the shortest paths between every pair of nodes in the network. We then compute the proportion of shortest paths that include the node in question for each \\(i,j\\) pair of nodes in the network. Those proportions are then summed to obtain a single number. If a node does not lie on any shortest paths, then its betweenness score will be 0 (e.g. if it is an isolate). It will have the maximum value if it lies on all shortest paths between all pairs of nodes in the network. Note that this is a systematic implementation of the general idea we considered earlier when noting that some nodes lie on more shortest paths than others.\nLet’s quickly visualize the distribution of betweenness scores with a histogram (Figure 15.4).\nsp_bet = pd.Series(nx.betweenness_centrality(G))\n\nax = sns.histplot(sp_bet, kde=True)\nax.set(xlabel='Shortest path betweenness centrality', \n      ylabel='Number of nodes')\nsns.despine()\n\nplt.savefig('figures/15_04.png', dpi=300)\n\n\n\n\n\n\nFigure 15.4: png\n\n\n\nMost nodes in the network have low shortest path betweenness; only a few have higher scores.\nUnlike shortest paths betweenness, current flow betweenness takes into account the strength of relationships when it conceptualizes how a contagion flows through a network. Current flow betweenness draws on the analogy of electrical current flowing through a resistance network, where edges are resistors. A detailed discussion of electromagnetism is beyond the scope of this book, so we will gloss over the details and focus on some quick takeaways.\nA circuit where resistors are arranged in a single line between the source and the target is a series circuit. The current can only flow through one path, so the current is the same between each node on the path between the source and the target. The effective resistance of the circuit is the sum of the resistances of all the resistors. Thus, for a given path, adding another edge at the end can only increase the effective resistance: information flows less well through longer chains. Consider the flow of people leaving a crowded stadium. This stadium is poorly designed and everyone has to leave via a single path consisting of a series of rooms. Each time you add a room, you add a door that has to be opened, you give people more chances to stumble, and generally the whole flow of people from the stadium (source) to the exit (target) will slow down.\nA circuit where resistors are arranged such that multiple paths lie between the source and the target is a parallel circuit. The current will split where paths branch and flow down each of the possible paths, with more current flowing through more efficient paths. As you add parallel resistors, the effective resistance of the whole circuit decreases. Consider the stadium example: if we have multiple exits, people will be leaving through all of them. Some exits will be more efficient because they have fewer rooms and/or larger doors, and people will flow through those faster. If you add another exit, even if it’s a small side door, people will necessarily be able to leave faster. In current flow betweenness, the strength of a relationship corresponds to how efficient the flow of current is between nodes. In our example, rooms with larger doorways between them could be represented with greater edge weights.\nThis example sounds like a directed network, but the network we are working with is undirected. To calculate the circuit flow betweenness of a node, we consider that each node in the network could be a source or a target, and calculate the current flow for that node averaged across every possible pairing of source and target nodes. Bringing it back to the stadium example, this is like shuffling the stadium seats and the stadium exit through all the rooms so that we consider every possible pairing and take the average flow through a room across all those pairings.\nThe code below computes current flow betweenness and then constructs a scatterplot (Figure 15.5) to compare the scores against the shortest path version.\ncf_bet = pd.Series(nx.current_flow_betweenness_centrality(G))\nbetweenness = pd.concat([sp_bet, cf_bet], axis=1)\nbetweenness.columns = ['Shortest Path Betweenness', 'Current Flow Betweenness']\nsns.jointplot(data=betweenness,\n              x='Shortest Path Betweenness',\n              y='Current Flow Betweenness',\n              alpha=.7)\nplt.savefig('figures/15_05.png', dpi=300)\n\n\n\n\n\n\nFigure 15.5: png\n\n\n\nWhile similar, the two measures are not equivalent to one another. At very low values, shortest path betweenness and current flow betweenness are quite densely clustered and the relationship between the two seems stronger, but as we look at the larger (and rarer) values, the relationship becomes much weaker. Consider why this might be. Being on a single shortest path will necessarily have a low shortest path and low current flow betweenness score because the flow of contagion/current can only be a small amount of the larger network. However, as we increase the number of shortest paths that a node is on, we are also increasing the chances of a node being part of a parallel circuits (metaphorically speaking). Current flow betweenness conceptualizes that flow between nodes take the routes other than the shortest path, albeit at a reduced rate. Thus, we would likely expect a wider distribution of current flow betweenness values.\nLet’s move on from the idea of things flowing through a network and consider another way of thinking about centrality.",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Centrality</span>"
    ]
  },
  {
    "objectID": "centrality.html#popularity-power-and-influence",
    "href": "centrality.html#popularity-power-and-influence",
    "title": "15  Centrality",
    "section": "15.6 POPULARITY, POWER, AND INFLUENCE",
    "text": "15.6 POPULARITY, POWER, AND INFLUENCE\nThe second centrality framework is focused less on the ideas of paths and flow, and more on popularity, status, and prestige. The idea is that more – and “better” – relationships are associated with greater popularity, status, and prestige.\nTo demonstrate this, let’s work with another network dataset collected from the same French high school students by the SocioPatterns team. The previous network represented reported contacts that we coerced into an undirected network. This network is a directed friendship network produced by students identifying other students as their friends. Unfortunately, some friendships are asymmetrical; one may feel the relationship is strong enough to nominate the other, but it may not be reciprocated. For this, we will use a DiGraph (directed graph).\nreported_friendships = pd.read_csv(\"data/SocioPatterns/Friendship-network_data_2013.csv\",\n                                   sep=' ')\n\nG_friendships = nx.from_pandas_edgelist(\n    reported_friendships,\n    'i', 'j', \n    create_using=nx.DiGraph()\n)\n\nG_friendships.name = 'Reported Friendships'\nprint(G_friendships)\nlayout = nx.nx_pydot.graphviz_layout(G_friendships)\n\n15.6.1 Degree, Degree Centrality, and Connection Inequality\nOne of the most critical pieces of information in this approach to centrality is the number of connections a node has. For an undirected network, that count is called degree. If I have 50 connections my degree is 50. Nodes with higher degree are, by definition, more connected. For a directed network, we distinguish between edges leaving a node, out-degree, and edges arriving at a node, in-degree. In a hypothetical advice network at a business where a directed edge represents “gave advice to”, we would expect the most senior and experienced employees to have higher out-degree (they are giving advice to more people) and the most junior would have higher in-degree (they are receiving from more people). While an individual node can have different in-degree and out-degree, the network as a whole will have a balanced in-degree and out-degree because each directed edge is necessarily an outgoing and an incoming edge for two different nodes. We can still use the concept of degree for directed networks, simply by adding a node’s in-degree and out-degree to count all edges connected to a node.\nLet’s compute the out and in-degrees for the nodes in the SocioPatterns contact network. Then we’ll visualize the network a couple of times, once with the node sizes as a function of indegree and once as a function of outdegree. Since we intend to execute the visualization code a couple of times, let’s define a custom function.\ndef visualize_digraph(\n    network, layout, node_size=50, title=''\n):\n    fig, ax = plt.subplots(figsize=(12, 8))\n    ax.set_title(title, fontsize=16)\n    nx.draw_networkx_nodes(network,\n                       pos=layout,\n                       node_size=node_size,\n                       node_color='#32363A')\n    nx.draw_networkx_edges(network,\n                       pos=layout,\n                       edge_color='#98989C',\n                       arrowsize=5,\n                       width=1)\n    plt.axis('off')\nin_degree = dict(G_friendships.in_degree())\nout_degree = dict(G_friendships.out_degree())\nIf we supply networkx with the original in and out degree scores as node sizes, even the most popular and active nodes will be extremely tiny. Instead we will multiply every score by 20 to get them into a range of values that are large enough to use as node sizes in the visualization (Figure 15.6).\nsized_by_indegree = [v * 20 for v in in_degree.values()]\nsized_by_outdegree = [v * 20 for v in out_degree.values()]\nvisualize_digraph(G_friendships, layout, sized_by_indegree)\nplt.savefig('figures/15_06.png', dpi=300)\n\n\n\n\n\n\nFigure 15.6: png\n\n\n\nAnd now we’ll do the same for outdegree (Figure 15.7).\nvisualize_digraph(G_friendships, layout, sized_by_outdegree)\nplt.savefig('figures/15_07.png', dpi=300)\n\n\n\n\n\n\nFigure 15.7: png\n\n\n\nIf you squint, you might notice some apparent differences in the popularity (indegree) and activity (outdegree) across nodes in the network, but for the most part it seems as if the nodes that have higher indegree also have higher outdegree. We can confirm our hunch by plotting the two degree scores (Figure 15.8).\nfig, ax = plt.subplots()\nsns.scatterplot(x=in_degree, y=out_degree, alpha = 0.2)\nsns.despine()\nax.set(xlabel='Indegree',\n       ylabel='Outdegree')\nplt.savefig('figures/15_08.png', dpi=300)\n\n\n\n\n\n\nFigure 15.8: png\n\n\n\nRecall we are working with reported friendships, where outdegree means that one node, \\(i\\), has nominated another node, \\(j\\), as a friend. If \\(i\\) and \\(j\\) are indeed friends, then they should both nominate each other. (Reciprocity!) It follows that in a network such as this one, nodes that have a high score on one of the two degree measures will likely also have a high score on the other. The network visualizations and the scatterplot clearly suggests that there are some high-activity students who both nominated, and were nominated by more people, and while closely related, it is also clear that in-degree and out-degree are not equal for all students. Not every friendship in this network was reciprocated.\n\n15.6.1.1 Connection Inequality\nInequalities in connectedness, visible as inequalities in node in-/out-/degree, can have many consequences. We are often concerned with extreme levels of inequality, which in network science are analyzed in terms of cumulative advantage processes – the rich get richer, the more well-connected become more connected (Merton 1968; Barabási and Albert 1999; Price 1965, 1986; DiPrete and Eirich 2006).\nEven moderate levels of inequality can have surprising effects, however. Consider what Scott Feld (1991) has called “the friendship paradox,” which states: on average, people’s friends have more friends than they do (on average, people have fewer friends than their friends do). To demonstrate, consider the following: a person (ego) has 20 friends (alters) who each have 10 friends. As a consequence of this inequality, every alter has a below-average number of friends. This connection inequality in personal networks can impact how we perceive the world around us and what we judge to be “normal.” Although they are a minority overall, the most well-connected people are over-represented in most people’s personal networks, and everyone else, though quantitatively more common, are under-represented in other people’s networks (Feld 1991).\nThe preferences and behaviours of the most popular people are more visible in our lives than those of their underrepresented peers. As a result, our personal connections are rarely a good indication of the types of preferences and behaviours that are more common in the general population because the most well-connected are disproportionately represented. In other words, our everyday perceptions of what is “normal” are biased.\nLet’s use degree as a measure of popularity for now, though in-degree may also be suitable. We can visualize connection inequalities by plotting the degree distribution. In ?fig-15_09, we see the number of nodes (\\(y\\)-axis) with each possible degree value (\\(x\\)-axis).\ndegree_sequence = sorted([d for n, d in G_friendships.degree()], reverse=True)  # degree sequence\ndegreeCount = collections.Counter(degree_sequence)\ndeg, cnt = zip(*degreeCount.items())\nfig, ax = plt.subplots(figsize=(6,4))\nplt.bar(deg, cnt, width=0.80, color=\"#32363A\")\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\nax.set_xlabel('Degree')\nax.set_ylabel('Number of nodes')\nplt.savefig('figures/15_09.png', dpi=300)\n\n\n\n\n\n\nFigure 15.9: png\n\n\n\nBecause there is often a lot of inequality in connectedness, especially in larger networks, it is often more informative to plot the degree distribution with both axes on a log scale. If the result is a relatively straight negative line from the upper right to the bottom left, you might want to formally check for a power law distribution, which would indicate extreme inequality and potentially some sort of cumulative advantage process.\nAnother way of inspecting the degree distribution is to produce a plot that ranks nodes based on their degree and plots their rank and degree on a log-log scale (?fig-15_10). The top ranked node in the graph is shown in the upper left of the graph (at \\(10^{0}\\), which is 1). As you would expect, nodes in this upper left corner have higher degree centrality than any other nodes in the network (27). We see the decreasing degree of each successive node in the ranked list as we move along the \\(x\\)-axis. As we get towards the end of the list, we start to see nodes with very low degree. As there are no isolates in this network, the lowest degree score is 1 (or \\(10^{0}\\)).\nfig, ax = plt.subplots(figsize=(6,4))\n\nax.loglog(degree_sequence, \n          'black', \n          marker='o', \n          markersize=3)\n\nplt.title(\"Degree Rank Plot\")\nplt.ylabel(\"Degree\")\nplt.xlabel(\"Rank\")\n\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\n\nplt.savefig('figures/15_10.png', dpi=300)\n\n\n\n\n\n\nFigure 15.10: png\n\n\n\nDegree is commonly treated as a proxy for “popularity,” but that’s not the only option. Our interpretation of any centrality measure must be based on the type of ties in the network. In the earlier example of the contact diary network, the edges in the network simply indicate whether two nodes came into contact with each other within a given time frame. One could make an argument for “popularity” there, but it would make more sense to think of degree in the contact network as something like mobility: moving more may produce more opportunities for contact. Returning to the idea of contagions flowing through a network, degree could be a measure for exposure, with more connections indicating more exposure. The specific details of the relations encoded in the network matter as much, or more than, the formal mathematics of the measure when it comes to interpreting centrality.\n\n\n\n15.6.2 Eigenvector Centrality\nDegree centrality is an intuitive way of thinking about how connected people are, but analytically it doesn’t get us very far on its own. However, many other centrality measures are built on degree, and can be used to operationalize more complex and interesting ideas. Eigenvector centrality is based on the simple idea that being connected to well-connected people matters: even if your degree doesn’t change, if your neighbour’s degree increases, your connection to the network also increases.\nConsider our friendship network again. This time we are going to look at the “neighborhoods” (immediate alters) of two specific nodes and their extended neighbourhoods (alters’ alters). I will pick two focal nodes, 1519 and 196 and assign them the color crimson. Their immediate alters will be plotted in black, their extended neighbourhood in dark gray, and the rest of the nodes light gray. I have chosen these two nodes to compare because they have the same degree (the size of their immediate neighbourhoods are identical).\nWe’ll use the plot_nodes() function defined below to simplify some of the visualization code. The resulting plots is shown in Figures ?fig-15_11 and Figure 15.12.\ndef plot_nodes(which_network, which_nodes, what_color, where):\n    if type(which_nodes) is list:\n        nx.draw_networkx_nodes(\n            which_network, \n            pos=where, \n            node_size=100, \n            node_color=what_color,\n            nodelist=which_nodes\n        ) \n    else:\n        nx.draw_networkx_nodes(\n            which_network, \n            pos=where, \n            node_size=100, \n            node_color=what_color,\n            nodelist=[which_nodes]\n        ) \nalters = nx.ego_graph(G_friendships, 1519, radius=1, undirected=True)\n\nalters_2 = nx.ego_graph(G_friendships, 1519, radius=2, undirected=True)\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\nplot_nodes(\n    G_friendships, \n    list(G_friendships.nodes()), \n    'lightgray', \n    layout\n)\n\nplot_nodes(\n    G_friendships, \n    list(alters_2.nodes()), \n    'gray', \n    layout\n)\n\nplot_nodes(\n    G_friendships, \n    list(alters.nodes()), \n    'black', \n    layout\n)\n\nplot_nodes(\n    G_friendships, \n    1519, \n    'crimson', \n    layout\n)\n\nnx.draw_networkx_edges(\n    G_friendships, \n    pos=layout,edge_color='lightgray',\n    arrowsize=3,\n    width=1\n)\n\nplt.axis('off')\nplt.savefig('figures/15_11.png', dpi=300)\n\n\n\n\n\n\nFigure 15.11: png\n\n\n\nalters = nx.ego_graph(\n    G_friendships, \n    196, \n    radius = 1, \n    undirected = True\n)\n\nalters_2 = nx.ego_graph(\n    G_friendships, \n    196, \n    radius = 2, \n    undirected = True\n)\n\nfig, ax = plt.subplots(figsize=(12,8))\n\nplot_nodes(G_friendships, list(G_friendships.nodes()), 'lightgray', layout)\n\nplot_nodes(\n    G_friendships, \n    list(alters_2.nodes()), \n    'gray', \n    layout\n)\n\nplot_nodes(\n    G_friendships, \n    list(alters.nodes()), \n    'black', \n    layout\n)\n\nplot_nodes(\n    G_friendships, \n    196, \n    'crimson', \n    layout\n)\n\nnx.draw_networkx_edges(\n    G_friendships, \n    pos=layout, \n    edge_color='lightgray',\n    arrowsize=3,\n    width=1)\n\nplt.axis('off')\nplt.savefig('figures/15_12.png', dpi=300)\n\n\n\n\n\n\nFigure 15.12: png\n\n\n\nDespite their immediate neighbourhoods (black) being the same size, 196 has much greater reach with their extended neighbourhood because their immediate neighbours are better connected.\nThink about influence in this context. 196 influences and is influenced by their direct alters, who in turn influence and are influenced by their direct alters. Influence on 196 is most strong from their immediate alters, followed by their alters’ alters, followed by her alters’ alters’ alters, and so on.\nLet’s consider how this is reflected in eigenvector centrality. Technically, a node’s eigenvector centrality is proportional to the sum of the centralities of their alters (Borgatti, Everett, and Johnson 2018). In this sense eigenvector centrality can also be interpreted as a measure of popularity, but it differs from degree centrality because a node can have high eigenvector centrality but low degree centrality (i.e., they are connected to only a few people, but those people are well-connected).\nBoth 1519 and 196 have degrees of 11 (5 reciprocated nominations and one unreciprocated), but when we look at the network we can probably intuit that they occupy different types of positions given who they are connected to, and how those people are connected. Just by eyeballing the network, we can see that 196’s connections are more connected than 1519’s are. When it comes to influence and power in a network, being connected to well-connected people is more useful than being connected to less well-connected people.\nEigenvector centrality is based on this fundamental idea that, when it comes to influence, the connections of the people we are connected to matter. We have to think, and “see” beyond our immediate social neighborhoods.\n\n15.6.2.1 Computing Eigenvector Centrality\nLet’s take a look at the distribution of eigenvector centrality scores to degree (Figure 15.13).\ndn = pd.Series(dict(nx.degree(G_friendships)))\nec = pd.Series(nx.eigenvector_centrality(G_friendships))\n\nfig, ax = plt.subplots()\nsns.scatterplot(x=dn, y=ec, alpha=.6)\nax.set(xlabel='Degree', ylabel='Eigenvector centrality')\nsns.despine()\nplt.savefig('figures/15_13.png', dpi=300)\n\n\n\n\n\n\nFigure 15.13: png\n\n\n\nWe can see that degree and eigenvector centrality are somewhat positively correlated. As degree increases, eigenvector tends to increase as well, though some individuals with high degree centrality are especially well-positioned and take a lion’s share of the eigenvector centrality.\n\n\n\n15.6.3 Bonacich Power Centrality\nEigenvector centrality is often used, among other things, as a proxy for power. However, as Phillip Bonacich (1987) pointed out, being more “central” is not necessarily the same thing as being more powerful. Being connected to well-connected others makes you more reachable, but less powerful, because the people you are connected to are connected to many others; they do not depend on you because they have alternative connections. On the other hand, if the people you are connected to are not themselves well-connected, then their connection to you matters more; they are more dependent on you, and therefore you are more powerful. For Bonacich, then, ego is more central when their alters are densely connected with one another, but this makes ego less powerful; instead, ego gains power when their alters have few connections to others.\nWe might expect to see this in access and exchange networks, where being connected to people who have a lot of other exchange partners reduces your “bargaining power.” Being connected to people who have few exchange partners, on the other hand, increases your bargaining power. Bonacich power centrality conceptualizes power in terms of an ego’s access to alters who are dependent on them.\nBonacich power centrality introduces an attenuation factor – the parameter \\(\\beta\\) – that controls how a node’s centrality is influenced by the centralities of its alters. \\(\\beta\\) can be positive or negative, reflecting different notions of power and centrality. For example, a positive \\(\\beta\\) indicates that being connected to well-connected alters increases your centrality. This is similar to the idea of eigenvector centrality, where centrality flows through well-connected nodes. A negative \\(\\beta\\) indicates that being connected to poorly connected alters increases your power. This captures the idea that you are more powerful when your alters depend on you because they lack alternative connections. Finally, \\(\\beta\\) of zero indicates that the centrality of a node is independent of the centralities of its alters, effectively reducing Bonacich centrality to degree centrality.\nIn this way, Bonacich power centrality is a flexible measure that can capture different dimensions of centrality and power depending on the value of \\(\\beta\\). However, this flexibility comes with the need to carefully choose an appropriate \\(\\beta\\) value, or range or \\(\\beta\\) values. Importantly, not all values of \\(\\beta\\) are valid for a given network. The attenuation factor \\(\\beta\\) must be within certain bounds to ensure that the centrality calculation is mathematically valid and numerically stable.\n\n15.6.3.1 Valid Ranges of \\(\\beta\\)\nComputing Bonacich power centrality involves inverting the matrix \\((I - \\beta A)\\), where \\(I\\) is the identity matrix and \\(A\\) is the adjacency matrix of the network. For the inverse \\((I - \\beta A)^{-1}\\) to exist, the matrix \\((I - \\beta A)\\) must be invertible, which depends on the eigenvalues of \\(A\\).\nFor positive attenuation factors, \\(\\beta\\) must be less than the absolute value of the recipricol of the largest eigenvalue of \\(A\\),\n\\[\n\\beta &lt; \\frac{1}{\\lambda_{\\text{max}}}\n\\]\nwhere \\(\\lambda_{\\text{max}}\\) is the largest eigenvalue of \\(A\\).\nFor negative attenuation factors, \\(\\beta\\) must be greater than the recipricol of the smallest eigenvalue of \\(A\\)\n\\[\n\\beta &gt; \\frac{1}{\\lambda_{\\text{min}}}\n\\]\nwhere \\(\\lambda_{\\text{min}}\\).\nThese upper and lower bounds ensure that the matrix \\((I - \\beta A)\\) is invertible, thereby avoiding issues with singularity and numerical instability.\nChoosing a single \\(\\beta\\) parameter within these bounds can be challenging, but that is not what we want to do in practice. Instead, we want to explore a range of \\(\\beta\\) values to understand how the centrality measures change.\nAt the time of writing, Bonacich power centrality is not implemented in NetworkX, graph-tool, or igraph for Python. However, I’ve implemented it in the networks module of the dcss package. It correctly handles the attenuation factor \\(\\beta\\) and the necessary bounds by default.\nLet’s compute Bonacich power centrality scores for a network constructed from the SocioPatterns contact diary data.\nG = nx.from_pandas_edgelist(\n    contact_diaries, \n    'i', 'j', \n    create_using=nx.Graph()\n)\n\nG.name = 'Reported Contacts (Diary Data)'\nLet’s define a range of \\(\\beta\\) values that are within the valid bounds for this network.\nattenuation_bounds = get_attenuation_bounds(G)\nattenuation_bounds\n\nbetas = generate_attenuation_factors(\n    attenuation_bounds,\n    num_points=19)\n\nprint(\"Valid beta values including zero:\")\nprint(betas)\nNow we can compute the Bonacich power centrality scores for the valid \\(\\beta\\) values.\ncentralities = bonacich_centrality_multiple_betas(\n    G, betas\n)\n\ncentralities_df = pd.DataFrame.from_dict(\n    centralities, orient='index'\n).T\n\ncentralities_df['Node'] = centralities_df.index\ncentralities_df = centralities_df.round(4)\ncentralities_df.head(10)\nLet’s compare the Bonacich power centrality scores for different \\(\\beta\\) values to normalized degree centrality. Figure 15.14 shows scatter plots of power centrality versus normalized degree centrality for a range of \\(\\beta\\) values, with each subplot corresponding to a different \\(\\beta\\).\ndegree_centrality = nx.degree_centrality(G)\n\ndegree_centrality_df = pd.DataFrame.from_dict(\n    degree_centrality, orient='index', columns=['degree_centrality']\n)\ndegree_centrality_df.reset_index(inplace=True)\ndegree_centrality_df.rename(columns={'index': 'Node'}, inplace=True)\n\ndf = pd.merge(centralities_df, degree_centrality_df, on='Node')\n\nbeta_columns = [\n    col \n    for col in df.columns \n    if col not in ['Node', 'degree_centrality']\n]\nnum_plots = len(beta_columns)\nnum_cols = 4  \nnum_rows = num_plots // num_cols + int(num_plots % num_cols &gt; 0)\n\nfig, axes = plt.subplots(\n    num_rows, \n    num_cols, \n    figsize=(20, \n    num_rows * 5), \n    sharex=True, \n    sharey=True\n)\n\naxes = axes.flatten()\n\nfor idx, beta_col in enumerate(beta_columns):\n    ax = axes[idx]\n    ax.scatter(df['degree_centrality'], df[beta_col], alpha=0.6)\n    ax.set_title(f\"\\nBeta = {round(beta_col, 4)}\", loc='left')\n    ax.set_xlabel('Normalized Degree Centrality')\n    ax.set_ylabel('Power Centrality')\n\nfor idx in range(num_plots, len(axes)):\n    fig.delaxes(axes[idx])\n\nplt.tight_layout()\nplt.savefig('figures/15_bpc_ndc.png', dpi=300)\n\n\n\n\n\n\nFigure 15.14: cap\n\n\n\nThese scatter plots reveal how the relationship between degree centrality and Bonacich power centrality changes as \\(\\beta\\) varies. At \\(\\beta = 0\\), Bonacich power centrality reduces to degree centrality, which is reflected in the perfect linear relationship in the corresponding subplot, where each node’s power centrality score is directly proportional to its normalized degree centrality. For positive \\(\\beta\\) values, the centrality scores begin to emphasize nodes connected to well-connected alters. The correlation with degree centrality remains positive, but with more dispersion. Nodes with high degree centrality connected to other well-connected nodes may have disproportionately higher power centrality scores.\nThe relationship between degree centrality and power centrality changes considerably for negative \\(\\beta\\) values. As Bonacich argued should be the case, nodes connected to poorly connected alters have higher power centrality scores. The scatter plots show negative trends, highlighting how nodes with lower degree centrality have higher power centrality, due to their alters’ dependence on them.\nTo deepen our understanding of how the attenuation factor \\(\\beta\\) influences the Bonacich power centrality scores, let’s visualize the network graphs with nodes sized and colored according to their power centrality scores for each \\(\\beta\\) value. ?fig-15_bpc_networks presents a series of network visualizations (small multiples), each corresponding to a different \\(\\beta\\) value.\ncentralities_df.set_index('Node', inplace=True)\n\npos = nx.nx_pydot.graphviz_layout(G)\nbeta_columns = centralities_df.columns.tolist()\n\nnum_plots = len(beta_columns)\nnum_cols = 4  # Number of plots per row\nnum_rows = num_plots // num_cols + int(num_plots % num_cols &gt; 0)\n\nfig, axes = plt.subplots(\n    num_rows,\n    num_cols,\n    figsize=(20, num_rows * 5),\n    constrained_layout=True\n)\naxes = axes.flatten()\n\nfor idx, beta_col in enumerate(beta_columns):\n    ax = axes[idx]\n    centrality = centralities_df[beta_col]\n    \n    # normalize centrality values within the beta, noit globally\n    beta_min = centrality.min()\n    beta_max = centrality.max()\n    norm = Normalize(vmin=beta_min, vmax=beta_max)\n    cmap = cm.viridis\n    \n    sm = ScalarMappable(cmap=cmap, norm=norm)\n    sm.set_array([])\n    \n    node_colors = [cmap(norm(centrality.loc[node])) for node in G.nodes()]\n    \n    min_size = 50\n    max_size = 300\n    size_range = max_size - min_size\n    sizes = [((centrality.loc[node] - beta_min) / (beta_max - beta_min) * size_range + min_size) if beta_max != beta_min else min_size + size_range / 2 for node in G.nodes()]\n    \n    nx.draw_networkx(\n        G,\n        pos=pos,\n        with_labels=False,\n        node_color=node_colors,\n        node_size=sizes,\n        edge_color='gray',\n        ax=ax\n    )\n    ax.set_title(r'$\\beta$' + f\" = {round(float(beta_col), 4)}\", loc='left')\n    ax.axis('off')\n    \n    cbar = fig.colorbar(\n        sm,\n        ax=ax,\n        orientation='horizontal',\n        fraction=0.046,\n        pad=0.04\n    )\n    cbar.ax.tick_params(labelsize=8)\n    cbar.set_label('Power Centrality', fontsize=8)\n\nfor idx in range(num_plots, len(axes)):\n    fig.delaxes(axes[idx])\n\nplt.savefig('figures/15_bpc_networks.png', dpi=300)\nplt.savefig('figures/15_bpc_small_multiples_force_directed.png', dpi=600)\n?fig-15_bpc_networks reveals a few notable patterns.\nAs previously noted, at \\(\\beta = 0\\) (all purple nodes), the node sizes and colors reflect degree centrality. As \\(\\beta\\) increases to positive values, nodes that are connected to well-connected alters have higher centrality. As such, nodes in densely connected clusters become larger and more prominently colored. This effect is similar to eigenvector centrality, where being connected to highly central nodes increases one’s own centrality. The visualizations show centrality clustering in densely connected parts of the network.\nAs \\(\\beta\\) decreases to negative values, the centrality scores highlight nodes connected to poorly connected alters. Nodes that serve as bridges to less-connected parts of the network become more central. In the visualizations, some previously large nodes shrink relative to their neighbors, while nodes connected to less-connected alters increase in size. This reflects the concept that power can come from brokering alters who lack alternative connections.\nThese graphs shed light on how the attenuation factor, \\(\\beta\\), affects what we learn about power and centrality in a network through the lens of Bonacich’s power centrality. As always, the choice of \\(\\beta\\) should align with the theoretical considerations of power and influence in the specific research context, and should always consider a range of parameter values.\n\nFurther Reading\nIf you are looking to learn more on centrality analysis, Chapter 10 of Borgatti, Everett, and Johnson’s (2018) Analyzing Social Networks provides a good overview. Borgatti and Everett (2020) contrast three different perspectives on network centrality. Kitts (2014) and Kitts and Quintane (2020) offer a useful perspective on social networks in “the era of computational social science,” with implications of how we interpret centrality measures with networks constructed using behavioural and interactional data.",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Centrality</span>"
    ]
  },
  {
    "objectID": "centrality.html#conclusion",
    "href": "centrality.html#conclusion",
    "title": "15  Centrality",
    "section": "15.7 CONCLUSION",
    "text": "15.7 CONCLUSION\n\n\n15.7.1 Key Points\n\nLearned two major ways of thinking about what centrality means: shortest paths through the network, and counting edges\nLearned some of the most common centrality measures\nConnected centrality measures to the theories and concepts they operationalize\nVisualized the distribution of different centrality measures within the same network\n\n\n\n\n\n\nBarabási, Albert-László, and Réka Albert. 1999. “Emergence of Scaling in Random Networks.” Science 286 (5439): 509–12.\n\n\nBonacich, Phillip. 1987. “Power and Centrality: A Family of Measures.” American Journal of Sociology 92 (5): 1170–82.\n\n\nBorgatti, Stephen, and Martin Everett. 2020. Three Perspectives on Centrality. Edited by Ryan Light and James Moody. Oxford University Press.\n\n\nBorgatti, Stephen, Martin Everett, and Jeffrey Johnson. 2018. Analyzing Social Networks. Sage.\n\n\nDiPrete, Thomas, and Gregory Eirich. 2006. “Cumulative Advantage as a Mechanism for Inequality: A Review of Theoretical and Empirical Developments.” Annu. Rev. Sociol. 32: 271–97.\n\n\nFeld, Scott. 1991. “Why Your Friends Have More Friends Than You Do.” American Journal of Sociology 96 (6): 1464–77.\n\n\nHanneman, Robert, and Mark Riddle. 2005. “Introduction to Social Network Methods.” University of California Riverside.\n\n\nKitts, James. 2014. “Beyond Networks in Structural Theories of Exchange: Promises from Computational Social Science.” In Advances in Group Processes. Emerald Group Publishing Limited.\n\n\nKitts, James, and Eric Quintane. 2020. “Rethinking Social Networks in the Era of Computational Social Science.” The Oxford Handbook of Social Networks, 71.\n\n\nMerton, Robert K. 1968. “The Matthew Effect in Science: The Reward and Communication Systems of Science Are Considered.” Science 159 (3810): 56–63.\n\n\nPrice, Derek De Solla. 1965. “Networks of Scientific Papers.” Science, 510–15.\n\n\n———. 1986. Little Science, Big Science... And Beyond. Vol. 480. Columbia University Press New York.\n\n\nStovel, Katherine, and Lynette Shaw. 2012. “Brokerage.” Annual Review of Sociology 38: 139–58.",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Centrality</span>"
    ]
  },
  {
    "objectID": "centrality.html#footnotes",
    "href": "centrality.html#footnotes",
    "title": "15  Centrality",
    "section": "",
    "text": "In the next chapter we will expand the idea of positions in a network, but for the time being we can think of position primarily in terms of centrality.↩︎",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Centrality</span>"
    ]
  },
  {
    "objectID": "mapping-network-structure.html",
    "href": "mapping-network-structure.html",
    "title": "16  Mapping network structure",
    "section": "",
    "text": "In Progress\n\n\n\nThis will be a heavily revised chapter! It’s essentially new. I’ll publish here in the coming weeks.",
    "crumbs": [
      "**EXPLORING**",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Mapping network structure</span>"
    ]
  },
  {
    "objectID": "supervised-learning.html",
    "href": "supervised-learning.html",
    "title": "17  Supervised Machine Learning",
    "section": "",
    "text": "Planned Revisions\n\n\n\nThis chapter will introduce ML with an emphasis on supervised learning. Dropping the connectionist paradigm until later in the book. Adding the generative / statistical?\nThe revised chapter will incorporate some content from the chapter on regression and classification. The examples in that chapter will be changed to text data. I plan to do this in a way that reflect the distinction between discovery and supervision in the text as data book. Of course, cross-validation will be part of that.",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Supervised Machine Learning</span>"
    ]
  },
  {
    "objectID": "prediction-and-classification.html",
    "href": "prediction-and-classification.html",
    "title": "18  Prediction and classification",
    "section": "",
    "text": "18.1 LEARNING OBJECTIVES",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Prediction and classification</span>"
    ]
  },
  {
    "objectID": "prediction-and-classification.html#learning-objectives",
    "href": "prediction-and-classification.html#learning-objectives",
    "title": "18  Prediction and classification",
    "section": "",
    "text": "Explain how Decision Trees classify data\nDescribe how to regularize (or trim) Decision Trees\nCompare individual Decision Trees to ensemble classification methods\nExplore how leveraging different metrics can help provide a better sense of how your classification models are performing",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Prediction and classification</span>"
    ]
  },
  {
    "objectID": "prediction-and-classification.html#learning-materials",
    "href": "prediction-and-classification.html#learning-materials",
    "title": "18  Prediction and classification",
    "section": "18.2 LEARNING MATERIALS",
    "text": "18.2 LEARNING MATERIALS\nYou can find the online learning materials for this chapter in doing_computational_social_science/Chapter_22. cd into the directory and launch your Jupyter Server.",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Prediction and classification</span>"
    ]
  },
  {
    "objectID": "prediction-and-classification.html#introduction",
    "href": "prediction-and-classification.html#introduction",
    "title": "18  Prediction and classification",
    "section": "18.3 INTRODUCTION",
    "text": "18.3 INTRODUCTION\nIn the previous chapter, we worked on some supervised machine learning algorithms based on relatively familiar statistical models that arose out of the symbolic paradigm of machine learning, OLS, Lasso, and Ridge linear regression models, and logistic regression models. This chapter will continue that discussion with decision trees, ensemble learning, Random Forests, and gradient boosted machines. We will finish with a description of model evaluation metrics, comparing accuracy, Precision, Recall and some ways we can make better use of these metrics.\n\n18.3.1 Imports\nimport pandas as pd \nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nimport graphviz\n\nfrom dcss.plots import plot_knn_decision_boundaries\nfrom dcss import set_style\n\nset_style()\n\n\n18.3.2 Preparing the Data\nAs in earlier chapters, we will be using the VDEM data on a country’s political and electoral freedoms to predict Internet freedoms drawn from the Freeman House dataset.\n# data downloaded in the previous chapter\nforml = pd.read_csv(\n    \"data/vdem_internet_freedom_combined/vdem_fh_combined.csv\"\n)\n\n\n18.3.3 The Train-Test Split and Cross-Validation\nAs discussed in Chapter 21, developing supervised machine learning models requires splitting our data into different sets: some for training the model, others for testing and validating the model. The most practical way to perform this split involves using cross-validation (introduced in the previous chapter).\nfrom sklearn.model_selection import train_test_split\n\nX = forml[[\n    'v2x_polyarchy', \n    'v2x_libdem', \n    'v2x_partipdem', \n    'v2x_delibdem', \n    'v2x_egaldem'\n]]\n\ny = forml[['Total Score']]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=23\n)",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Prediction and classification</span>"
    ]
  },
  {
    "objectID": "prediction-and-classification.html#rules-based-learning-with-trees",
    "href": "prediction-and-classification.html#rules-based-learning-with-trees",
    "title": "18  Prediction and classification",
    "section": "18.4 RULES-BASED LEARNING WITH TREES",
    "text": "18.4 RULES-BASED LEARNING WITH TREES\n\n18.4.1 Decision Trees\nDecision trees, and some more sophisticated models based on decision trees that we will discuss shortly, are the workhorse models of rules-based learning. They can be used for classification and regression tasks. We will focus on a classification task in the example here, but the process is more or less the same for a regression problem.\nIn machine learning, a decision tree is a directed network that starts with a single node ‘containing’ every instance in your dataset. From there on, it’s like playing a highly skilled game of twenty questions, to borrow a clever analogy from Pedro Domingos (2015). In this game, the model is going to ‘ask’ a series of ‘questions’ to figure out the correct label if it’s a classification problem, or the correct value if it’s a regression problem. In a moment, we will learn how the model decides which question to ask, but for now just know that the model will always ask the most informative question possible. The questions will always concern the value for some specific feature for each instance, such as ‘does Canada hold free and fair elections’ or ‘is Canada’s score for freedom on the press higher than the median score?’\nEverytime the model asks a question, a node containing some subset of instances in our dataset splits off into two new nodes. Depending on the answer to the question, each observation moves from the parent node into one of the two child nodes. This process continues until (a) all of the observations contained in a node share the same value for the outcome you want the model to be able to predict, or (b) your tree model runs out of room to ask more questions. When one of these two conditions is met, the branch of the tree terminates in a node called a ‘leaf’. The path from the root node (every instance in the dataset) to each leaf in the tree constitutes a rule. We can collect all of these rules into a single hierarchical rule base that is relatively easy for humans to interpret and understand.\nNow that we understand the basics, it’s time to answer a critical question: how does the model decide which question to ask next? How does it know what the “most informative” question is? The most common method is to use the concept of entropy from information theory. In information theory, entropy is a measure of how much information something contains, expressed in terms of uncertainty.\nTo use a simplified example, let’s say we want to figure out which of the nations in the vdem dataset are democracies. If you think elections are all you need to be considered a democracy, then you could just ask one question for each case – do they hold elections? However, not all elections are the same, and democracies are about much more than elections. So you keep asking questions until you are confident you can make a judgement. The more questions you need to ask to arrive at a confident judgement, the more accurate your classificaiton of the observations into ‘democracies’ and ‘autocracies’ will be. The more purely separated those two classes become, the lower ‘entropy’ in your model. In the context of a decision tree analysis, the model will always ask the question that will result in the biggest decrease in entropy, usually expressed in terms of ‘information gain’, which quantifies the decrease in entropy that resulted from asking the question.\nAt this point, there shouldn’t be much doubt about how easily the VDEM dataset we’ve been using throughout the book can be classified; nevertheless, we’re going to use it here again. We’re not going to do so because it will provide us with a better classification (we already achieved very good scores using a logistic regression), but rather because the resultant decision tree model will allow us to easily see what information the model finds most useful when deciding whether a nation is an autocracy or a democracy.\nWe’ll start, as usual, by splitting our dataset into a matrix, \\(X\\), and an outcome vector, \\(y\\).\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom graphviz import Source\nfrom sklearn.preprocessing import LabelEncoder\n\ndem_indices = pd.read_csv(\n    \"data/vdem_internet_freedom_combined/dem_indices.csv\"\n)\n\nX = dem_indices[[\n    'v2smgovdom_osp', # Government dissemination of false information domestic\n    \"v2smgovfilprc_osp\", # Government internet filtering in practice\n    \"v2smgovsmcenprc_osp\", # Government social media censorship in practice\n    \"v2smonper_osp\", # Diversity of online media perspectives (0 = gov't only, 4 = any perspective)\n    \"v2smarrest_osp\", # Arrests for political content disseminated online\n]]\n\ninterpretable_names = [\n    'Domestic Misinformation',\n    'Internet Filtering',\n    'Social Media Censorship',\n    'Online Media Diversity',\n    'Arrests for Political Content'\n]\n\nregime_types = [\n    'Autocracy',\n    'Democracy',\n]\n\nle = LabelEncoder()\nlabels = le.fit_transform(regime_types)\n\ny = np.where(dem_indices[\"v2x_regime\"] &lt;= 1, 0, 1).copy()\nThe technique we’re using to convert the 4-point v2x_regime scale into a binary variable is identical to the one we employed in Chapter 21.\nWith \\(X\\) and \\(y\\) created, we can create our training and test sets, and then create and fit our decision tree classifier using cross-validation (in much the same way as we did in the previous chapter; consult Chapter 21 for more detail on cross-validation).\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import ShuffleSplit\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=23\n)\n\nshuffsplit = ShuffleSplit(n_splits=5, test_size=0.3, random_state=42)\n\ndtclass = DecisionTreeClassifier(random_state=0)\ndt_scores = cross_val_score(\n    dtclass, X_train, y_train, cv=shuffsplit\n)\n\nprint(dt_scores)\nprint(f\"Mean: {dt_scores.mean()}\")\nNot bad! In order to get a sense of what our tree is doing under the hood, the below diagram represents our decision tree. You start at the top node, which contains all of the observations (countries in this case). The top line in that node (and every non-leaf node in the remainder of the tree) indicates the rule it will use to split the data. All of the countries for which that statement is true will travel along the ‘True’ path for further subdivision. All of the nations for whom this condition does not apply travel along the ‘False’ path.\nFigure 18.1 shows the resulting image without color because it keeps the cost of the print book down. If you change the argument filled=False below to True, you can get a color version. In the color versions, the ‘strength’ of the colour represents how ‘pure’ each node is. If there’s an equal mix of both classes, the colour should desaturate entirely. The code below also writes the figure to disk. To display it in a notebook, wrap the entire function in graphviz.Source(). The same is true for the other decision trees later in the chapter.\n\nTODO: Update this.\n\nfrom sklearn import preprocessing\n\ndt_fitted = dtclass.fit(X_train, y_train)\n\nexport_graphviz(\n    dtclass,\n    out_file='../graphical_models/classified_1.gv', \n    filled=False,\n    rounded=True,\n    feature_names=interpretable_names,\n    class_names=le.classes_,\n)\n\n\n\n\n\n\nFigure 18.1: cap\n\n\n\n\n18.4.1.1 What About Overfitting?\nAs you may be starting to suspect, decision trees are prone to overfitting. The tree grows bigger with every question, and by the time we’ve reached the leaves, we know everything we need to know to make predictions that are 100% right 100% of the time… for the data we trained the model on. This extreme overfitting is sometimes called “memorizing” the training data. We don’t want to do that.\nOne way to address the overfitting problem with decision trees is to “prune” them. Remember that the model always asks the most informative question first. This means that as the trees get deeper and deeper – as we ask more questions – each feature is weaker or less predictive than those that came before it. As we move further and further out, we risk making decisions based on noise and overfitting the model to the data we have. The full tree, then, is typically worse than a pruned tree because it includes weak features that could be specific to our dataset.\nWe constrain the depth of the tree by restricting the number of questions or decisions that the model is allowed to ask, and in doing so, we improve the ability of our model to generalize to data it hasn’t seen before. If we set the maximum depth of our tree to 6, for example, the models can only ask the 6 most informative questions, at which point it must make its prediction. Obviously this reduces the accuracy on the training data, but not as much as you might think. It’s the unseen data we care most about, and the pruned model will make much better predictions when it is not overfitted.\nIn Sklearn, we specify the maximum depth of the tree in advance. This can be done using the max_depth argument for the DecisionTreeClassifier(). Let’s set it to 3. This will produce a very shallow tree, but that’s desirable; we want it to have to make the best decisions it can in broad strokes. This way, the model will be less likely to overfit the training data.\ndtclass_pruned = DecisionTreeClassifier(\n    max_depth=3, random_state=0\n)\n\ndt_scores = cross_val_score(\n    dtclass_pruned, X_train, y_train, cv=shuffsplit\n)\n\nprint(dt_scores)\nprint(f\"Mean: {dt_scores.mean()}\")\ndtclass_pruned.fit(X_train, y_train)\n\nexport_graphviz(\n    dtclass_pruned,\n    out_file='../graphical_models/pruned.gv',\n    filled=False,\n    rounded=True,\n    feature_names=interpretable_names,\n    class_names=le.classes_,\n)\ndtclass_pruned.score(X_test, y_test)\nLooking good! We’ve already seen a modest improvement, which probably represents a slight reduction in overfitting (something that cross-validation automatically assesses). Let’s examine the tree again (Figure 18.2):\n\n\n\n\n\n\nFigure 18.2: cap\n\n\n\nYou can see the influence of setting the max_depth parameter to 3 in the tree: rather than a sprawling monstrosity, we now have a tree that neatly terminates each branch at the same level. Decisioin Trees have other parameters you can tweak, such as min_samples_leaf; it’s worth looking at the documentation to see the options available to you! Using only max_depth, we managed to get a good result, but we’re unlikely to be able to do much better using regularization alone. As we saw with Ridge and Lasso regression, regularization usually reaches a ‘sweet spot’ at some modest value, but as the strength of the regularization increases, the model’s performance nosedives. Decision trees have, by their nature, low granularity. You can’t perform fine-grained regularization on a single decision tree the same way you could for an ‘alpha’ parameter on a Ridge or Lasso regression (what would a max_depth of 3.5 even look like?). It’s likely that no regularization of a single-tree model will eliminate overfitting entirely. Instead, we’ll have to turn to a method which will allow us to combine many, many trees.",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Prediction and classification</span>"
    ]
  },
  {
    "objectID": "prediction-and-classification.html#ensemble-learning",
    "href": "prediction-and-classification.html#ensemble-learning",
    "title": "18  Prediction and classification",
    "section": "18.5 ENSEMBLE LEARNING",
    "text": "18.5 ENSEMBLE LEARNING\nOne very effective way to get around the over-fitting problem is to take an ensemble approach, which combines predictions from multiple models into a single prediction that is better than that of any individual model. As you will soon learn, this approach tends to produce excellent results and does not require any pruning. Ensembles of decision trees produce better results than any one decision tree, including any of the decision trees in the ensemble.\nTo work with an ensemble of decision trees, we first draw many bootstrapped samples of instances from our overall dataset. In a bootstrapped sample, replacement is allowed, which means that the same instance can be sampled more than once. For each sample we fit a decision tree and record the model’s predictions. The final predictions are made by an ‘election’ of sorts, where each tree ‘votes’ on the class they think each observation belongs to. If we take 200 samples, we would fit 200 decision trees. These modes are used collectively – as an ensemble – to make predictions on new instances by taking averages of the predictions made by the models that make up the ensemble. This process is called ‘bagging’ or ‘bootstrapped aggregation’, and it can be applied not only to decision trees, but to a wide variety of the classification models implemented in scikit learn! For now, we’ll stick to applying it to decision trees.\nBagging / bootstrapped aggregation goes a very long way in addressing the overfitting problem. One major advantage is that we don’t have to prune our decision trees. In fact, it’s better if we don’t! If we let each tree grow to be as deep and complex as it likes, we will end up with an ensemble that has high variance but low bias. That’s exactly what we want when we go to make our final aggregated predictions. The important choice you must make is how many bags to use, or rather, how many bootstrapped samples of instances to draw, and the number of total trees we want to end up with. Let’s see what the combination of 100 trees can bring us:\nfrom sklearn.ensemble import BaggingClassifier\n\nbag_of_trees = BaggingClassifier(\n    DecisionTreeClassifier(),\n    n_estimators=100,\n    bootstrap=True,\n    random_state=0\n)\n\nbt_scores = cross_val_score(\n    bag_of_trees, X_train, y_train, cv=shuffsplit\n)\n\nprint(bt_scores)\nprint(f\"Mean: {bt_scores.mean()}\")\nThe unregularized bagging classifier has produced an even better score than regularized decision tree did! There may yet be more room for improvement if we alter how each of the trees functions using a Random Forest model.\n\n18.5.1 Random Forests\nOne issue with the bagging approach we just learned is that the resulting trees tend to be correlated with one another, mainly due to the fact that they are all trying to maximize the same thing when they ask questions – information gain. If there are some very powerful attributes in our dataset, as there almost always are, the tree we fit for each bag will lean heavily on those features, which makes the whole ensemble approach a lot less useful and degrades the quality of the final prediction. It would be much better for us if the trees are not correlated, or are at best weakly correlated.\nRandom Forests accomplish this with one simple, but highly effective, modification: they constrain the features that any given node is allowed to ask questions about. The result is a collection of decision trees that are uncorrelated, or weakly correlated, with one another, and this leads to more accurate predictions when they are aggregated.\nRandom Forests are straightforward to train, and because of their clever design, they do a good job of dealing with noise and preventing overfitting, so it is not necessary to trim / prune our trees. They also only take two hyperparameters: the number of trees in your forest (i.e. the number of samples of instances to draw) and size of the random sample to draw when sampling the features that any given decision tree will select from. You can and should experiment with cross-validation to select values for these hyperparameters that result in the most accurate predictions (we’re not doing so here because space is limited).\nfrom sklearn.ensemble import RandomForestClassifier\n\nrforest = RandomForestClassifier(\n    n_estimators=100,\n    max_features=2,\n    random_state=0\n)\n\nrforest_scores = cross_val_score(\n    rforest, X_train, y_train, cv=shuffsplit\n)\n\nprint(rforest_scores)\nprint(f\"Mean: {rforest_scores.mean()}\")\nIt would appear that our Random Forest model, with modest parameters, is producing the exact same result as we got with our bagging classifier.\nThe downside of Random Forests is that – unlike garden-variety decision trees – the results are not so easy to interpret. For this reason, Random Forests and other ensemble models are generally considered to be less “interpretable” than simple decision trees, linear and logistic regressions, or \\(k\\)-nearest neighbours. It’s worth knowing that you can inspect any of the trees in your Random Forest Classifier! This process is complicated somewhat by the fact that our model contains 100 distinct trees, meaning that you can’t easily determine how significant any one tree was to the overall decision making process. Nevertheless, it’s a good idea to select a tree at random and take a look at what it has done with the data. Of course, you can do this many different times, if you like. Just select different trees each time. One such tree is shown in Figure 18.3.\nrforest.fit(X_train, y_train)\n\nexport_graphviz(\n    rforest.estimators_[6],\n    out_file='../graphical_models/rf_classified.gv',\n    filled=False,\n    rounded=True,\n    feature_names=interpretable_names,\n    class_names=le.classes_,\n)\n\n\n\n\n\n\nFigure 18.3: cap\n\n\n\nThere are other ways you that can help you interpret your Random Forest models, such as using rforest.feature_importances to get a sense of which features in your dataset had the greatest impact on predictive power.\nWhile our Random Forest classifier has outperformed decision trees, regularized decision trees, and tied the bagging classifier, there’s one last technique we might use to squeeze out a bit more performance…\n\n\n18.5.2 Gradient Boosted Machines\nWhile Random Forests remain one of the best and most widely-used approaches to supervised machine learning, a slightly newer approach to ensembling decision trees has recently started outperforming Random Forests and is widely considered to be one of the best algorithms for doing machine learning on anything other than image / perception data (Chollet 2018). This technique is called “Gradient Boosting”, and it differs from the Random Forest approach in that, rather than allowing all of the decision trees to randomly pursue the best answer possible in isolation (as Random Forest does), it attempts to fit trees that better account for the mis-classified observations from previous trees. In this way, each tree tackles the ‘room for improvement’ left behind by the tree that immediately preceded it. The effect here is that Gradient Boosted Trees can reach a remarkably high degree of accuracy using only a small handful of estimators (but are accordingly prone to overfitting). Let’s try creating one now:\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngboost = GradientBoostingClassifier(\n    n_estimators=100,\n    random_state=0\n)\n\ngboost_scores = cross_val_score(\n    gboost, X_train, y_train, cv=shuffsplit\n)\n\nprint(gboost_scores)\nprint(f\"Mean: {gboost_scores.mean()}\")\nThe Gradient Boosted Trees achieved worse performance than our previous two models. Usually, we would expect a Gradient Boosted Trees model to outperform all of our other decision tree models (ensemble or otherwise), but that shouldn’t be interpreted as a good reason to skip straight to Gradient Boost without bothering to specify and fit any other models. What we’ve seen here is evidence to that point; there’s value in fitting ‘intermediate’ models to see how their performance and idiosyncracies compare to the cutting-edge techniques. There are a few reasons why this is a vital practice:\nAdvanced, complicated methods are not intrinsically better than simple methods: Not only is this true in our example – given that one of the most demonstrably powerful and widely-applicable algorithms, Gradient Boosting, failed to outperform Random Forests – but it is often true in general. Cutting-edge methods are indispensible for their ability to tackle cutting-edge issues, but they’re often overkill for the kinds of problems they get applied to.\nDon’t Sacrifice Interpretability Without Good Cause: Explicable, interpretable, transparent models that slightly underperform are often more valuable than top-performing ‘black-box’ models that appear to be more accurate, but for reasons that are hard to establish. Gradient Boost models are more difficult to interpret than Decision Tree models, so the advantages of the former over the latter should be considered in light of the interpretability trade-off.\nAny problem in machine learning should be tackled using multiple approaches. Even if you feel like you can’t improve on your model, there may be undiscovered issues lurking beneath the surface. Applying a multitude of modelling strategies to a problem – even in cases where your first model is performing well – may help confirm the defensibility of your primary approach, give you more inferential insight, or uncover contingencies that need to be addressed.\nOne problem common to all tree-based models (ensemble or otherwise) is that they require an abundance of data and are especially prone to overfitting in cases where such data is not forthcoming. That said, there are many ways to make up for a lack of data; in future chapters, we’ll explore methods you can use to get even more out of limited dataset.\nBefore we move on, let’s take a moment to compare how each of our tree-based models perform on the test set which we split off from the training data right at the beginning of this section and haven’t touched since:\nmodel_list = [\n    dtclass,\n    dtclass_pruned,\n    bag_of_trees.fit(X_train, y_train),\n    rforest,\n    gboost.fit(X_train, y_train)\n]\n\nfor model in model_list:\n    print(model.score(X_test, y_test))",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Prediction and classification</span>"
    ]
  },
  {
    "objectID": "prediction-and-classification.html#evaluation-beyond-accuracy",
    "href": "prediction-and-classification.html#evaluation-beyond-accuracy",
    "title": "18  Prediction and classification",
    "section": "18.6 EVALUATION BEYOND ACCURACY",
    "text": "18.6 EVALUATION BEYOND ACCURACY\nSo far, we have been using accuracy to evaluate the quality of our machine learning models. While accuracy is important, it isn’t everything. Consider two issues.\nFirst, bad models can have high accuracy rates. This can happen when the events we want to predict are rare, or the categories we want to classify instances into are very imbalanced. In such scenarios, we can achieve very high accuracy rates by making the same guess all the time. We haven’t actually learned anything useful, we just learned that if something is rare, predicting the rare thing didn’t happen is often correct. Similarly, bad models can achieve high accuracy rates by learning from the wrong things. For example, a deep learning model differentiating pictures of huskies from pictures of wolves – with a high level of accuracy, of course – was shown to be relying on whether or not the image contained snow. That’s a bad model. We will return to this example later in the chapter.\nSecond, depending on the purpose of your machine learning, there may be aspects of your model’s performance that are more important to optimize than accuracy. Imagine you were tasked with designing a machine learning algorithm capable of detecting when a patient in a hospital requires a dose of analgesic medication; although false negatives in such a setting would be bad, false positives could be deadly and should be avoided at all costs. In that case, we could set a hard constraint on the ratio of false positives to false negatives and ignore any marginal increases that break that constraint.\nThere are a number of useful measures we can use when accuracy is not ideal. Selecting the best from among them depends on factors such as the data you have, the types of question you are trying to answer, the consequences of false positives or false negatives, and so on. In other words, once again, you need to know what you are trying to do, why you are doing it, and where the risks are. Your evaluation measures should be aligned with those larger goals.\nLet’s work through some examples of other evaluation measures. In the examples below, we will focus primarily on evaluation measures for classification models, but we will also discuss evaluation for regression models.\n\n18.6.1 Balancing False Positives and False Negatives in Classification Models\nWe can evaluate the quality of a machine learning model in terms of the balance between false positives and false negatives, or Type I and Type II errors, respectively. In the context of binary classification, false positives are when we predict that a negative instance is positive, and false negatives are when we predict that a positive instance is negative. Imagine you have a binary classification model to predict whether any given country is an autocracy. In this model, ‘autocracy’ is the “positive” class and ‘not autocracy’ (i.e. democracy) is the “negative class.” If your model predicted that New Zealand was an autocracy in 2020, that’s a false positive / Type I error. If it predicted that North Korea was a not an autocracy in 2020, that’s a false negative / Type II error.\nIn some cases, we may prefer a model with slightly lower accuracy if it minimizes false positives or false negatives while still having good overall accuracy. This is especially important in models that might have some kind of real-world impact. Think about whether there are any potentially negative consequences that could result from one type of error versus the other, and if so, how much overall accuracy would you be willing to accept to reduce the risk of false positives or negatives?\nGenerally, it’s easier to work with single score summaries-+ for evaluation metrics, like accuracy. The most common alternative single score evaluation metrics are Precision and Recall.\nPrecision is a measure of the number of predicted positive cases that are actually positive. Specifically, Precision is the number of true positives the model identified divided by the number of total positives (true and false) the model identified. Precision gives us the proportion of correct predictions (true positives). Recall is a measure of the number of true positives that the model identified divided by the total number of true positive cases (regardless of whether or not the model identified them as such). Recall is the number of true positives divided by the number of actual positives in the data. In other words, the proportion of positive cases that we were able to identify.\n\\[\\begin{equation}\nPrecision = \\frac{True Postives}{True Positives + False Positives}\n\\end{equation}\\]\n\\[\\begin{equation}\nRecall = \\frac{True Postives}{True Positives + False Negatives}\n\\end{equation}\\]\nPrecision should be used when you are trying to reduce false positives, and Recall should be used when you are trying to limit false negatives. However, it is generally a bad idea to focus on one of these measures without considering the other. If you only care about avoiding false positives, the solution is trivial. You will never produce a false positives if you never predict the positive case! So while one type of error may be more serious than the other, you should consider them together. The goal is to strike an acceptable balance between the two types of errors.\nOne way of considering preciPrecisionsion and Recall together is to use a measure such as F-Score, which combines Precision and Recall into a single measure by computing their harmonic mean, shown below.\n\\[\\begin{equation}\nF_1 = \\frac{2}{Recall^{-1} + Precision^{-1}}\n\\end{equation}\\]\nRecalling that Precision and recall are proportions, and therefore range between 0 and 1, as Precision and recall improve (get closer to 1), the F-Score will approach 1. As Precision and recall get closer to 0, the sum of their inverses grows towards infinity and the F-score will approach 0. In short: F-Scores that are close to 0 are bad, close to 1 are good.\nSklearn’s implementation can be imported from the metrics module. Conveniently for us, Sklearn will report the Precision, Recall, and F-Score together in a classification report. The final column in the report – ‘support’ – is the number of true instances in that class, or the ‘ground truth’. Each class category in the report has it’s own line, as this is an example of binary classification.\n\n\n18.6.2 Improving Binary Classification with Curves\nPredictions about class labels – was New Zealand an autocracy in 2020? – depend on some underlying probability threshold. When our models classifies New Zealand as not an autocracy, it predicted the negative class because New Zealand is above some probability threshold that separates the boundaries between classes. In other words, the probability of it being a “not autocracy” is greater than the probability of it being an autocracy. For example, if the probability of a country being an autocracy is greater than .5, classify it as an autocracy, otherwise classify it as not an autocracy.\nWhat if there was a different probability threshold? What if, instead, our model would only classify countries as autocracies if the probability of them being in the autocratic class was above .8 instead of above .5? Shifting the threshold in this way reduces the false positive rate because it would make it harder for any given case to be classified as positive, i.e. autocratic. But what would that do to our rate of false negatives? Does it really matter if we inaccurately classify some autocracies as not autocracies if we prevent our models from incorrectly classifying some non-autocracies as autocracies?\nAs always, it depends on your case, the questions you are trying to answer, the problems you are solving, and – more than anything else – the consequences of both types of error. You need to think carefully about these questions every time you begin a machine learning project. Once you have a sense of what an ideal model might look like (e.g. one with very few false positives), you can use Precision-Recall Curves to understand how different probability thresholds separating the positive and negative cases change the balance between false positives and false positives. We will briefly discuss these, as a full exploration is beyond the scope of this chapter.\n\n18.6.2.1 Precision-Recall Curves\nHow good would your model be if you needed to ensure a minimum of 90% Recall – that is, if you needed to correctly identify at least 90% of the true positives in the data? Again, depending on the specifics of your project, maximizing your models performance on one dimension, while still being good enough on other dimensions, is better than relentlessly pursuing small improvements in overall accuracy.\nPrecision-Recall curves let us visualize the tradeoffs between these two metrics and understand their impact on the quality of our classifiers at various probability thresholds. Models with high Precision and high Recall are better, so what we are looking for is a model where the curve is as close as possible to 1 on both axes. Note, however, that we will never actually get to 1,1 because of the inherent tradeoff between these two measures.\nAlternatively, we can compute the area under the curve, or AUC, to get a one-number summary of the quality of this model. The AUC is not necessarily a better approach when we are assessing one model, but since it is a single number summary it does make it easier to compare the performance of multiple models. Very simply, consider a randomly chosen pair of a true positive \\(p\\) and a true negative \\(q\\): The AUC is a measure of how likely the model is to rank \\(p\\) higher than \\(q\\), ranging between 0 and 1. A perfect classifier would always rank true positives higher than true negatives, so scores closer to 1 are better. Precision-Recall Curves are very helpful and informative when the number of cases in each class label are imbalanced.\nIf you want further information and examples on Precision-Recall Curves, and ROC Curves, I suggest looking into (Géron 2019) and (Müller and Guido 2016).\n\n\n18.6.2.2 Beyond Binary Classifiers\nSo far we have only considered measures for evaluating binary classifiers, but the evaluation metrics for multi-class predictions build on those of the binary tasks, so we can extend what we have just learned to these more complex models. We don’t have the room to cover them here, but if you’re interested in exploring multi-class evaluation metrics, feel free to check out our section on them which can be found in the online supplemental material for this textbook. Looks like the training results match up nicely with the test results!\n\nFurther Reading\nAs with the previous chapter, if you want to learn more about doing supervised machine learning with the models covered in this chapter, and many others, I recommend consulting the relevant chapters from Andreas M{\"u}ller and Sarah Guido’s (2016) Introduction to Machine Learning with Python: A Guide for Data Scientists or Aur{'e}lien G{'e}ron’s (2019) Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems.",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Prediction and classification</span>"
    ]
  },
  {
    "objectID": "prediction-and-classification.html#conclusion",
    "href": "prediction-and-classification.html#conclusion",
    "title": "18  Prediction and classification",
    "section": "18.7 CONCLUSION",
    "text": "18.7 CONCLUSION\n\n18.7.1 Key Points\n\nLearned how to set up, build, fit, and interpret supervised learning tree-based classifiers, including decision trees and ensemble classifiers\nExplored how pipelines help prevent data leakage while also facilitating grid searches and cross-validation\nDemonstrated the use of parallel processing to expedite model fitting\nLearned how to use a variety of performance metrics to balance between false positives and false negatives\nCreated and interpreted graphical measures of threshold tradeoffs, including Precision-Recall and reciever operating characteristic curves\n\n\n\n\n\nChollet, Francois. 2018. Deep Learning with Python. Vol. 361. Manning New York.\n\n\nDomingos, Pedro. 2015. The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World. Basic Books.\n\n\nGéron, Aurélien. 2019. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O’Reilly Media.\n\n\nMüller, Andreas, and Sarah Guido. 2016. Introduction to Machine Learning with Python: A Guide for Data Scientists. \" O’Reilly Media, Inc.\".",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Prediction and classification</span>"
    ]
  },
  {
    "objectID": "causality.html",
    "href": "causality.html",
    "title": "20  Causality",
    "section": "",
    "text": "In Progress\n\n\n\nThis is a “shitty first draft,” with some sections being shittier than others. Normally I wouldn’t share work so early in it’s development, but I’m interested in your feedback. Feel free to skim, but know that I am actively developing this chapter and it will be going through some very extensive changes in the coming weeks.",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Causality</span>"
    ]
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "20  Probability 101",
    "section": "",
    "text": "20.1 LEARNING OBJECTIVES",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Probability 101</span>"
    ]
  },
  {
    "objectID": "probability.html#learning-objectives",
    "href": "probability.html#learning-objectives",
    "title": "20  Probability 101",
    "section": "",
    "text": "Explain the meaning of key concepts in probability theory, including\n\nrandom variable\nsample space\n\nDifferentiate between (i) discrete probability distributions and Probability Mass Functions (PMFs) and (ii) continuous probability distributions and Probability Density Functions (PDFs)\nLearn how to select probability distributions for modelling by thinking through their assumptions and learning about the parameters they take\nExplain the differences between marginal, joint, and conditional probabilities, and the notation used to express these probabilities\nUnderstand Bayes’ theorem and its component parts",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Probability 101</span>"
    ]
  },
  {
    "objectID": "probability.html#learning-materials",
    "href": "probability.html#learning-materials",
    "title": "20  Probability 101",
    "section": "20.2 LEARNING MATERIALS",
    "text": "20.2 LEARNING MATERIALS\nYou can find the online learning materials for this chapter in doing_computational_social_science/Chapter_26. cd into the directory and launch your Jupyter Server.",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Probability 101</span>"
    ]
  },
  {
    "objectID": "probability.html#introduction",
    "href": "probability.html#introduction",
    "title": "20  Probability 101",
    "section": "20.3 INTRODUCTION",
    "text": "20.3 INTRODUCTION\nIn the previous chapter, we discussed how the Frequentist paradigm differs from the Bayesian interpretation probability and contrasted discriminative models with generative models. Throughout that discussion, we didn’t actually discuss the mathematics of probability, which are the same regardless of your philosophical persuasion, or the role that probabilities play in the models you develop.\nThis chapter is a primer on probability theory. Unlike other introductions to probability, we won’t be getting into any mathematical proofs; there are plenty of those available elsewhere. Instead, we will clarify some foundational concepts and aim to build a bit of intuition about how different types of probability distributions work through simple simulations. Though not especially broad or deep, this introduction will provide enough knowledge of probability that you will be able to understand, develop, critique, interpret, and revise generative models for structured, network / relational, and text data.\n\n20.3.1 Imports\nimport math\nimport numpy as np\nnp.random.seed(42)\nimport pandas as pd\nfrom collections import Counter\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom dcss import set_style\nset_style()",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Probability 101</span>"
    ]
  },
  {
    "objectID": "probability.html#foundational-concepts-in-probability-theory",
    "href": "probability.html#foundational-concepts-in-probability-theory",
    "title": "20  Probability 101",
    "section": "20.4 FOUNDATIONAL CONCEPTS IN PROBABILITY THEORY",
    "text": "20.4 FOUNDATIONAL CONCEPTS IN PROBABILITY THEORY\nFrequentists and Bayesians differ in their philosophies but not in their mathematics. Both paradigms are built on a solid mathematical foundation of probability distributions that define the relative probability of all potential events that could result from some pre-defined system, experiment, or trial.\nThe starting point for probability theory is the sample space, sometimes referred to using the symbol \\(S\\). The sample space is simply an exhaustive list of all the possible events (or outcomes) that could result from some trial or experiment. The sample space for a coin toss, for example, consists of Heads and Tails, but not 42 or a bag of catnip. Once we have defined the sample space, we assign probabilities to every possible event.\nProbability distributions are just sample spaces where every possible event has an associated probability. They are governed by three axioms:\n\nThe probability of any event occurring is equal to or greater than 0. There can be no negative probabilities.\nThe probability of the entire sample space is 1. If the probability distribution is discrete (i.e., distinct events that can be individually counted, like the result of a coin flip) then the sum of those probabilities must equal 1. If the probability distribution is continuous (i.e., you can’t count the distinct events because they are uncountably infinite) then the probabilities of each event must add up to 1 using an infinite sum operation.\nIf events A and B are mutually exclusive (a coin can’t land both Heads and Tails), or disjoint, then the probability of either event occurring is equal to the probability of A + B.\n\nThis third axiom, the “additivity axiom,” is often expressed using notation from set theory. You may see it expressed in one of two forms, one that is common in more mathematical discussions of probability:\n\\[\n\\text{if } P(\\text{A} \\cap \\text{B}) = \\emptyset, \\text{ then } P(\\text{A} \\cup \\text{B}) = P(A) + P(B)\n\\]\nand one that is more common in the statistical discussions of probability:\n\\[\n\\cap P(A_{i}) = \\emptyset, \\text{ then } \\cup P(A_i) = P(A_1) + P(A_2) + ... + P(A_n)\n\\]\nThese two expressions are saying the same thing. The \\(\\cup\\) represents the set theoretic concept of the union of two events and the \\(\\cap\\) symbol represents their intersection. Consider the simple venn diagram in Figure 20.1.\n\nTwo events, A and B, intersect in the top of the figure. The point at which they intersect is the portion of the venn diagram where the two events overlap one another. This intersection is represented by the symbol \\(\\cap\\). If the two events do not intersect, as in the bottom of the figure, then the intersection (\\(\\cap\\)) of the two sets is empty. We represent this emptiness with the symbol \\(\\emptyset\\). In essence, all the third axiom is saying is that if two events are disjoint, then the probability of either of those two events happening is the probability of the first event plus the probability of the second event. That’s it.\nThese iron-clad rules are paraphrased versions of the original trio, known as ‘Kolmogorov Axioms’ after the mathematician Andrey Kolmogorov. Together, they produce a number of useful features that we’ll explore and exploit throughout the rest of this book.\nAnother essential concept in probability theory is that of a “random variable.” Consider the example of a coin flip once again. There is no inherent numerical value for the outcome of a coin flip. When we flip it, it will either land heads up or tails up. The trouble is that neither ‘heads’ nor ‘tails’ has any inherent mathematical meaning, so it’s up to us to create something that allows two worlds (the world of the coin and the world of statistics) to come together.\nOne way to do this is to say that there is a “random variable” with values 0 and 1 that represent the outcomes of the coin flip; heads up = 1, tails up = 0. At this point, writing \\(X = \\text{heads}\\) means the same thing as \\(X = 1\\), and writing \\(X = \\text{tails}\\) means the same thing as \\(X = 0\\). What’s more, we can use probability distributions to describe the probability of the coin flip taking on each value it is capable of producing. Random Variables may take on more than two values; you might use one to describe income, occupation, or height. In short, random variables are what enable us to connect the tangible worlds of coin tosses, income inequality, and so on, with the mathematical world of probability and statistical inference.\nNow, let’s start learning about the properties of some specific probability distributions. I’ll be perfectly honest, most of this is not inherently interesting to applied researchers, but a bit of knowledge here goes a very long way. In the chapters that follow, I will assume you know the contents of this chapter in particular, even though you might be flipping back to it from time to time to remind yourself of the details. That’s perfectly fine!\nFinally, as with other chapters in this book we’ll be discussing some equations. As promised at the start of the book, these equations are not meant to carry the explanatory weight here. If you are used to seeing and thinking about equations, great. If not, that’s OK too. You still need to understand how the distributions work and what kinds of parameters they take, but it’s possible to gain that knowledge from the simulations instead of the formulas. Ideally, the simulations will help you understand the equations, and the two can work together.",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Probability 101</span>"
    ]
  },
  {
    "objectID": "probability.html#probability-distributions-and-likelihood-functions",
    "href": "probability.html#probability-distributions-and-likelihood-functions",
    "title": "20  Probability 101",
    "section": "20.5 PROBABILITY DISTRIBUTIONS AND LIKELIHOOD FUNCTIONS",
    "text": "20.5 PROBABILITY DISTRIBUTIONS AND LIKELIHOOD FUNCTIONS\nThe mathematics of probability vary depending on whether we are working with discrete or continuous distributions, and then there are further differences based on the specific discrete or continuous distributions. To help you understand some of the differences and build some intuition, we’ll walk through a few examples of widely-used distributions. We’ll start with discrete distributions and “probability mass functions,” which represents the probability that a discrete random variable for a probability distribution is a specific value. Then we’ll move on to continuous distributions and “probability density functions.”\n\n20.5.1 Discrete Distributions, Probability Mass Functions\nDiscrete distributions are used when we have a limited number of distinct countable outcomes. For example, we can flip Heads or Tails on a coin, but nothing in between. Similarly, we can roll a 3 or a 4 on a regular die, but we can’t roll a 3.5. Below, we will learn a bit about three discrete probability distributions that are commonly used in models: the Uniform, Bernoulli, and Binomial distributions.\n\n20.5.1.1 Everything is Equally Likely: The Uniform Distribution\nThe uniform distribution is probably familiar to you in practice, if not by name. A uniform probability distribution describes a collection of possible outcomes for a random process where each outcome is equally (uniformly) likely. To compute the probability of an event given this assumption that all events are equally likely, we simply divide by the number of possible events,\n\\[\nP(x) = \\frac{1}{n}\n\\]\nWhere \\(x\\) is a particular value within the range of the distribution, and \\(n\\) is the number of possible values in the range. This simple equation is an example of a probability mass function (PMF), and in this case it applies only to uniform distributions. If we want to know the probability that our random variable is some value, and we are using a uniform distribution, all we need to know is the number of possible events (\\(n\\)), and that \\(x\\) is one of those possible events.\nFor example, if we wish to know the probability of rolling a 7 on a die, we need only to know how many sides are on the die (\\(n\\)), and be sure that 7 (\\(x\\)) is on one of those sides. If we are rolling a classic six-sided die, our distribution is defined only for values 1 through 6: you cannot roll a 7. However, if we are rolling a 10-sided die, and 7 is on one of those sides, we can plug those numbers into the above PMF to see that:\n\\[\nP(7) = \\frac{1}{10} = 0.1\n\\]\nSince the uniform distribution assigns the same probability to all events, all events are equally likely. If we assign values to each event, then the expected value is simply the weighted average value of that distribution. It is calculated as follows:\n\\[\n\\text{E}[X] = \\sum_{i=1}^{n} x_i p_i\n\\]\nWhere \\(\\text{E}[X]\\) represents the expected value of \\(X\\), \\(i\\) is an iterator, \\(n\\) is the number of different values \\(X\\) can take on, \\(x_i\\) is the value of one of the events represented by \\(X\\), and \\(p_i\\) is the probability of that event. In this case, because we’re using the uniform distribution, the weights \\(p_i\\) will all be identical, and \\(\\text{E}[X]\\) will just be the average value of \\(X\\).\nLet’s use numpy’s random number generators to simulate a uniform distribution with different sample sizes and ranges and then visualize the differences. We’ll simulate rolling a six-sided die 10,000 times by generating an array of 10,000 random integers between 1 and 6 (the number of possible events). If you set the same seed I did (42, in the imports) you should draw the same numbers. If you don’t set a seed, or you set it to something different, yours will be different.\nn_possible_events = 6\nsamples = np.random.randint(1, n_possible_events+1, 10_000)\nCounter(samples)\nGood! That’s exactly what we would expect to see (1 / n_possible_events * 10_000 = 1666.666) when drawing so many samples from a uniform distribution. But what if we had drawn a smaller number of samples? Let’s do few more simulations and visualize the results (?fig-25_02). We will add a red line showing \\(E[X]\\), the expected value.\nuniform_sim_1 = np.random.randint(1, n_possible_events+1, 6)\nuniform_sim_2 = np.random.randint(1, n_possible_events+1, 100)\nuniform_sim_3 = np.random.randint(1, n_possible_events+1, 1_000)\nuniform_sim_4 = np.random.randint(1, n_possible_events+1, 10_000)\ndef get_percentages(simulation_array, n_samples):\n    s = pd.Series(simulation_array).value_counts().div(n_samples)\n    return s\nfig, ax = plt.subplots(2, 2, sharex=True, sharey=True)\n\nsns.barplot(\n    x=get_percentages(uniform_sim_1, 6).index,\n    y=get_percentages(uniform_sim_1, 6), \n    ax=ax[0, 0], \n    color='gray'\n)\n\nax[0,0].axhline(1 / 6, color='crimson')\nax[0,0].set(title='6 samples')\n\nsns.barplot(\n    x=get_percentages(uniform_sim_2, 100).index,\n    y=get_percentages(uniform_sim_2, 100), \n    ax=ax[0, 1], \n    color='gray')\n\nax[0,1].axhline(1 / 6, color='crimson')\nax[0,1].set(title='100 samples')\n\nsns.barplot(\n    x=get_percentages(uniform_sim_3, 1_000).index,\n    y=get_percentages(uniform_sim_3, 1_000), \n    ax=ax[1, 0], \n    color='gray'\n)\n\nax[1,0].axhline(1 / 6, color='crimson')\nax[1,0].set(title='1,000 samples')\n\nsns.barplot(x=get_percentages(\n    uniform_sim_4, 10_000).index, \n    y=get_percentages(\n        uniform_sim_4, 10_000), ax=ax[1, 1], color='gray'\n    )\n\nax[1,1].axhline(1 / 6, color='crimson')\nax[1,1].set(title='10,000 samples')\n\nsns.despine()\nplt.tight_layout()\nplt.savefig('figures/25_01.png', dpi=300)\n\n\n\n\n\n\nFigure 20.1: png\n\n\n\nThese four simple simulations show that we get closer and closer to the expected value the more samples we draw. Go ahead and vary the number of samples and re-run the code. You’ll find that the red line is always on the same y-value. Given a uniform distribution and in the case where the number of possible events is 6, the probability of any specific outcome of rolling our die is ~0.1666…, or ~16.67%.\n\n\n20.5.1.2 The Bernoulli and Binomial Distributions\nThe Bernoulli distribution is a bit different than the other distributions we examine in this chapter. It’s actually special case of the binomial distribution, which we will discuss in a moment. The Bernoulli distribution describes an experiment with only a single sample, where the outcome of the experiment is binary (e.g., 0 or 1, yes or no, heads or tails, tested positive for COVID-19 or not), and described by a single probability \\(p\\). Since we only have two possible outcomes, we only need to know the probability of one of those outcomes because the sum of the probabilities of all outcomes must equal 1. Necessarily, if the probability of testing positive for COVID-19 is 20%, the probability of testing negative is 80%.\nThe binomial distribution is the extended case of the Bernoulli distribution. The binomial distribution models observing certain events over some kind of interval. Specifically, the events are Bernoulli trials: events with binary outcomes with a probability \\(p\\) describing one outcome and \\(q\\) describing the other. The interval is a discrete range of number of trials.\nThe PMF for the binomial distribution models the number of events corresponding to probability \\(p\\) observed over \\(n\\) trials. The formula is:\n\\[\nP(x) = {n \\choose x}p^{x}q^{n-x}\n\\]\nWhere:\n\n\\(x\\) represents observing a specific number of outcomes corresponding to the probability \\(p\\)\n\\(n\\) is the number of trials\n\\(p\\) is the probability of observing the chosen outcome\n\\(q\\) is the probability of observing the other outcome and is equal to \\(1 - p\\)\n\nTo make this more concrete, let’s return to the somewhat tiresome, but useful, example of flipping fair coins. Since this is a binomial distribution, it’s composed of a series of Bernoulli trials. If we flip the coin 10 times, we are conducting 10 Bernoulli trials (\\(n=10\\)). Across all 10 trials, what’s the probability of seeing heads \\(x\\) times?\nSince we have decided to select heads as our success condition, we shall set the probability of observing heads equal to \\(p\\). Given that we have a fair coin, both sides are equally likely, so we know that the \\(p = 0.5\\), and by extension the probability of tails is \\(1 - p = 0.5\\). We also know that the number of Bernoulli trials is \\(n = 10\\) because we are flipping the coin 10 times.\nAs with the other PMFs, we can get the probability of seeing heads \\(x\\) times by plugging the value for \\(x\\) into the formula. If we wanted to determine the probability of getting heads 3 times out of the 10 flips:\n\\[\\begin{align}\nP(3) &= {10 \\choose 3}0.5^{3}0.5^{10-3} \\\\\nP(3) &= {10 \\choose 3}0.5^{3}0.5^{7} \\\\\nP(3) &= 0.1171875\n\\end{align}\\]\nIf we flip a fair coin 10 times, we should expect to get exactly 3 heads approximately 12% of the time.\nAgain, let’s use simulations to deepen our understanding. We’ll need to provide the number of Bernoulli trials we would like to run. Since we can calculate the one probability using the other, we only need the probability of the “success” outcome, \\(p\\), of our Bernoulli trial. We also provide the number of random samples we draw from the binomial distribution.\nIt is worth stressing the difference between the number of Bernoulli trials and the number of samples we draw. In the above example, \\(n\\) is the number of Bernoulli trials, or coin flips. The number of samples we draw does not feature in the equation: each time we draw a sample, we are essentially flipping 10 coins and tallying the results.\nBefore running the simulations, consider what would you expect to see given different values for the probability of the success condition, \\(p\\). If \\(p\\) were 0.8, for example, what do you think you might see with an \\(n\\) of 40? How do you think distribution would change with different values for \\(n\\) and \\(p\\)?\nThe results are shown in Figure 20.2.\nbinomial_sim_1 = np.random.binomial(20, 0.5, 10_000)\nbinomial_sim_2 = np.random.binomial(20, 0.8, 10_000)\nbinomial_sim_3 = np.random.binomial(10, 0.5, 10_000)\nbinomial_sim_4 = np.random.binomial(10, 0.8, 10_000)\nbinomial_simulations = pd.DataFrame(\n    [\n        binomial_sim_1, binomial_sim_2, binomial_sim_3, binomial_sim_4\n    ]\n).T\nfig, ax = plt.subplots(2, 2,figsize=(8, 6))\n\nt = list(range(1, 21))\n\nsns.countplot(\n    x=binomial_simulations[0], ax=ax[0, 0], color='gray', order=t\n)\nax[0, 0].set(\n    xlabel=\"\", \n    title = r'$n=20$ and $p=0.5$'\n)\n\nsns.countplot(\n    x=binomial_simulations[1], ax=ax[0, 1], color='gray', order=t\n)\nax[0, 1].set(\n    xlabel=\"\", \n    ylabel=\"\", \n    title = r'$n=20$ and $p=0.8$', \n    xticks=range(0, 20)\n)\n\nsns.countplot(\n    x=binomial_simulations[2], ax=ax[1, 0], color='gray', order=t\n)\nax[1, 0].set(xlabel=\"\", title = r'$n=10$ and $p=0.5$')\n\nsns.countplot(\n    x=binomial_simulations[3], ax=ax[1, 1], color='gray', order=t\n)\nax[1, 1].set(xlabel=\"\", ylabel=\"\", title = r'$n=10$ and $p=0.8$')\n\nsns.despine()\nplt.savefig('figures/25_03.png', dpi=300)\n\n\n\n\n\n\nFigure 20.2: png\n\n\n\nThere are, of course, other discrete probability distributions that are commonly used in probabilistic models. There is little to no point in trying to introduce them all here, and there are many fine introductions that go into considerable technical depth. But now you should have a pretty good understanding of the basic concepts and ideas, and you should know to expect unfamiliar distributions to have (i) some set of assumptions that make them more or less appropriate to use given the nature of what you are trying to model, and (ii) some set of parameters that govern the distribution, and which can be used to compute probabilities for different samples.",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Probability 101</span>"
    ]
  },
  {
    "objectID": "probability.html#continuous-distributions-probability-density-functions",
    "href": "probability.html#continuous-distributions-probability-density-functions",
    "title": "20  Probability 101",
    "section": "20.6 CONTINUOUS DISTRIBUTIONS, PROBABILITY DENSITY FUNCTIONS",
    "text": "20.6 CONTINUOUS DISTRIBUTIONS, PROBABILITY DENSITY FUNCTIONS\nEverything we’ve seen so far in this chapter pertains to discrete distributions. Things are a bit different with continuous distributions, as there are an uncountably infinite number of different values a continuous distribution can take on. Counterintuitively, this means that the probability of any specific value in any continuous distribution (e.g., 8.985452) is 0; instead, we must describe the probability present across a range of values. Instead of using Probability Mass Functions (PMFs) to compute probabilities, we use Probability Density Functions (PDFs). Let’s see how this works by focusing on the ubiquitous normal distribution.\n\n20.6.1 The Normal Distribution\nThe normal distribution (often called the ‘Gaussian’ distribution) is foundational to traditional statistics. We don’t have the room to cover why it appears so frequently (in both nature and the scientific study thereof), but you’ve almost certainly seen it many times before. Regardless of the moniker we apply, the Normal Distribution describes a process that trends towards some mean value (\\(\\mu\\)) with data evenly spread around it in proportions that diminish the further away from the mean they are. We use the term ‘Standard Deviation’ (\\(\\sigma\\)) to describe how quickly the data diminishes; you might also encounter publications and software that describe it as ‘width’ or ‘scale’. The PDF for the normal distribution is this monstrosity:\n\\[\nP(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}\n\\]\nWhere:\n\n\\(\\mu\\) is the mean of the distribution\n\\(\\sigma\\) is the standard deviation\n\\(e\\) is the mathematical constant Euler’s Number (\\(\\approx\\) 2.71828…)\n\\(\\pi\\) is the mathematical constant pi (\\(\\approx\\) 3.14159…)\n\nA standard deviation of 0 indicates that every observation is the same as the mean value. The larger the standard deviation, the further away from the mean the average observation will be.\nLet’s start simulating! We’ll use Numpy’s random.normal() to perform four simulations, each pulling 10,000 samples from normal distributions with slightly different parameters. The first two arguments indicate the mean and standard deviation for the normal distribution, and the third indicates the number of samples we’ll draw. Results are shown in Figure 20.3.\nnormal_sim_1 = np.random.normal(0, 0.1, 10_000)\nnormal_sim_2 = np.random.normal(0, 0.2, 10_000)\nnormal_sim_3 = np.random.normal(0, 0.3, 10_000)\nnormal_sim_4 = np.random.normal(0.5, 0.2, 10_000)\nb = np.linspace(-1, 1, 30)\n\nfig, ax = plt.subplots(2,2, sharex=True, sharey=True, \n                       figsize=(6,4))\nsns.histplot(normal_sim_1, ax = ax[0,0], kde=True, bins=b)\nax[0,0].set_title(r'$\\mu$ = 0 and $\\sigma$ = 0.1')\n\nsns.histplot(normal_sim_2, ax = ax[0,1], kde=True, bins=b)\nax[0,1].set_title(r'$\\mu$ = 0 and $\\sigma$ = 0.2')\n\nsns.histplot(normal_sim_3, ax = ax[1,0], kde=True, bins=b)\nax[1,0].set_title(r'$\\mu$ = 0 and $\\sigma$ = 0.3')\n\nsns.histplot(normal_sim_4, ax = ax[1,1], kde=True, bins=b)\nax[1,1].set_title(r'$\\mu$ = 0.5 and $\\sigma$ = 0.2')\n\nsns.despine(left=True)\nplt.tight_layout()\nplt.savefig('figures/25_04.png', dpi=300)\n\n\n\n\n\n\nFigure 20.3: png\n\n\n\nAs usual, deepen your understanding of how the normal distribution behaves by experimenting with different values of mu (\\(\\mu\\)) and sigma (\\(\\sigma\\)).\n\n20.6.1.1 The Exponential Distribution\nAnother important continuous distribution is the exponential distribution. Among other things, the exponential distribution is often used to model the half-life of radionuclide decay, which describes the amount of time it takes for half of a given mass of radioactive atoms to decay into other atoms. Like the other distributions that have shown up so far, we’ll be using it a few times in the chapters that follow.\nThe PDF of the exponential distribution is normally given as:\n\\[\nP(x) = \\lambda e^{\\lambda x}\n\\]\nWhere:\n\n\\(\\lambda\\) is the rate parameter of the events, and must be greater than 0.\n\\(e\\) is Euler’s number (~ 2.71828…)\n\\(x\\) is the time until the next event\n\nAlthough ‘rate parameter of the events’ is a precise definition, it isn’t a particularly intuitive one. You might be more familiar with \\(\\lambda\\) representing ‘rate of decay’ or ‘half-life’.\nNote that what we presented above is not the only formulation of the exponential distribution PDF. These functions can be written in many ways, and for various reasons like interpretability or ease of calculation, some people prefer one over another. This is relevant because we will be drawing samples using numpy’s exponential distribution function, which uses the scale parameter \\(\\beta = \\frac{1}{\\lambda}\\) rather than \\(\\lambda\\).\nThis gives the following PDF:\n\\[\nP(x) = \\frac{1}{\\beta} e^{-\\frac{x}{\\beta}}\n\\]\nLet’s jump right into the simulations. Results shown in Figure 20.4.\nexponential_sim_1 = np.random.exponential(1, 10000)\nexponential_sim_2 = np.random.exponential(2, 10000) \nfig, ax = plt.subplots()\n\nsns.histplot(\n    exponential_sim_1, color='crimson', label=r'$\\beta = 1$'\n)\n\nsns.histplot(\n    exponential_sim_2, color='lightgray', label=r'$\\beta = 2$'\n)\n\nsns.despine()\nplt.legend()\nplt.savefig('figures/25_05.png', dpi=300)\n\n\n\n\n\n\nFigure 20.4: png\n\n\n\nAs you can see from the simulations above, the Exponential distribution always assigns the greatest probability density to values closest to 0, with a long tail to the right. The value of \\(\\beta\\) or \\(\\lambda\\) influences how much of the probability density is in the tail.\nThe Exponential distribution always assigns a probability of 0 to any events that are less than 0. This is a useful property for us, as it allows us to use an Exponential distribution to describe processes that cannot have negative values.",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Probability 101</span>"
    ]
  },
  {
    "objectID": "probability.html#joint-and-conditional-probabilities",
    "href": "probability.html#joint-and-conditional-probabilities",
    "title": "20  Probability 101",
    "section": "20.7 JOINT AND CONDITIONAL PROBABILITIES",
    "text": "20.7 JOINT AND CONDITIONAL PROBABILITIES\n\n20.7.1 Joint Probabilities\nUp to this point, we’ve been primarily focused on marginal probabilities, though we haven’t called them that. Marginal probabilities describe events that are unconditional on other events (which is why you’ll also see us use unconditional probability to refer to the same kind of thing – the two terms are interchangeable). Joint probabilities, on the other hand, describe two or more events occurring together. Let us consider some simple examples.\nThink of a standard deck of cards without jokers: 52 cards with two colours (red and black) divided into 4 suits (Clubs, Diamonds, Hearts, Spades), each with 13 cards having values of Ace through 10, Jack, Queen, and King. If we wanted to know the probability of randomly drawing a single Jack of any suit from the deck, then we are talking about a marginal probability, because the probability of drawing a Jack is independent of other events in this scenario. As there are 4 Jacks in the 52 cards, we can express this probability with the following:\n\\[\\begin{align}\nP(\\text{Jack}) &= \\frac{\\text{Number Of Jacks}}{\\text{Number Of Cards}} \\\\\nP(\\text{Jack}) &= \\frac{4}{52} = \\frac{1}{13}\n\\end{align}\\]\nConversely, if we wanted to know the marginal probability of drawing a Diamond:\n\\[\\begin{align}\nP(\\text{Diamond}) &= \\frac{\\text{Number Of Diamonds}}{\\text{Number Of Cards}} \\\\\nP(\\text{Diamond}) &= \\frac{13}{52} = \\frac{1}{4}\n\\end{align}\\]\nSometimes we want to know the probability of two independent events occurring simultaneously, which again is known as a joint probability. When we want to represent the joint probability of two independent events which we will arbitrarily call \\(A\\) and \\(B\\), we use \\(P(A \\cap B)\\), which represents the intersection of these two events. To get joint probabilities, we multiple the marginal probability of one event by the marginal probability of the other event, which can be expressed as:\n\\[\nP(A \\cap B) = P(A) \\times P(B)\n\\]\nNow consider the probability of drawing the Jack of Diamonds. The event we are interested in can be expressed as two events occurring: drawing a Jack and drawing a Diamond; in order to be both a Jack and a Diamond, our card must be the Jack of Diamonds, of which there is only one in the deck. We know there are 4 Jacks and 13 Diamonds in the 52 cards.\n\\[\\begin{align}\nP(\\text{Jack} \\cap \\text{Diamond}) &= P(\\text{Jack}) \\times P(\\text{Diamond}) \\\\\nP(\\text{Jack} \\cap \\text{Diamond}) &= \\frac{\\text{Number Of Jacks}}{\\text{Number Of Cards}} \\times \\frac{\\text{Number Of Diamonds}}{\\text{Number Of Cards}} \\\\\nP(\\text{Jack} \\cap \\text{Diamond}) &= \\frac{4}{52} \\times \\frac{13}{52} = \\frac{1}{52}\n\\end{align}\\]\nFinally, we have been representing joint probabilities here with the \\(\\cap\\) symbol. You may also see joint probabilities represented with commas, such as \\(P(\\text{Jack},\\text{Diamond})\\). There are no differences between the two; they mean the same thing.\n\n\n20.7.2 Conditional Probability\nWhereas marginal probabilities represent the probability of an event independent of other events and joint probabilities represent the probability of two or more events occurring together, conditional probabilities represent the probability of an event occurring given that another has already occurred. You’ll often see this relationship expressed using a statement like “the probability of A conditional upon B” or “the probability of A given B”.\nOnce again we’ll think of drawing Jack of Diamonds from a deck of 52 cards, but under slightly different circumstances. Imagine this time that someone has already drawn a card and informed us that it’s a Diamond, and we would like to know the the probability that the card in their hand is the Jack of Diamonds. We’ll assuming our friend is honest and the card they’ve removed is indeed a Diamond, which means that \\(P(Diamond) = 1\\). Now we need to update our probabilities to account for this new certainty. Since that we know we’re dealing with Diamonds only, there is only one Jack that we could have drawn. But it could only have been drawn from the pool of 13 Diamonds.\nTo represent the probability of an event, say \\(A\\), given that another event, say \\(B\\), has occurred, we use the notation \\(P(A \\mid B)\\). You can read the \\(\\mid\\) as “given;” in this case, the probability of observing a specific value for \\(A\\) given a specific value for \\(B\\) that you have already observed. Knowing these new pieces of information, we can adjust our previous probabilities to the following:\n\\[\\begin{align}\nP(\\text{Jack} \\mid \\text{Diamond}) &= \\frac{\\text{Number Of Jacks That Are Diamonds}}{\\text{Number Of Cards That Are Diamonds}} \\\\\nP(\\text{Jack} \\mid \\text{Diamond}) &= \\frac{1}{13}\n\\end{align}\\]\nWe’ve used an easy case here. Other data can be much more complicated, making the above process more complicated to puzzle through. Fortunately there is a more formal and generalizable definition we can use. We won’t discuss the proof here, but know that:\n\\[\nP(\\text{Jack} \\mid \\text{Diamond}) = \\frac{P(\\text{Jack} \\cap \\text{Diamond})}{P(\\text{Diamond})}\n\\]\nRecalling that \\(P(\\text{Jack} \\cap \\text{Diamond}) = \\frac{1}{52}\\) and \\(P(\\text{Diamond}) = \\frac{1}{4}\\) we can plug these probabilities we found earlier into this equation and we should get the same result as above.\n\\[\nP(\\text{Jack} \\mid \\text{Diamond}) = \\frac{\\frac{1}{52}}{\\frac{1}{4}} = \\frac{1}{13}\n\\]",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Probability 101</span>"
    ]
  },
  {
    "objectID": "probability.html#bayesian-inference",
    "href": "probability.html#bayesian-inference",
    "title": "20  Probability 101",
    "section": "20.8 BAYESIAN INFERENCE",
    "text": "20.8 BAYESIAN INFERENCE\nSo far, we’ve played around with the nuts and bolts of a few discrete and continuous probability distributions to better understand how they work. But all of this is, of course, just a means to an end. We want to understand how these distributions work because we want to use them to develop models! To develop models that are more interesting than the distributions we’ve considered to this point, we need to introduce one more piece of probability theory: Bayes’ theorem.\n\n20.8.1 Bayes’ Theorem\nThe term “Bayesian” is derived from the surname of Reverend Thomas Bayes – a British statistician and Presbyterian minister of the first half of the 18th century. He’s primarily famous for two things:\n\nArticulating a special-case solution for finding the probability of an unobserved random variable\nHis use of probability to describe not just frequencies, but also uncertainty in states of knowledge and belief\n\nOf the two, the latter is more distinctly ‘Bayesian’, and is largely responsible for the move to associate his surname with the Bayesian statistical paradigm. What we now call Bayes’ theorem was originally proposed by Bayes to compute what he called ‘inverse probability,’ a term that has since fallen out of favour. The modern form of the theorem is used for finding the probability of an unknown variable, \\(P(A|B)\\), given three known variables: \\(P(A)\\), \\(P(B)\\), and \\(P(B|A)\\). It has a very impressive mathematical lineage. Though initially proposed by Bayes, the modern version of the theorem we know and love owes quite a lot to the Welsh mathematician and philosopher Richard Price and the French polymath Pierre-Simon Laplace. Really, it should be probably be named the Bayes-Price-Laplace theorem, but anyway.\nIf you’re reading this book, you’ve probably encountered Bayes’ theorem at some point.\n\\[\nP(A|B) = \\frac{P(B|A)\\times P(A)}{P(B)}\n\\]\nYou can read this as “The probability of A conditional on B is equal to the probability of B conditional upon A, times the marginal probability of A, all divided by the marginal probability of B.”\nWith the theorem introduced, I have some potentially surprising news to share. There’s nothing uniquely – or even distinctly – Bayesian about using Bayes’ Theorem! Using it doesn’t make you a Bayesian. Much of what we cover in later chapters will use Bayes’ theorem in some capacity, but the same would be true if you were using Frequentist methods! Understanding Bayes’ Theorem is an important and necessary stepping stone along the path to working with a more flexible view of probability (which is a distinct feature of Bayesian analysis), but it is not a sufficient one. Not by itself, at least.\nNow that I’ve spilled “The Big Dirty Secret of Bayes’ Theorem,” the natural next step is to explain what, exactly, we need to do to make Bayes’ theorem “Bayesian.”\n\n20.8.1.1 How To Make Bayes’ Theorem Bayesian\nSimply put, the best way to make Bayes’ theorem Bayesian is to apply it to a hypothesis or a state of knowledge. In other words, we assign a probability to a hypothesis and then use Bayes’ theorem to determine the probability of that hypothesis given the data we’ve observed. Isn’t that a beautiful idea?\nIn the Bayesian paradigm, Bayes’ theorem can be applied to hypotheses and data. In other words, just as we might use Bayes’ theorem to compute \\(P(\\text{Jack} \\mid \\text{Diamond})\\), we can compute \\(P(\\text{Hypothesis} \\mid \\text{Data})\\), or expressed another way, \\(P(\\text{Hypothesis} \\mid \\text{Evidence})\\).\nIn order to introduce a little more clarity into our equations, you’ll often find a slightly different form of notation used for the hypothesis-based version of Bayes’ theorem. Below, we use the symbols \\(\\theta\\) or \\(\\text{H}\\) to represent a hypothesis. We represent data with \\(\\text{D}\\), or evidence with \\(\\text{E}\\).\n\\[\nP(\\theta|D) = \\frac{P(D|\\theta)\\times P(\\theta)}{P(D)}\n\\]\nAnother equivalent rendition:\n\\[\nP(H|E) = \\frac{P(E|H)\\times P(H)}{P(E)}\n\\]\nYou can read either of these versions of the theorem in a very similar way as the form we described earlier. In this case, one might read: “the probability of a specific hypothesis conditional upon the data/evidence is equal to the probability of that data conditioned upon the hypothesis, times the unconditional probability of the hypothesis divided by the unconditional probability of the data.” Whew! That was a mouthful. We’re going to be referring back to the first of these forms of Bayes’ theorem a whole lot (the one with \\(\\theta\\) and \\(D\\)), so it might be a good idea to jot it down in your notes or take a picture of it. To reiterate, the reason why this particular form of Bayes’ theorem can be considered ‘Bayesian’ is because we’re using a fixed interpretation of data to update probabilistically-described hypotheses.\nThis hypothesis form of Bayes Theorm has several components, and each component has a specific name that you’ll need to know if you want to be conversant in Bayesian inference and data analysis, and to think deeply and systematically about probabilistic / generative modelling. We’ll cover each of them shortly, but first, an apology: the terminology we must cover now is, for lack of a better word, tragic. Among other things it will involve drawing a distinction between two words that are near-perfect synonyms in colloquial English. The distinction between them only matters in the specialized setting we currently operate within, and the differences in their meanings are confusing and oblique. I’ll do my best to differentiate between them clearly, as the distinction is vitally important, but I’m sorry to have to ruin two perfectly good words for you. Those words are ‘Probability’ and ‘Likelihood’.\n\n\n\n20.8.2 The Components of Bayes’ Theorem:\nIn this final part of the chapter I’m going to walk through each of the components of Bayes’ Theorem. Specifically, the\n\nthe prior probability, or “priors,”\nthe likelihood, and\nthe normalizing constant\n\nTogether, these three components are used to compute something called the “posterior probability.” Everything we do is in search of understanding the posterior.\nWhen people think of the Bayesian paradigm, they generally think of two things: (i) Bayes’ theorem (which, as we’ve established, isn’t especially Bayesian), and (ii) the use of prior probabilities. While it is certainly true that Bayesian methods make extensive use of priors, they aren’t the point of Bayesian methods. Having the ability to manipulate priors can be useful, but you should think of them as the price that Bayesians pay in order to enjoy principled access to the complete distribution of posterior probabilities (which we will get to soon) as opposed to the point estimates and confidence intervals that Frequentists use. So when thinking about what makes something Bayesian, don’t focus on the priors - they’re just the cost of entry, and you can develop models that minimize their influence anyhow. Instead, focus on the posteriors. Since you can’t really make much sense of posteriors before understanding the other components, we’ll save them for last.\n\n20.8.2.1 Prior Probability, or “Priors”\nA ‘Prior Probability’ is a probability that Bayesians place on any unobserved variable. In the strictest sense, Bayesian priors are intended to serve as a quantified representation of an individual’s “state of belief” about a hypothesis under examination.\n\nFurther Reading\n“Belief” is widely-used term here, but many Bayesians (myself included) think the term is a bit misleading while still being technically accurate. It’s probably at least partly responsible for the persistent but outdated and inaccurate characterization of Bayesian models as “subjective.” A better way of thinking about priors, which I encountered via Andrew Gelman’s widely-read blog “Statistical Modeling, Causal Inference, and Social Science”, is to think of priors as “an expression of information” that is relevant to the modelling task. As far as I know, this is better represents how most statisticians and scientists who would call themselves Bayesian themselves think about the role of priors in modelling. When the word “belief” is thrown around in relation to Bayesian models, it does not refer to just any old opinion you might have, it’s a tentatively-held “belief” about what you think is going on in any given modelling context; it’s a hypothesis grounded in relevant information. While this could have a “subjective” source, it’s really a way of leveraging theory and other kinds of knowledge, such as from previous empirical research.\n\nImagine you’ve got some coins in an opaque jar. Some are fair coins, others are trick coins, weighted so that they tend to land heads up far more frequently than a fair coin (a fact that only becomes obvious once one starts flipping the trick coins). In this rather contrived scenario, you’re going to select a coin from the jar and make as good a guess as possible about the probability that the coin – when flipped – would land heads-up.\nIf you didn’t know there were some trick coins in the jar, then the best guess you could make is that any given coin has a 50% chance of landing heads up. Think of this as a hypothesis; we “believe” that there is a 50% chance of getting heads when we flip this coin.\n\\[\nP(\\theta) = 0.5\n\\]\nIf you knew about those trick coins, however, you might have a good reason to adjust your prior upwards somewhat. You’d do this to account for the slim but nonzero chance that the coin you randomly grabbed from among all the coins in your jar would produce many more heads than tails. With that additional knowledge, maybe you hypothesize that the probability of getting heads is actually 0.65, for example.\n\n\n20.8.2.2 Likelihood\nLikelihood is, in many ways, the opposite of probability. For our purposes, Likelihood describes the relative plausibility of some data if we assume a given hypothesis is true. All of the Likelihoods we’re going to consider are going to be conditional upon a hypothesis, which as a brief reminder is “the probability of thing A in light of the fact that we know thing B has already occurred.” In this case, we’re not talking about conditioning on cards that we’ve observed, we’re talking about conditioning data we’ve observed upon a hypothesis. In Bayes’ theorem, it’s this part:\n\\[\nP(D|\\theta)\n\\]\nTo briefly illustrate how likelihood operates, imagine we are testing the hypothesis that the coin we’re flipping is biased such that it produces heads 80% of the time; if we assume that’s the case, the likelihood of the coin landing heads is 0.8, and tails is 0.2.\n\\[\nP(\\text{D = Heads} | \\theta \\text{ = 0.8}) = 0.8\n\\]\nand therefore\n\\[\nP(\\text{D = Tails} | \\theta \\text{ = 0.8}) = 0.2\n\\]\nAn important thing to keep in mind here is that Likelihoods are useful in that they let us compare the plausibility of data given a hypothesis relative to the same data given other hypotheses. Likelihood is not, however, equivalent to probability. There are many implications that stem from this distinction, but one of the more salient ones is that likelihoods do not need to sum (or integrate) to 1; an individual likelihood can, in fact, be greater than 1! Even when multiplied by a prior (which is a probability), a likelihood isn’t ready to be used as a probability just yet. For that, we need to add one more piece of the puzzle.\n\n\n20.8.2.3 The Normalizing Constant\nThe normalizing constant is what converts the unstandardized ‘Bayes numerator’ (a term that refers to the product of the likelihood and the prior: \\(P(D|\\theta) \\times P(\\theta)\\)) back into a standardized probability. If you recall, all probabilities must sum to 1, and the product of the prior and the likelihood very rarely do. In Bayes’ theorem, the normalizing constant is \\(P(D)\\), or \\(P(E)\\), and is often referred to as the ‘Total Probability’.\nThe normalizing constant is interesting because it’s simultaneously the least important element in Bayes’ Theorem and the most difficult to calculate. It’s the least important because Bayes’ theorem is capable of working at nearly full power without it. \\(P(D|\\theta) \\times P(\\theta)\\) often won’t sum (or integrate) to 1, so it can’t be a probability, but it’ll be exactly the same shape as the standardized posterior. From an inferential standpoint, they’re almost identical. The normalizing constant is the most difficult to calculate because it is often unclear what the marginal probability of any given data was. What’s more: even if one knows the marginal probability of the data, determining an exact analytical solution often involves performing some truly horrific multiple integrals, some of which have no closed-form solution (read: can’t be solved exactly, and must be estimated).\nNevertheless, one the great advantages offered by the Bayesian paradigm is the ability to take the result from one model (in the form of the Posterior probability) and use it as the prior for another model (a process called Bayesian updating, which we’ll introduce later). In order to do that, we must make use of the normalizing constant.",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Probability 101</span>"
    ]
  },
  {
    "objectID": "probability.html#posterior-probability",
    "href": "probability.html#posterior-probability",
    "title": "20  Probability 101",
    "section": "20.9 POSTERIOR PROBABILITY",
    "text": "20.9 POSTERIOR PROBABILITY\nWe’ve saved the best for last. The whole point of all of this is to compute the posterior probability, which represents our “belief” in the hypothesis (\\(\\theta\\)) once we’ve considered it in light of the empirical evidence, our data. It is typically depicted like so:\n\\[\nP(\\theta|D)\n\\]\nIn the outputs of many models – including regression analysis – Frequentists will typically report two or three statistics designed to give you a rough idea of how the variables in your model are related. You’ll commonly find a coefficient, the standard error associated with that coefficient, and the significance of that coefficient in light of its standard error (the significance is typically displayed using one or more stars next to the coefficient).\nBayesians don’t skip immediately to summarizing model outputs like the Frequentists do; instead, Bayesian data analysis requires that you report the entire posterior probability of your model. In other words, we are not just interested in knowing what the “best” estimate is and a bit about how much certainty to place in that estimate. We want to know the relative plausability of every hypothesis in the form of a distribution. That’s what working with Bayes gives us.\nOnce you have the posterior probability, you can easily calculate statistics that mimic what the Frequentists report directly – it’s generally simple to calculate the mean or median value of an effect size, its variance, credible intervals (the Bayesian equivalent of confidence intervals), and so on. The important thing here is that a Bayesian has delivered the fullest and most complete answer they can once they’ve produced a posterior. Everything else is just designed to make the posterior easier to digest.",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Probability 101</span>"
    ]
  },
  {
    "objectID": "probability.html#conclusion",
    "href": "probability.html#conclusion",
    "title": "20  Probability 101",
    "section": "20.10 CONCLUSION",
    "text": "20.10 CONCLUSION\n\n20.10.1 Key Points\n\nProbability distributions are the cornerstone of most statistical analysis, and use Probability Mass Functions (for discrete distributions) or Probability Density Functions (for continuous functions) to describe how their probability is distributed\nJoint and conditional probability are useful tools for describing how the probabilities of dependent random variables are related\nBayes theorem is a foundational concept in all statistics, but is most commonly associated with the Bayesian paradigm, where it allows for the development of a posterior based on a prior, likelihood, and description of total probability",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Probability 101</span>"
    ]
  },
  {
    "objectID": "credibility.html",
    "href": "credibility.html",
    "title": "21  Credibility",
    "section": "",
    "text": "21.1 LEARNING OBJECTIVES",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Credibility</span>"
    ]
  },
  {
    "objectID": "credibility.html#learning-objectives",
    "href": "credibility.html#learning-objectives",
    "title": "21  Credibility",
    "section": "",
    "text": "Understand the basic logic of developing a regression model within the Bayesian paradigm\nDifferentiate between variables in a Bayesian model based on their “origin”\nDevelop a Bayesian model by repeatedly asking yourself “what’s that?”\nExplain how stochastic sampling methods enable us to fit Bayesian models that would otherwise be intractable\nExplain what a Markov chain is\nExplain how Metropolis-Hastings and Hamiltonian Monte Carlo allow us to efficiently explore posterior distributions",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Credibility</span>"
    ]
  },
  {
    "objectID": "credibility.html#learning-materials",
    "href": "credibility.html#learning-materials",
    "title": "21  Credibility",
    "section": "21.2 LEARNING MATERIALS",
    "text": "21.2 LEARNING MATERIALS\nYou can find the online learning materials for this chapter in doing_computational_social_science/Chapter_27. cd into the directory and launch your Jupyter Server.",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Credibility</span>"
    ]
  },
  {
    "objectID": "credibility.html#introduction",
    "href": "credibility.html#introduction",
    "title": "21  Credibility",
    "section": "21.3 INTRODUCTION",
    "text": "21.3 INTRODUCTION\nPierson Browne, one of my PhD students, was once sitting in on a “Mathematics for Statisticians” lecture at the University of Michigan when a professor of mathematics settled an in-class debate by boldly stating: “there are many, many more functions then there are formulae.” He was trying to hammer home the idea that some numerical relationships are knowable, but cannot be readily described using a single algebraic equation. This might, at first, seem like a counterintuitive claim because much of our mathematical instruction is focused on manipulating functions whose behaviour can be precisely expressed using an equation (most of which are defined for inputs along the real number line). It may come as a surprise to you that there are some functions that cannot be accurately described using an equation. Form(ula) Follows Function.\nIn the last chapter, we saw how the Bayesian paradigm uses statements of likelihood \\(P(\\text{D | }\\theta)\\) and total probability \\(P(\\text{D})\\) to condition a prior \\(P(\\theta)\\) on data, producing a posterior probability \\(P(\\theta \\text{ | D})\\). The function that describes this process, however, is not often accompanied by a well-behaved formula. Consequently, for the majority of the 20th century, the Bayesian paradigm required frequent use of daedal calculus and often produced algebraic dead-ends, all of which severely hampered the development and adoption of Bayesian methods.\nFortunately, recent advances in computational Bayesian statistics and probabilistic programming have allowed the Bayesian paradigm to largely slip free from intractable integrals by approximating the posterior. As far as this book is concerned, the two main ways of doing this are:\n\nstochastic sampling, especially with the family of Markov Chain Monte Carlo (MCMC) methods, and\nvariational inference, which approximates the posterior by using a simpler but very similar distribution as a proxy.\n\nThe primary purpose of this chapter is to demystify stochastic sampling with MCMC methods. We’ll set variational inference aside until Chapter 31.\nUnderstanding stochastic sampling with MCMC is our goal; we won’t actually start there. Instead, I’ll start by setting up a scenario that demonstrates the practical utility of MCMC methods with a detailed work-through of a hypothetical Bayesian regression model based on principles established in previous chapters. This will also help you understand how Bayesians approach regression analysis (which will be the focus of the next two chapters). Then, I’ll introduce MCMC methods with the goal of helping you develop an intuitive understanding of how they work.\nIn this chapter, I assume that you’ve been introduced to linear regression (beyond its brief appearance in Chapter 21), and more specifically, the classic Frequentist approach of Ordinary Least Squares (OLS). A typical introductory quantitative methods class in the social sciences should suffice. If OLS is entirely new to you, it’s worth taking a moment to familiarize yourself with the basic framework.",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Credibility</span>"
    ]
  },
  {
    "objectID": "credibility.html#bayesian-regression",
    "href": "credibility.html#bayesian-regression",
    "title": "21  Credibility",
    "section": "21.4 BAYESIAN REGRESSION",
    "text": "21.4 BAYESIAN REGRESSION\nOne of the models you’re going to encounter ad nauseum in the social sciences is linear regression, often called ’ordinary least squares’ (OLS). Linear regression is a workhorse in many fields, and is notable for its ease of computation and interpretation (categorical variables make a lot of intuitive sense in OLS, and do not in many other models). We’re going to use it to deepen your understanding of Bayesian Data Analysis.\nTackling linear regression from a Bayesian perspective still involves using data to condition priors and turn them into posteriors. In doing so, we’re going to use a continuous range of hypotheses about the numerical relationship between two or more variables (including exactly one ‘dependent’ variable and some number of ‘independent’ variables). As a result our “hypotheses” are going to become significantly more complex. We might ask a question like “how much does a 1,000-dollar increase in yearly salary affect a person’s life span?” This requires a numerical answer. We will consider an infinite number of such answers at the same time. That might sound impressive, but isn’t: it’s the natural consequence of using continuous variables to describe hypotheses.\nAs with other chapters in this book, my goal is to build intuition and understanding with practical examples. However, this will mean I have to hand-wave the specifics of how Bayes theorem is being used at times. The basic logic is the same, but more complex, when we generalize it to multiple variables and higher dimensions. I don’t think that it’s necessary to have a deep understanding of the maths behind generalizing the theorem to these conditions, so when we get to places where there is a precise-yet-notationally-baffling explanation for the logical leaps we’re making, I’m just going to mention that it Just Works \\(^{(TM)}\\).\n\n21.4.1 Playing the “What’s That?” Game\nWhen developing a Bayesian regression model, you can get pretty far by asking a bunch of annoying “What’s That?” questions. Unleash your inner child! I’ll show you what I mean by walking through the development of a hypothetical regression model. This is not likely to be a good model; it’s designed with pedagogical goals in mind, so there are some purposeful problems with the model.\nLet’s imagine we have a couple thousand observations about individual-level wealth around the world. Since wealth is a continuous variable (or nearly enough so that we can treat it as such), and can hypothetically take on any value on the real number line, it can be expressed as a random variable drawn from the Normal distribution. By doing this, we’re effectively hypothesizing that individual wealth is distributed following a rough ‘bell curve’, with some mean \\(\\mu\\) (the Greek letter mu, pronounced ‘mew’) and some standard deviation \\(\\sigma\\) (the Greek letter sigma). This is, of course, a very naive hypothesis (remember, model criticism in the context of Box’s loop, introduced in Chapter 8).\nWe can express the above using model notation, like so:\n\\[\n\\text{Wealth} \\sim \\text{Normal}(\\mu, \\sigma)\n\\]\nIn one line, we’ve concisely defined the relationship between our three variables, \\(\\text{Wealth}\\), \\(\\mu\\), and \\(\\sigma\\). The little squiggly line (called a tilde) separating \\(\\text{Wealth}\\) from the rest of the model notation indicates “is distributed as”. Using this notation, we’re saying that “\\(\\text{Wealth}\\) has the same distribution as a Normal Distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\)”.\nWe don’t yet have a complete model, though. For a Bayesian, you can’t just conjure a variable out of thin air, it must have an origin of some sort. You should ask: where did this variable come from? There are, broadly speaking, three different types of origin for a variable.\n\nA variable can be observed. In almost all cases, observed variables come from data. Their origin is the real world, or perhaps a simulation.\nA variable can be calculated. Its origin is a combination of other variables.\nA variable can be unobserved. Unobservered variables are often referred to as latent or hidden variables, or parameters. If we haven’t observed enough to know much about a variable, and the variable isn’t calculated by mathematically combining other variables, then we must use our brains to produce a prior distribution for it (which serves as the origin).\n\n\nThis is not the place to belabour the point, but Bayesian statistics provides a powerful framework for working with unobserved variables in a wide variety of contexts. For now, we’ll focus on regression problems and refer to ‘parameters’ since you’re already acquiring a lot of new technical vocabulary very quickly, and discussing parameters in the context of regression modelling is likely more familiar that describing regression modelling in terms of latent or hidden variables.\nThe downside of this approach is that ‘parameter’ generally implies a single value that estimated from some sort of model – a ‘point estimate.’ Whereas linear regression in the Frequentist paradigm produces point estimates with standard errors, Bayesian regression, produces a full distribution. It is possible to produce a point estimate from that distribution (which is almost always the same as what you would get from a Frequentist point estimate).\nIn later chapters, we’ll drop the language of parameters to speak more generally about “latent” and “hidden variables.” Mathematically and statistically nothing will change; what we are calling “parameters in Chapters 27-29 are the same thing as latent and hidden variables. But once you have a slightly firmer grasp on the logic of Bayesian data analysis and inference, switching up our language a bit will help you get your head around the wider world of Bayesian latent variable models. We’ll focus on drawing inferences about latent structure in social networks and latent thematic structure (topics) in large text datasets, but these two are also only a small subset of what’s possible. Once you ‘get’ the bigger picture of latent variable modelling in a Bayesian framework, you’re well on you way to developing high-quality bespoke probabilistic models for all kinds of really interesting research problems.\n\nOur model has three variables. One is observed: \\(\\text{Wealth}\\). Both \\(\\mu\\) and \\(\\sigma\\) are not calculated anywhere in our model specification, and we don’t have data on them, so – by process of elimination – they are unobserved, and we must imbue them with a prior.\nYou can probably see the value of interrogating your models with the “What’s That?” game as you construct them. Every time you write down a variable, make sure you ask yourself where it comes from. If you can’t identify a pre-existing origin, you must make one by supplying a prior. This will seem like a clunky and exhausting process at first, but it becomes second nature after a while.\nSince both \\(\\mu\\) and \\(\\sigma\\) are unobserved, we’re going to have to come up with priors for them. Since \\(\\mu\\) simply represents the middle point of our Normal distribution, we can probably come up with a sensible prior for it. If you take the total amount of wealth in the world, convert everything into USD, and divide the result by the number of humans on the planet, you get approximately 7,000. You might be tempted to update your model specification like so:\n\\[\\begin{align}\n\\text{Wealth} &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n\\mu &= 7000\n\\end{align}\\]\nWhile that might be a prior (of a sort), it’s not a very good one. In fact, it’s a very, very bad one. Among other things, it’s equivalent to saying that you are perfectly confident that the value of \\(\\mu\\) is exactly 7,000 and will never change for any reason.\nIf we want our Bayesian model to be able to update our priors to produce the posteriors, we must inject some uncertainty into them. Rather than describing \\(\\mu\\) using an integer, we’ll describe it using a full probability distribution. Since we know that \\(\\mu\\) represents the number of dollars per capita, and given that these dollars are the same unit (and thus follow the same rules) as our wealth variable, we might as well use a Normal distribution here, too. Since we’re pretty sure of our mean value, we can afford to use a comparatively small value for the standard deviation of this distribution; if we use a value of 1,000, we’re saying that about 68% of the probability will lie between 6,000 and 8,000. If you’re wondering why in the world it’s permissible to pull numbers out of a hat like this, stay tuned: we’ll cover the dark art of prior selection in more detail in the next chapter. If you’re really concerned and can’t wait, know that in most actual models with anything other than very small datasets, the evidence generally overwhelms the priors, and they have little effect.\nWe’re going to have to go through the same process for \\(\\sigma\\) as we did for \\(\\mu\\). The standard deviation parameter in a normal distribution is a continuous variable that can take on any value from 0 to positive infinity. That means that we should be careful to assign a prior that can’t produce negative values. There are many good candidates, but we’ll use the exponential distribution, which covers the same domain (from 0 to positive infinity). The exponential distribution takes only one parameter - \\(\\beta\\). For simplicty’s sake, let’s assign a large value for \\(\\beta\\), which will help encode our lack of prior knowledge about the variability of wealth. When we put it all together, our model looks like this:\n\\[\\begin{align}\n\\text{Wealth} &\\sim \\text{Normal}(\\mu, \\sigma)  &\\text{[Likelihood]}\\\\  \n\\mu &\\sim \\text{Normal}(7000, 1000) &[\\mu \\text{  Prior]}\\\\\n\\sigma &\\sim \\text{Exponential}(4000) &[\\sigma \\text{  Prior]}\n\\end{align}\\]\nAt this point, we have a complete model. You can play the “What’s That?” game on any portion of it, and another part of the model definition will give you an answer. However, the model isn’t very informative at this point. All we’ve done is lump all of our data into one big bin and described the shape of that bin by specifying where the middle is and how wide it is. If we had actual data, we could produce posterior probabilities for each of our priors and see how close our initial guesses were to the final answers (hint: they’d probably be way, way off). For now, let’s focus on two specific limitations with what we’ve done:\n\nThe model isn’t even remotely interesting or informative.\nIt isn’t yet a linear model. (For it to be a linear model, we’d need to have an independent variable upon which wealth depends.)\n\nThese two problems are related, and we’ll attempt to solve them both in the subsequent section.\n\n\n21.4.2 Introducing a Predictor\nIn order to turn our normal model into a linear model, we’re going to need to introduce another variable. Let’s say you’ve been reading some international development and globalization research and learn there is a correlation between the absolute value of latitude and wealth per capita (after the Industrial Revolution). Whether you go North or South, per capita wealth is higher the further you get from the equator. How strong is this relationship? Maybe you want to know, for example, how much of a difference a 10-degree shift of latitude has on wealth. To show how we’d go about modelling this, let’s rebuild our model, starting from the likelihood:\n\\[\\begin{align}\n\\text{Wealth}_i &\\sim \\text{Normal}(\\mu_i, \\sigma) &\\text{[Likelihood]}\\\\  \n\\end{align}\\]\nThat looks almost exactly the same as the Normal model we specified before! The only difference is that there are now subscripted ’i’s after \\(\\text{Wealth}\\) and \\(\\mu\\) - what gives?”\nThe subscripted ‘i’ is a powerful clue. It means that rather than trying to find a single value for \\(\\mu\\) that applies to the entire dataset (which, in effect, gives us overall average wealth), we’re going to be producing a different value of \\(\\mu\\) for each observation of \\(\\text{Wealth}\\) in our dataset. Pay attention to subscripts (like ‘i’ or ‘j’) going forward: their appearance in some part of the model indicates that we’re going to be allowing that part of the model to take on many different values – usually, one value for each observation in the data.\nIn this case, rather than plunking a normal distribution somewhere along the real number line and trying to configure it to best account for all of the data we have, we’re going to let it move about. Every time we calculate a \\(\\mu\\) value for one of the observations in the data, we’ll plug it in as a parameter in our Normal distribution, which will cause the distribution to scoot around the real number line in an attempt to get as close as possible to the observed data.\nIf we’re serious about allowing our likelihood distribution to move, we can’t put a prior directly on \\(\\mu\\). Instead, we’re going to re-cast \\(\\mu\\) as a statistic, and calculate it as a combination of other variables. This is where our linear model comes in!\n\\[\\begin{align}\n\\text{Wealth}_i &\\sim \\text{Normal}(\\mu_i, \\sigma)  &\\text{[Likelihood]}\\\\\n\\mu_i &= \\alpha + (\\beta \\times \\text{Latitude}_i)  &\\text{[Linear Model]}\\\\\n\\end{align}\\]\nNote the new line uses \\(=\\) rather than \\(\\sim\\). This indicates that the calculation of \\(\\mu\\) is now based on a deterministic combination of its constituent parts. This line is called the ‘linear model,’ and it’s how we tell our Bayesian model that we want to use a line to approximate the relationship between latitude and wealth. If you squint and blur your eyes a bit, you might even begin to recognize similarities between the linear model and the equation for a straight line:\n\\[\ny = mx + b\n\\]\nWhere \\(m\\) is the slope of the line and \\(b\\) is the intercept. We’re doing the exact same thing here, except rearranging things, using \\(\\alpha\\) instead of \\(b\\), and using \\(\\beta\\) instead of \\(m\\). It’s a simple model, but simplicity is often a virtue in statistics. All we have to do to complete it is play the ‘What’s That?’ game until we’ve covered all of our bases. Let’s start from the top:\n\nWe already know that Wealth is observed, and so it doesn’t need to appear anywhere else in the model.\nWe know that \\(\\mu_i\\) is unobserved, but unlike the previous model we made, it is now calculated from other variables in the model. As such, it doesn’t need a prior – we already have a line telling us where it comes from. That line is a linear model.\nNo such luck with \\(\\sigma\\); we’re going to need a prior just like before.\nSimilarly, we do not have information about \\(\\alpha\\) or \\(\\beta\\), and so they’re both going to need priors.\nLatitude is observed, so we can leave it as-is.\n\nConsider what these terms might mean in the model, and then to try and extrapolate some sensible priors. Pay attention to what values the parameters can take. Recall, that you can’t have a negative standard error, and so it’s vitally important that you assign a prior to \\(\\sigma\\) that can’t take on any negative values. Conversely, it’s important to make sure that you don’t artificially limit what values a variable can take. If you assign a probability of 0 to a value, you’ve made that particular value impossible; from that point onward, it will never receive any probability from the model. If you ever assign any value a probability of 0, make sure that you’ve got a really good reason for doing so (a model predicting wealth using age probably shouldn’t allow negative ages). If you think a particular value is unlikely but still theoretically possible, then it’s far safer to use a distribution that will place a vanishingly small but still non-0 probability on those unlikely values.\nPrior specification is a complex debate. Don’t worry about about it for now; until you’re comfortable with Bayesian analysis, your focus should be making sure that you don’t unintentionally make the impossible possible, or vice versa. When you have lots of data and a simple model, the exact form of your priors won’t matter because they’ll get overwhelmed by the evidence! Even horrifically mis-specified priors will be “washed out” and have next-to-no impact on inference.\nWhen you’ve thought this through a bit, feel free to take a look at what I’ve selected. Got any criticisms? Good. That’s a vital part of the process. Write them down.\n\\[\\begin{align}\n\\text{Wealth}_i &\\sim \\text{Normal}(\\mu_i, \\sigma) &\\text{[Likelihood]}\\\\  \n\\mu_i &= \\alpha + (\\beta \\times \\text{Latitude}_i) &\\text{[Linear Model]}\\\\\n\\alpha &\\sim \\text{Normal}(4000, 2000)             &[\\alpha\\text{ Prior]}\\\\\n\\beta &\\sim \\text{Normal}(1000, 500)               &[\\beta\\text{ Prior]}\\\\\n\\sigma &\\sim \\text{Exponential}(1000)              &[\\sigma\\text{ Prior]} \\\\\n\\end{align}\\]\nAnd now, for the anticlimax: we don’t have any data for this model, so we can’t produce a posterior distribution. A shame, I know, but that wasn’t the point. The point was to work through the process of developing a rudimentary Bayesian regression model using only hypotheticals to keep your focus as much as possible on the structure and logic of these regressions, including the use of a few priors that stretch credibility in order to emphasize the importance of criticism in model development. This is a theme we will return to often.\nNow that you’ve built a bivariate linear regression, you can easily extrapolate what you’ve learned to add more variables to your model. Suppose we wanted to add another variable to the model we just finished specifying. We could do so by simply adding another term to the linear model equation and creating another prior for the coefficient!\n\\[\\begin{align}\n\\text{Wealth}_i &\\sim \\text{Normal}(\\mu_i, \\sigma)  &\\text{[Likelihood]}\\\\                                              \n\\mu_i &= \\alpha + (\\beta_1 \\times \\text{Latitude}_i) + (\\beta_2 \\times \\text{NewVariable}_i) &\\text{[Linear Model]}\\\\\n\\alpha &\\sim \\text{Normal}(4000, 2000) &[\\alpha\\text{ Prior]}\\\\\n\\beta_1 &\\sim \\text{Normal}(1000, 500) &[\\beta_1\\text{ Prior]}\\\\\n\\beta_2 &\\sim \\text{Normal}(-150, 100) &[\\beta_2\\text{ Prior]}\\\\\n\\sigma &\\sim \\text{Exponential}(1000) &[\\sigma\\text{ Prior]} \\\\\n\\end{align}\\]\nRather take a deep dive into the mathematics of Bayesian inference we’re going to skip to the cutting edge of Bayesian analysis and discuss the first of two computational approaches to approximating the posterior: stochastic sampling. Together with variational inference (introduced in Chapter 30), stochastic sampling has played a major role in the meteoric rise of Bayesian methods.",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Credibility</span>"
    ]
  },
  {
    "objectID": "credibility.html#stochastic-sampling-methods",
    "href": "credibility.html#stochastic-sampling-methods",
    "title": "21  Credibility",
    "section": "21.5 STOCHASTIC SAMPLING METHODS",
    "text": "21.5 STOCHASTIC SAMPLING METHODS\nThroughout the next few chapters, we’re going to be making frequent use of stochastic sampling methods to produce posterior distributions for a variety of Bayesian models. Stochastic sampling methods represent the cutting-edge of a remarkably adaptable approach to fitting otherwise difficult or impossible models. What they are not is a one-size-fits-all panacea. Unlike many other approaches, we can’t simply feed our data and model specification into a sampler and reliably get an intelligible answer. You’re going to have to know:\n\nhow your sampler works,\nhow to read and interpret the output it produces, and, most importantly,\nhow to help a sampler that’s fallen sick.\n\nFortunately, you don’t need a rigorous understanding of the underlying math in order to become a pretty good sampler medic; you will, however, need a strong intuitive understanding of how they work. In what follows, and over the course of the next two chapters, my goal is to help you build that essential intuition. Rather than wasting your time starting from first principles and working our way up to something interesting, I’m going to briefly introduce an especially important concept, Markov Chains. Then we’ll dive straight into the deep end of a grotesquely extended metaphor. We’ll get into the details of diagnosing and fixing problems with samplers in the chapters to come. Let’s begin.\n\n21.5.0.1 Markov Chains\nAt the root of everything we’re going to cover in this section is the Markov Chain. Named after Russian mathematician Andrey Markov, a Markov Chain is a simple machine that transitions from one state to another based on some pre-defined set of inter-state probabilities. Markov Chain models are ‘memoryless,’ which is a fancy way of saying that when they decide to switch states, they do so using information about the current state of the machine and nothing else. Figure 23.1 is a model that describes (pretty accurately) how my two cats, Dorothy and Lando Catrissian, spend their days.\n\n\n\n\n\n\nFigure 21.1\n\n\n\nAll we have to do is choose an initial state and some kind of looping time interval which governs when we check for a state transition. Let’s say we start on the ‘Play’ node, jumping in boxes and pawing at strings. Every 5 minutes, we’ll check to see if we transition to a different node. No matter which node we’re on, there’s a non-zero chance that we’ll end up on any of the nodes (including the one we’re currently on). From the Play node, there’s a 60% chance that we’ll stay exactly where we are, a 20% chance that we’ll end up on the ‘Nap’ node, and a 20% chance of wandering over to the food bowl for a snack.\nimport numpy as np\nnp.random.seed(3)\nnp.random.choice(['Play', 'Snack', 'Nap'], p=[0.6, 0.2, 0.2])\nThe choice is ‘Play,’ so we’ll keep batting at strings. That was the most probable outcome (60% chance). After a further 5 minutes of wondering what you have to do to get a human to break out a laser pointer, we’ll run the check once more:\nnp.random.seed(4)\nnp.random.choice(\n    ['Play', 'Snack', 'Nap'], \n    p=[0.6, 0.2, 0.2]\n)\nNap time! While on the Nap node, we’re very likely to stay where we are: a 70% chance. Of the remaining probability, there’s a 20% probability of getting up for another snack and a 10% chance of more play. Let’s see what happens;\nnp.random.seed(5)\nnp.random.choice(['Play', 'Snack', 'Nap'], p=[0.1, 0.2, 0.7])\nSleeping is hard work! Time to reward all that effort with a well-deserved snack.\nAt this point, the pattern should be pretty clear: a Markov Chain switches between some set of pre-defined states according to a set of probabilities that can be different for each of the nodes in the model. Crucially, Markov Chain models converge, over long periods of time, to a calculable equilibrium state. This feature will come in handy in just a moment…\n\n\n21.5.1 Markov Chain Monte Carlo\nAs it happens, we can fruitfully apply Markov Chains to probability distributions by replacing the bespoke probabilities we used in the previous section (Nap, Snack, Play) with probabilities computed on-the-fly based on a distribution. This is known as Markov chain Monte Carlo (MCMC), and is useful because it – much like the simpler Markov Chains we already covered – converges with the probability distribution it is being asked to traverse. Very useful!\nThe issue with MCMC is that – while simple and elegant in theory – implementing them in practical settings involves a number of trade-offs. This has caused a plethora of specific implementations to emerge: one workhorse from among this stable is the “Metropolis-Hastings Algorithm”, which uses a proposal distribution to quasi-randomly walk around the parameter space. Instead of transitioning between abstract ‘states’, as in the case of the pure Markov Chain above, we can imagine Metropolis-Hastings stepping between different parameter values. Let’s say that we think a certain parameter in a model can only take on one of 5 discrete ordinal values (1 through 5), each of which might be more or less plausible. Metropolis-Hastings chooses a random parameter value to start with and then – for a predetermined number of iterations – starts stepping from value to value according to the following logic:\n\nRandomly select an adjacent parameter value that’s 1 higher or lower than the current parameter value. We’ll call it the ‘proposal’. If the proposal is outside the range of values (e.g., 0 or 6), wrap around to the other side of the value range.\nCalculate the probability at the proposal, and compare it to the probability of the current parameter value. If the proposal has a higher probability, move to it immediately and return to step 1. Otherwise, move to step 3.\nSince the proposal’s probability is equal to or lower than the current node’s, randomly choose from between the two with a probability proportional to the difference between them. (For example, if the proposal has half the probability of the current value, then there’s a 1/3 chance that the algorithm will move to the proposal, and a 2/3 chance it will stay where it is).\n\nThat’s it! Collectively, these rules ensure that the Metropolis-Hastings algorithm will trend towards the parameter values with the highest posterior probability, but won’t entirely ignore the ones with lower probability.\nDespite the Metropolis-Hastings algorithm not knowing about the shape of the distributions it is tasked with exploring, its stochastic meandering will eventually cause it to visit every portion of a probability distribution in proportion to the probability density at that location. Thinking back to the hypothetical Bayesian model we created in the first half of this chapter, using a sampling method like Metropolis-Hastings would allow us to create reliable estimates of the posterior distributions for all of our unobserved parameters (\\(\\alpha\\), \\(\\beta\\), and \\(\\sigma\\)), provided we had data to feed into our linear model/likelihood (which we don’t).\nMetropolis-Hastings “Just Works\\(^{(TM)}\\),” but sometimes it doesn’t work quickly or efficiently enough for our purposes. It’s not enough to employ an algorithm that will eventually provide us with a satisfactory approximation; we want to find one that will do so efficiently and in a reasonable amount of time, even when the shape of the posterior is irregular.\nRather than skipping straight to the answer, we’re going to take a diversion into the realm of an extended thought experiment that will – with luck – provide you with an intuition for how one might go about efficiently exploring convoluted continuous parameter spaces. It’s a bit of a weird thought experiment, but learning about stochastic sampling for the first time is a bit mind-bending anyway, so let’s just have a bit of fun, shall we?\n\n\n21.5.2 Mapping a Skate Bowl\nImagine we’ve made a bet with a friend that we can create a topographical map of the bottom of a skate bowl. The bowl is highly irregular in shape and depth, with several local minima scattered around, and there’s no easy way to mathematically describe it. This poses a bit of a challenge already, but the real challenge is that the rules of the bet prevent us from ever seeing the skate park! All we know in advance is that there are several low areas scattered throughout the skate bowl (relative to their steeply sloping surroundings), and that our friend is more interested in the lower areas of the bowl than the steeply sloping sides of the bowl. They’re completely uninterested in the completely flat, high area surrounding the bowl. The 3D plots in Figure 23.2 offer three perspectives that are similar, but not the same, as the shape of the skate bowl we’re trying to map in this example.\n\n\n\n\n\n\nFigure 21.2\n\n\n\nThis is already kind of a weird example, so let’s just lean into the weird. Our friend has provided two unusual tools to help us: a frictionless, perfectly elastic marble which, once set into motion, will stop – dead – after a configurable length of time has elapsed. This marble is a marvel of modern engineering (and may potentially break several laws of physics), as it is capable of coming to an immediate and complete standstill whilst halfway up a slope that any other round object would immediately begin to roll down. What’s more, the marble, once it has come to a complete stop, will send you an unnervingly accurate three-dimensional readout of its current position.\nThe other tool is a robot. We’re allowed to program the robot to traverse the skate park, find the physics-defying marble, and move the marble either by picking it up and putting it down elsewhere, or by imparting some kind of force onto the marble. The robot is always aware (relative to its current position) of where the marble is, where the marble last came to a stop, and where the skate park’s walls are – it is otherwise incapable of perceiving anything about its surroundings.\nOur objective is to use the robot and the marble to ‘map’ the contours of the skate bowl (but none of the surrounding area) as efficiently as possible. How might we approach such a task? Let’s think it through.\n\n21.5.2.1 Gradient Descent\nThose of you who saw the ‘efficiently as possible’ qualifier above might have started thinking something akin to: “why not just instruct the robot to repeatedly roll the marble over very small intervals until it descends into the skate bowl, and keep going until it reaches the bottom? We could use the resulting data as an approximation of best fit!” That would be very similar to the “Gradient Descent” approach discussed in Chapter 23.\nWhile this technique certainly gets top marks for ease of implementation, our friend wouldn’t be sufficiently impressed to concede the bet. For starters, short of conducting several such throws, we’d have no way of knowing whether or not the marble had ended up in a ‘local minima,’ i.e., one of the smaller sub-bowls in the diagram above that are quite a bit shallower than a nearby ‘global minima,’ which is the actual lowest point in the skate bowl. What’s more, recall that to win the bet our friend expects us to describe low points throughout the entire bowl, not just an approximation of the single lowest point.\n\n\n21.5.2.2 Quadratic Approximation\nSince having a single point isn’t good enough, we could use the data gathered as our marble slowly descended into the bowl (remember, it stopped frequently on the way down) to estimate the curve it followed as it descended?” If you were thinking along these lines, it might be fair to say that you had hoped to employ a ‘Quadratic Approximation’ which involves using a analytically-defined ‘good-enough’ parabolic curve to describe the shape of the bowl.\nSince many statistical models make extensive use of the Normal distribution, and given that the Normal distribution can be fairly well-approximated using a parabola, Quadratic Approximation is commonly called upon to help provide useful approximations of posterior distributions in simple (and a few not-so-simple) Bayesian models. Unfortunately, based on the description of the bowl our friend provided us with (and the simulation of one possible bowl above), the skate bowl is not symmetric, has multiple ‘lowest points’ (multiple local minima), and undulates (not monotonic). Under such conditions, there’s no easy way to produce an accurate quadratic approximation: the best-fitting curve will look nothing like the actual bowl.\n\n\n21.5.2.3 Grid Approximation\nYou may now be thinking “okay, the quick-and-easy approach is out, so how about we double down on accuracy and try to systematically cover every inch of the skate park?” This is a method akin to ‘Grid Approximation’ or ‘Grid Search’, wherein we would systematically cover every part of the skate bowl by breaking the entire skate park into a regularly-spaced grid, and then taking a sample at each intersection in that grid.\nUsing this approach, you’d be guaranteed to map the entire skate bowl. The problem here, though, is that you’re going to spend a whole lot of time – a WHOLE lot – exploring areas of the skate bowl that aren’t of any interest. Let’s say the park is 100 metres by 100 metres. Even if you only take one measurement every two metres, you’re going to have to take 2,500 measurements to cover the entire park. If you double the resolution of your search to take one measurement every metre, the number of measurements balloons to 10,000. Further increases in resolution will result exponentially larger numbers of required measurements.\nIf we were immortal, fine, grid search can be usefully applied to complex, continuous spaces. If, however, you want to settle this bet sometime between now and the eventual heat death of the universe, you’re going to have to find a faster way.\n\n\n21.5.2.4 Randomly Whack The Marble Around\nThose of you with a keen sense of irony may have seen something like this coming: rather than employing sophisticated mathematical approximations of our skate bowl, our best option overall involves instructing our robot to give the marble a good thump in a random direction with a random force, wait until it stops (after a fixed period of time), and then repeat the process from the marble’s new location. This “Randomly Whack The Marble Around” approach is known as ‘Hamiltonian Monte Carlo’ (HMC). The unusual thing about it is that – with the exception of a few edge cases – it is a reasonable, reliable, and comparatively efficient method for exploring the shape of a distribution, even if the distribution is very complex or has many different dimensions.\nProviding any form of rigorous proof - mathematical or otherwise - of the effectiveness of Hamiltonian Monte Carlo is FAR beyond the scope of this book. You’ll have to take it for granted that this method Just Works\\(^{(TM)}\\). You can get a good look under the hood with some of the recommended sources at the end of this chapter.\n\nBox. If you want to learn more about HMC, and it’s use in regression analysis, I recommend McElreath’s (2020) classic Bayesian statistics textbook Rethinking Statistics. Note, however, that you’ll want to build up more of a foundation before jumping into that book, or others like it. Lambert (2018) and Kruschke (2014) are also excellent introductions to Bayesian statistics in the social and cognitive sciences that include discussions of various approaches to approximate and exact Bayesian inference.\n\n\n\n21.5.2.5 Go See The Marbles Move\nI’ve tried to make everything we’ve just covered as concrete and easy-to-picture as possible, but obviously all of this remains very abstract. This material can be incredibly difficult to grasp, especially if you’re encountering it for the first time, and dually so when we try to extend the intuitions we’ve built in 3 dimensions to higher number of dimensions. It is, sadly, impossible to imagine a marble rolling around in a 16-dimensional skate bowl.\nIt might be helpful to view an animated representation of what’s happening. Since you’re most likely reading this textbook on paper, I recommend reading Richard McElreath’s blog post “Build a Better Markov Chain” https://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/. You can spend a bit of time observing and playing around with a few animated stochastic samplers to deepen your understanding.\nIn particular, we’d like to draw your attention to the section on the No-U-Turn-Sampler, or NUTS for short. NUTS is a sort of special case of HMC, wherein the marble is capable of intelligently detecting when it has pulled a U-turn and is headed back towards its starting location.\nWhen you’re all done watching the imaginary marbles zip around the imaginary skate bowls, we can move on to specifying some models in the next chapter.",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Credibility</span>"
    ]
  },
  {
    "objectID": "credibility.html#conclusion",
    "href": "credibility.html#conclusion",
    "title": "21  Credibility",
    "section": "21.6 CONCLUSION",
    "text": "21.6 CONCLUSION\n\n21.6.1 Key Points\n\nIn this chapter, we developed an intuitive understanding of how Bayesian regression models are specified using a set of mathematical conventions\nIntroduced the concept of Markov Chains and some of the sampling techniques based thereon\nUsed an extended metaphor to develop an intuitive understanding of how the Hamiltonian Monte Carlo sampling algorithm works\n\n\n\n\n\nKruschke, John. 2014. “Doing Bayesian Data Analysis: A Tutorial with r, JAGS, and Stan.”\n\n\nLambert, Ben. 2018. A Student’s Guide to Bayesian Statistics. Sage.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. CRC press.",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Credibility</span>"
    ]
  },
  {
    "objectID": "measurement-and-missingness.html",
    "href": "measurement-and-missingness.html",
    "title": "22  Measurement and missingness",
    "section": "",
    "text": "Coming in fall 2024…",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Measurement and missingness</span>"
    ]
  },
  {
    "objectID": "linear-regression.html",
    "href": "linear-regression.html",
    "title": "23  Bayesian Regression Models with Probabilistic Programming",
    "section": "",
    "text": "23.1 LEARNING OBJECTIVES",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Bayesian Regression Models with Probabilistic Programming</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#learning-objectives",
    "href": "linear-regression.html#learning-objectives",
    "title": "23  Bayesian Regression Models with Probabilistic Programming",
    "section": "",
    "text": "Specify a Bayesian linear regression model with PyMC\nUnderstand the logic of using Python’s context management to develop models with PyMC\nUse PyMC to conduct a prior predictive check to ensure that our model is not overly influenced by our priors\nRead a traceplot to assess the quality of a stochastic sampler\nAssess and interpret models by\n\nConstructing and interpreting credible intervals using the Highest Density Interval method\nConducting Posterior Predictive Checks\nPlotting uncertainty",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Bayesian Regression Models with Probabilistic Programming</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#learning-materials",
    "href": "linear-regression.html#learning-materials",
    "title": "23  Bayesian Regression Models with Probabilistic Programming",
    "section": "23.2 LEARNING MATERIALS",
    "text": "23.2 LEARNING MATERIALS\nYou can find the online learning materials for this chapter in doing_computational_social_science/Chapter_28. cd into the directory and launch your Jupyter Server.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Bayesian Regression Models with Probabilistic Programming</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#introduction",
    "href": "linear-regression.html#introduction",
    "title": "23  Bayesian Regression Models with Probabilistic Programming",
    "section": "23.3 INTRODUCTION",
    "text": "23.3 INTRODUCTION\nIn this chapter, we’ll actually develop some Bayesian regression models. We will slowly develop a simple linear model, explaining the ins and outs of the process using a package for probabilistic programming called PyMC. Then we’ll criticize the model we’ve built and use those critiques to build a much better model in the next chapter.\nOur example here, and in the next chapter, will be the influence of money on voting outcomes by state in the 2020 American General Election. Given that we would like data that is regionally representative and as numerous as possible, we’re going to focus on the electoral contests that took place across America’s 435 congressional districts.\nIt’s almost a truism to state that money wins elections. In light of this fact, one of the most critical decisions a political party can make is where and how to allocate their funds. It’s far from an easy problem to solve: every dollar spent on a race where the result is a foregone conclusion represents a dollar that might have helped shift the result in a more tightly-contested district. In the US, both the Democratic and Republican parties are perpetually attempting to outdo each other by allocating their limited resources more efficiently, but their task is an asymmetric one: Republicans might, for instance, get better returns (measured in votes) on their investment in Alabama than Democrats would in the same state for the same amount. Of course, given that Alabama swings so heavily Republican, it might be a mistake for any party to invest funds there, given that the races in most of Alabama’s districts were probably over before they began. Let’s see what we can learn.\n\n23.3.1 Imports\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport pymc as pm\nimport arviz as az\n\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\n\nfrom dcss.bayes import plot_2020_election_diff, plot_2020_election_fit\n\nfrom dcss import set_style, download_dataset\nset_style()\n\n\n23.3.2 Data\nThe data we will use for this chapter is stored in a CSV called 2020_election/2020_districts_combined.csv. Rather than take you through the entire process of cleaning and pre-processing the data, we’ve done it for you this time; it’s ready to go! It’s worth noting, however, that the cleaning and pre-processing steps we’ve taken for this data (and the models we’re going to fit in this chapter) are very similar to those that we’ve taken in previous chapters.\nus_election_2020_data_url = \"https://www.dropbox.com/scl/fo/gcotab57xtv9a0ga5vums/ANB2gm71cIXW1NcLwA5ezXY?rlkey=nai1uun6mkl10a66ekzs692ux&st=rmbsufjx&dl=0\"\n\ndownload_dataset(\n    us_election_2020_data_url, 'data/2020_election/'\n)\ndf = pd.read_csv('data/2020_election/2020_districts_combined.csv')\ndf.head()\nIn this chapter, we’re only going to be utilizing a small subset of the available variables: going forward, I’m going to restrict my discussion to only those that are pertinent to this chapter (the rest will come into play in the subsequent chapter).\n\n\n23.3.3 Checking and Cleaning the Data\nWe’ll start by summarizing the variables we intend to use. Doing so helps us get a sense of what those variables look like, where on the number line they lie, and how they might best be modelled. We can do this with using Panda’s .describe() method.\npd.options.display.float_format = \"{:.2f}\".format\n\ndf[['vote', 'spend', 'districts']].describe()\nThe state and districts variables are straightforward: they represent the state and numerical identifier associated with the congressional district in question. The vote and spend columns are a little more involved. For the past 29 years, American federal elections have been an almost completely two-party affair. Almost all viable candidates at almost every level of government belong to either the Democratic or Republican parties. There are some notable exceptions (such as the technically independent senators Bernie Sanders and Angus King), but almost all politically viable independent politicians in the US are Democrats in all but name (they often even receive the official endorsement of the Democratic party, and are not opposed by any member). Given the ubiquity of this political duopoly, we can simplify our data by focusing solely on the differential in votes and spending between the two major parties.\nWe’ve decided to treat Republicans as our ‘negative’ case and the Democrats as our ‘positive’ case. Casting the two in diametric opposition allows the vote and spend variables to represent the differential between the two parties: when vote is positive, it means the Democrats received more votes than the Republicans. A negative vote value means the Republicans received more votes than the Democrats. Ditto for spend.\nAlthough this helps us simplify our model immensely, it also comes at a cost: we can only include districts where both Democrats and Republicans officially ran, spent campaign funds, and received votes. This limitation has reduced our data from 435 districts to 371; a steep cost, but not an altogether unwarranted one. More advanced models could incorporate and model the dropped data, but we’re keeping it simple.\nNow that the data is loaded, let’s create a scatterplot so we can see how it is distributed (Figure 23.1).\nplot_2020_election_diff(df)\nplt.savefig('figures/27_01.png', dpi=300)\n\n\n\n\n\n\nFigure 23.1: png\n\n\n\nIn the above scatterplot, each point represents a single congressional district in one of the 50 states. The x-axis represents the Democrats’ ‘spending differential’, which is just the amount of money the Democrats spent in a congressional race minus the amount the Republicans spent in the same. The y-axis, ‘vote differential’, is similar: it represents the amount of votes the Democrats received minus the amount the Republicans received.\nI’ve broken the plot into four quadrants and labelled them. The upper-left quadrant represents the best-case scenario for the Democrats: districts here were won by Democratic candidates despite the fact that the Republicans spent more money on the race. The lower-right is the inverse; it represents the worst-case scenario for the Democrats, wherein they outspent the Republicans yet still lost. You might notice that comparatively few districts fall into these two quadrants: this might imply that both parties are fairly adept at avoiding overspending in districts where they’re unsure of victory.\nThe final two quadrants, upper-right and lower-left, contain the districts where the winning party spent more money than their opponents did (which, for the most part, is what we’d expect). Now let’s prepare the model for inclusion in our model.\n\n23.3.3.1 Standardize Data, Process Categoricals\nGenerally speaking, it’s a good idea to standardize any non-categorical data you plan to use in a modelling context. We do this by first shifting the numerical value so that its mean is 0. Then, we divide each observation by the standard deviation of the data, which converts the variable into a value whose units are ‘standard deviations’, or z-scores. We’re also going to tackle our non-numerical categorical variable, state, which is currently a list of strings (the districts variable is also categorical, but it’s already numerical and is thus good-to-go as-is). We’re going to use Pandas to convert state into an explicitly categorical object, extract numerical codes from it, and then use those codes to determine how many different states we’re working with (remember, some may have been dropped when we cleansed our data of ~60 districts). The code cell below accomplishes all this; there are more efficient ways to accomplish our task, and we’ve even covered some of them elsewhere in the book. Nevertheless, we’re going to do them manually here to help give you a better sense of what’s going on.\nspend_std = (df.spend - np.mean(df.spend)) / np.std(df.spend)\nvote_std = (df.vote - np.mean(df.vote))/ np.std(df.vote)\nstate_cat = pd.Categorical(df.state)\nstate_idx = state_cat.codes\nn_states = len(set(state_idx))",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Bayesian Regression Models with Probabilistic Programming</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#developing-our-bayesian-model",
    "href": "linear-regression.html#developing-our-bayesian-model",
    "title": "23  Bayesian Regression Models with Probabilistic Programming",
    "section": "23.4 DEVELOPING OUR BAYESIAN MODEL",
    "text": "23.4 DEVELOPING OUR BAYESIAN MODEL\nUsing the modelling language we established in the previous chapter, let’s create a model that uses spending differential to predict vote differential in congressional districts:\n\\[\\begin{align}\n\\text{vote}_i &\\sim \\text{Normal}(\\mu_i, \\sigma)\\\\\n\\mu_i &= \\alpha + (\\beta \\cdot \\text{spend}_i)  \n\\end{align}\\]\nBased on the hypothetical model we developed last chapter, this format should look familiar: the top line is our likelihood, and the linear model on the second line determines where the mean of the likelihood function falls. Now that we have our likelihood and linear model specified, we can play the “What’s That?” game, which will see us through to the creation of a fully-specified model. Let’s look at our model definition again; we’ll start with the data, which are the variables whose values we have observations of. They are:\n\n\\(\\text{vote}_i\\)\n\\(\\text{spend}_i\\)\n\nWe have real, actual numerical values for both of the above, so we don’t need to do any guessing about them. Next, let’s turn our gaze to the statistics - the variables whose values are (at least in part) derived from other variables:\n\n\\(\\mu_i\\) - mean parameter for likelihood function\n\\(\\alpha\\) - the intercept\n\\(\\beta\\) - coefficient for spend\n\\(\\sigma\\) - standard deviation parameter for likelihood function\n\nSince we don’t have any strong reasons to think that any of those variables should take on any particular values, we can use uninformative priors for each. We have a large amount of data to work with, so as long as our priors are not unduly mis-specified, they will likely be overwhelmed by the weight of evidence and have no noticeable impact on our posterior distributions. Here’s what I’ve elected to use (feel free to play around with different priors at your leisure). The text on the right (Likelihood, Linear Model, etc.) is not necessary, but it’s a nice reminder of what each line in the model represents.\n\\[\\begin{align}\n\\text{vote}_i &\\sim \\text{Normal}(\\mu_i, \\sigma)& \\text{[Likelihood]}  \\\\\n\\mu_i &= \\alpha + (\\beta \\cdot \\text{spend}_i)  & \\text{[Linear Model]} \\\\\n\\alpha &\\sim \\text{Normal}(0, 2)                & \\text{[alpha Prior]} \\\\\n\\beta  &\\sim \\text{Normal}(1, 2)                & \\text{[beta Prior]} \\\\\n\\sigma &\\sim \\text{Exponential}(2)              & \\text{[sigma Prior]} \\\\\n\\end{align}\\]\n\n23.4.1 Making the Model with PyMC\nSince we’ve already discussed how and why to use stochastic samples to approximate the posterior distribution in a Bayesian model, we’ll go straight into using stochastic samplers using a package called PyMC. PyMC is designed to facilitate the specification, fitting, and simulation of Bayesian models, and it includes state-of-the-art stochastic samplers. While far more sophisticated than anything we’ve described in this book thus far, PyMC is conceptually similar to – and based upon – the Markov Chain and related techniques covered in the previous chapter.\nPyMC is expansive and constantly evolving – any attempt to capture even a modest percentage of its contents would be futile. As with other packages discussed in this book, you will likely use a very small portion of it extensively, and the rest much more rarely. I encourage you to avail yourselves of PyMC’s extensive documentation and helpful tutorials. For now, we will focus on what you need to build your own Bayesian regression from scratch.\nBefore we actually make the model, we have to introduce a bit of Python programming knowledge that we’ve used before but have not actually explained: context management.\n\nFurther Reading\nSalvatier, Wiecki, and Fonnesbeck (2016) provide a detailed introduction to PyMC, and Martin (2018) provides an excellent in-depth introduction to statistical modelling and probabilistic programming with PyMC. If you want to go beyond the Bayesian methods we discuss in this book, I especially recommend working though Martin (2018).\n\n\n23.4.1.1 Context Management for Modelling with PyMC\nAlthough the PyMC package has a wide variety of use cases, we’ll exclusively use it for modelling. PyMC uses an unusual (though convenient) convention to simplify the necessary syntax for modelling. To understand it, we first have to briefly cover what a ‘Context’ is in Python.\nPython contexts are immediately recognizable by their use of the with statement, and are usually employed to manage system resources that are in limited supply. That’s why you’ll frequently see them used with I/O operations, where files are being read from or written to disk. Rather than leaving those files open and available for further editing, the with block ensures that the files are opened and closed in perfect lockstep with when they’re needed. A typical I/O context might look like this:\nwith open(\"data/hello.txt\", 'w') as file:\n    file.write(\"hello\")\nPyMC’s approach to modelling seeks to simplify the syntax by requiring that their models be used within the bounds of a context. It looks something like this:\nwith pm.Model() as test_model: \n    testPrior = pm.Normal(\"testPrior\", mu=0, sigma=1)\nAnytime you want to create a model, add variables to a model, or specify any other aspect of the model or how you plan to fit it, you can do so using PyMC’s context management. In the code block above, we defined a new model and called it test_model. That object now persists in our global namespace, and we can call it directly, which will prompt PyMC to give us a (slightly confusing) printout of the model specification:\ntest_model\nWe can also examine the individual variables, which also exist in the namespace by themselves:\ntestPrior\nFinally, we can also call the model directly with the with statement to add more variables (or do whatever else we please):\nwith test_model:\n    anotherTest = pm.Normal(\"anotherTest\", mu=2.5, sigma=10)\n        \ntest_model\n\n\n23.4.1.2 Specifying the Model in PyMC\nNow, we can start describing our model. We’re going to do this in chunks, starting with the priors:\nwith pm.Model() as pool_model:\n    # Priors\n    alpha = pm.Normal(\"alpha\", mu=1, sigma=2)\n    beta = pm.Normal(\"beta\", mu=1, sigma=2)\n    sigma = pm.Exponential(\"sigma\", lam=2)\nWe used one line per prior to define a distribution for each. The distributions themselves were drawn from PyMC’s library of distributions, which contains all of the distributions we discussed in Chapter 26 and other well-known distributions.\nEach call to pm.Normal in the code above included 3 arguments, the first of which is always a string representation of the variable’s name. It’s up to you how you name your variables. If at all possible, I prefer to name them so that they’re a one-to-one match with their Python counterparts. Doing so makes it much easier to read model output without cross-referencing against your model specification. The second and third arguments were passed as keyword arguments (they don’t need to be, but we wanted to make it explicit here); these are the \\(\\mu\\) and \\(\\sigma\\) we know and love, and they represent the mean and standard deviation for each of the Normal distributions we used.\nThere’s only one exception to the pattern above, which comes in the form of the pm.Exponential distribution we used for the standard deviation of the outcome. It still took in a name as its first argument, but we provided a lam argument, which represents the distribution’s ‘rate’ (and, conveniently, is also the inverse of its mean value).\nNow, let’s make another call to our model to add the line which represents the linear model – the part that’s responsible for combining all of the observed variables and priors we specified above:\nwith pool_model:\n    # Linear Model\n    mu = alpha + beta * spend_std\nThe line we used to specify the linear model should look very familiar to you - it’s nearly a dead ringer for the line we’ve been using in the formal model specification! The major difference is that we used spend_std, rather than spend – the former is the standardized version of the latter, and PyMC almost always prefers standardized variables. At this point, all that remains is to add the likelihood:\nwith pool_model:\n    # Likelihood\n    votes = pm.Normal(\"votes\", mu=mu, sigma=sigma, observed=vote_std)\nOur specification of the likelihood should appear as a straightforward representation of what we had built earlier, but with one major addition: the ‘observed’ parameter. When we pass data to this parameter, PyMC knows to treat this variable as a likelihood as opposed to a prior. Notice that if we were to remove the observed=vote_std argument, we would be supplying something that’s functionally identical to the priors we added in step 1.\nAnd that’s it! We now have a fully-specified PyMC model! All we need to do to get it to run is to add one more line, which we’ll do in the following major section. But before we do, we’re going to take a brief detour to make sure that our model isn’t totally off-base.\n\n\n\n23.4.2 Prior Predictive Check\nOne of the most oft-repeated criticisms of the Bayesian paradigm is the use of potentially indefensible prior distributions. Yeah, sounds bad. Is it?\nI’ve mentioned previously – and, statisticians with far more expertise than I have have demonstrated elsewhere – that most models are simple enough and are conditioned on large enough volumes of data that any combination of priors, regardless of how off-base they are, will be overwhelmed by the likelihood of the evidence, leaving inference more-or-less unaffected. The only really important exception here is an entirely off-base prior that assigns probabilities of 0 to important parts of the parameter space. Hopefully this is some cause for comfort, but the fact that our models are usually ‘safe’ from prior-based bias does not mean that we can become complacent.\nOne of the rituals we use to stave off complacency is the Prior Predictive Check. As we learned in previous chapters, one model’s prior is another model’s posterior; from a mathematical (but not inferential) standpoint, posteriors and priors are largely identical. This is convenient for us, because it means that we can draw samples from our model’s prior distribution much the same way we’d draw samples from any other distribution. In so doing, we can give ourselves a picture of what our model thinks is likely to occur before it has seen any data.\nFortunately, PyMC has built-in functionality for sampling from the prior (which simply draws sample values from the distributions we’ve already defined). We’ll re-use the model context to achieve this and save the results in a new variable:\nwith pool_model:\n    prior_predictive = pm.sample_prior_predictive(\n        samples=50, var_names=['alpha', 'beta', 'sigma', 'votes'], random_seed=42)\nThe prior_predictive object that we just created is an arviz.InferenceData object. We can examine the groups available in this object:\nprior_predictive\nWe can see that it contains the groups: prior, prior_predictive, and observed_data. To access the samples of our priors and simulated observations, we can use the following:\n\nPrior samples are in prior_predictive.prior\nSimulated observations are in prior_predictive.prior_predictive\n\nFor example, we can access the samples of alpha and beta from the prior:\nalpha_samples = prior_predictive.prior['alpha'].values.flatten()\nbeta_samples = prior_predictive.prior['beta'].values.flatten()\nTake some time to flip through the values in the prior_predictive object, and you’ll notice that they’re stored as xarray DataArrays. If you examine alpha_samples.shape, you’ll see that there are 50 samples (the number of samples we asked for). Similarly for beta_samples.\nNow that that’s done, we can just plug the parameter samples into a simple reproduction of our linear model.\nResults are shown in Figure 23.2.\nspend_grid = np.linspace(-20, 20, 50)\n\nplt.xlim((-10, 10))\nplt.ylim((-10, 10))\n\nalpha_samples = prior_predictive.prior['alpha'].values.flatten()\nbeta_samples = prior_predictive.prior['beta'].values.flatten()\n\nfor a, b in zip(alpha_samples, beta_samples):\n    # This is the same linear model that appeared in our PyMC definition above\n    vote_sim = a + b * spend_grid \n    plt.plot(spend_grid, vote_sim, c=\"k\", alpha=0.4)\n\nplt.axhspan(-2, 2, facecolor='black', alpha=0.2)\nplt.axvspan(-2, 2, facecolor='black', alpha=0.2)\n    \nplt.xlabel(\"Expenditure differential (standard deviations)\")\nplt.ylabel(\"Vote differential (standard deviations)\")\nplt.savefig('figures/26_02.png', dpi=300)\n\n\n\n\n\n\nFigure 23.2: png\n\n\n\nThe above plot contains 50 different regression lines drawn from our model’s prior distributions – a quick glance shows that our priors leave a whole lot of room for improvement. Here’s how you can tell: the intersecting grey areas in the plot represent two standard deviations on both of our variables, which means that roughly 95% of our data points will fall somewhere within the darker grey area of overlap. We can see that the majority of the regression lines we sampled from our model cross through the darker grey area from the lower-left to the upper-right, albeit at slightly too sharp an angle. A great many of the lines, though, only barely skim the edges or corners of the box; some fail to cross it altogether. If your model produces one or two highly suspect regression lines, that’s not a cause for concern. When your model produces a great many (as is the case with ours), it might be time to consider making your priors a little more informative.\nTake a look at what we can do by tightening our priors a little. The results are shown in Figure 23.3.\nwith pm.Model() as regularized_model:\n\n    # Priors\n    alpha = pm.Normal(\"alpha\", mu=0, sigma=0.5)\n    beta = pm.Normal(\"beta\", mu=0.5, sigma=1)\n    sigma = pm.Exponential(\"sigma\", lam=1)\n\n    # Linear Model\n    mu = alpha + beta * spend_std\n\n    # Likelihood\n    votes = pm.Normal(\"votes\", mu=mu, sigma=sigma, observed=vote_std)\n\n    reg_prior_pred = pm.sample_prior_predictive(\n        samples=50, var_names=['alpha', 'beta', 'sigma', 'votes'], random_seed=42)\nspend_grid = np.linspace(-20, 20, 50)\n\nplt.xlim((-10, 10))\nplt.ylim((-10, 10))\n\nalpha_samples = reg_prior_pred.prior['alpha'].values.flatten()\nbeta_samples = reg_prior_pred.prior['beta'].values.flatten()\n\nfor a, b in zip(alpha_samples, beta_samples):\n    # This is the same linear model that appeared in our PyMC definition above\n    vote_sim = a + b * spend_grid \n    plt.plot(spend_grid, vote_sim, c=\"k\", alpha=0.4)\n\nplt.axhspan(-2, 2, facecolor='black', alpha=0.2)\nplt.axvspan(-2, 2, facecolor='black', alpha=0.2)\n    \nplt.xlabel(\"Expenditure differential (standard deviations)\")\nplt.ylabel(\"Vote differential (standard deviations)\")\nplt.savefig('figures/26_03.png', dpi=300)\n\n\n\n\n\n\nFigure 23.3: png\n\n\n\nBased on the above plot, we can see that our new regularized model has a very strong preference for regression lines that hem closely to the origin (0 on both axes), and feature a moderately positive relationship between spend_std and vote_std (most regression lines have a positive slope). There’s still quite a bit of variability in the predictions: owing to their steeper incline, some of the regression lines travel through a limited span of the middle area. Others are more or less flat (predicting no relationship between spending and votes), and our model even permits a few of the lines to reverse the trend entirely and predict that increased spending is correlated with fewer votes received. All said, MUCH better!\nWhen selecting priors for a model, I like to use two simple heuristics:\n\nPriors shouldn’t make the impossible possible\nPriors shouldn’t make the possible impossible\n\nThe process of setting good priors involves more than simply following these two heuristics of course, but this is a good starting point. Once you’ve gotten the hang of setting priors following basic guidelines, you should feel free to descend into the particulars at your leisure. A good place to start doing so is cite this guide from the developers of another probabilistic programming tool for Bayesian data analysis called STAN.\nNow that we’ve created a better model using more sensible priors, we’re going to abandon it and forge ahead using the worse one. Why? I’ve got two didactic reasons:\n\nBy proceeding with the worse model, we’ll be able to see how even modest amounts of evidence can overwhelm poorly-specified priors with ease.\nIt won’t happen until next chapter, but we’ll see how models with poorly-specified priors can do ruinous things to more complex models.\n\n\n\n23.4.3 Running Our Model\nOur model is ready to run – all we need to do is to add one more line to get it started! This is where we tell PyMC to sample our model and produce a posterior distribution (which, in PyMC-speak, is contained in a ‘trace’ object). By default, PyMC draws 2,000 samples for each of the 4 chains, resulting in a grand total of 8,000 samples. The first 1,000 samples in each chain will be ‘tuning’ samples, used to get our proverbial marble into the right ballpark before we start drawing samples that we’ll incorporate into the posterior. In terms of the skate bowl metaphor from the previous chapter, you can think of each of the different chains as representing a different marble-robot pair. Each of those 4 pairs will repeat the ‘randomly whack the marble’ process 1,000 times, and the result of all of the marble whacks in aggregate will form our posterior. Let’s get a-whackin’!\nwith pool_model:\n    # Run Sample Traces\n    trace_pool = pm.sample()\nIf everything’s working correctly, our PyMC model should spit out a collection of preliminary text followed by a progress bar that should fill up in relatively short order. Running this last line of code hasn’t actually done anything to our model proper, but it has produced a ‘trace’ object that contains all the information we need to see how our model performed under sampling. First, let’s use the trace variable to produce a summary (for which we’ll use the arviz package, which is a companion module to the PyMC package, and which facilitates diagnosis and inference). The standard az.summary printout provides an overwhelming amount of data, so we’re going to artificially limit what it shows us for now. We’ll get to the other important variables a little later:\nsummary = az.summary(trace_pool, round_to=2)\nsummary[['mean', 'sd', 'r_hat']]\nEach of the rows in the dataframe above are dimensions of our posterior distribution and the three columns represent different summary statistics ArviZ has calculated for us. The three statistics we care about right now are the mean, the standard deviation, and the ‘r_hat’ (or \\(\\hat{r}\\)) of each dimension.\nIf you’ve fit and interpreted regression models before, you might find the mean and sd variables familiar: they simply represent the centre and width of the posterior distribution for that particular dimension. In a Frequentist regression, we would be implicitly comparing each of these hypotheses (one for each covariate) to the assumed ‘null hypothesis’ and deciding whether or not to reject the null hypothesis based on the strength of the evidence. You would usually look for a series of little stars to rapidly assess the statistical significance of each alternative hypothesis. Since this is a Bayesian regression, you’ll find no such machinery here: the numbers we’ve printed here are just a summary of the full answer we’ve tasked ourselves with providing, which is always the full shape of the entire posterior distribution. A good Bayesian is obsessed with retaining as much information and uncertainty as possible throughout the modelling process.\nIf you are not familiar, the r_hat statistic is a purely diagnostic statistic and is not normally interpreted. If all is well with your model, you would expect to see all of the r_hat values to be 1.00, or very close to. Anything higher than that (even 1.02 or greater) is a sign that something has gone wrong in your model.\n\n\n23.4.4 Checking the Traceplot\nOne of the most important steps in any Bayesian regression model involves checking your model’s ‘traces’ to ensure that nothing went awry behind the scenes. ArviZ has some really nice built-in tools for this, shown for our trace pool model in Figure 23.4.\naz.plot_trace(trace_pool, var_names=['alpha', 'beta', 'sigma'], compact=True)\nplt.savefig('figures/26_04.png', dpi=300)\n\n\n\n\n\n\nFigure 23.4: png\n\n\n\nEach row in the foregoing grid of plots corresponds to a row in the dataframe summary we produced above. The left column of plots presents you with the shape of the posterior distribution corresponding to one variable in the model (or, equivalently, one dimension of the posterior distribution). The right column of plots shows you the ‘trace’ of the PyMC sampler as it attempted to fit your model. You can think of each line in each traceplot representing a single marble being swatted around a high-dimensional skate bowl, and each row of the figure (there’s one per parameter) is one of those dimensions. This might seem a bit unintuitive at first, but the x-axis in each of the plots on the left represents the exact same thing as the y-axis of their counterpart in the same row on the right! They both represent the parameter’s value: the left is showing you the estimated posterior distribution of the parameter, and the right is showing you how the marbles moved to produce it (the x-axis for each plot on the right is the ‘sample number’; you can think of the marbles as moving from left to right within each plot).\nAnother thing you might notice is that all of our parameters look normally distributed now; that isn’t much of a surprise for \\(\\alpha\\) and \\(\\beta\\), but what about \\(\\sigma\\)? Since we used the Exponential distribution as its prior, shouldn’t we expect its posterior to be Exponentially distributed, too? Not at all; the only reason we were using the Exponential distribution was to prevent our model from making the mistake of using negative numbers as potential parameter values for our Normal distribution’s \\(\\sigma\\) parameter (which is undefined for all numbers below 0). Even if you use a non-normal distribution for a prior, you’ll often find that your posterior distribution for that parameter is Normal. Nothing to worry about.\nWhat you should be worried about is the shape of your traces. There are three things we want to see in a ‘good’ trace:\n\nWe want to make sure that the algorithm is stationary, meaning that it has located the area of highest posterior probability and is spending all of its time bouncing around near it. When chains wander around and never settle in one region of the parameter space, it’s a bad sign. To spot a non-stationary trace, look for lines that spend a good amount of time in one part of the posterior and then suddenly switch to another area and stay there for an extended period.\nWe want to make sure that our samplers are exploring the posterior space rapidly and efficiently, which is called ‘good mixing’. When a chain is mixing well, it will appear to be darting around from one side of the posterior distribution to the other rapidly. Chains that aren’t mixing well might be stationary in the long run, but take a long time to move back and forth. To spot a poorly-mixed trace, look for lines that slowly and gradually move around (as opposed to the frenetic, zippy movement of the healthy lines we see above).\nWe want to make sure that each of the various chains we use have converged, meaning they all spent most of their time in the same region of the posterior; if 3 chains are stationary in one area of the posterior, but the 4th chain is spending all of its time a good distance away, there’s a problem afoot. It’s often easier to spot non-stationary traces on the left-hand side of the trace plot, where it’s easy to notice if one of the traces’ distributions differs significantly from the others. The small amount of wiggliness we see in the \\(\\sigma\\) plot above is no big deal at all.\n\nThe trace plots we’ve produced here are all ideal. Later on, we’ll show you some that are far from ideal. If you can’t wait to find out what bad trace plots look like, you can find lots of detail at this blog post: https://jpreszler.rbind.io/post/2019-09-28-bad-traceplots/. It features a bunch of examples that are more extreme than anything we’re going to see in this book, but is worth taking a look at nonetheless!\nLet’s return to those nice-looking distributions on the left-hand side of the diagram again. You might notice that there are a few different lines in each plot – each of the 4 different chains we used to fit our model is separately represented, each with a different line pattern. In fact, those 4 separate lines appear in the trace plots on the right-hand side, too; they’re just much harder to see individually (which is a good thing - that means our marbles were well-behaved).\nSince each of the four lines in each of our distribution plots are in broad agreement (they differ slightly, but not even remotely enough to indicate any problems), we can use these distributions to get an accurate idea of where our model thinks the parameter values are most likely to fall.\n\n\n23.4.5 Establishing Credible Intervals\nNow, let’s dig into each of our variables in a bit more detail; we can do so using ArviZ’s plot_posterior function. Our focus will be on something called the ‘HDI’, which stands for the ‘Highest Density Interval’. The HDI is the closest thing you’re going to see to the Frequentist ‘95% confidence interval’ (or similar) in Bayesian data analysis. Statistically, the HDI represents the shortest possible interval in one dimension of the posterior distribution which contains a predetermined amount of probability. We use the HDI interval to provide us a sense of the area in the distribution that we’re confident (to a predetermined extent) contains the best-fitting parameter value.\nIt’s up to us to determine how much of the posterior probability we want to appear inside our HDI. In his classic Bayesian text, Richard McElreath (2020) uses an abundance of cheek when suggesting that Bayesians should employ a prime number for no other reason than the fact that it is prime. He portrays this as a way of subtly jabbing Frequentists for their automatic use of an arbitrarily-set significance threshold of .05, whose progenitor specifically indicated should not be adopted as a default. Hilarious! (Though to be fair, many Frequentists are themselves trying to get other Frequentists to stop doing that.)\nWe’ll follow in McElreath’s footsteps and use 0.89, but there’s no good reason why we couldn’t use something like 0.83 or 0.79. The default for most ArviZ plots is 94%; having made our point, we’ll leave the HDI intervals at their defaults from here on out. Results are shown in Figure 23.5.\nfig, axs = plt.subplots(3, 1, sharex=True, sharey=True, figsize=(6, 6))\naz.plot_posterior(trace_pool,\n                  ax=axs,\n                  var_names=['alpha', 'beta', 'sigma'],\n                  hdi_prob=0.89)\nfig.tight_layout()\n\nplt.savefig('figures/26_05.png', dpi=300)\n\n\n\n\n\n\nFigure 23.5: png\n\n\n\nWe decided to force PyMC and ArviZ to plot all three posterior distributions (and their HDIs) on the same unified axis so you could directly compare their positions and widths. The black bars under each of the plotted distributions represent the span of our chosen HDI. The numbers that appear to the left and right of the black bar represent the HDI’s upper and lower bounds – this gives us a precise numerical range within which our chosen probability density can be found.\nRemember that unlike the Frequentist paradigm, the Bayesian paradigm allows us to apply probability and probabilistic statements to hypotheses. That’s exactly what we’re doing when we create a credible interval! The credible interval represents the region of the posterior probability within which we expect the underlying parameter value to fall, conditional on a predetermined amount of uncertainty. The lower we set our HDI interval, the tighter it becomes, but the less certain of it we are. In our example above, we used an 89% interval; had we set that interval to, say, 79%, it would occupy a smaller proportion of the number line, but we would also have less confidence that the interval contains the ‘true’ parameter value (if such a thing can be said to exist).\nThe more certain we are of a parameter’s value (as a result of having a posterior distribution with a smaller standard deviation), the more narrow and concentrated our HDI becomes. But even if we had nearly limitless data to feed into our Bayesian machine, we’d never reach perfect certainty about a parameter value, at least not while using a continuous range of hypotheses. If you think back to our probability primer in Chapter 26, this is because our probability density is an integrated value, and the value of any integral on a span of 0 length is 0: thus, the probability of any single hypothesis (such as \\(\\beta = 1\\)) will also be 0. We can only ever speak of probability as accumulating within a range of hypotheses.\nThe HDI is a common and well-understood method of constructing a credible interval. It is not, however, the only means of doing so. We don’t have the time to cover them in detail, but it’s worth weighing the merits of HDI against other techniques for developing a credible interval. Some place more emphasis on ensuring that the credible interval has the same amount of probability on either side of it, ensuring that it is in the ‘middle’ of the posterior distribution. Others mimic the HDI, but allow it to split in the middle so as to cover a 2-humped posterior. Good options abound, many of which can be found in the details of the ArviZ’s plot_posterior documentation.\n\n\n23.4.6 Posterior Predictive Checks\nJust in case you hadn’t yet seen enough plots of roughly normal-looking distributions, we’re going to do one more. In much the same way as we drew samples from our model’s prior distribution to perform a prior predictive check, we can draw samples from our model’s posterior distribution to perform a posterior predictive check. While the purpose of the prior predictive was to ensure that our model wasn’t out to lunch, the posterior predictive is designed to see how well it performs at retrodicting the evidence we fed to it.\nJust as with the prior predictive, we start by drawing samples from our model:\nwith pool_model:\n    ppc = pm.sample_posterior_predictive(trace_pool, var_names=['votes', 'alpha', 'beta', 'sigma'])\nIf you inspect it, you’ll find that the resulting ppc object is an arviz.InferenceData object. We can use this object directly with the ArviZ functions. In the code cell below, we use the az.plot_ppc function to produce a plot of our posterior predictive (Figure 23.6):\naz.plot_ppc(ppc, num_pp_samples=100, legend=False)\nplt.savefig('figures/26_06.png', dpi=300)\n\n\n\n\n\n\nFigure 23.6: png\n\n\n\nIn the above plot, observations from our outcome variable (the standardized vote differential) are arranged along the x-axis, and the frequency (or density) of an observation of that value is tracked along the y-axis. The light wispy lines represent all of the retrodictions made by one set of posterior parameter values (of which we sampled 100); the dashed line represents the overall average of each sample. The solid black line represents the observed data.\nIdeally, we’d want to see our model adhere more closely to the observed data: as it stands, our model tends to underpredict the number of congressional districts that the Republicans won by a single standard deviation and greatly overpredicts the number of extremely close races (in and around the origin).\n\n\n23.4.7 Plotting Uncertainty\nI know I’ve said it quite a lot already, but one of the reasons why we use Bayesian methods in the first place is because we want to preserve uncertainty to the greatest extent possible throughout the entire modelling process. You’ll often find that other approaches to regression analysis produce a ‘line of best fit’ or a ‘predictor line’ or something similar. In Bayesian analysis, we instead produce a range of such lines, each of which is probabilistically drawn from our posterior distribution, and each of which differs from the others. Since it’s difficult to appreciate information at this scale directly, Bayesian regression leans heavily on visualization techniques to provide intuitive guides to inference. Here, we’re going to draw samples of predicted outcomes and parameter values from our posterior distribution (using a PyMC function designed for just such a task), feed those sampled values through our linear model, and plot the 94% HDI range of the results (Figure 23.7).\nplot_2020_election_fit(spend_std, vote_std, trace_pool, ppc)\nplt.savefig('figures/26_07.png', dpi=300)\n\n\n\n\n\n\nFigure 23.7: png\n\n\n\nIn the above plot, the black line represents the mean (or average) predictor line. Its value was produced by averaging over thousands of such lines, 94% of which fall entirely within the smaller, darker band around the black line; that band represents our model’s uncertainty in the regressor. Our model also models predictive uncertainty – or, in simpler terms, the width of the band within which it expects 94% of the data to fall (which is controlled by our model’s sigma parameter, which we also have some uncertainty about). It’s uncertainty piled upon uncertainty (and so on ad infinitum), but it produces a set of results and visualizations that are remarkably intuitive to read and interpret.\nNevertheless, we can now produce a preliminary interpretation of what our model is telling us: using the posterior predictive plot and the various parameters summaries from earlier, our model is indicating an increase of 1 standard deviation in spending differential tends to correlate with a roughly 0.45 standard deviation increase in vote differential.\nThere’s just one catch, which you may or may not have noticed by looking at the plot. This model sucks. We can do better. That’s what the next chapter is all about.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Bayesian Regression Models with Probabilistic Programming</span>"
    ]
  },
  {
    "objectID": "linear-regression.html#conclusion",
    "href": "linear-regression.html#conclusion",
    "title": "23  Bayesian Regression Models with Probabilistic Programming",
    "section": "23.5 CONCLUSION",
    "text": "23.5 CONCLUSION\n\n23.5.1 Key Points\n\nBayesian regression is a powerful, flexible approach to regression analysis\nJust because simple Bayesian regression models with plenty of data aren’t all that sensitive to the priors placed on their latent variables doesn’t mean that you should be complacent about setting priors: a prior predictive check can be helpful in this regard\nBayesian regression emphasizes preserving and visualizing uncertainty whenever and however possible\n\n\n\n\n\nMartin, Osvaldo. 2018. Bayesian Analysis with Python: Introduction to Statistical Modeling and Probabilistic Programming Using Pymc and ArviZ. Packt Publishing Ltd.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. CRC press.\n\n\nSalvatier, John, Thomas Wiecki, and Christopher Fonnesbeck. 2016. “Probabilistic Programming in Python Using Pymc.” PeerJ Computer Science 2: e55.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Bayesian Regression Models with Probabilistic Programming</span>"
    ]
  },
  {
    "objectID": "multilevel-regression.html",
    "href": "multilevel-regression.html",
    "title": "24  Multilevel regression",
    "section": "",
    "text": "24.1 LEARNING OBJECTIVES",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Multilevel regression</span>"
    ]
  },
  {
    "objectID": "multilevel-regression.html#learning-objectives",
    "href": "multilevel-regression.html#learning-objectives",
    "title": "24  Multilevel regression",
    "section": "",
    "text": "Explain what a Hierarchical linear regression model is\nSpecify a Hierarchical linear regression model using mathematical notation\nSpecify a Hierarchical linear regression model using PyMC code\nExplain what “pooling” is in a Hierarchical linear regression\nDifferentiate between no pooling, partial pooling, and complete pooling\nUse informative priors to fix a problematic sampler\nAssess how well Hierarchical models fit the data\nInterpret the results of a Hierarchical model",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Multilevel regression</span>"
    ]
  },
  {
    "objectID": "multilevel-regression.html#learning-materials",
    "href": "multilevel-regression.html#learning-materials",
    "title": "24  Multilevel regression",
    "section": "24.2 LEARNING MATERIALS",
    "text": "24.2 LEARNING MATERIALS\nYou can find the online learning materials for this chapter in doing_computational_social_science/Chapter_29. cd into the directory and launch your Jupyter Server.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Multilevel regression</span>"
    ]
  },
  {
    "objectID": "multilevel-regression.html#introduction",
    "href": "multilevel-regression.html#introduction",
    "title": "24  Multilevel regression",
    "section": "24.3 INTRODUCTION",
    "text": "24.3 INTRODUCTION\nGenerally speaking, most introductory and intermediate quantitative methods classes for social science students do not teach hierarchical linear regression models except as a special case of ‘default’ linear regression models. This is probably due to the fact that simple linear models are much easier to teach than complex ones, and because of the wise notion that, where possible, we should favour simple models over complex models. And so hierarchical linear models are banished to “advanced” electives that you might get to after years of learning ANOVA-like statistical tests (now with 300+ flavours!). This is all a bit silly given the philosophical gymnastics required of “simple” statistical tests and linear models in the Frequentist tradition. We need a new normal in which our “default” regressions are hierarchical. I’m going to assume that you, like me, were not taught statistics this way and that you may not even know what a hierarchical regression is. Let’s change that.\n\n24.3.1 Imports\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport pymc as pm\nimport arviz as az\n\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\nfrom dcss import set_style\nset_style()\n\nfrom dcss.bayes import plot_2020_no_pool, plot_2020_partial_pool",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Multilevel regression</span>"
    ]
  },
  {
    "objectID": "multilevel-regression.html#so-whats-a-hierarchical-model",
    "href": "multilevel-regression.html#so-whats-a-hierarchical-model",
    "title": "24  Multilevel regression",
    "section": "24.4 SO, WHAT’S A HIERARCHICAL MODEL?",
    "text": "24.4 SO, WHAT’S A HIERARCHICAL MODEL?\nLinear regression models of any variety can justifiably be called “hierarchical” if they use data at different ‘levels’ to estimate parameters and make predictions. I’ll come back to ‘data at different levels’ in a moment, but first I want to acknowledge a very common source of confusion. For no good reason, hierarchical models go by a wide variety of different names, including ‘random effects’, ‘mixed effects,’ and ‘multilevel.’ There are many others as well. The terminology here is hopelessly and inextricably muddled. In some fields, the various terms have specific meanings; in others, they don’t. Sometimes the specific meanings are at odds with one another.\nThankfully none of that really matters for our purposes here. Most of the time I’ll stick with hierarchical, but when I don’t, I mean the same thing. Since you have now been introduced to two different languages for describing your models with precision: code and mathematical notation. A description of your model using either of these two languages is enough to banish any ambiguity about the type of model you’re working with.\nWhat do I mean when I say ‘data at different levels?’ Good question. Anytime I mention something along those lines (including references to ‘pools,’ ‘clusters,’ etc.), I’m referring to data where observations can be reasonably grouped together because they share some sort of context. (You might remember me alluding to this type of model when I introduced relational thinking in Chapter 14, where I compared network analysis to multilevel analysis.) The model we were working with last chapter – the one that investigated if you could predict a Democratic candidate’s margin of victory (or loss) from the amount by which the Democrats outspent (or were outspent by) the Republicans – used clustered data: each of the observations was drawn from one of the United States of America’s 435 Federal Congressional Districts, and each of those districts belonged to one of the 50 States. In this way, the State variable could have acted as a ‘cluster’, in the sense that two Congressional Districts from within the same state are more likely to share similarities with one another than two Congressional Districts chosen at random from the entire dataset.\nThe real power of a hierarchical model stems from its ability to balance the influence of individual level observations (individual congressional districts) with the influence of whole clusters (each individual state). Bayesian hierarchical models permit this tradeoff between countervailing influences by permitting the slopes (\\(\\beta\\)) and intercepts (\\(\\alpha\\)) of each cluster to vary from one another, while still forcing all of the slopes and intercepts to be drawn from a simultaneously-estimated prior. This is all getting a little too abstract, so let’s get practical.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Multilevel regression</span>"
    ]
  },
  {
    "objectID": "multilevel-regression.html#goldilocks-and-the-three-pools",
    "href": "multilevel-regression.html#goldilocks-and-the-three-pools",
    "title": "24  Multilevel regression",
    "section": "24.5 GOLDILOCKS AND THE THREE POOLS",
    "text": "24.5 GOLDILOCKS AND THE THREE POOLS\nWhilst working your way through the previous chapter, you might have noticed the word ‘pool’ showing up in our model. I didn’t explain it at the time, but the nomenclature was indicative of the modelling strategy we were using. In a regression model, the ‘pooling’ you use determines how the various categorical variables in your model can influence one another. The election data we used had one categorical variable – state – that indicated which of the 50 US States each congressional district belonged to. It might not have seemed so at the time, but our decision to omit the state variable by excluding it from the model entirely was an intentional choice: by preventing our model from accessing the information contained therein, it was forced to treat every congressional district from every state as if they had all come from one giant state (or, equivalently, no state at all). Doing so might have seemed like the ‘default’ option, but the only reason we went that route was for the sake of simplicity.\nNow that we’ve seen a full example of developing a simple Bayesian regression model, it’s time to take our categorical data more seriously. The idea here, which I am taking from McElreath’s classic Statistical Rethinking, is that our modelling choices should reflect serious consideration of the options provided to us by the information available to us in our datasets. In this case, we should be asking ourselves: “how should we handle the various U.S. states that appear in this dataset?” Let’s discuss three primary options.\nOption 1, Complete Pooling: All 50 U.S. States are identical.\nThis is the approach we took in the previous chapter, treating all congressional districts from all states as if there was no meaningful difference between them. This approach is known as ‘Complete Pooling’, because it puts all of the observations into one big ‘pool’ and estimates parameters therefrom (now you know where all those ‘pool’ references come from). This is a simple approach, which is nice, but it’s rarely the best one. It can be overly simplistic, obliterates differences between clusters, and is prone to underfitting. It is highly unlikely, for instance, that increased Democratic election spending will do anything to sway voters in overwhelmingly Republican state of Wyoming. Ditto for Hawaii, where most voters are already committed Democrats. Best to avoid any impulse to artificially impose homogeneity.\nOption 2, No Pooling: All 50 U.S. States are utterly unique.\nThis approach – called “No Pooling” – would allow each state to have its own intercept (\\(\\alpha\\)) and slope (\\(\\beta\\)), completely free from any other influences. This would mean that there would be practically no statistical commonalities between them, aside from the (very weak) regularizing influence of our priors. Going this route ensures that nothing our model learns about one state (or all of the states as a whole) can tell us anything about any of the others as individuals. Since the model is now free to create the best fit for each state based on the data available, this approach is very susceptible to overfitting.\nOption 3, Partial Pooling: The U.S. States differ from one another, but there are commonalities about them that we can infer and apply productively.\nThis approach – which we’ll call ‘Partial Pooling’ – allows states to differ from one another, but places limitations on how they may differ. Rather than giving each state free rein over its own parameters, this approach allows the model to simultaneously learn about each state’s parameters from the data, as well as overall trends for the states in general by way of shared priors.\nLogically and statistically, this approach usually makes the most sense: each state differs, but all are political entities within the United States of America, carrying all of the shared norms, values, and traditions incumbent upon belonging to the Union. This is the approach we primarily will use as we dive into hierarchical modelling. Before we do, though, let’s take a brief detour to examine what a ‘No Pooling’ model might look like.\n\n24.5.1 Load Data\nSince our exploration of Bayesian hierarchical linear models builds off of the model we developed in the previous chapter, we’re going to re-use the same 2020 House of Representatives Election dataset. We’ll start by loading, standardizing, and previewing the data:\ndf = pd.read_csv(\n    'data/2020_election/2020_districts_combined.csv'\n)\n\nspend_std = (df.spend - np.mean(df.spend)) / np.std(df.spend)\nvote_std = (df.vote - np.mean(df.vote)) / np.std(df.vote)\nstate_cat = pd.Categorical(df.state)\nstate_idx = state_cat.codes\nn_states = len(set(state_idx))\ndem_inc = df.dem_inc\nrep_inc = df.rep_inc\npvi_std = (df.pvi - np.mean(df.pvi)) / np.std(df.pvi)\ndf.head()\nPart of our objective in this chapter is to incorporate more of the available data into our model - as you may recall, we only utilized the vote and spend variables in the previous chapter. Now, we’re going to expand our model to incorporate information from the state, dem_inc, rep_inc, and pvi variables. Before proceeding, let’s take a moment to summarize each of the new variables and consider what they represent:\ndf[['dem_inc', 'rep_inc', 'pvi']].describe()\nThe three new variables in our lineup, from left to right in the table above, represent Democratic Incumbent, Republican Incumbent, and Cook Partisan Voting Index, respectively.\nThe two incumbency variables are straightforward: both are binary categorical variables (whose only possible values are 1 or 0), and they represent which of the parties (if either) has an incumbent in the race. We can’t really combine them in the same way we did with vote and spend, because some districts have no incumbent at all, and it’s not yet clear that the effect of incumbency is the same for Republicans and Democrats alike. We’ll have to keep them separate for now. The ‘Cook Partisan Voting Index’ (pvi) measures how strongly a given congressional district tends to lean towards one of the two major U.S. political parties. It’s based on voting data gathered from the two previous presidential elections, and – for this election – ranges from a minimum of -33 (the deep-red Texas Panhandle), to 43 (the true-blue Bronx).\nWithout looking at any regression results, I’d expect all three of these variables to play a strong role in our model: collectively, they speak volumes about how each congressional district has voted in the past. In fact, I’d be willing to bet that their collective influence on the model, regardless of its final form, will be stronger than the spend variable’s will be, but that’s fine: the purpose of our model is to tell us what the spend variable’s influence is whilst controlling for things like statewide preferences and historical trends. If, after the control variables are added, our model finds that spend isn’t that important, that’s a perfectly valid result.\nOf course, we’re not yet certain how things are going to turn out; there’s a lot of modelling to be done between now and then! As a prelude, let’s take a moment to remind ourselves about the fully-pooled model (i.e., “All 50 states are identical”) we used last chapter:\n\\[\\begin{align}\n\\text{vote}_i &\\sim \\text{Normal}(\\mu_i, \\sigma)& \\text{[Likelihood]}  \\\\\n\\mu_i &= \\alpha + (\\beta \\cdot \\text{spend}_i)  & \\text{[Linear Model]} \\\\\n\\alpha &\\sim \\text{Normal}(0, 2)                & \\text{[alpha Prior]} \\\\\n\\beta  &\\sim \\text{Normal}(1, 2)                & \\text{[beta Prior]} \\\\\n\\sigma &\\sim \\text{Exponential}(2)              & \\text{[sigma Prior]} \\\\\n\\end{align}\\]\nThe above model only uses a single value for \\(\\alpha\\) and a single value for \\(\\beta\\), which means that every observation (regardless of which state they come from) must use the same slope and intercept. When we build hierarchical models, we allow the slope and intercept to vary by state. Consequently, we’re going to have to re-build our model such that it is capable of accommodating multiple slopes and multiple intercepts. Rather than use ‘dummy’ variables for each state (as would be the standard Frequentist practice), we’re going to use an unordered categorical ‘index variable’. We can write it like so:\n\\[\\begin{align}\n\\mu_i &= \\alpha_{\\text{state[i]}} + (\\beta_{\\text{state[i]}} \\cdot \\text{spend}_i)\n\\end{align}\\]\nTranslated into plain English, the above line is saying that “The value of \\(\\mu_i\\) for a given observation \\(i\\) is equal to the \\(\\alpha\\) for that observation’s state plus the product of the \\(\\beta\\) for that observation’s state and that observation’s spend value.” This makes it explicit that our model will now accommodate as many different values for \\(\\alpha\\) and \\(\\beta\\) as there are states in the dataset (48, in our case, since two were dropped).\nNow let’s update the rest of the model:\n\\[\\begin{align}\n\\text{vote}_i &\\sim \\text{Normal}(\\mu_i, \\sigma)\\\\\n\\mu_i &= \\alpha_{\\text{state[i]}} + (\\beta_{\\text{state[i]}} \\cdot \\text{spend}_i)     \\\\\n\\alpha_{\\text{state[i]}} &\\sim \\text{Normal}(0, 2)   &  \\text{for state[i]} = 0 ... 47            \\\\\n\\beta_{\\text{state[i]}}  &\\sim \\text{Normal}(1, 2)   &  \\text{for state[i]} = 0 ... 47                \\\\\n\\sigma &\\sim \\text{Exponential}(2)               \\\\\n\\end{align}\\]\nEven though each of the 48 \\(\\alpha\\) and \\(\\beta\\) parameters are completely separate and will have no influence on one another, they all share the same respective priors. Additionally, the \\(\\text{state[i]}\\) that shows up everywhere. I’m particularly fond of the \\(\\text{state[i]}\\) nomenclature, because it very closely mirrors how a Python object would behave. What we’re saying in the model definition above is that \\(\\text{state}\\) is a mapping that accepts an integer, \\(\\text{i}\\) (which can range from 0 to 370), and outputs an integer between 0 and 47. In so doing, it has mapped the observation number (0 - 370) into a state number (0 to 47).\nWe can replicate this behaviour using variables we’ve already defined:\ndistrict_3_state = state_idx[3]\nprint(district_3_state)\nprint(state_cat.categories[district_3_state])\nFeel free to go check what state the corresponding row in the dataset belongs to; you should see that it’s a perfect match!\nNow that we’ve specified our model mathematically, let’s feed it into PyMC:\n\n\n24.5.2 No Pooling Model\nWe’ll start by specifying the full model. We won’t go through it step by step, though, as we’re tight on space and we’ve only made a few changes from the model in the previous chapter. Those changes are:\n\nWe added a shape=n_states parameter to our \\(\\alpha\\) and \\(\\beta\\) priors\nWe added [state_idx] to the \\(\\alpha\\) and \\(\\beta\\) parameters in the linear model\n\nAs a result of these changes, the \\(\\alpha\\) and \\(\\beta\\) parameters are no longer one-dimensional scalars. Instead, they are each vectors of length 48 – one for each of the states in the dataset. Second, during fitting, our model will now seek out the \\(\\alpha\\) and \\(\\beta\\) parameters that correspond to the \\(i\\)-th district’s state. Here’s what it looks like in action:\nwith pm.Model() as no_pool_model:\n    # Priors\n    alpha = pm.Normal(\"alpha\", mu=0, sigma=2, shape=n_states)\n    beta = pm.Normal(\"beta\", mu=1, sigma=2, shape=n_states)\n    sigma = pm.Exponential(\"sigma\", lam=2)\n    \n    # Linear Model\n    mu = alpha[state_idx] + beta[state_idx] * spend_std\n    \n    # Likelihood\n    votes = pm.Normal(\"votes\", mu=mu, sigma=sigma, observed=vote_std)\n    \n    # Run Sample Traces\n    trace_no_pool = pm.sample()\nIf everything works correctly, PyMC should sample without issues. Let’s check the trace plots for the hyperparameters (Figure 24.1):\nwith no_pool_model:\n    az.plot_trace(trace_no_pool, ['sigma'], compact=True)\n    plt.savefig('figures/28_01.png', dpi=300)\n\n\n\n\n\n\nFigure 24.1: png\n\n\n\nEven though we can’t display all the traces for alpha and beta (since there are 48 of each), a quick glance at our trace for sigma seems to indicate that everything is well and good. This assumption is backed up by the fact that the PyMC sampler didn’t have any grievances to air. Operating under the assumption that our model was well-sampled, let’s proceed with our examination of the results (Figure 24.2):\nwith no_pool_model:\n    ppc = pm.sample_posterior_predictive(trace_no_pool, var_names=['votes', 'alpha', 'beta', 'sigma'])\nplot_2020_no_pool(\n    no_pool_model, \n    trace_no_pool,\n    n_states, \n    state_idx,\n    spend_std,\n    vote_std,\n    ppc,\n    state_cat\n)\nplt.savefig('figures/28_02.png', dpi=300)\n\n\n\n\n\n\nFigure 24.2: png\n\n\n\nEach state in the model has a different average regression line, and all of them seem to be doing a good job of fitting the data. While most of the states still show a positive relationship between spending differential and vote differential, not all do: Maine, Alabama, Massachusetts, and New Mexico have completely reversed the trend. Our model has determined that Democrats who outspend Republicans in these states tend to do worse than their colleagues who don’t. According to our model, the only thing the Democrats would have to do to sweep Alabama is stop spending any money there! Though hilarious, this is clearly not a reasonable conclusion for the model to draw. Such is the peril of allowing a model to fit the data as closely as it can.\nIf you squint and look closely, you might be able to see some small bands around each of the regression lines, covering the interval that we have data for. It might come as little surprise, then, that those little bands are the exact same as the bands we used to surround our regression line from the previous chapter. As a brief refresher: the bands represent the model’s uncertainty about the best-fit regression line (inner band, 94% HDI) and its uncertainty about where the data points themselves lay in relation to the regression line (outer band, 94% HDI; parameterized as ‘\\(\\sigma\\)’ in our model’s likelihood, a.k.a. the standard deviation of the normal distribution in our likelihood).\nWe’re not going to dwell too much on the specifics here: the important takeaway is that our unpooled model has allowed the data for each state to completely determine their own intercept and slope parameters, even when there are only a small number of observations. The only regularizing forces present are the relatively uninformative (and, therefore, weak) priors that we established for this model in the previous chapters (they haven’t changed between now and then). With nothing stopping the model from rushing straight for the best possible fit, we’ve allowed it to descend into the dread valley of overfitting. Damn. Our model does an excellent job at fitting the data we have, but it is, in effect, painting a bullseye around an arrow that had already lodged itself into a wall. In order to curb these tendencies in a principled way, we’re going to turn to the regularizing properties of the hierarchical linear model.\n\n24.5.2.1 Partially Pooled Model\nOur objective for the hierarchical election model we are developing here is to permit slopes and intercepts to vary between States, but to ensure that each is being drawn from a set of higher-level distributions that encode our model’s knowledge of the States in general. This means that we’re going to have to do away with the numbers (\\(\\mu\\) and \\(\\sigma\\)) we’ve been using thus far to specify our \\(\\alpha\\) and \\(\\beta\\) priors and replace them with parameters. Let’s do that now, even though the result will be incomplete:\n\\[\\begin{align}\n\\text{vote}_i &\\sim \\text{Normal}(\\mu_i, \\sigma)\\\\\n\\mu_i &= \\alpha_\\text{state[i]} + (\\beta_\\text{state[i]} \\cdot \\text{spend}_i)     \\\\\n\\alpha_\\text{state[i]} &\\sim \\text{Normal}(\\alpha_\\mu, \\alpha_\\sigma)   \\\\\n\\beta_\\text{state[i]}  &\\sim \\text{Normal}(\\beta_\\mu, \\beta_\\sigma)   \\\\\n\\sigma &\\sim \\text{Exponential}(2)               \\\\\n\\end{align}\\]\nOkay, great! We’ve now configured our \\(\\alpha\\)s and \\(\\beta\\)s so that they’ll be drawn from a common, higher-level distribution. This gives us four new variables to play the “what’s that?” game with. Since\n\n\\(\\alpha_\\mu\\)\n\\(\\alpha_\\sigma\\)\n\\(\\beta_\\mu\\), and\n\\(\\beta_\\sigma\\)\n\nare all unobserved, they’re going to need priors. You might be thinking to yourself “aren’t \\(\\alpha\\) and \\(\\beta\\) already priors? Does this mean we’re going to be giving priors to our priors?”\nYes! Exactly! In order to keep things as conceptually clear as possible, a ‘prior for a prior’ has a special name: ‘Hyperprior’. Let’s fill those in now, using similar numerical values as in earlier models. I’ve included line breaks to help clarify which type of prior is which.\n\\[\\begin{align}\n\\text{vote}_i &\\sim \\text{Normal}(\\mu_i, \\sigma)\\\\\n\\mu_i &= \\alpha_\\text{state[i]} + (\\beta_\\text{state[i]} \\cdot \\text{spend}_i)     \\\\\n\\\\\n\\alpha_\\text{state[i]} &\\sim \\text{Normal}(\\alpha_\\mu, \\alpha_\\sigma)   \\\\\n\\beta_\\text{state[i]}  &\\sim \\text{Normal}(\\beta_\\mu, \\beta_\\sigma)   \\\\\n\\sigma &\\sim \\text{Exponential}(2)               \\\\\n\\\\\n\\alpha_\\mu &\\sim \\text{Normal}(1, 2) \\\\\n\\beta_\\mu &\\sim \\text{Normal}(1, 2) \\\\\n\\alpha_\\sigma &\\sim \\text{Exponential}(1) \\\\\n\\beta_\\sigma &\\sim \\text{Exponential}(1) \\\\\n\\end{align}\\]\n\nNow that we have priors, and that our priors have priors (most of them, anyways; good ol’ sigma remains untouched), let’s translate everything into PyMC:\nwith pm.Model() as partial_pool_model:\n    \n    # Hyperpriors\n    alpha_mu = pm.Normal(\"alpha_mu\", mu=1, sigma=2)\n    beta_mu = pm.Normal(\"beta_mu\", mu=1, sigma=2)\n    alpha_sigma = pm.Exponential(\"alpha_sigma\", lam=1)\n    beta_sigma = pm.Exponential(\"beta_sigma\", lam=1)\n    \n    # Priors\n    alpha = pm.Normal(\"alpha\", mu=alpha_mu, sigma=alpha_sigma, shape=n_states)\n    beta = pm.Normal(\"beta\", mu=beta_mu, sigma=beta_sigma, shape=n_states)\n    sigma = pm.Exponential(\"sigma\", lam=2)\n    \n    # Linear Model\n    mu = alpha[state_idx] + (beta[state_idx]*spend_std) \n    \n    # Likelihood\n    votes = pm.Normal(\"votes\", mu=mu, sigma=sigma, observed=vote_std)\nLooking good! Surely, nothing will go wrong when we attempt to fit this model.\nwith partial_pool_model:\n    trace_partial_pool = pm.sample(random_seed=42)\nSomething went wrong when we attempted to fit this model.\n\n24.5.2.1.1 The Peril is in the Priors\nRoughly translated, the series of warnings we received can be interpreted as: “The sampling process didn’t go well”. One of the benefits to working with PyMC’s default sampler is that it is very noisy. It will loudly complain whenever anything goes wrong. As annoying as it can be sometimes, you would do well to view this behaviour as a good thing: whenever your sampler is having trouble with your model, it means there’s probably something wrong with your model.\nOur largest cause for concern is the number of ‘divergences’ that the sampling process returned. The sampler records a divergence whenever the proverbial marble in the idiomatic skate bowl ends up somewhere that shouldn’t be physically possible: it has ended up buried beneath the terrain, or bounced completely clear of the skate park and is zipping around the city causing chaos.\nLet’s examine our trace plot (Figure 24.3) to see the extent of the damage:\nwith partial_pool_model:\n    az.plot_trace(trace_partial_pool, ['alpha_mu', 'beta_mu', 'alpha_sigma', 'beta_sigma', 'sigma'], compact=True)\n    plt.savefig('figures/28_03.png', dpi=300)\n\n\n\n\n\n\nFigure 24.3: png\n\n\n\nThis isn’t a completely unmitigated disaster, but the problems are apparent enough that we should go through them in detail. The first thing you might notice here is that our traces don’t meet the criteria we laid out last chapter:\n\nThe chains are not stationary: some of the traces in beta_mu and beta_sigma seem to occasionally meander away from the overall mean and then get stuck in a local minima for long periods of time.\nThe chains are not mixing well: some of the traces alternate between rapidly zipping from one extreme to another (which is fine) and slowly moving in a single direction for 50 samples at a time or more (which is not fine).\nThe chains have not converged: the lower end of beta_sigma has some real issues.\n\nDespite everything, overall state of this posterior sampling trace isn’t too bad; if you went ahead with the model as-is, you would probably draw inferences that are pretty close to what you would have gotten from a better-behaved trace… I wouldn’t be in a hurry to submit this to any peer-reviewed journal, though. The bottom line is this: we can do better. To do so, we’re going to have to find a less chaotic way of throwing our marble around this 101-dimensional skate bowl.\n\n\n\n\n24.5.3 Informative Priors: A Spoonful of Information Makes the Sampler Calm Down\nThere are many ways to tame a rebellious model. We don’t have the space here to cover some of the better ones (reparameterization is the usual go-to), but we wanted to show that informative priors can be used to improve sampling.\nIn order to keep things simple, we’ve specified each of our spend-vote models thus far using only two different distributions:\n\nThe Normal Distribution, which we’ve used for parameters that could, theoretically, take on any value on the real number line, and\nThe Exponential Distribution, which we’ve used as priors for our standard deviation parameters, which must fall somewhere between 0 and positive infinity.\n\nOur strategy for calming down our out-of-control model is going to involve tightening up our Normal distributions and swapping out our Exponential distributions for Gamma distributions (another distribution we did not explicitly discuss in Chapter 26, but which you now know how to learn about).\nFirst, “tightening our normals.” All I mean by this is that instead of using the wide, nearly flat priors we’ve been using thus far, we’re going to shrink them down to cover a much smaller part of the real number line. You can see what I mean by looking at the priors for the model specified below.\nThe switch from an Exponential distribution to a Gamma distribution for our standard deviation parameters needs a bit more explanation. The Gamma distribution is similar to the Exponential in the sense that both can only take on values from 0 to positive infinity (a property we need for our standard deviations), and that they tend to peak early and have long tails. In the interests of avoiding technical jargon, the Gamma distribution will let us ‘scoop’ a bit of the probability density away from 0, which is likely the cause of our woes here.\nA quick note: the priors we’re using here are designed to demonstrate how information can be used to combat model degeneracy. As such, they’re a little more strongly informative than you might expect to see in published literature, but not by that much. With these informative priors in place (and no other changes), let’s examine how our model behaves:\nwith pm.Model() as partial_pool_model_regularized:\n    \n    # Hyperpriors\n    alpha_mu = pm.Normal(\"alpha_mu\", mu=0.1, sigma=0.3)\n    beta_mu = pm.Normal(\"beta_mu\", mu=0.1, sigma=0.3)\n    alpha_sigma = pm.Gamma(\"alpha_sigma\", alpha=4, beta=0.10)\n    beta_sigma = pm.Gamma(\"beta_sigma\", alpha=4, beta=0.10)\n    \n    # Priors\n    alpha = pm.Normal(\"alpha\", mu=alpha_mu, sigma=alpha_sigma, shape=n_states)\n    beta = pm.Normal(\"beta\", mu=beta_mu, sigma=beta_sigma, shape=n_states)\n    sigma = pm.Gamma(\"sigma\", alpha=4, beta=0.10)\n    \n    # Linear Model\n    mu = pm.Deterministic(\"mu\", alpha[state_idx] + (beta[state_idx]*spend_std))\n    \n    # Likelihood\n    votes = pm.Normal(\"votes\", mu=mu, sigma=sigma, observed=vote_std)\n    \n    # Run Sample Traces\n    trace_partial_pool_regularized = pm.sample(\n        random_seed=42\n    )\nBoom! No more divergences! The sampler still had a few grievances to air (at least one of our parameters was sampled very inefficiently), but we should interpret the lack of divergences as permission to manually examine our trace plots (Figure 24.4):\nwith partial_pool_model_regularized:\n    az.plot_trace(trace_partial_pool_regularized, ['alpha_mu', 'beta_mu', 'alpha_sigma', 'beta_sigma', 'sigma'], compact=True)\n    plt.savefig('figures/28_04.png', dpi=300)\n\n\n\n\n\n\nFigure 24.4: png\n\n\n\nThere are a couple of hiccups, and the alpha_sigma traces are verging on non-convergence, but there’s nothing to be too concerned about.\nwith partial_pool_model_regularized:\n    ppc = pm.sample_posterior_predictive(trace_partial_pool_regularized, var_names=['votes', 'alpha_mu', 'beta_mu', 'alpha_sigma', 'beta_sigma', 'alpha', 'beta', 'sigma', 'mu'])\nWe won’t include the alpha or beta parameters in our ArviZ summary because there are 96 of them in total. No sense reading them all. Instead, we’ll focus on our hyperpriors:\nwith partial_pool_model_regularized:\n    summary = az.summary(trace_partial_pool_regularized, round_to=2, var_names=['alpha_mu', 'beta_mu', 'alpha_sigma', 'beta_sigma', 'sigma'])\n    \nsummary[['mean', 'sd', 'r_hat']]\nIt appears that there was a bit of an issue with beta_sigma; our assessment of beta_sigma is backed by the r_hat value of 1.01 (anything noticeably greater than 1.00 indicates something’s amiss). It’s not high enough to be a true cause for concern, but it’s worth pointing out.\nNow that we’ve fit our hierarchical model, let’s visualize the results (Figure 24.5):\nplot_2020_partial_pool(\n    partial_pool_model_regularized,\n    trace_partial_pool_regularized,\n    trace_no_pool,\n    n_states, \n    state_idx,\n    spend_std,\n    vote_std,\n    ppc,\n    state_cat\n)\nplt.savefig('figures/28_05.png', dpi=300)\n\n\n\n\n\n\nFigure 24.5: png\n\n\n\nThe above plots should look familiar: they’re very similar to the ones we used to investigate the results from our unpooled model above. All of the elements they share in common are the same, only for our latest model:\n\neach of the points represents a single congressional district\nthe black line represents the regression line from our partially-pooled model, and\nthe small and large bands around the lines represent the 94% highest density interval of posterior probability for the regressor and the predictions, respectively.\n\nThere’s one additional element here, though. The grey lines represent the regression lines from the unpooled model; I included them here to facilitate comparison between the partially-pooled and unpooled models.\n\n\n24.5.4 Shrinkage\nLet’s dig into these lines a little. First of all, a cursory glance at the previous model’s more outlandish conclusions shows that things have been calmed down considerably. Each of the states with downward-sloping regression lines (predicting worse voting outcomes in districts where Democrats spent more) – such as Alabama, Maine, and New Mexico – have been pulled back from the brink. In the opposite direction, some of the more steeply positive states (such as Kentucky, where the unpooled model predicted that a single standard deviation increase in relative spending for the Democrats would net two standard deviations’ worth of votes) have been reined in.\nAnother thing you might notice is that each of the single-district states1 (Wyoming, Vermont, Rhode Island, etc.) have had their regression lines change from perfect fits (where the grey line travels straight through the sole point of data) to more ‘standard’ fits (where the black line misses the point, often by a good margin). That’s not to claim that all of their black lines are identical: they’re not (compare Rhode Island with Montana). Instead, what the model is telling us is that the posterior distribution for each of these individual states isn’t all that much different from the distribution all states are drawing from.\nWhat’s happening here is that all of the states who have had their regression lines ‘calmed down’ by the model are being regularized by the impact of our model’s prior. Unlike a single-level model, however, we didn’t choose this prior: the model learned it from the data!\nThis is the power of the hierarchical model; it adaptively learns how to straddle the line between underfitting and overfitting, leveraging regularizing probability distributions to calm down overeager predictions. The net effect is that our partially pooled model, at least compared with the unpooled model, has ‘shrunk’ the posterior distribution, causing the model’s predictions to crowd more tightly around a more conservative predictor. This phenomenon is known as ‘shrinkage’.\nA final parting thought on this topic: you may have noticed that the larger states such as Texas, New York, and California – all of which already had fairly reasonable regression lines – barely changed at all. Each of them were endowed with enough observations that they could largely overwhelm the regularizing influence of the priors.\n\n\n24.5.5 Does the Model Fit? Posterior Predictive Plots\nAdding variables to a model can be an excellent way to explore relationships that simply couldn’t be tackled without some form of multivariate regression. It is unfortunate, then, that adding even a small handful of variables results in a model that the human brain can no longer perceive visually. A regression with just a predictor and an outcome variable is simple: you can capture everything in a two-dimensional scatterplot, with the predictor on the horizontal axis and the outcome on the vertical. A regression with two independent variables (say, a predictor and a control) and one outcome variable is less simple, but doable: you’ll occasionally see them visualized as a 3-dimensional plot with the two independents on the horizontal axes, and with a plane representing the regressor. Anything more than this is practically impossible, as the ‘line’ of best fit becomes a hyperplane that our brains are incapable of visualizing.\nIn this sad state of affairs, we’re forced to turn to plots that collapse high-dimensional space back down into a 2-dimensional plane that allows us to see the distance between our model’s predictions and the true value of the observations. The resulting plot is called a ‘posterior predictive plot’, and is useful for assessing how well our model has fit the data. The main strength of this method is that it scales well into an arbitrarily large number of dimensions: we can picture all of the observations using a single plot. The downside is that we lose some of the nuance that we’d get from other visualizations. Here’s how to make one (results shown in Figure 24.6):\nmu_hpd = az.hdi(ppc.posterior_predictive[\"mu\"], 0.89)\nD_sim = ppc.posterior_predictive[\"votes\"].mean(dim=(\"chain\", \"draw\")).values\n\n# Calculate the error bars using the correct dimension names from mu_hpd\nyerr_lower = D_sim - mu_hpd.sel(hdi='lower').mu.values\nyerr_upper = mu_hpd.sel(hdi='higher').mu.values - D_sim\nyerr = np.vstack([yerr_lower, yerr_upper])\n\nfig, ax = plt.subplots(figsize=(6, 6))\n\n# Apply the calculated error bars\nplt.errorbar(\n    vote_std,\n    D_sim,\n    yerr=yerr,\n    fmt=\"C0o\",\n)\n\nax = sns.scatterplot(x=vote_std, y=D_sim, s=1, color='darkgray')\n\nmin_x, max_x = vote_std.min(), vote_std.max()\nax.plot([min_x, max_x], [min_x, max_x], \"k--\")\n\nax.set_ylabel(\"Predicted vote differential\")\nax.set_xlabel(\"Observed vote differential\")\n\nsns.despine()\nplt.savefig('figures/28_06.png', dpi=300)\n\n\n\n\n\n\nFigure 24.6: png\n\n\n\nEach of the points is an observation from our dataset. They’re arranged along the horizontal axis according to their observed (actual) value, and along the vertical axis according to where our model thinks they should be. The vertical lines around each point indicate the range that contains 94% of the posterior probability for that particular observation (remember, in a Bayesian model, everything comes with a healthy dose of uncertainty).\nExamining the plot above, we can see that our model does a passable job of retrodicting the voting data using nothing but the ‘state’ and ‘spend’ variables. We can, however, see some real problems at the far-right side of the plot: not only is our model incorrect about almost every district which lays more than ~1.5 standard deviations above the mean, it is confidently incorrect about them. In the next section, we’re going to see if we can do better.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Multilevel regression</span>"
    ]
  },
  {
    "objectID": "multilevel-regression.html#the-best-model-our-data-can-buy",
    "href": "multilevel-regression.html#the-best-model-our-data-can-buy",
    "title": "24  Multilevel regression",
    "section": "24.6 THE BEST MODEL OUR DATA CAN BUY",
    "text": "24.6 THE BEST MODEL OUR DATA CAN BUY\nNow that we have established a Hierarchical baseline and introduced a method for visualizing results from models whose regression ‘lines’ are in higher dimensions (and, thus, aren’t lines any longer, but rather hyperplanes), we can start to add variables in an effort to improve model fit. Unfortunately, we don’t have the room here to report all the proper checks every time we add a variable. Instead, I’ll run you through how to add each of the remaining variables in the dataset and then present the finished model.\nwith pm.Model() as full_hierarchical_model:\n    \n    # Hyperpriors\n    alpha_mu_state = pm.Normal(\"alpha_mu_state\", mu=0.1, sigma=0.3)\n    alpha_sigma_state = pm.Gamma(\"alpha_sigma_state\", alpha=4, beta=0.10)\n    beta_mu_spend = pm.Normal(\"beta_mu_spend\", mu=0.1, sigma=0.3)\n    beta_sigma_spend = pm.Gamma(\"beta_sigma_spend\", alpha=4, beta=0.10)\n    \n    # Priors from Hyperpriors\n    alpha_state = pm.Normal(\"alpha_state\", mu=alpha_mu_state, sigma=alpha_sigma_state, shape=n_states)\n    beta_spend = pm.Normal(\"beta_spend\", mu=beta_mu_spend, sigma=beta_sigma_spend, shape=n_states)\n    \n    # Priors\n    beta_pvi     = pm.Normal(\"beta_pvi\", mu=1, sigma=0.3)\n    beta_rep_inc = pm.Normal(\"beta_rep_inc\", mu=-0.5, sigma=0.2)\n    beta_dem_inc = pm.Normal(\"beta_dem_inc\", mu=0.5, sigma=0.2)\n    sigma = pm.Gamma(\"sigma\", alpha=4, beta=0.10)\n    \n    # Linear Model\n    mu = pm.Deterministic(\"mu\", \n                         alpha_state[state_idx] + \n                         beta_spend[state_idx] * spend_std +\n                         beta_pvi * pvi_std +\n                         beta_rep_inc * rep_inc +\n                         beta_dem_inc * dem_inc \n                         )\n    \n    # Likelihood\n    votes = pm.Normal(\"votes\", mu=mu, sigma=sigma, observed=vote_std)\n    \n    # Run Sample Traces\n    trace_full_hierarchical_model = pm.sample(\n        target_accept=0.97,\n        random_seed=42\n    )\nLet’s produce the trace plots for our full_hierarchical_model (Figure 24.7).\nwith full_hierarchical_model:\n    az.plot_trace(trace_full_hierarchical_model, \n                  [\n                      'alpha_mu_state', \n                      'alpha_sigma_state', \n                      'beta_mu_spend', \n                      'beta_sigma_spend', \n                      'beta_pvi',\n                      'beta_rep_inc',\n                      'beta_dem_inc',\n                      'sigma',\n                  ], compact=True)\n    plt.savefig('figures/28_07.png', dpi=300)\n\n\n\n\n\n\nFigure 24.7: png\n\n\n\nThere are a few worrying signs here (the alpha_mu_state isn’t sampling as efficiently as we would prefer), but nothing serious enough to call the model into question entirely! Time to take a peek at our model fit (Figure 24.8):\nwith full_hierarchical_model:\n    ppc = pm.sample_posterior_predictive(trace_full_hierarchical_model, var_names=['votes', 'mu'])\nmu_hpd_mean = mu_hpd['mu'].mean(dim=\"hdi\").values\n\nyerr = np.abs(ppc.posterior_predictive[\"votes\"].mean(dim=(\"chain\", \"draw\")).values - mu_hpd_mean)\n\nplt.errorbar(\n    vote_std,\n    ppc.posterior_predictive[\"votes\"].mean(dim=(\"chain\", \"draw\")).values,\n    yerr=yerr,\n    fmt=\"C0o\",\n)\n\nax = sns.scatterplot(x=vote_std, y=D_sim, s=1, color='darkgray')\n\nmin_x, max_x = vote_std.min(), vote_std.max()\nax.plot([min_x, max_x], [min_x, max_x], \"k--\")\n\nax.set_ylabel(\"Predicted vote differential\")\nax.set_xlabel(\"Observed vote differential\")\n\nsns.despine()\nplt.savefig('figures/28_06.png', dpi=300)\n\n\n\n\n\n\nFigure 24.8: png\n\n\n\nMuch, much better. The addition of two categorical variables (Democratic and Republican incumbency), and one continuous variable (the Cook Partisan Voting Index) has allowed our model’s predictions to fall closer to the observed values across the board.\nEven though the above plot does a good job of showing us how well our model fit the data in general, it tells us nothing about the parameters of interest! For that, we’re going to need to turn to a form of plot that can condense large amounts of coefficient information into a relatively small space (we have 51 to examine, after all). It’s called a forest plot. Figure 24.9 is the forest plot for our model.\nstate_labels = list(state_cat.categories) \nadditional_labels = ['PVI', 'Democratic Incumbency', 'Republican Incumbency', 'Spending Differential']\nall_labels = state_labels + additional_labels\n\nax_array = az.plot_forest(trace_full_hierarchical_model,\n                          var_names=['beta_pvi', 'beta_dem_inc', 'beta_rep_inc', 'beta_spend'],\n                          combined=True,\n                          quartiles=False)\n\nax = ax_array[0]\n\n# print(f\"Number of labels: {len(all_labels)}\")\n# print(f\"Number of tick locations: {len(ax.get_yticks())}\")\n\nif len(all_labels) &gt; len(ax.get_yticks()):\n    all_labels = all_labels[:len(ax.get_yticks())]\n\nax.set_yticklabels(all_labels)\nax.set_title(\"Coefficients for spending differentials, incumbency, and PVI\")\n\nsns.despine(left=False, bottom=False, top=False, right=False)\nplt.show()\n\n\n\n\n\n\nFigure 24.9: png\n\n\n\nAfter all that, we can finally settle down to the task of interpreting our handiwork.\nLooking at the forest plot above, you might be struck by two countervailing trends. The first is that two out of the three variables we added in this latest model have a strong impact on its predictions. It should come as no surprise that the Partisan Voting Index is strongly positively correlated with vote differential, and that Republican incumbency has a predictably negative effect, showing the impact of Republicans’ prior experience in helping them win their races.\nThe other trend that you might notice is that most of the rest of the model’s claims are relatively tepid. Starting at the top: Democratic incumbency has a positive impact on the Democrats’ vote margins, but it isn’t as significant a boost as was the case for their Republican counterparts – a little under half. Finally, the 94% Highest Density Interval for most of the states’ spending coefficients show only a weak effect, if any at all. Most of the coefficient estimates are above zero, but a good portion of their HDI ranges straddle 0, meaning that our model hasn’t really ruled out the idea that spending has no (or even a negative) effect on Democratic margins of victory. Of all the States under consideration, only in Georgia, Maryland, Michigan, North Carolina, Oregon, and Wisconsin does our model see unambiguous evidence of a positive effect from Democrats outspending Republicans.\nOne interpretation of these results is that Democratic spending advantages don’t often translate into vote advantages. An equally valid interpretation, and one which takes into account the specific historical context, is that Democrats did a good job of funneling their resources towards close races in states whose districts were ripe for a Democratic breakthrough. They didn’t succeed in all cases, but were able to translate their funding advantage in North Carolina and Georgia into a total of three seats flipped in their favour (the Democrats neither gained nor lost seats in any of the other states mentioned above).\nIs this enough? Should we, at this point, be satisfied?\nNo.\nAnytime we create a model with more than two variables, it is incumbent upon us to think through the causal implications of what we have done. To once again paraphrase Richard McElreath, whose Statistical Rethinking has had a major influence on our discussion of Bayesian regression, a regression model is implicitly asking a set of questions about each of its variables simultaneously: that question almost always boils down to something like “how much predictive information does this variable contain, once the effect of all the other variables is known?” Since we’re interested in the influence of spending differentials on vote differentials across different states, we’re implicitly using our regression model to ask: “what is the value of knowing the effect of the spending differential once the effects from incumbency and the partisan voting index are already known?”\nHere’s a plausible explanation for what’s happening here. The negative effect that we’re seeing in Texas might be indicative of a concerted Democratic push to try and flip historically Republican-leaning districts. If you paid attention to the 2020 American election, you might know that Democrats generally underperformed in the 2020 House races, and as such, our model may be picking up on an effect wherein Democrats funneled cash into break-even or Republican-leaning districts, only to be rebuffed. To do this, they would have presumably had to move cash away from safer Democratic-leaning districts in metro Houston and Austin. Under such circumstances, it might be entirely plausible that, once we control for all the other variables in the model, Democrats were more likely to lose ridings they overspent on, and win those they underspent on.\nAnother problem is that our model is helpless to tell us if the presumed causal relationship underpinning the whole shebang (namely, that money helps win elections) is justifiable. It’d be just as easy to claim that our predictor (money spent) and outcome (votes received) variables share no direct causal link, and instead share ‘political popularity’ as a common cause. The logic there being that popular candidates might be more likely to attract donations AND votes. Modelling can’t help us here. The best regression model in the world, be it Bayesian or Frequentist, wouldn’t be able to help you determine if there’s any validity to your assumptions of causality. If you’re interested, computational social science is in the midst of a causal inference renaissance, and thanks primarily to people like the brilliant computer scientist Judea Pearl, that renaissance is Bayesian. Unfortunately, this is another area of computational social science that we simply don’t have the space to cover in this book.\nThere is, in fact, a lot more we could get into here, but our hands are tied by space constraints and by the fact that this book is not a Bayesian regression textbook. Hopefully you’ve seen that it is possible, and even fun, to build an interesting, functional, and sophisticated statistical model from scratch. Its imperfections represent room for improvement, not something to be feared or ashamed of.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Multilevel regression</span>"
    ]
  },
  {
    "objectID": "multilevel-regression.html#the-fault-in-our-lack-of-stars",
    "href": "multilevel-regression.html#the-fault-in-our-lack-of-stars",
    "title": "24  Multilevel regression",
    "section": "24.7 THE FAULT IN OUR (LACK OF) STARS",
    "text": "24.7 THE FAULT IN OUR (LACK OF) STARS\nSomewhere, someone in the world is saying something like “Hold up, John. This is a chapter on regression analysis! Where are the hypothesis tests!? Where are the little stars that tell me if I can publish my findings or not?”\nAs these chapters have probably made abundantly clear, that’s not what we’re doing here. The Bayesian statistical paradigm is capable of comparing hypotheses in the way Frequentists think of such things, but are generally loath to do so. This is because we already have access to a very broad range of hypotheses defined by a posterior probability distribution, and that distribution already contains all of the information we can possibly derive from our model for a given set of data. Anything else we do – plotting, calculating HDI, hypothesis testing – is simply a summarization of that posterior distribution.\nIf you feel like comparing hypotheses in the style of a Frequentist, go for it. All of the Bayesian regression models we fit today contain infinite hypotheses (in multiple dimensions!) and the probability of any individual from among them (say, \\(\\beta = 3\\)) being ‘true’ (whatever that means) is 0. We’ve already covered why that’s the case.\nYou could compare ranges of hypotheses against a ‘null’ of sorts, but the Bayesian paradigm ensures that a simple posterior plot is all that is needed to quickly ascertain whether or not most of the posterior probability for any given parameter is credibly distant from 0, which is all null-hypothesis significance testing does really does anyhow.\nInstead of using null-hypothesis significance testing, consider treating each model as its own ‘hypothesis’ of sorts. Gelman and Shalizi (2013) advocate a paradigm wherein whole models are judged by how well they fit data (both in-sample and out-of-sample). Accepted models are used until their flaws become too egregious to ignore, at which point new, better models are developed using insights from the failings of the previous one. It’s a different way of doing science than you might be used to, but it’s worth knowing about: the winds of change are blowing decisively away from the traditional null hypothesis testing paradigm.\n\nFurther Reading\nAs I mentioned in several previous chapters, there are a number of outstanding resources that you can now turn to to continue your journey into Bayesian regression analysis. I especially recommend McElreath (2020), Lambert (2018), Kruschke (2014), and Martin (2018). Finally, Lynch and Bartlett (2019) offer a literature review of the use of Bayesian statistics in sociology.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Multilevel regression</span>"
    ]
  },
  {
    "objectID": "multilevel-regression.html#conclusion",
    "href": "multilevel-regression.html#conclusion",
    "title": "24  Multilevel regression",
    "section": "24.8 CONCLUSION",
    "text": "24.8 CONCLUSION\n\n24.8.1 Key Points\n\nAny regression dataset that features at least one clustering variable should be modelled, by default, using partially pooled hierarchical regression; any simpler models should only be used if justified\nWide, uninformative priors can cause a large number of divergences during sampling using an HMC sampler; using tighter, more informative priors can help ameliorate this.\nHigher-dimensional regression models (those with more than two variables) are difficult (if not impossible) to fully visualize – we can instead turn to specialized visualizations to assess model fit (via retrodiction) and model parameters (via forest plots)\nThe First Rule of Bayes Club is that we don’t do p-values, stars, or null hypothesis significance testing (exceptions apply)\n\n\n\n\n\nGelman, Andrew, and Cosma Rohilla Shalizi. 2013. “Philosophy and the Practice of Bayesian Statistics.” British Journal of Mathematical and Statistical Psychology 66 (1): 8–38.\n\n\nKruschke, John. 2014. “Doing Bayesian Data Analysis: A Tutorial with r, JAGS, and Stan.”\n\n\nLambert, Ben. 2018. A Student’s Guide to Bayesian Statistics. Sage.\n\n\nLynch, Scott, and Bryce Bartlett. 2019. “Bayesian Statistics in Sociology: Past, Present, and Future.” Annual Review of Sociology 45: 47–68.\n\n\nMartin, Osvaldo. 2018. Bayesian Analysis with Python: Introduction to Statistical Modeling and Probabilistic Programming Using Pymc and ArviZ. Packt Publishing Ltd.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. CRC press.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Multilevel regression</span>"
    ]
  },
  {
    "objectID": "multilevel-regression.html#footnotes",
    "href": "multilevel-regression.html#footnotes",
    "title": "24  Multilevel regression",
    "section": "",
    "text": "Note that many of these states aren’t actually single-district states, but rather only have one valid district in the dataset because of the filtering we had to do↩︎",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Multilevel regression</span>"
    ]
  },
  {
    "objectID": "generalized-linear-models.html",
    "href": "generalized-linear-models.html",
    "title": "25  Generalized Linear Models",
    "section": "",
    "text": "Coming in fall 2024.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Generalized Linear Models</span>"
    ]
  },
  {
    "objectID": "structural-causal-models.html",
    "href": "structural-causal-models.html",
    "title": "26  Structural causal models",
    "section": "",
    "text": "Coming in fall 2024.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Structural causal models</span>"
    ]
  },
  {
    "objectID": "modeling-texts-lda.html",
    "href": "modeling-texts-lda.html",
    "title": "27  Modeling text with LDA topic models",
    "section": "",
    "text": "27.1 LEARNING OBJECTIVES",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Modeling text with LDA topic models</span>"
    ]
  },
  {
    "objectID": "modeling-texts-lda.html#learning-objectives",
    "href": "modeling-texts-lda.html#learning-objectives",
    "title": "27  Modeling text with LDA topic models",
    "section": "",
    "text": "Explain how variational inference differs from Hamiltonian Monte Carlo sampling, conceptually\nDescribe the distinction between deterministic and stochastic/generative topic models\nExplain what Latent Dirichlet Allocation is and how it works\nExplore the use of Semantic Coherence as an evaluation metric",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Modeling text with LDA topic models</span>"
    ]
  },
  {
    "objectID": "modeling-texts-lda.html#learning-materials",
    "href": "modeling-texts-lda.html#learning-materials",
    "title": "27  Modeling text with LDA topic models",
    "section": "27.2 LEARNING MATERIALS",
    "text": "27.2 LEARNING MATERIALS\nYou can find the online learning materials for this chapter in doing_computational_social_science/Chapter_30. cd into the directory and launch your Jupyter Server.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Modeling text with LDA topic models</span>"
    ]
  },
  {
    "objectID": "modeling-texts-lda.html#introduction",
    "href": "modeling-texts-lda.html#introduction",
    "title": "27  Modeling text with LDA topic models",
    "section": "27.3 INTRODUCTION",
    "text": "27.3 INTRODUCTION\nThis chapter serves three purposes: (1) introduce you to generative topic modelling and Bayesian latent variable modelling more generally, (2) explain the role that graphical models can play in developing purpose-made generative models, and (3) introduce you to another computational approach for approximating the posterior called “variational inference.”\nWe’ll start by introducing the logic behind generative topic modelling. Then, we will discuss the technical details of one of the most widely-used topic models: latent Dirichlet allocation (LDA). Then, we’ll cover the basics of approximating the posterior using an alternative to HMC called variational inference. In the second section, we’ll start developing LDA topic models with Gensim, discuss quantitative measures of coherence, and show how to visualize topic models.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Modeling text with LDA topic models</span>"
    ]
  },
  {
    "objectID": "modeling-texts-lda.html#generative-topic-models",
    "href": "modeling-texts-lda.html#generative-topic-models",
    "title": "27  Modeling text with LDA topic models",
    "section": "27.4 GENERATIVE TOPIC MODELS",
    "text": "27.4 GENERATIVE TOPIC MODELS\nYou’ve likely noticed that “latent” is a not-so-latent theme in this book. Recall Chapter 9. Sometimes our data has highly correlated variables because they arise from a shared latent factor or process. That may even be by design, like when we collect data on low-level indicators of an abstract and unobservable concept that we want to measure. We later extended that idea to text analysis by discussing latent semantic analysis, which used deterministic matrix factorization methods (truncated SVD) to construct a set of latent thematic dimensions in text data. In other chapters, we’ve touched on “latent” variables in a variety of different ways, including regression models and latent network structure.\nLatent variables are central to Bayesian thinking. When we develop models in the Bayesian framework, we define joint probability distributions with both latent and observed variables, and then use an inference algorithm such as HMC or variational inference (introduced below) to approximate the posterior distribution of each latent variable conditional on the evidence provided by the observed variables. This is an extremely flexible and mathematically-principled way of working with latent variables that you can use to develop probabilistic models for just about any research. So, what exactly would a Bayesian approach to modelling latent thematic structure – topics – in text data look like?\nGenerative topic models got their start when the computer scientists David Blei, Andrew Ng, and Michael Jordan (2003) published a classic paper proposing the model we will focus on in this chapter. This model and many variants of it are widely used in the social sciences and digital humanities, some of them developed by social scientists (e.g., M. Roberts et al. 2013; M. E. Roberts et al. 2014). Broadly speaking, generative topic models are a family of Bayesian models that assume, like Latent Semantic Analysis, that documents are collections of thematically linked words. Rather than using matrix factorization methods to understand latent themes, most generative topic models approach this as just another latent variable problem.\nAlthough applicable in many contexts, we should be especially open to using generative topics models if:\n\nWe have data in the form of many text documents.\nWe know that each document contains words that represent different themes, or topics, in various proportions.\nWe want to infer the distribution of latent topics across documents given the words we observe in them.\nWe have a plausible mechanism, or a “simple generative story,” of the relationship between our latent and observed variables that will help us accomplish (3): documents and their specific combinations of words are “generated from” a mixtures of latent themes, which are themselves mixtures of words.\n\nIn the case of text data, posing a generative mechanism means thinking through the reasons why some words co-occur in documents while others tend not to. Something influences those relationships; our word choices are not random. What might lead a politician making a speech, or a scientist writing journal articles, to select some combination of words but not others? Why does elephant floral own snowstorm aghast the rock cat? (see what I did there?)\nMany generative mechanisms have been posited and tested by different types of topic models, including some that are designed to take information about speakers / authors into account in a regression model-like framework (e.g., M. Roberts et al. 2013; M. E. Roberts et al. 2014; Rosen-Zvi et al. 2012) or to account for the ordering / sequence of words (e.g., D. M. Blei and Lafferty 2006; Wang, Blei, and Heckerman 2012), but the foundational generative mechanism that unites all topic models is that the particular mixture of words that show up in documents are related to a latent set of underlying themes, latent topics. The topics we discuss are one of the key factors that determine the probability of selecting one word over another.\nWe learn about the underlying latent topics by constructing a probabilistic model and then using an inference algorithm to approximate the posterior. We will construct a latent Dirichlet allocation topic model (a subtype within the more general class of mixed membership models), which revolutionized natural language processing and probabilistic machine learning in the early 2000s. It remains a widely used model, alongside many variations.\nThe goal of approximating the posterior with an LDA topic model is learning about the distribution of latent topics over: (i) documents and (ii) words. Say we have a journal article about social movements focused on energy transitions. There are likely quite a few latent topics in this article but it’s safe to say that the dominant ones are probably “social movements” and “energy transitions.” Of course, the model doesn’t actually know what “social movements” and “energy transitions” are, so it might tell us that the article in question is 17% “topic 11,” 12% “topic 2,” and then many other topics in much smaller proportions. Note that these are mixtures; documents always consist of multiple topics, though one may be dominant.\nEvery word in our document has a different probability of appearing in each of the topics we find. The words “movement,” “mobilization,” “collective,” and “protest” may have a high probability of appearing in topic 11 (which we interpret as “social movements”). The words “environmental,” “transition,” “energy,” “oil,” “renewable,” and “pipeline” may have a high probability of appearing in “energy transitions and politics,” “topic 2,” but a relatively low probability of appearing in social movements topics. Other words, such as “medical,” and “healthcare” will have a low probability of appearing in either topic (assuming they appear at all), but they might have a high probability of appearing in a topic about “health” (which itself has a low probability of appearing in the article.\nThis notion that words have different probabilities of appearing in each topic makes it possible for the same word to have a high probability of appearing in more than one topic depending on its use (we discuss this more in the next chapter). For example, the word “policy” might have a high probability of appearing in both topics. This turns out to be a major benefit of generative topic models. As prominent sociologists of culture have pointed out, this brings the topic modelling framework close to relational theories of language and meaning that have been influential in the social sciences for quite some time (e.g. DiMaggio, Nag, and Blei 2013; Mohr and Bogdanov 2013).\nNow that we’ve built a bit of intuition about what a generative topic model might look like, let’s get into some of the technical modelling details. Then we’ll use these models to analyze latent topics in 1,893,372 speeches made in Parliament by Canadian politicians over a thirty-year period (1990 - 2020).\n\n27.4.1 Latent Dirichlet Allocation (LDA)\n\n\n27.4.2 LDA as a Graphical Model\nIn previous chapters, we developed and described our Bayesian models using statistical model notation, and we briefly saw graphical models as an alternative in the context of regression modelling. Graphical models are a powerful tool for developing, critiquing, and communicating our probabilistic models in part because they make three key things explicit:\n\nthe origin of every variable in our model as either observed or latent (we’re always playing the “what’s that?” game introduced in Chapter 27), and\nour assumptions about the structure of statistical dependencies between all the variables in our model, and relatedly\nour assumptions about the generative processes that give rise to the data we observe.\n\nGraphical models are a favoured tool in probabilistic machine learning in particular (Koller and Friedman 2009; McElreath 2020; Pearl and Mackenzie 2018; Jordan 2004, 2003), and you’ll see them everywhere in the generative topic modelling literature. Though they can be a little confusing at first, they are transparent once you know how to read them. Let’s break down the graphical representation of a vanilla LDA topic model, shown in ?fig-30_01.\n\n\nIt is possible to produce graphical models like this automatically using pymc. Doing so can be very useful if you want to examine how your Bayesian models are structured. Consult the pymc documentation if you want to learn more!\n\nNow, what does it mean?\nFirst, each node in the graph represents a random variable, with observed variables shaded and latent variables unshaded. The black nodes are model hyperparameters. Each arrow in the graph represents a statistical dependency, or to be precise, conditional independence. The boxes (called plates) represent repetition over some set of items (replicates), like words or documents. Plates notation is very useful for condensing your graphical models. For example, without it, you would need a node for each document in a text analysis. We will soon analyze 1,893,372 political speeches. Now, imagine how many nodes we would need for the \\(\\text{W}_{d,n}\\) node.\nLet’s break down this specific graphical model starting with the plates. The large outer plate with the \\(D\\) in the bottom right corner represents all of the documents in our document collection. When we get to our model for political speeches, \\(D\\) will equal 1,893,372. Everything inside the document plate is repeated for each individual document in the document collection. In other words, it pertains to \\(D_i\\), where the index \\(_i\\) represents any given document in the dataset. The small inner plate with \\(N\\) in the bottom right represents the specific words and their position in the probability distribution for each topic. We’ll come back to this momentarily. The third plate, with \\(K\\) in the bottom right, represents the latent topics whose distributions we are computing. If we model 100 topics, then \\(\\beta_{k}\\) would be 100 probability distributions over terms.\nEvery document in our dataset is composed of a mixture of topics, with each topic being a probability distribution over words. Inside the document plate, then, \\(\\theta_{d}\\) represents the topic proportions for each document. Picture a matrix with documents in the rows and latent topics (represented by arbitrary numerical IDs) in the columns. Each document in our collection is made up of words. The gray node \\(W_{d,n}\\) represents each observed word \\(_n\\) for each document \\(_k\\), while \\(\\text{Z}_{d,n}\\) represents the topic assignments for each word in each document for each topic. In other words, each word in each document has a probability associating it with each topic. Imagine a matrix of probabilities with words in the rows and latent topics in the columns. \\(\\beta_{k}\\) represents the topics themselves, with \\(k\\) being the number of latent topics to model. The value of \\(k\\) is selected by the researcher; we’ll discuss that process shortly.\nThat leaves the black nodes \\(\\alpha\\) and \\(\\eta\\). These are priors for the parameters of the Dirichlet distribution, and we’ll discuss the options for these below. \\(\\alpha\\) is the “proportions parameter” and represents text-topic density. Think of this as the prior probability that a document will be associated with a topic. If we set \\(\\alpha\\) to a high value – say close to 1 – the probability of texts being associated with topics increases, and when \\(\\alpha\\) is set to a low value – say 0.1 – the probability decreases. \\(\\eta\\), on the other hand, represents topic-word densities. It’s known as the “topic parameter. When \\(\\eta\\) is set to a high value, the probability of a word being associated with a topic increases. When it is set low, the probability decreases.\nPutting this all in one convenient place, then,\n\n\\(\\beta_{k}\\) represents the latent topics themselves;\n\\(\\theta_{d}\\), inside the document plate, represents the latent topic proportions for each document;\n\\(\\text{Z}_{d,n}\\) represents the latent topic assignments for each word \\(_n\\) in each document \\(_d\\);\n\\(\\text{W}_{d,n}\\) represents each observed word \\(_n\\) in each document \\(_d\\)\n\\(\\alpha\\) represents the portions hyperparameter (the prior probability that a document is associated with a topic), and\n\\(\\eta\\) represents the topic hyperparameter (the prior probability that a word is associated with a topic).\n\nWe are representing a three-level Hierarchical Bayesian latent variable model with each document in a document collection modelled as a finite mixture of hidden topics in varying proportions, and with each topic modelled as an infinite mixture of words in varying proportions. It posits a generative relationship between these variables in which meaningful patterns of co-occurring words arise from the specific mixtures of latent themes. Altogether, it descibes the joint probability distribution for (a) the latent topics, (2) their distribution over documents, and (3) their distribution of words, or\n\\[\\begin{align}\nP(\\beta, \\theta, Z, W).\n\\end{align}\\]\nBut, we want to know the posterior, which is the probability of the topics, their distribution over documents, their distribution of words conditional on the observed words, or\n\\[\\begin{align}\nP(\\beta, \\theta, Z | W).\n\\end{align}\\]\nAs with other Bayesian models, we can’t derive the posterior from the joint distribution analytically because of the intractable denominator in Bayes theorem, and because the number of potential latent topical structures is exponentially large, so we turn to approximate posterior inference. That’s where variational inference comes in.\n\n\n27.4.3 The Dirichlet in Latent Dirichlet Allocation\nLike the probabilistic models we’ve developed, probabilistic topic models are built out of probability distributions! The ‘Dirichlet’ portion in Latent Dirichlet Allocation (often written as Dir(\\(\\alpha\\)) is just another probability distribution of the kind discussed in Chapter 26. It’s a generalization of the idea of a triangle (called the simplex), only it can have an arbitrary number of sides… What?\nThese kinds of descriptions (generalization of a triangle) are useful for those already deeply familiar with mathematical geometry or multidimensional probability distributions, but they’re unlikely to get the rest of us very far. That said, with a little scaffolding, this will quickly make sense. In the probability primer chapter, we established that some probability distributions only cover some parts of the real number line; the exponential distribution, for instance, only supports positive values. The ‘beta distribution’ takes this idea a bit further: it only supports values from 0 to 1, inclusive. It takes two parameters, \\(\\alpha\\) and \\(\\beta\\), which jointly control the shape of the curve. You can think of the two as representing inversely correlated axes, both trying to pull more of the probability density towards the side of the distribution that they’re more positive in (so \\(\\alpha\\) pulls to the right, towards 1, \\(\\beta\\) pulls to the left, towards 0). Here’s an example of one where \\(\\beta\\) is doing more of the pushing (?fig-30_02):\n\nThe beta distribution is remarkably flexible: you should look up some examples of the shapes it can take!\nSince the beta distribution only supports values from 0 to 1, what would it look like if we tacked on a second dimension to this distribution? See for yourself, in ?fig-30_03.\n\nBehold the Dirichlet distribution! The Dirichlet is a multi-dimensional generalization of the Beta distribution. In the diagram above, instead of 2 parameters (\\(\\alpha\\) and \\(\\beta\\)) having a tug-of-war along a real number line, we have 3 parameters having a 3-way tug-of-war (the probability is concentrated in areas closer to the red end of the colour spectrum). The shape they’re battling over is a simplex in 2 dimensions (which is just a triangle). If we add a third dimension, then our triangle becomes a pyramid (a 3-dimensional simplex), and we’ll have 4 parameters duking it out in a 4-way tug-of-war. Remember that because the Dirichlet distribution is a probability distribution, its density must integrate to 1; this makes the Dirichlet very useful for describing probability across a large number of mutually-exclusive categorical events.\nLike the other Bayesian models we’ve seen, LDA topic models require priors. The \\(\\alpha\\) and \\(\\eta\\) hyperparameters inform the generation of the Dirichlet distribution, and understanding them gives you much greater control over your model. If this discussion of priors reminds you of the chapters on Bayesian Regression, good! LDA models function in a very similar framework. In fact, we can present LDA in a similar format to those chapters!\nA few notes first. We’re going to include a long list of variables, including what each of them mean. Normally, we don’t do this kind of thing, because the variables in Linear Regression models are usually self-evident. In the case of LDA, most of the ‘data’ variables we’re using are calculated using some aspect of the corpus and beg explanation. The first three sections that follow (Data, Hyperparameters, and Latent Variables) are all simple descriptions. They all come together in the 4-line Model section at the end.\nData\n\\[\\begin{align}\nV &: ~~~\\text{integer} &  \\text{ [Number of Unique Terms in Vocabulary]}\\\\\nD &: ~~~\\text{integer} &  \\text{ [Number of Documents]}\\\\\nd &: ~~~\\text{integer, values [min:1, max:} D] &  \\text{ [Document ID]}\\\\\nN &: ~~~\\text{integer} &  \\text{ [Total Word Instances]}\\\\\nn &: ~~~\\text{integer, values  [min:1, max:} N] &  \\text{ [Word Instance]}\\\\\nK &: ~~~\\text{integer} &  \\text{ [Number of Topics]} \\\\\nk &: ~~~\\text{integer, values [min:1, max:}K] &  \\text{ [Topic]}\\\\\n\\end{align}\\]\nHyperparameters\n\\[\\begin{align}\n\\alpha &: ~~~\\text{vector of real numbers, length  } K & \\text{[Topic-in-Document Prior Hyperparameter]} \\\\\n\\eta  &: ~~~\\text{vector of real numbers, length  } V & \\text{[Term-in-Topic Prior Hyperparameter]} \\\\\n\\end{align}\\]\nLatent Variables\n\\[\\begin{align}\n\\theta_d &: ~~~K\\text{-simplex, Dirichlet-distributed}   &  \\text{ [Topic Distribution for Document  } d]  \\\\\n\\beta_k &: ~~~V\\text{-simplex, Dirichlet-distributed}    & \\text{[Word Distribution for Topic  } k] \\\\\n\\end{align}\\]\nModel\n\\[\\begin{align}\n\\theta_d &\\sim \\text{Dirichlet}(\\alpha)  && \\text{ for } d \\text{ in } 1 ... D  & \\text{  [Topic-in-Document Prior]} \\\\\n\\beta_k  &\\sim \\text{Dirichlet}(\\eta)   && \\text{ for } k \\text{ in } 1 ... K  & \\text{  [Term-in-Topic Prior]} \\\\\nz_{d,n}  &\\sim \\text{Categorical}(\\theta_d)  && \\text{ for } d \\text{ in } 1 ... D, n \\text{ in } 1 ... N &\\text{[Document-Topic Probability]} \\\\\nw_{d,n}  &\\sim \\text{Categorical}(\\beta_{z[d,n]}) && \\text{ for } d \\text{ in } 1 ... D, n \\text{ in } 1 ... N &\\text{[Likelihood]}\n\\end{align}\\]\nWhew, that’s a lot of variables! We’ve already discussed what some of them are (and how they function), but some remain enigmatic. Let’s discuss them in the abstract here.\n\n27.4.3.1 Understanding the \\(\\alpha\\) hyperparameter\nThe \\(\\alpha\\) parameter can be a relatively naive setting, or more informed. If it’s a simple scalar (ie. single value) it will be propagated into a matrix of expected topic probabilities for each document. In all cases, this matrix has a shape of n_topics x n_documents. When a single value is used for all of the topic, this is a symmetric prior. As you will soon see, this “a-priori” assumption actually matters, even though the LDA model will modify these values a lot. A symmetric prior essentially tells the model “I expect the probability of each topic being a topic in each document to be the same, and you will have to work very hard to tell me otherwise.” There are times where this assumption might actually be helpful for the model. In most cases, though, we want to use LDA to tell us something we don’t know about a corpus, with an unknown distribution of topics. In this case, an asymmetric prior is essential(H. M. Wallach, Mimno, and McCallum 2009; Syed and Spruit 2018). In the example from (Hoffman, Bach, and Blei 2010) this value is set at 1/num_topics, but they mention that this is for simplicity, and reference (H. M. Wallach, Mimno, and McCallum 2009) that asymmetric priors can also be used.\nAn asymmetric prior tells the LDA model that the probability of each topic in a given document is expected to be different, and that it should work on determining what those differences are. Unlike the symmetric prior, there is a lot of flexibility in the \\(\\alpha\\) hyperparameter for an asymmetric prior in Gensim. If a scalar is given, the model will incorporate the values from 1 to num_topics when it generates the prior. This means that each document has an array of topic probabilities that are all different, although each document will have the same array. Rather than a single scalar value, it’s also possible to pass an \\(\\alpha\\) array of expected probabilities that is informed by prior knowledge. This could be domain knowledge, but then we would again be left wondering whether we want to learn something or just confirm what we already know. Perhaps the most exciting option here, then, is to use Bayes to inform Bayes. A crucial part about the use of asymmetric priors for the \\(\\alpha\\) hyperparameter is that the LDA model becomes a lot less sensitive to the number of topics specified. This decreased sensitivity means we should be able to trust the assignment of topics by the model regardless of how much choice we give it.\nOf course, there’s more than one way to do this. Gensim implements an automatic hyperparameter tuning method, based on work by (Ypma 1995), where the model priors are updated at regular intervals during iterations of model training. This is convenient for a number of reasons, but in particular: we can train an LDA model on a random sample of the data, setting the model to update the priors as frequently and for as many iterations as we have time for. Then, the updated \\(\\alpha\\) prior can be used to model the entire corpus. As you will read about in the next chapter, this is a form of transfer learning, and it comes with many advantages.\nThe posterior results, theta, will be these priors fitted to the corpus, with which we can estimate unique topic probabilities for each document.\n\n\n27.4.3.2 Understanding the \\(\\eta\\) hyperparameter\nThe \\(\\eta\\) hyperparameter functions quite similarly to \\(\\alpha\\) in terms of technical implementation, but has very different assumptions and conceptual implications. The prior constructed from \\(\\eta\\) is the expected probability for each word being a part of each topic. This can, again, be initialized in a relatively simple way by providing a single value - Hoffman and Blei again use 1/num_topics. This time, the single value is used to populate an array of shape n_topics x n_words. This again results in a symmetric prior, but the conceptual implication is actually what we want - if we told the model that the probability of each word’s topic contribution should be different from the beginning, we would be directing the model to prefer some words to others. This could bias words away from contributing to topics that they should, or towards topics that they shouldn’t. This issue would tend to smooth out with a large amount of training data, but it’s safer to just start this parameter with uniform word probabilities. In reality, the words we use are versatile and many of them are very likely to be used in all topics. So conceptually, this prior should actually be a symmetric one, and we’ll look at some evidence for this shortly (H. M. Wallach, Mimno, and McCallum 2009).\nNonetheless, there are also asymmetric options for eta, although they’re very similar to alpha, so we won’t spend too much time rehashing the technical details. We can provide an array of probabilities for each word that will be their prior expected probabilities for each topic, or a matrix of shape num_topics x n_words to make the expected word probabilities specific to each word. The last option is to use the same prior update method introduced above for the \\(\\alpha\\) prior. We will demonstrate below that the latter method indicates that \\(\\eta\\) becomes fairly symmetrical after model training, suggesting that a simple symmetrical prior is the most efficient choice and will not result in a loss of accuracy.\nThe posterior results, beta, will be these \\(\\eta\\) priors fitted to the corpus, which can be used to calculate unique topic probabilities for each word, as well as the top probabilities of words forming each topic.\n\n\n\n27.4.4 Variational Inference\nWe have just described the structure of our generative model. The structure is independent of the inference algorithm that we use to approximate the posterior probabilities for \\(\\beta\\), \\(\\theta\\), and \\(Z\\). We’ve seen this kind of thing before; in the chapters on Bayesian Linear Regression, we defined our models using priors, likelihood, and a linear model, and then sampled from those models’ posteriors to produce final posterior distributions. We used pymc’s Hamiltonian Monte Carlo-like (HMC) sampler to accomplish this (it’s an easy, efficient, general-purpose approach), but we could have used any number of other techniques, such as a Gibbs Sampler, grid approximation, quadratic approximation, and so on. Our models would have remained the same regardless of approximation techniques.\nIn this section, we’re going to introduce Variational Inference (VI), which is another approach to approximating the posterior of a Bayesian model (D. Blei, Ng, and Jordan 2003). The goal of VI is identical to that of HMC; both seek to efficiently approximate an entire posterior distribution for some set of latent variables. However, whereas HMC is based on the idea that we can learn about posterior distributions by sampling from them, variational inference attempts to approximate posteriors by using a parametric distribution (or some combination thereof) that gets as close as possible. For this brief introduction, the point is that we will still be approximating the posterior, but without imaginary robots hurling imaginary marbles around an unfathomably large sample space. Sampling methods like HMC construct an approximation of the posterior by keeping a ‘tally’ of where the marble ends up in space, building a jagged pyramid of sorts, and then sanding down the edges and filling in the gaps to produce the smooth posterior curves you saw in the model outputs of chapters 28 and 29. It’s worth knowing that VI, by contrast, approaches the problem by doing the rough equivalent of taking a bendy piece of wire and trying to warp it so that it closely matches the posterior. The key here is that VI provides us with a close-as-possible approximation of posterior distributions using a distribution that we can describe mathematically. Remember, at the beginning of Chapter 27, I recounted a parable about there being “more functions than formulae”? The results that we get from a sampling-based approach to approximating the posterior (HMC, MCMC, Gibbs), gives us the equivalent of a function without a formula. We know what those posteriors look like and the values they take on, but we can’t use a mathematical formula to describe them. Variational inference, on the other hand gives us a function with a formula. It’s not a perfect analogy, but it should help you grasp the difference between the two.\nThe major breakthroughs in generative topic modelling are due, in part, to variational inference. It provides a proxy which we can use to calculate an exact analytical solution for the (still approximate) posterior distribution of the latent variables \\(p(Z | X)\\). To do that, we posit a family of distributions with variational parameters over the latent variables in our model, each of which is indexed by the parameter \\(\\nu\\). It’s written like this:\n\\[\\begin{align}\nq(Z; \\nu)\n\\end{align}\\]\nWe pick some initial value for \\(\\nu\\) and then gradually modify it until we find parameter settings that make the distribution as close to the posterior \\(p(Z | X)\\) as possible. We assess closeness by measuring the distance between the two distrubutions using a measure from information theory called KL Divergence. Once we know those parameter settings, we can use \\(q(Z; \\nu)\\) as a proxy for the posterior.\nThis is represented in ?fig-30_04, which is adapted from D. Blei (2017). We represent the family of distributions \\(q(Z ; \\nu)\\) as an ellipse, and every position within that ellipse represents a specific instantiation of the variational family, indexed by \\(\\nu\\). The squiggly gray line represents different realizations along the way to finding the parameterized distribution that is closest to the posterior, measured with KL divergence.\n\n\n\nAn illustration of the logic of variational inference, adapted from David Blei (2017)\n\n\nRemember that, as with HMC, we are approximating the posterior. Only instead of approximating it by drawing samples, we approximate it by finding another very similar but not identical distributions that can serve as exact analytical proxies for the posterior. The general process works as I’ve decscribed above, but the specifics are a thriving area of research in machine learning. Discussions of variational inference in the technical literature involve a healthy dose of dense mathematics, but most of the technical specifics are not really necessary to understand as an applied researcher. It “Just Works\\(^{(TM)}\\).” It is especially useful when working with very large datasets, as we do in text analysis, and it’s a good bit faster than HMC in cases like these, but is just as accurate.\nI have just covered the basic goals and ideas behind LDA topic models and the importance of thinking through the generative mechanisms. You should also understand generative topics models using graphical models with plate notation, and the basics of how variational inference works. There’s one final issues left to address: selecting the number of topics. Here, once again, the emphasis is on iterative multi-method workflows that leverage as much information and careful interpretive and critical work as possible.\n\nFurther Reading\nIf you want another friendly introduction to LDA topic models, I recommend D. Blei (2012). If you are looking to develop a deeper understanding of variational inference aside from its specific application in LDA topic models, I would recommend Chapters 21 and 22 of Murphy’s (2012) comprehensive Machine Learning: A Probabilistic Perspective.\n\n\n\n27.4.5 Selecting the Number of Topics\nWith LDA topic models, we need to specify the number of topics, \\(K\\), in advance. We are defining a random variable whose values we will infer from the posterior. Selecting the number of topics in a generative topic model is a bit of a dark art; due to the nature of the problem, there is no “correct” number of topics, although some solutions are certainly better than others. If we tell our topic model to identify 12 topics, it will. It will model the probability distribution of those topics over a set of documents and a probability distribution of words for each of the 12 topics. So how do we know how many topics to look for, and what are the consequences of selecting a number that is too large or too small?\nLet’s explore a comparison. Imagine using a simple clustering method like \\(k\\)-means as a rudimentary topic model: you want to identify groups of documents that are thematically similar, so you create a bag-of-words representation of the documents, perform some dimensionality reduction with PCA, and then pass some number of principal components into the \\(k\\)-means algorithm along with the number of clusters to look for. With \\(k\\)-means, each observation (i.e., document) can only be assigned to a single cluster, and if clusters are thematically distinct, then they can only be assigned to a single theme. Continuing with our previous example, a hypothetical article about social movements focused on energy transitions would have to be assigned a single topic (either social movements, energy transitions, or a single topic capturing both of these things), which makes it very likely that documents will be assigned to clusters that don’t fit them very well. There is no “correct” value for \\(k\\), but solutions that set the value of \\(k\\) too high or too low will result in clusters containing many documents that have no business being there.\nThough topic models also require the researcher to choose the number of topics, the consequences of using a sub-optimally calibrated topic model are different from clustering methods like \\(k\\)-means. To reiterate: in topic modelling, documents are always conceptualized as a mixture of topics. If the number of topics that we specify is too small, our model will return extremely general and heterogeneous topics. To a human reader, these topics often appear incoherent. On the other hand, if we set the number of topics too high, then the model will return extremely specific topics. This can seem like taking one topic and splitting it into two topics that are differentiated by things that don’t really matter. It’s the topic modelling version of the narcissism of minor differences. We don’t want that either.\nLet’s continue with our hypothetical example for a bit longer. Say we pick a large number of topics and the result is that we split our social movement topic into multiple social movement topics. Is this a good thing or a bad thing? The short answer is “it depends.” If we are lucky, that split may make some sense, such as separating content on resource mobilization theory (McCarthy and Zald 1977) from other theoretical perspectives in social movement research, such as frame analysis (Benford and Snow 2000; Snow et al. 2014; Benford 1993), political process theory (McAdam 2010; Caren 2007), multi-institutionalism (Armstrong and Bernstein 2008), or strategic adaptation (H. McCammon 2012, 2009; H. J. McCammon et al. 2007). Or perhaps it would differentiate between cultural approaches and structural approaches (Smith and Fetner 2009). In reality, we may not find topics that align so neatly with our own mental models but the take home message here is that general (fewer topics) and specific (more topics) solutions can both be good or bad; the “best” solution depends on what we are trying to learn.\nLooking for fine distinctions with a small number of topics is like trying to compare pedestrians’ gaits while standing on the rooftop patio of an extremely tall building. It’s not “wrong” but if you really want to analyze gait, you would be better off getting a little closer to the action. On the other hand, if you were looking for a more general perspective on the flow of foot traffic in the neighbourhood, the top of a tall building is a perfectly fine place to be. The key thing to realize here is that your goal makes one vantage point better or worse than the other. Luckily, the same research that found LDA results to be greatly improved by an asymmetric prior also found that the artificial splitting of topics was greatly diminished. This means that, in general, we’re better off choosing too many topics than choosing too few, so long as we’re using an asymmetrical prior. On the other hand, if you’re using LDA on a corpus where you actually do expect a homogenous set of topics to be equally likely in the documents, you might want to use a symmetric prior, in which case you will also want to experiment more with the number of topics. However, if you know enough about the data to determine this is the prior you need, then you probably also have a ballpark idea about how many topics to expect! The two a-priori assumptions go hand-in-hand.\nIn short, we can make bad decisions when topic modelling, and these bad decisions can have major implications for what we find. But the risks are different than they are for methods like \\(k\\)-means clustering because documents are always a mix of topics. Most of the time, the risk of a bad topic solution is that we will be either too zoomed in or zoomed out to learn what we want to learn. The best course of action here is to develop many different models with different numbers of topics. And the best way to do this is in an iterative framework like Box’s loops, or better yet a framework like computational grounded theory (discussed in Chapter 11) that is designed specifically for multi-method text analysis.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Modeling text with LDA topic models</span>"
    ]
  },
  {
    "objectID": "modeling-texts-lda.html#topic-modelling-with-gensim",
    "href": "modeling-texts-lda.html#topic-modelling-with-gensim",
    "title": "27  Modeling text with LDA topic models",
    "section": "27.5 TOPIC MODELLING WITH GENSIM",
    "text": "27.5 TOPIC MODELLING WITH GENSIM\nThere are a number of options for developing topic models with Python. In this chapter we’ll use Gensim because it’s mature, well-maintained, and has good documentation. It offers some really nice implementations of widely-used models, is computationally efficient, and scales well to large datasets.\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom dcss import set_style, download_dataset\nset_style()\n\nfrom dcss.text import preprocess, bow_to_df\n \nfrom gensim import corpora\nfrom pprint import pprint\nfrom gensim.models import LdaModel\nfrom gensim.models.ldamulticore import LdaMulticore\nfrom gensim.models.coherencemodel import CoherenceModel\nimport pickle\nca_hansard_data_url = \"https://www.dropbox.com/scl/fo/5voxfrx6qeqgdrjuc979k/AD63UZhKpxF64b58Jp65w18?rlkey=2bbaqw1bwjgvodbwqhox494e2&st=99fldjgr&dl=0\"\n\ndownload_dataset(ca_hansard_data_url, 'data/canadian_hansard/')\ndf = pd.read_csv('data/canadian_hansard/lipad/canadian_hansards.csv', low_memory=False)\ndf.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 946686 entries, 0 to 946685\nData columns (total 16 columns):\n #   Column           Non-Null Count   Dtype \n---  ------           --------------   ----- \n 0   index            946686 non-null  int64 \n 1   basepk           946686 non-null  int64 \n 2   hid              946686 non-null  object\n 3   speechdate       946686 non-null  object\n 4   pid              824111 non-null  object\n 5   opid             787761 non-null  object\n 6   speakeroldname   787032 non-null  object\n 7   speakerposition  202294 non-null  object\n 8   maintopic        932207 non-null  object\n 9   subtopic         926996 non-null  object\n 10  subsubtopic      163963 non-null  object\n 11  speechtext       946686 non-null  object\n 12  speakerparty     787692 non-null  object\n 13  speakerriding    686495 non-null  object\n 14  speakername      923416 non-null  object\n 15  speakerurl       763264 non-null  object\ndtypes: int64(2), object(14)\nmemory usage: 115.6+ MB\nThe text data is stored in the speechtext Series. We’ll use the dcss preprocess() function to perform the same pre-processing steps that we’ve used a few times since. As a reminder, this function passes each document through SpaCy’s nlp pipeline and returns a list of tokens for each document, each token being a lemmatized noun, proper noun, or adjective that is longer than a single character. The function also strips out English language stopwords.\ntexts = df['speechtext'].tolist()\nprocessed_text = preprocess(texts, bigrams=False, detokenize=False, n_process = 32)\nlen(processed_text)\n1893372\nSince pre-processing 1.8 million speeches takes a good amount of time, we’ll pickle the results. Then we can easily re-load then again later rather than needlessly waiting around.\nwith open('data/preprocessed_speeches_canadian_hansards_no_bigrams.pkl', 'wb') as handle:\n    pickle.dump(processed_text, handle, protocol=pickle.HIGHEST_PROTOCOL)\nprocessed_text = pickle.load( open( 'data/preprocessed_speeches_canadian_hansards_no_bigrams.pkl', 'rb'))\n\n27.5.0.0.1 Creating a Bag-of-Words with Gensim\nTo topic model our data with Gensim, we need to provide our list of tokenized texts to the Dictionary() class. Gensim uses this to contruct a corpus vocabulary that assigns each unique token in the dataset to an integer. If you run dict(vocab) after creating the vocab object, you’ll see this this is just a Python dictionary that stores a key:value pairing of the integer representation (the key) of each token (the value).\nWe’ll also create a corpus object using the doc2bow method of the Dictionary class. The resulting object stores information about the specific tokens and token frequencies in each document. If you print the corpus object, you will see a lot of numbers. The corpus object itself is just a list, and each element of the list represents an individual document. Nested inside each document are tuples (e.g. (147, 3)) that represent (a) the unique integer ID for the token (stored in our vocab object) and (b) the number of times the token appears in the document.\nWe’re going to filter the vocabulary to keep tokens that only appear in 20 or more speeches, as well as tokens that don’t appear in more than 95% of speeches. This is a fairly inclusive filter but still reduces ~160K words to ~36K words. You will want to experiment with this, but one obvious advantage is that a lot of non-words from parts of the text data that are low quality should end up removed, while non-differentiating words that would probably crowd the topic space will also be left out.\nvocab = corpora.Dictionary(processed_text) # id2word\nvocab.save('../models/lda_vocab.dict')\nThe file saved above is easy to reload, so you can experiment with different filter parameters at will.\nvocab = corpora.Dictionary.load('../models/lda_vocab.dict')\nvocab.filter_extremes(no_below=20, no_above=0.95)\ncorpus = [vocab.doc2bow(text) for text in processed_text]\nlen(vocab)\n36585\n\nNote that we’re not using TF-IDF weights in our LDA models, whereas we did use them in the context of LSA. While TF-IDF weights are appropriate in some cases (such as LSA), they are not in LDA models (D. Blei and Lafferty 2009). The reason is because of LDA’s generative nature. It makes sense to say that word frequencies are generated from a distribution, as LDA posits, but it does not make sense to say that TF-IDF weights are generated from that distribution. Consequently using TF-IDF weights in a generative topic model generally worsens the results. In general TF-IDF weights work well in deterministic contexts but less so in generative ones.\n\n\n\n27.5.1 Running the Topic Model\nWe are now ready to fit the topic model to our data. We will do so using Gensim’s LdaModel first, rather than LdaMulticore which is designed to speed up computation by using multiprocessing. They do almost the same things, though with one key difference that we’ll discuss shortly.\nThe code block below estimates a model for 100 topics, which is an initial value that I selected arbitrarily. Later we will discuss other ways of selecting a number of topics. We’re going to start with a random sample from the corpus and the processed list of text, because even with Gensim’s efficiency and using algorithms designed to perform well on large datasets, this can take a while to run, and this will not be our final model.\nimport random\nrandom.seed(100)\nsample_corpus, sample_text = zip(*random.sample(list(zip(corpus,processed_text)),100000))\nldamod_s = LdaModel(corpus=sample_corpus,\n                      id2word=vocab,\n                      num_topics=100,\n                      random_state=100,\n                      eval_every=1,\n                      chunksize=2000,\n                      alpha='auto',\n                      eta='auto',\n                      passes=2,\n                      update_every=1,\n                      iterations=400\n                  )\nWe’ll pickle the results to easily load them later without having to wait for our code to run again.\nwith open('data/lda_model_sample.pkl', 'wb') as handle:\n    pickle.dump(ldamod_s, handle, protocol=pickle.HIGHEST_PROTOCOL)\nldamod_s = pickle.load(open( 'data/lda_model_sample.pkl', 'rb'))\nGensim provides a number of useful functions to simplify working with the results of our LDA model. The ones you’ll likely turn to right away are\n\n.show_topic(), which takes an integer topic ID and returns a list of the words most strongly associated with that topic,\n.get_term_topics(), which takes a word and, if it’s in the corpus vocabulary, returns the word’s probability for each topic, and\n\nAs you can see below, we can find the topics that a word is associated with.\nldamod_s.get_term_topics('freedom')\n[(53, 0.02927819)]\nGensim provides the weights associated with each of the top words for each topic. The higher the weight, the more strongly associated with the topic the word is. The words in this case make quite a lot of intuitive sense - freedom has to do with the law, rights, principles, society, and is a fundamental concept.\nldamod_s.show_topic(53)\n[('right', 0.15800211),\n ('human', 0.04227337),\n ('freedom', 0.029263439),\n ('law', 0.022657597),\n ('Canadians', 0.018386548),\n ('canadian', 0.018030208),\n ('citizen', 0.017851433),\n ('society', 0.015541217),\n ('fundamental', 0.014947715),\n ('principle', 0.013568851)]\nWhen we look at how parliament talks about criminals, we can see that the associated words are pretty intuitive although we might have to dig a bit further into the terms to find more particular term associations.\nldamod_s.get_term_topics('criminal')\n[(20, 0.059014548)]\nldamod_s.show_topic(20)\n[('crime', 0.07494005),\n ('criminal', 0.058972023),\n ('victim', 0.055283513),\n ('justice', 0.047199916),\n ('offence', 0.03621511),\n ('law', 0.03601918),\n ('offender', 0.03377842),\n ('sentence', 0.032146234),\n ('system', 0.022435088),\n ('person', 0.020964943)]\nLet’s look a little closer at something that’s a bit more controversial, like ‘marriage’.\nldamod_s.get_term_topics('marriage')\n[(28, 0.042418264)]\nWe can specify the return of a few more terms for a topic by adding an argument for topn. You can see that when marriage is discussed in parliament, it’s around fairly controversial concepts such as equality, gender, tradition, and even abuse.\nldamod_s.show_topic(28, topn=30)\n[('woman', 0.26727995),\n ('man', 0.069456935),\n ('violence', 0.06529136),\n ('marriage', 0.04248659),\n ('girl', 0.023184145),\n ('Women', 0.02255594),\n ('equality', 0.021070031),\n ('Canada', 0.019696228),\n ('society', 0.018637668),\n ('gender', 0.01841013),\n ('abuse', 0.015813459),\n ('issue', 0.015377659),\n ('action', 0.012950255),\n ('practice', 0.01211937),\n ('female', 0.011507524),\n ('equal', 0.011195933),\n ('Status', 0.011139394),\n ('medicare', 0.011124585),\n ('group', 0.010348747),\n ('physical', 0.008267313),\n ('psychological', 0.0075966706),\n ('prescription', 0.0070270123),\n ('traditional', 0.006817099),\n ('Speaker', 0.0067508616),\n ('killing', 0.006746756),\n ('status', 0.006714445),\n ('sexual', 0.0065426086),\n ('victim', 0.0060332483),\n ('government', 0.005900839),\n ('country', 0.0058119465)]\n\n27.5.1.1 Evaluating the Quality of Topic Models by Measuring Semantic Coherence\nThe model we just estimated found 100 topics because we told it to. Was 100 a good number? How do we pick a good number of topics, at least as a starting point? Again, my advice here is that you develop your model iteratively, by zooming in and zooming out, each time learning a little more about your data. You should supplement this by reading samples of documents from the corpus. All of this will help you develop a better model.\nYou can also supplement this with quantitative measures. The ideal number of topics ultimately comes down to interpretability and usefulness for the task at hand. Strange as it might seem, there are quantitative approaches to measuring human readability – in this case by measuring “semantic coherence.” These measures can be used to help guide our decisions about how many topics to search for in our topic model. The higher the topic coherence, the more human readable the topics in our model should be. Note that quantitative measures of semantic coherence should help you make a decision, not make it for you. You still want to be a human “in the loop,” reading things and thinking deeply about them.\nMost semantic coherence measures work by segmenting the corpus into topics, taking the top words in a topic, putting them in pairs, computing their similarity in vector space, and then aggregating those scores to produce an overall summary of the semantic coherence in your model.1 There is a growing technical literature on computing semantic coherence (see Röder, Both, and Hinneburg 2015 for a good introduction to measures used in some Python implementations). Higher coherence means that the words in a topic are closer to one another in vector space. The closer they are, the more coherent the topic. The general idea here is that words in a topic are likely to come from similar positions in the vector space. (I’m being a bit hand-wavey here because this is essentially what the next three chapters are about.)\nModels with high semantic coherence will tend to have a smaller number of junk topics but, ironically, this sometimes comes with a reduction of the quality of the topics in your model. In other words, we can avoid having any really bad topics but the ones we are left with might themselves be middling. Imagine two models. In the first, you have 40 topics, 37 of which seem good, and 3 of which seem like junk. You can ignore the junk topics and focus on the good ones. In the second, you have 33 topics and end up with a higher coherence score. There are no junk topics in this model, but the 33 topics you got are not really as informative as the 37 good topics from your 40 topic model. Which do you prefer? Can you tolerate a few junk topics or not? Personally, I prefer the solution with more interpretable topics and a few junk topics but, again: there is no absolutely correct answer.\nHuman qualitative evaluation is labour intensive and, yes, inevitably subjective. Quantitative methods for selecting the number of topics sometimes produce models that seem worse, and – like human qualitative evaluation methods – there may not be agreement that the results are more informative even if the model overall has better semantic coherence. What’s the value of eliminating human judgment from a process that is intended to help you iteratively learn things you didn’t know before? Besides, the whole point of Bayesian analysis is to provide principled ways of integrating information from multiple sources. This includes qualitative interpretation and cricitism of the model. Semantic coherence measures are very useful when it comes to making what might seem like arbitrary decisions in developing your topic model, but ultimately you need to use your human abilities and make informed judgements. This is integral to Box’s loop, as I’ve emphasized troughout the book. It’s better to produce many models and look at them all, interpreting, thinking, critiquing. The more you learn, the better! Finally, to repeat a point I have now made many times, you absolutely must read. There’s no getting around it.\nGensim makes it fairly straightforward to compute coherence measures for topic models. There are numerous coherence measures available “out of the box,” each of which works slightly differently. The measure we will use below – \\(C_v\\) – was shown by Röder, Both, and Hinneburg (2015) to be the most highly correlated with all available data on human interpretation of the output of topic models.\n\nThere are four steps involved in computing the \\(C_v\\) measure. First, it selects the top (i.e. most probable) \\(n\\) words within any given topic. Second, it computes the probability of single top words and the joint probability of pairs of co-occurring top words by counting the number of texts in which the word or word pair occurs, and dividing by the total number of texts. Third, it vectorizes this data using a measure called normalized Pointwise Mutual Information (NPMI), which tells us whether a pair of words \\(W_i\\) and \\(W_j\\) co-occur more than they would if they were independent of one another. Finally, \\(C_v\\) computes the cosine similarity for all vectors. The final coherence score for a topic model is the mean of all of these cosine similarity scores.\n\nThe C_v measure is already implemented in Gensim, so we can compute it with very little code. To start, let’s compute the “coherence” of our 100 topic model. Note that for C_v, coherence scores range from 0 for complete incoherence to 1 for complete coherence. Values above 0.5 are fairly good, while we can’t expect to find values much above 0.8 in real world text data.\ncoherence_model_s = CoherenceModel(model=ldamod_s, \n                                     texts=sample_text, \n                                     dictionary=vocab, \n                                     coherence='c_v')\n\ncoherence_lda_s = coherence_model_s.get_coherence()\nprint('Coherence Score: ', coherence_lda_s)\nCoherence Score:  0.3882056267381639\nwith open('data/coherence_model_sample.pkl', 'wb') as handle:\n    pickle.dump(coherence_model_s, handle, protocol=pickle.HIGHEST_PROTOCOL)\ncoherence_model_s = pickle.load( open( 'data/coherence_model_sample.pkl', 'rb'))\nNow that we can calculate \\(C_v\\) scores, we can gauge topic solution’s. But what if 21, 37, 42, or \\(n\\) other number of topics would be better? As discussed earlier, we’ve used an asymmetric, trained prior, so even if we selected too many topics, the quality of the best ones should still be pretty good. Let’s take a look at their coherence scores in a dataframe, sorted highest to lowest on coherence, and lowest to highest on standard deviation (although the latter will only have an effect if we have any identical coherence scores).\ntopic_coherence_s = coherence_model_s.get_coherence_per_topic(with_std = True)\ntopic_coherence_df = pd.DataFrame(topic_coherence_s, columns = ['coherence','std'])\ntopic_coherence_df = topic_coherence_df.sort_values(['coherence', 'std'], ascending=[False,True])\nThe top 10 most coherent topics actually have fairly high scores!\ntopic_coherence_df.head(10).mean()\ncoherence    0.640070\nstd          0.233939\ndtype: float64\ntopic_coherence_df.tail(10).mean()\ncoherence    0.213738\nstd          0.182295\ndtype: float64\n\nFurther Reading\nH. Wallach et al. (2009) and Mimno et al. (2011) offer useful guidelines for evaluatating topic models using semantic coherence.\n\n\n\n27.5.1.2 Going Further with Better Priors\nAs promised, let’s examine selecting a few different \\(\\alpha\\) and \\(\\eta\\) hyperparameters. We’ll use the same data as before, but rather than a sample, let’s use the whole corpus. For the most part, you can probably feel safe using the “auto” setting for both since your priors will be informed by the data rather than being arbitrary. Unfortunately, with the amount of data in the full corpus, you want to use LdaMulticore rather than the base LdaModel, but the “auto” option is not implemented for \\(\\alpha\\) in the much faster multicore option. Your built-in options are either a uniform scalar probability for each topic, or “asymmetrical”. The code block below shows how an asymmetrical \\(\\alpha\\) is constructed, as well as the simple scalar value for \\(\\eta\\) used in the paper that informs Gensim’s LDA implementation (Hoffman, Bach, and Blei 2010).\nalpha_asym = np.fromiter(\n                    (1.0 / (i + np.sqrt(100)) for i in range(100)),\n                    dtype=np.float16, count=100,\n                    )\neta_sym = 1/100\nBut we can also use the automatically updated \\(\\alpha\\) and \\(\\eta\\) hyperparameters from our earlier model on the sample of the corpus as long as we plan to use the same number of topics. Because we ran LdaModel with 1) frequent model perplexity evaluations 2) frequent hyperparameter updates and 3) two full passes over the sample data along with 400 iterations per document, we can expect the \\(\\alpha\\) prior to have a lot of fine-grained nuance. There are more complex ways to sample the data in order to produce trained asymmetric priors, such as the Bayesian slice sampling detailed in,(Syed and Spruit 2018) but they are outside the scope of this chapter. Taking the output from one Bayesian model to use in another is a lot like the transfer learning methods that are covered in the final chapters of this book.\nalpha_t = ldamod_s.alpha\neta_t = ldamod_s.eta\nLet’s get a sense of the difference between the options we’ve discussed for priors, by calculating the average of the probabilities as well as the amount of variance they have.\nprint(\"Trained alpha variance: \" + str(np.round(np.var(alpha_t), 4)))\nprint(\"Asymmetric alpha variance: \" + str(np.round(np.var(alpha_asym), 4)))\nprint(\"Trained alpha avg: \" + str(np.round(alpha_t.sum()/len(alpha_t), 4)))\nprint(\"Asymmetric alpha avg: \" + str(np.round(alpha_asym.sum()/len(alpha_asym), 4)))\n\nprint(\"Trained eta variance: \" + str(np.round(np.var(eta_t), 4)))\nprint(\"Symmetric eta variance: \" + str(np.round(np.var(eta_sym), 4)))\nprint(\"Trained eta avg: \" + str(np.round(eta_t.sum()/len(eta_t),4)))\nprint(\"Symmetric eta avg: \" + str(np.round(eta_sym, 4)))\nTrained alpha variance: 0.0006\nAsymmetric alpha variance: 0.0004\nTrained alpha avg: 0.0304\nAsymmetric alpha avg: 0.0244\nTrained eta variance: 0.0003\nSymmetric eta variance: 0.0\nTrained eta avg: 0.0098\nSymmetric eta avg: 0.01\nAs you can see, the trained \\(\\alpha\\) prior has around 1.5x the variance of the simpler asymmetric version and around 1.25x the average topic probability. The trained \\(\\eta\\) priors, on the other hand, end up with only half the variance of the trained alpha, and the average word probability is very close to the simple 1/100 symmetrical prior. Although the automatic updates for priors add only linear computation complexity, it’s always nice to trim computation time wherever possible, so you might find that it’s just as good to use a simple scalar for the \\(\\eta\\) hyperparameter.\nldamod_f = LdaMulticore(corpus=corpus,\n                      id2word=vocab,\n                      num_topics=100,\n                      random_state=100,\n                      chunksize=2000,\n                      alpha=alpha_t,\n                      eta=eta_t,\n                      passes=1,\n                      iterations=10,\n                      workers=15,\n                      per_word_topics=True)\nwith open('data/lda_model_full.pkl', 'wb') as handle:\n    pickle.dump(ldamod_f, handle, protocol=pickle.HIGHEST_PROTOCOL)\nldamod_f = pickle.load( open( 'data/lda_model_full.pkl', 'rb'))\ncoherence_model_full = CoherenceModel(model=ldamod_f,\n                                     texts=processed_text,\n                                     dictionary=vocab,\n                                     coherence='c_v')\ncoherence_full = coherence_model_full.get_coherence()\nwith open('data/coherence_model_full.pkl', 'wb') as handle:\n    pickle.dump(coherence_model_full, handle, protocol=pickle.HIGHEST_PROTOCOL)\ncoherence_model_full = pickle.load( open( 'data/coherence_model_full.pkl', 'rb'))\nWe have actually gained a slight amount of topic coherence after training on 1.8 million documents rather than 100,000!\ncoherence_full\n0.3943192191621368\nLet’s look at per-topic results in a dataframe, to compare to the results from the sample corpus. First we’ll compare the average coherence scores for the top 30 and bottom 30 topics.\ntopic_coherence_f = coherence_model_full.get_coherence_per_topic(with_std = True)\ntopic_coherence_f_df = pd.DataFrame(topic_coherence_f, columns = ['coherence','std'])\ntopic_coherence_f_df = topic_coherence_f_df.sort_values(['coherence', 'std'], ascending=[False,True])\nprint(\"Full model average coherence top 30 topics: \" + str(topic_coherence_f_df['coherence'].head(30).mean()))\nprint(\"Sample model average coherence top 30 topics: \" + str(topic_coherence_df['coherence'].head(30).mean()))\nprint(\"Full model average coherence bottom 30 topics: \" + str(topic_coherence_f_df['coherence'].tail(30).mean()))\nprint(\"Sample model average coherence bottom 30 topics: \" + str(topic_coherence_df['coherence'].tail(30).mean()))\nFull model average coherence top 30 topics: 0.4943150877317519\nSample model average coherence top 30 topics: 0.550659531826426\nFull model average coherence bottom 30 topics: 0.2952359669399737\nSample model average coherence bottom 30 topics: 0.2517402193295241\nWe’ve actually lost a bit of coherence in the top 30 topics, while gaining some in the bottom 30. One thing to keep in mind is that coherence scores are a convenient, objective way to assess a topic model, but they are not a substitute for subjectively inspecting the topics themselves! Below we can see that topic 20 remains the most coherent, while a few others also remain in the top 10 but at different positions.\ntopic_coherence_f_df.head(10)\n    coherence       std\n20   0.604434  0.331337\n77   0.565511  0.242574\n90   0.545323  0.344012\n55   0.534403  0.306397\n73   0.530510  0.295568\n88   0.523907  0.287610\n10   0.521217  0.340641\n89   0.521041  0.333885\n54   0.519937  0.309211\n75   0.515672  0.339481\ntopic_coherence_df.head(10)\n    coherence       std\n20   0.721814  0.228599\n65   0.676778  0.116802\n12   0.657578  0.283031\n55   0.640755  0.331486\n74   0.632465  0.197373\n42   0.630215  0.211954\n17   0.626714  0.253870\n80   0.609308  0.241425\n4    0.607440  0.215214\n75   0.597635  0.259641\nNow that we have run through all of these models, let’s actually examine the topics with some visualization of the most coherent ones!\n\nFurther Reading\nH. M. Wallach, Mimno, and McCallum (2009) provides some usful advice on thinking through the use of priors in LDA topic models.\n\n\n\n27.5.1.3 Visualizing Topic Model Output with PyLDAVis\nOne way to explore a topic model is to use the pyLDAvis package to interact with dynamic browser-based visualizations. This package is designed to hook into Gensim’s data structures seamlessly. We will provide pyLDAvis with (1) the name of the object storing our LDA model, (2) the corpus object, and (3) the vocab. We will then write the results to an HTML file that you can open in your browser and explore interactively. This can take a while to run, so if you just want to explore the results seen below, you can find the HTML file in the data/misc directory.\nimport pyLDAvis.gensim_models as gensimvis\nfrom pyLDAvis import save_html\nvis = gensimvis.prepare(ldamod_f, corpus, vocab)\nsave_html(vis, 'data/misc/ldavis_full_model.html')\n\nOf course, I can’t show the interactive results here on the printed page, but what you will see in your browser will be something like the graph shown in ?fig-30_05. These maps contain a lot of information, so it’s worth taking your time to explore them fully.\nHow do you read this? On the left we have a two-dimensional representation of the distances between topics. The distances between topics are computed with Jensen-Shannon divergence (which is a way of measuring the distance between two probability distributions (Lin 1991)), and then a principal components analysis is performed on the results. The interactive graph says the map is produced “via multidimensional scaling” because MDS is a general class of analysis and PCA is a specific method. Topics that are closer together in the map are more similar. Topics that are further away from one another are dissimilar. However, recall that we don’t know how much variance in the data is actually accounted for with these two dimensions, so we should assume that, like other text analyses, the first two principal components don’t actually account for much variance. Accordingly, we should interpret the spatial relationships between topics with a healthy dose of scepticism. Finally, the size of the topic in this map is related to how common it is. Bigger topics are more common. Smaller topics are rare. Sizing points like this is generally not considered best data visualization practice, but we are not focused on comparing topics on their size, so it’s generally okay.\nOn the right of the graph, we have horizontal bar graphs that update as you mouse over a topic. These are the words that are most useful for interpreting what a given topic is about. The red shows you how common the word is in the topic, and the blue shows you how common it is in the rest of the corpus. So topics with a lot of red but not a lot of blue are more exclusive to the topic. If you mouse over the words in the bar graphs, the MDS map changes to show you the conditional distribution over topics on the MDS map.\nFinally, you can change the meaning of “words that are most useful for interpreting what a given topic is about” by changing the value of the \\(\\lambda\\) parameter. You do this moving the slider. If \\(\\lambda\\) = 1, then the words provided are ranked in order of their probability of appearing in that specific topic. Setting them at 0 reorders the words displayed by their “lift” score, !keyword(lift scores) which is defined as the ratio of their probability within the topic to its marginal probability across the corpus. The idea, sort of like with TF-IDF, is that words that have a high probability of occurring across the whole corpus are not helpful in interpreting individual topics. You want to find some sort of balance that helps you understand what the topics are about. If you are following along with the code, take some time exploring the results in your browser.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Modeling text with LDA topic models</span>"
    ]
  },
  {
    "objectID": "modeling-texts-lda.html#conclusion",
    "href": "modeling-texts-lda.html#conclusion",
    "title": "27  Modeling text with LDA topic models",
    "section": "27.6 CONCLUSION",
    "text": "27.6 CONCLUSION\n\n27.6.1 Key Points\n\nGenerative topic models are Bayesian models used to understand latent themes in documents containing thematically linked words\nDeveloped latent Dirichlet allocation models using Gensim\nVariational inference is an alternative to Hamiltonian Monte Carlo (HMC) that provides a function with an analytical solution for the approximate posterior distribution\nEvaluated topic coherence using quantitative measures\nVisualized topic models with PyLDAVis\n\n\n\n\n\nArmstrong, Elizabeth, and Mary Bernstein. 2008. “Culture, Power, and Institutions: A Multi-Institutional Politics Approach to Social Movements.” Sociological Theory 26 (1): 74–99.\n\n\nBenford, Robert. 1993. “Frame Disputes Within the Nuclear Disarmament Movement.” Social Forces 71 (3): 677–701.\n\n\nBenford, Robert, and David Snow. 2000. “Framing Processes and Social Movements: An Overview and Assessment.” Annual Review of Sociology 26 (1): 611–39.\n\n\nBlei, David. 2012. “Probabilistic Topic Models.” Communications of the ACM 55 (4): 77–84.\n\n\n———. 2017. “Variational Inference: Foundations and Innovations.” Simons Institute: Computational Challenges in Machine Learning.\n\n\nBlei, David M, and John D Lafferty. 2006. “Dynamic Topic Models.” In Proceedings of the 23rd International Conference on Machine Learning, 113–20.\n\n\nBlei, David, and John Lafferty. 2009. “Topic Models.” Text Mining: Classification, Clustering, and Applications 10 (71): 34.\n\n\nBlei, David, Andrew Ng, and Michael I Jordan. 2003. “Latent Dirichlet Allocation.” The Journal of Machine Learning Research 3: 993–1022.\n\n\nCaren, Neal. 2007. “Political Process Theory.” The Blackwell Encyclopedia of Sociology.\n\n\nDiMaggio, Paul, Manish Nag, and David Blei. 2013. “Exploiting Affinities Between Topic Modeling and the Sociological Perspective on Culture: Application to Newspaper Coverage of US Government Arts Funding.” Poetics 41 (6): 570–606.\n\n\nHoffman, Matthew, Francis Bach, and David Blei. 2010. “Online Learning for Latent Dirichlet Allocation.” In Advances in Neural Information Processing Systems, 856–64. Citeseer.\n\n\nJordan, Michael. 2003. “An Introduction to Probabilistic Graphical Models.” preparation.\n\n\n———. 2004. “Graphical Models.” Statistical Science 19 (1): 140–55.\n\n\nKoller, Daphne, and Nir Friedman. 2009. Probabilistic Graphical Models: Principles and Techniques. MIT press.\n\n\nLin, Jianhua. 1991. “Divergence Measures Based on the Shannon Entropy.” IEEE Transactions on Information Theory 37 (1): 145–51.\n\n\nMcAdam, Doug. 2010. Political Process and the Development of Black Insurgency, 1930-1970. University of Chicago Press.\n\n\nMcCammon, Holly. 2009. “Beyond Frame Resonance: The Argumentative Structure and Persuasive Capacity of Twentieth-Century US Women’s Jury-Rights Frames.” Mobilization: An International Quarterly 14 (1): 45–64.\n\n\n———. 2012. The US Women’s Jury Movements and Strategic Adaptation: A More Just Verdict. Cambridge University Press.\n\n\nMcCammon, Holly J, Courtney Sanders Muse, Harmony D Newman, and Teresa M Terrell. 2007. “Movement Framing and Discursive Opportunity Structures: The Political Successes of the US Women’s Jury Movements.” American Sociological Review 72 (5): 725–49.\n\n\nMcCarthy, John D, and Mayer N Zald. 1977. “Resource Mobilization and Social Movements: A Partial Theory.” American Journal of Sociology 82 (6): 1212–41.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. CRC press.\n\n\nMimno, David, Hanna Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. 2011. “Optimizing Semantic Coherence in Topic Models.” In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 262–72.\n\n\nMohr, John, and Petko Bogdanov. 2013. “Introduction—Topic Models: What They Are and Why They Matter.” Elsevier.\n\n\nMurphy, Kevin. 2012. Machine Learning: A Probabilistic Perspective. MIT press.\n\n\nPearl, Judea, and Dana Mackenzie. 2018. The Book of Why: The New Science of Cause and Effect. Basic books.\n\n\nRoberts, Margaret E, Brandon M Stewart, Dustin Tingley, Christopher Lucas, Jetson Leder-Luis, Shana Kushner Gadarian, Bethany Albertson, and David G Rand. 2014. “Structural Topic Models for Open-Ended Survey Responses.” American Journal of Political Science 58 (4): 1064–82.\n\n\nRoberts, Margaret, Brandon Stewart, Dustin Tingley, and Edoardo Airoldi. 2013. “The Structural Topic Model and Applied Social Science.” In Advances in Neural Information Processing Systems Workshop on Topic Models: Computation, Application, and Evaluation, 4:1–20. Harrahs; Harveys, Lake Tahoe.\n\n\nRöder, Michael, Andreas Both, and Alexander Hinneburg. 2015. “Exploring the Space of Topic Coherence Measures.” In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, 399–408.\n\n\nRosen-Zvi, Michal, Thomas Griffiths, Mark Steyvers, and Padhraic Smyth. 2012. “The Author-Topic Model for Authors and Documents.” arXiv Preprint arXiv:1207.4169.\n\n\nSmith, Jackie, and Tina Fetner. 2009. “Structural Approaches in the Sociology of Social Movements.” In Handbook of Social Movements Across Disciplines, 13–57. Springer.\n\n\nSnow, David, Robert Benford, Holly McCammon, Lyndi Hewitt, and Scott Fitzgerald. 2014. “The Emergence, Development, and Future of the Framing Perspective: 25+ Years Since\" Frame Alignment\".” Mobilization: An International Quarterly 19 (1): 23–46.\n\n\nSyed, Shaheen, and Marco Spruit. 2018. “Selecting Priors for Latent Dirichlet Allocation.” In 2018 IEEE 12th International Conference on Semantic Computing (ICSC), 194–202. IEEE.\n\n\nWallach, Hanna M, David M Mimno, and Andrew McCallum. 2009. “Rethinking LDA: Why Priors Matter.” In Advances in Neural Information Processing Systems, 1973–81.\n\n\nWallach, Hanna, Iain Murray, Ruslan Salakhutdinov, and David Mimno. 2009. “Evaluation Methods for Topic Models.” In Proceedings of the 26th Annual International Conference on Machine Learning, 1105–12.\n\n\nWang, Chong, David Blei, and David Heckerman. 2012. “Continuous Time Dynamic Topic Models.” arXiv Preprint arXiv:1206.3298.\n\n\nYpma, Tjalling J. 1995. “Historical Development of the Newton–Raphson Method.” SIAM Review 37 (4): 531–51.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Modeling text with LDA topic models</span>"
    ]
  },
  {
    "objectID": "modeling-texts-lda.html#footnotes",
    "href": "modeling-texts-lda.html#footnotes",
    "title": "27  Modeling text with LDA topic models",
    "section": "",
    "text": "Some alternative approaches are based on computations of conditional probabilities for pairs of words.↩︎",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Modeling text with LDA topic models</span>"
    ]
  },
  {
    "objectID": "modeling-networks.html",
    "href": "modeling-networks.html",
    "title": "28  Latent structure in networks",
    "section": "",
    "text": "28.1 Imports and Setup",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Latent structure in networks</span>"
    ]
  },
  {
    "objectID": "modeling-networks.html#imports-and-setup",
    "href": "modeling-networks.html#imports-and-setup",
    "title": "28  Latent structure in networks",
    "section": "",
    "text": "import math\nimport pickle\nimport random\nfrom pprint import pprint\n\nimport graph_tool.all as gt\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import homogeneity_score\n\nimport icsspy\nfrom icsspy.networks import (\n    get_consensus_partition_from_posterior,\n    plot_line_comparison,\n)\nfrom icsspy.paths import enron\n\n\nicsspy.set_style()\nprint(f'Using graph-tool version {gt.__version__}')",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Latent structure in networks</span>"
    ]
  },
  {
    "objectID": "modeling-networks.html#load-the-enron-data",
    "href": "modeling-networks.html#load-the-enron-data",
    "title": "28  Latent structure in networks",
    "section": "28.2 Load the Enron Data",
    "text": "28.2 Load the Enron Data\nWe can load the Enron email data (crick2022enron?) from the icsspy course package. The network itself has already been prepared and can be loaded directly into graph-tool.\nenron_email_network = str(enron / 'enron_graph.gt')\ng = gt.load_graph(enron_email_network)\nprint(g)\nLike the political blogs network, this network has internal property maps containing data about node and edge attributes, as well as the graph itself. We can list the available property maps:\ng.list_properties()\n\nlabel is a string variable containing the email address.\nposition is a string variable containing information about the job position associated with the email account.\nedge weight are counts of the number of emails that vertex \\(i\\) sent vertex \\(j\\) (since this is a directed network).",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Latent structure in networks</span>"
    ]
  },
  {
    "objectID": "modeling-networks.html#prepare-for-modelling",
    "href": "modeling-networks.html#prepare-for-modelling",
    "title": "28  Latent structure in networks",
    "section": "28.3 Prepare for Modelling",
    "text": "28.3 Prepare for Modelling\nSince we’ll be creating a series of identical visualizations for each model, let’s define a simple visualization function so we can avoid repeating ourselves. ☂︎\ndef draw_state(state, g, hvprops, heprops, filename=None):\n    state.draw(\n        vertex_text = g.vp['position'],\n        hvprops=hvprops,\n        heprops=heprops,\n        vertex_size=5,\n        output_size=(1200, 1200),\n        bg_color=[1, 1, 1, 1],\n        output=filename,\n)\nWe’ll also initialize a couple of empty dictionaries that we’ll use to collect information about our models once they’ve been fit. The first will store each model’s Minimum Description Length (MDL), the second will store homogeneity scores.\nmodel_mdl_scores = {}\nmodel_homogeneity_scores = {}",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Latent structure in networks</span>"
    ]
  },
  {
    "objectID": "modeling-networks.html#model-1-a-nested-sbm-with-binary-edges",
    "href": "modeling-networks.html#model-1-a-nested-sbm-with-binary-edges",
    "title": "28  Latent structure in networks",
    "section": "28.4 Model 1: A Nested SBM with Binary Edges",
    "text": "28.4 Model 1: A Nested SBM with Binary Edges\nAs before, let’s start with a simple nested SBM and visualization. We’ll use gt.minimize_nested_blockmodel_dl() with the default parameters, which is the same default nested SBM we fit in our analysis of the political blogs networks in the previous notebook. This model will select the best fitting posterior partition, where “best fitting” means the posterior partition with the shortest Minimum Description Length (MDL).\nSince we have not passed an argument for edge weights, the model will treat this network as binary; the only information we are using to estimate the latent hierarchical block structure of this network is the presence or absence of edges.\nmodel_1 = gt.minimize_nested_blockmodel_dl(g)\nNow that we’ve fit the model, let’s visualize the results.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Latent structure in networks</span>"
    ]
  },
  {
    "objectID": "modeling-networks.html#model-1-a-nested-sbm-with-binary-edges-1",
    "href": "modeling-networks.html#model-1-a-nested-sbm-with-binary-edges-1",
    "title": "28  Latent structure in networks",
    "section": "28.5 Model 1: A Nested SBM with Binary Edges",
    "text": "28.5 Model 1: A Nested SBM with Binary Edges\nhvprops = {\"fill_color\": \"black\", \"size\": 30}\nheprops = {\"color\": \"black\", \"pen_width\": 2}\ndraw_state(state=model_1, g=g, hvprops=hvprops, heprops=heprops)\nThis visualization conveys a lot of information about the estimated network structure. Each node’s job title is printed on the node (although you’ll really have to squint to read it). You may notice that some blocks contain clusters of folks with the same or similar job titles. This makes some intuitive sense, especially when considered in terms of positional network analysis and the concept of stochastic equivalence. It’s plausible, likely even, that the formal roles people hold in the organization generate different relational patterns, however noisy, and that these patterns would be picked up by the blockmodel.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Latent structure in networks</span>"
    ]
  },
  {
    "objectID": "modeling-networks.html#model-1-a-nested-sbm-with-binary-edges-2",
    "href": "modeling-networks.html#model-1-a-nested-sbm-with-binary-edges-2",
    "title": "28  Latent structure in networks",
    "section": "28.6 Model 1: A Nested SBM with Binary Edges",
    "text": "28.6 Model 1: A Nested SBM with Binary Edges\nTo get a better sense of how homogeneous blocks are with respect to job titles, we can get the position labels from our internal property map position and compute a homogeneity score for the combination of job positions and block membership. To do so, we’ll iterate over the vertices in the graph and collect the information we need from the appropriate property maps.\nm1_l0_prop_map = model_1.levels[0].b # block assignment property map\n\nvertex_data = []\nfor v in g.vertices():\n    vertex_data.append((int(v), g.vp.position[v], m1_l0_prop_map[v]))\n\nvertex_data = pd.DataFrame(vertex_data)\nvertex_data.columns = [\"VertexID\", \"JobPosition\", \"M1BlockID\"]",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Latent structure in networks</span>"
    ]
  },
  {
    "objectID": "modeling-networks.html#model-1-a-nested-sbm-with-binary-edges-3",
    "href": "modeling-networks.html#model-1-a-nested-sbm-with-binary-edges-3",
    "title": "28  Latent structure in networks",
    "section": "28.7 Model 1: A Nested SBM with Binary Edges",
    "text": "28.7 Model 1: A Nested SBM with Binary Edges\nvertex_data.head(10)",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Latent structure in networks</span>"
    ]
  },
  {
    "objectID": "modeling-networks.html#model-1-a-nested-sbm-with-binary-edges-4",
    "href": "modeling-networks.html#model-1-a-nested-sbm-with-binary-edges-4",
    "title": "28  Latent structure in networks",
    "section": "28.8 Model 1: A Nested SBM with Binary Edges",
    "text": "28.8 Model 1: A Nested SBM with Binary Edges\nWe’ll use sklearn’s homogeneity score to calculate the uncertainty (via entropy) of the job positions within each block ID. The closer the score is to 1, the more homogeneous the blocks are with respect to job titles.\nm1_homogeneity = homogeneity_score(\n    vertex_data['JobPosition'], vertex_data['M1BlockID']\n)\n\nmodel_homogeneity_scores['Model 1'] = round(float(m1_homogeneity), 4)\npprint(model_homogeneity_scores)\nRemember, this is not supervised learning! We are not trying to group nodes based on their job titles, we are just hypothesizing that at least some of the network structure is driven by the formal roles people in the network hold. In that sense, the score doesn’t tell us anything about how good or bad our model is, but once we’ve fit a few more models we can compare homogeneity scores to see whether some models result in more homogeneous blocks than others, and whether patterns show up across models.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Latent structure in networks</span>"
    ]
  },
  {
    "objectID": "modeling-networks.html#model-2-a-nested-sbm-with-edge-weights",
    "href": "modeling-networks.html#model-2-a-nested-sbm-with-edge-weights",
    "title": "28  Latent structure in networks",
    "section": "28.9 Model 2: A Nested SBM with Edge Weights",
    "text": "28.9 Model 2: A Nested SBM with Edge Weights\nWe fit Model 1 using binary edges. In Model 2, we’ll complicate things a wee bit by including edge weights, which are counts. Before we model our edges, let’s get an initial sense of how they are distributed by plotting them as an ECDF (basically a cumulative histogram).\nsns.ecdfplot(g.ep.edge_weight.a)\nplt.xlabel(\"\\nNo. of emails sent from $i$ to $j$\")\nplt.ylabel(\"Proportion of edges\\n\")\nplt.title(\"EDCF of edge weights\\nin the Enron email network\\n\", loc=\"left\")\nplt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{int(x):,}')) # comma format the x-axis\nplt.savefig('output/enron_edge_weight_ecdf.png', dpi=300)",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Latent structure in networks</span>"
    ]
  },
  {
    "objectID": "modeling-networks.html#model-2-a-nested-sbm-with-edge-weights-1",
    "href": "modeling-networks.html#model-2-a-nested-sbm-with-edge-weights-1",
    "title": "28  Latent structure in networks",
    "section": "28.10 Model 2: A Nested SBM with Edge Weights",
    "text": "28.10 Model 2: A Nested SBM with Edge Weights\nIn the SBM framework, we can include edge weights as covariates in our generative model and assume that they are sampled from some probability distribution conditioned on the block partitions (i.e., a prior distribution).\nGiven that this data is over-dispersed (i.e., the variance is greater than the mean), a Negative Binomial distribution would be an appropriate choice to model these edges, but as of fall 2024 it is not implemented in graph-tool. While we can extend the distributions available in graph-tool by applying transformations,1 we’ll keep things relatively simple here by modelling the edges with a Poisson distribution. This should still provide a reasonable approximation, though it may not do the best job of characterizing the heavy tail of the distribution. Be mindful of this model limitation!",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Latent structure in networks</span>"
    ]
  },
  {
    "objectID": "modeling-networks.html#model-2-a-nested-sbm-with-edge-weights-2",
    "href": "modeling-networks.html#model-2-a-nested-sbm-with-edge-weights-2",
    "title": "28  Latent structure in networks",
    "section": "28.11 Model 2: A Nested SBM with Edge Weights",
    "text": "28.11 Model 2: A Nested SBM with Edge Weights\nWe’ll use the minimize_nested_blockmodel_dl() function again to fit Model 2, but this time we’ll add an argument called state_args. state_args contains two important pieces of information:\n\nthe edge weight property map, passed to recs, and\nthe probability distribution we want to use to model the edge weights, passed to rec_types.\n\nmodel_2 = gt.minimize_nested_blockmodel_dl(\n    g,\n    state_args = dict(\n        recs=[g.ep.edge_weight],\n        rec_types = ['discrete-poisson']\n    )\n)\nAs we did with Model 1, let’s plot the network’s hierarchical block structure (using the same hvprops and heprops customization for our draw_state() function).\ndraw_state(model_2, g, hvprops, heprops)\nWe’ll also add the new block assignments to the vertex_data dataframe we created above.\nm2_l0_prop_map = model_2.levels[0].b  # block assignment property map\n\nm2_vertex_data = {}\nfor v in g.vertices():\n    m2_vertex_data[int(v)] = m2_l0_prop_map[v]\n\nvertex_data['M2BlockID'] = vertex_data['VertexID'].map(m2_vertex_data)\nRemember that the integer block IDs are just labels; they aren’t meaningful in any numerical sense. The labels that get assigned depend on the model itself, so it doesn’t mean anything if a new model puts a node in a block with a different integer label. It may be the case that the node has been assigned to a different block, but the labels do not tell us if that is the case.\n\n28.11.0.1 MDL and Homogeneity\nVisually (?fig-enron_model_2) we can already see that the inclusion of edge weights has changed the model’s best estimate of the latent block structure. What did it do the MDL?\nm2_mdl = model_2.entropy()\nmodel_mdl_scores[\"Model 2\"] = int(round(m2_mdl))\npprint(model_mdl_scores)\nThe MDL score is much higher! However, this is to be expected, as the jump from a binary to a weighted network requires a huge increase in bits to adequately compress the network. We’ll see how this model compares to the other weighted models we’ll fit below.\nMDL is Always Larger for Models with Edge Weights and Other Covariates\nRecall that MDL is computed by compressing the model and the data. Models with more parameters will always have larger MDLs, and models with edge weights always have more parameters than binary networks. Weighted networks contain more information than binary networks, and therefore require more bits to compress.\nAlthough MDL is a model-agnostic way of comparing model fits, this doesn’t mean that the models using binary edges are to be preferred over weighted networks. We want to strike a good balance between having simple parsimonious models on the one hand and models that are complex enough to adequately explain our data on the other hand. This will often mean that, given the choice, we’ll want to a select models with the lowest MDL given a set of models that sufficiently capture our network, and it may be that models of binary networks are not considered sufficient when compared to their weighted counterparts. This is a matter of judgement.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Latent structure in networks</span>"
    ]
  },
  {
    "objectID": "modeling-networks.html#model-2-a-nested-sbm-with-edge-weights-3",
    "href": "modeling-networks.html#model-2-a-nested-sbm-with-edge-weights-3",
    "title": "28  Latent structure in networks",
    "section": "28.12 Model 2: A Nested SBM with Edge Weights",
    "text": "28.12 Model 2: A Nested SBM with Edge Weights\nLet’s plot the MDLs for the models we’ve fit so far for easy comparison (?fig-model_comparison_mdl_M1_M2). We’ll use the plot_line_comparison() function from the icsspy course package.\nplot_line_comparison(\n    models_dict=model_mdl_scores,\n    title=\"Model Comparison\\nMinimum Description Length (MDL)\\n\",\n    xlabel=\"\\nMDL\",\n    padding=5_000,\n    filename = 'output/model_comparison_mdl_M1_M2.png')",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Latent structure in networks</span>"
    ]
  },
  {
    "objectID": "modeling-networks.html#model-2-a-nested-sbm-with-edge-weights-4",
    "href": "modeling-networks.html#model-2-a-nested-sbm-with-edge-weights-4",
    "title": "28  Latent structure in networks",
    "section": "28.13 Model 2: A Nested SBM with Edge Weights",
    "text": "28.13 Model 2: A Nested SBM with Edge Weights\nWhat about the homogeneity of job titles within blocks?\nm2_homogeneity = homogeneity_score(\n    vertex_data['JobPosition'], vertex_data['M2BlockID']\n)\n\nmodel_homogeneity_scores['Model 2'] = round(float(m2_homogeneity), 4)\npprint(model_homogeneity_scores)\nWhen we model edge weights rather than just presence/absence, the best posterior partition aligns more closely with the formal job descriptions themselves.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Latent structure in networks</span>"
    ]
  },
  {
    "objectID": "modeling-networks.html#model-2-a-nested-sbm-with-edge-weights-5",
    "href": "modeling-networks.html#model-2-a-nested-sbm-with-edge-weights-5",
    "title": "28  Latent structure in networks",
    "section": "28.14 Model 2: A Nested SBM with Edge Weights",
    "text": "28.14 Model 2: A Nested SBM with Edge Weights\nLet’s plot these as well.\nplot_line_comparison(\n    models_dict=model_homogeneity_scores,\n    title=\"Model Comparison\\nBlock homogeneity wrt formal job titles\\n\",\n    xlabel=\"\\nHomogeneity\",\n    xrange=(0, 1),  # homogeneity scores range from 0 to 1\n    print_decimals=True,\n    filename = 'output/model_comparison_homogeneity_M1_M2.png')",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Latent structure in networks</span>"
    ]
  },
  {
    "objectID": "modeling-networks.html#model-3-weighted-poisson-distributed-refined",
    "href": "modeling-networks.html#model-3-weighted-poisson-distributed-refined",
    "title": "28  Latent structure in networks",
    "section": "28.15 Model 3: Weighted, Poisson-distributed, Refined",
    "text": "28.15 Model 3: Weighted, Poisson-distributed, Refined\nOnce again, we are modelling our edges as Poisson-distributed. The difference between Models 2 and 3 is that Model 3 includes some additional refinements on the partition we found with Model 2. Let’s take a moment to understand what this means.\n\n28.15.0.1 Why Refine Nested SBMs?\nMinimizing a model’s description length is equivalent to maximizing it’s posterior probability. In theory, when we find a good partition with minimal description length, we’ve also found the partition with maximum posterior probability. But this isn’t always the case, as our search space – the posterior distribution of all possible partitions – is vast and complex. It’s possible that the partition we find with minimize_nested_blockmodel_dl() came from a local minima rather than the global minima. If it came from a local minima, it may be very good but not the best.\nTo improve out estimates, we’ll further explore the posterior distribution, enough to escape any local minima and to find the global minima. This involves making iterative refinements using Markov Chain Monte Carlo (MCMC) sampling. We sample from our posterior distribution to find partitions that further lower MDL and maximize posterior probability. This process starts with our initial partition, rather than randomly, which makes the MCMC sampling process in graph-tool very efficient.\nIn short, you can think of the initial partition returned from the minimize_nested_blockmodel_dl() function as the best guess about the best partition, and the refined estimate as the best partition,2 identified by more thoroughly exploring the posterior distribution.\nHow do we do this in graph-tool? In short, we want to run a merge-split MCMC refinement algorithm multiple times, each time storing the resulting blockstates and description lengths. At the end, we select the model with the shortest description length. Let’s fit Model 3!",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Latent structure in networks</span>"
    ]
  },
  {
    "objectID": "modeling-networks.html#model-3-weighted-poisson-distributed-refined-1",
    "href": "modeling-networks.html#model-3-weighted-poisson-distributed-refined-1",
    "title": "28  Latent structure in networks",
    "section": "28.16 Model 3: Weighted, Poisson-distributed, Refined",
    "text": "28.16 Model 3: Weighted, Poisson-distributed, Refined\nSince this model is a refinement of Model 2, we use Model 2 as a base and accept new partitions only if they have shorter description lengths than the Model 2 partition.\nmodel_3 = model_2.copy() # will update if there are improvements to be had\nm3_mdl = model_2.entropy()\n\nnum_refinements = 10 # no. of times to refine the initial state using merge-split MCMC\nnum_mcmc_calls = 2_000 # no. of times to call multiflip_mcmc_sweep within a refinement loop\nnum_mcmc_iters_per_call = 10 # no. iterations to perform per single multiflip_mcmc_sweep call\n\nfor _ in range(num_refinements):\n    temp_state = model_2.copy()\n    for _ in range(num_mcmc_calls):\n        temp_state.multiflip_mcmc_sweep(beta=np.inf, niter=num_mcmc_iters_per_call)\n\n    if temp_state.entropy() &lt; m3_mdl:\n        model_3 = temp_state\n        m3_mdl = temp_state.entropy()\n\n# draw the model\ndraw_state(state=model_3, g=g, hvprops=hvprops, heprops=heprops)\nThat code block will take a while to run. As you wait for it to finish, let’s unpack the code a bit. Above, we set three variables that have a massive impact on how extensively we search the posterior distribution: num_refinements, num_mcmc_calls, and num_mcmc_iters_per_call. Let’s break down what each of these are, and how they work together to influence our search.\nThe first parameter, num_refinements, represents the number of separate searches of the posterior distribution we will conduct. Each starts from the same initial state – Model 2! – but is independent from the other searches. By performing a number of independent searches, in this case {{ num_refinements }}, we make it much less likely that our final result comes from a local minima.\nThe second, num_mcmc_calls, determines the number of times we call the multiflip_mcmc_sweep() function within a single refinement loop. If we set to 10_000, each refinement loop will execute multiflip_mcmc_sweep() 10,000 times, and hence will explore more of the parameter space. Once again, this increases the chances of finding improvements for our partition.\nFinally, num_mcmc_iters_per_call determines the number of iterations that multiflip_mcmc_sweep() performs in each individual call. Each iteration attempts to refine the current partition by proposing changes to the MCMC algorithm, and then either accepting or rejecting the change. When we increase this number, we again search the posterior distribution more thoroughly, increasing the chances of improving our partition.\nHow thoroughly we search the posterior distribution depends on all three of these parameters. The larger the numbers we provide our three search parameters, the more thoroughly we search the posterior distribution, and the more likely we are to find refinements for our model. The cost, of course, is increased computation time.\nm3_l0_prop_map = model_3.levels[0].b  # block assignment property map\n\nm3_vertex_data = {}\nfor v in g.vertices():\n    m3_vertex_data[int(v)] = m3_l0_prop_map[v]\n\nvertex_data['M3BlockID'] = vertex_data['VertexID'].map(m3_vertex_data)\nHow does Model 3 compare to Models 1 and 2?\nmodel_mdl_scores[\"Model 3\"] = int(round(m3_mdl))\npprint(model_mdl_scores)\n\nplot_line_comparison(\n    models_dict=model_mdl_scores,\n    title=\"Model Comparison\\nMinimum Description Length (MDL)\\n\",\n    xlabel=\"\\nMDL\",\n    padding=5_000,\n    filename = 'output/model_comparison_mdl_M1_M2_M3.png')\nm3_homogeneity = homogeneity_score(\n    vertex_data['JobPosition'], vertex_data['M3BlockID']\n)\n\nmodel_homogeneity_scores['Model 3'] = round(float(m3_homogeneity), 4)\npprint(model_homogeneity_scores)\n\nplot_line_comparison(\n    models_dict=model_homogeneity_scores,\n    title=\"Model Comparison\\nBlock homogeneity wrt formal job titles\\n\",\n    xlabel=\"\\nHomogeneity\",\n    xrange=(0, 1),\n    print_decimals=True,\n    filename = 'output/model_comparison_homogeneity_M1_M2_M3.png')",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Latent structure in networks</span>"
    ]
  },
  {
    "objectID": "modeling-networks.html#posterior-inference",
    "href": "modeling-networks.html#posterior-inference",
    "title": "28  Latent structure in networks",
    "section": "28.17 Posterior Inference",
    "text": "28.17 Posterior Inference\nIn this next section of the notebook, we’ll do two things that require more intensive analysis of the posterior distribution of block partitions.\nFirst, we’ll develop another Nested SBM that creates a “consensus partition” by averaging over a large sample of partitions from the posterior distribution, weighted by their posterior probabilities. This quantifies uncertainties in the blockmodelling process, including determining the number of blocks at each level, and block membership at each level (i.e. the block assignment marginal probabilities).\nSecond, we’ll perform some additional analyses of the posterior distribution with the goal of figuring out whether it contains other high-probability partitions that could represent plausible competing explanations, or “data stories,” for the network structure we’ve observed.\n\n28.17.1 Model 4: Weighted, Poisson-distributed, Consensus Partition\nIn Model 3, we refined the partition from Model 2 by more thoroughly searching the posterior distribution of partitions. Those refinements are optimizations of the Model 2 partition. In other words, we searched the posterior distribution for ways to improve our best guess about the best partition. We can improve our analysis even further by averaging over many posterior partitions, weighted by their posterior probabilities, rather than attempting to find the single best fitting partition. This quantifies uncertainty in our node-level block assignments and accounts for variability across many different but plausible partitions. We can use get_consensus_partition_from_posterior from the course package to simplify this process.\nmodel_4 = get_consensus_partition_from_posterior(model_3, g, force_niter=2000)\nLet’s take a look at our model’s MDL, and compare it to Models 1-3.\nm4_mdl = model_4.entropy()\nmodel_mdl_scores[\"Model 4\"] = int(round(m4_mdl))\n\nplot_line_comparison(\n    models_dict=model_mdl_scores,\n    title=\"Model Comparison\\nMinimum Description Length (MDL)\\n\",\n    xlabel=\"\\nMDL\",\n    padding=10_000,\n    filename = 'output/model_comparison_mdl_M1_M2_M3_M4.png')\nThe get_consensus_partition_from_posterior() function assigns each node to a block in the consensus partition (averages from partitions in the posterior weighted by posterior probability). We can extract that block information to consider homogeneity with respect to job titles for this model (below).\nAs we go through the usual process below, we’ll also extract information from a newly created internal property map that contains counts the times each node was assigned to each partition in our posterior samples. We can normalize these counts to get the each node’s marginal probability for it’s block assignment. We’ll store that information in our vertex_data dataframe as well and will look at it shortly.\nm4_l0_prop_map = model_4.levels[0].b\nmarginal_counts_consensus_partition = g.vertex_properties[\"pv\"]\n\nm4_vertex_assignments = {}\nm4_vertex_marginal_probs = {}\n\nfor v in g.vertices():\n    assigned_block = m4_l0_prop_map[v]\n    m4_vertex_assignments[int(v)] = assigned_block\n    # get the count for the assigned block\n    count_of_assigned_block = marginal_counts_consensus_partition[v][assigned_block]\n    # normalize the count to get the probability\n    total_count = sum(marginal_counts_consensus_partition[v])\n    prob_of_assigned_block = count_of_assigned_block / total_count\n    m4_vertex_marginal_probs[int(v)] = prob_of_assigned_block\n\nvertex_data['M4BlockID'] = vertex_data['VertexID'].map(m4_vertex_assignments)\nvertex_data['MargProbsConsPart'] = vertex_data['VertexID'].map(m4_vertex_marginal_probs)\nWhat do our homogeneity look like now, using the consensus partition?\nm4_homogeneity = homogeneity_score(\n    vertex_data['JobPosition'], vertex_data['M4BlockID']\n)\n\nmodel_homogeneity_scores['Model 4'] = round(float(m4_homogeneity), 4)\n\nplot_line_comparison(\n    models_dict=model_homogeneity_scores,\n    title=\"Model Comparison\\nBlock homogeneity wrt formal job titles\\n\",\n    xlabel=\"\\nHomogeneity\",\n    xrange=(0, 1),\n    print_decimals=True,\n    filename = 'output/model_comparison_homogeneity_M1_M2_M3_M4.png')\nWe can also use the pv property map to change the appearance of the nodes in our visualizations to fractions of a pie, reflecting the marginal probabilities of block assignments. We won’t use our draw_state() function this time, since we’re using some new arguments that we didn’t include in that function: vertex_shape and vertex_pie_fractions.\nhvprops['size'] = 15\n\nmodel_4.draw(\n    vertex_shape=\"pie\",\n    vertex_pie_fractions=g.vp['pv'],\n    hvprops=hvprops,\n    heprops=heprops,\n)\n\n\n28.17.2 Competing Explanations?\nWhen developing generative models, the posterior distribution may contain multiple explanations of the network structure with nearly equal probability. This is exactly why Bayesian generative modelling emphasizes analysis of the full posterior distribution rather than selecting a single best estimate.\nIn the case of SBMs, even when we create a consensus partition from the posterior, we may still want to know whether the posterior contains clusters of similar partitions that differ from the one we selected, but which are still very plausible. Each these different partitions may be offering different generative explanations of the observed network, which is definitely something we would want to know!\nTo determine whether this is the case, we can infer the modes of the posterior distribution to identify clusters of graph partitions that are similar to one another but different from those in other clusters (see Peixoto (2021)).\n\n\n28.17.3 Competing Explanations?\nn_partitions_sample = 5000 # the larger the number, the more accurate the estimates\n\nstate = gt.NestedBlockState(g) # initialize\ngt.mcmc_equilibrate(state,\n    force_niter=1000,\n    mcmc_args=dict(niter=10)\n)\n\nbs = []\n\ndef collect_partitions(s):\n   global bs\n   bs.append(s.get_bs())\n\ngt.mcmc_equilibrate(state,\n    force_niter=n_partitions_sample,\n    mcmc_args=dict(niter=10),\n    callback=collect_partitions\n)\n\n# infer partition modes in posterior\npmode = gt.ModeClusterState(bs, nested=True)\n\n# minimize the mode state itself\ngt.mcmc_equilibrate(pmode, wait=1, mcmc_args=dict(niter=1, beta=np.inf))\n\n# get inferred modes\nmodes = pmode.get_modes()\n\nfor i, mode in enumerate(modes):\n    b = mode.get_max_nested()    # mode's maximum\n    pv = mode.get_marginal(g)    # mode's marginal distribution\n\n    print(f\"Mode {i} with size {mode.get_M()/len(bs)}\")\n    state = state.copy(bs=b)\n    state.draw(\n        vertex_shape=\"pie\",\n        vertex_pie_fractions=pv,\n        output_size=(1200, 1200),\n        output=f\"output/enron-partition-mode-{i}.png\")\n😎 We don’t have competing explanations in our posterior distribution! While there are minor differences in the partitions – as we’ve seen from our initial models in this notebook – there are no competing clusters with comparable high probability.\nThis code will list each mode in the posterior along with the collective posterior probability of the partitions in each mode. The number of modes detected will depend on the posterior distribution for any given network, but of course the probabilities of all the modes combined will sum to 1. It will also create a series of plots, one for each mode in the posterior, that plots the marginal node block assignment probabilities by drawing the nodes as small pie charts, just like we did above. If there are multiple modes, this will help us compare the different stories or explanation that each partition tells.\nTomorrow, we’ll develop your generative modelling toolkit to include simulation and agent-based modelling!\n\nTODO: This is the original text from the print edition. Update it / replace it with the new content I wrote for FCIT / GESIS. It’s much better.\n\n\n\n28.17.4 Bayesian Hierarchical Stochastic Blockmodels\nUnlike their deterministic counterparts, Bayesian stochastic blockmodels conceptualize network structure as a latent variable problem to be addressed with a generative model. Just as LDA assumes that specific combination of words observed in documents are generated from shared latent themes, SBMs assume that specific patterns of ties between nodes in social networks are generated from some latent network structure that influences the formation and dissolution of relationships. The types of latent structure that we are interested in varies, and we can develop models for specific types of structure.\nHaving a probabilistic model of how this works, grounded in plausible generative mechanisms, is an important part of developing models that don’t under or overfit our data. It helps us differentiate structure from random noise in the process of moving from concrete connections between concrete nodes to general connections between categories of nodes. This allows us to overcome some of the limitations of deterministic approaches, which can be tripped up by structure that is caused by random fluctuations rather than some meaningful network-driven social process.\nTiago T. P. Peixoto (2019) summarizes the Bayesian response to this problem in one pithy paragraph:\n\n“The remedy to this problem is to think probabilistically. We need to ascribe to each possible explanation of the data a probability that it is correct, which takes into account modeling assumptions, the statistical evidence available in the data, as well as any source of prior information we may have. Imbued in the whole procedure must be the principle of parsimony – or Occam’s razor – where a simpler model is preferred if the evidence is not sufficient to justify a more complicated one” (page 4).\n\nAs with LDA, the underlying logic of developing a Bayesian generative model here is the same as in other contexts. To continue drilling that underlying logic:\n\nwe have observed data (connections between nodes in a network) and unobserved latent variables (block or community membership);\nwe want to infer the distributions of the latent variables (i.e., the assignment of nodes into latent blocks) conditional on the observed data;\nto do so, we construct a joint probability distribution of every possible combination of values for our latent and observed variables (i.e., the numerator in Bayes theorem) and then perform approximate posterior inference to determine the probabilities of different distributions on the latent variables conditional on the observed data.\n\nWe are after the posterior probabilities of many different partitions of the network conditioned on the connections we observe. In other words, we want to know the conditional probability that some node partition \\(b\\) could have plausibly generated an observed network \\(G\\),\n\\[\\begin{align}\nP(\\text{b}|\\text{G})\n\\end{align}\\]\nAs with all Bayesian models, we need to play the “what’s that” game, providing priors for all latent variables. The natural tendency here is to prefer uniform priors. If you recall from Chapter 28, using a uniform distribution for our priors means assigning an equal probability to every possible value of the latent variable. T. P. Peixoto (2019) has shown, however, that this strategy often results in suboptimal results with network models, as it has an a priori preference for solutions with number of blocks comparable to the number of nodes in the network. Who wants that? Nobody. Instead, T. P. Peixoto (2019) proposes a three-level hierarchical Bayesian approach where we sample (1) the number of blocks, (2) the sizes of each block, and the (3) the partition of the observed network into those blocks.\nThis hierarchical model is much less likely to overfit our data, and it does so without requiring us to determine the number of groups in advance, or indeed making any assumptions about the higher-order structure of the networks we are interested in. We will use this model exclusively below. It’s known as a nested Stochastic Blockmodel. T. Peixoto (2014) describes a number of interesting variations on inference algorithms for this hierarchical model. One very important thing to know about the SBM implementation in graph-tool is that rather than strictly considering equivalence, it also considers the probability of nodes connecting to other nodes, in the more standard sense of network models we’ve looked at previously. This means that the network partitions from graph-tool will be based on a mixture of assortative community structure (as we’ve seen in Chapter 15 with Louvain and Leiden) along with disassortative (structural equivalence). Incorporating edge weights into the SBM estimation tends to push the balance in the results towards the assortative side, which makes some intuitive sense - a highly weighted connection between two nodes could drown out the latent influence of structural equivalence. We will examine this shortly.\nThis has all been very abstract. Let’s get our hands dirty with some code.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Latent structure in networks</span>"
    ]
  },
  {
    "objectID": "modeling-networks.html#blockmodelling-with-graph-tool",
    "href": "modeling-networks.html#blockmodelling-with-graph-tool",
    "title": "28  Latent structure in networks",
    "section": "28.18 BLOCKMODELLING WITH GRAPH-TOOL",
    "text": "28.18 BLOCKMODELLING WITH GRAPH-TOOL\nWhen it comes to the fitting Bayesian stochastic blockmodels, there’s no beating Tiago Peixoto’s graph-tool, in Python or otherwise. It has astonishing performance in terms of both speed and memory, and as a result it can handle exceptionally large networks efficiently. This performance is achieved by offloading most of the heavy lifting to C++ on the back-end. The cost of these performance improvements, however, is that using graph-tool is less “Pythonic” than you might be used to by this point in the book. Graph-tool is considerably more complex than the network analysis packages we’ve seen so far (Networkx and NDLib).\nThe additional overhead and less Pythonic nature that gives graph-tool it’s superior performance capabilities also means that I have to spend more time upfront describing how things work. It is entirely possible some of this won’t really “sink in” until you start working with graph-tool. That’s OK! Once you get your hands dirty with some models and have built up a bit of intuition, you can always come back to this content to deepen your understanding.\n\n28.18.1 Installing graph-tool\nThe easiest way to get up and running with graph-tool is to install it via conda-forge with the following command. Because of its numerous dependencies, I strongly recommend that you do this inside a Conda environment (such as the dcss environment, if you’ve been following along with the supplementary learning materials). As a reminder, Conda environments were introduced in Chapter 2.\nconda install -c conda-forge graph-tool\nIf you haven’t been using an environment already, you can also install graph-tool inside a conda environment designed specifically for graph-tool. You can use that environment the same way you use any other conda environment. To download and activate the graph-tool environment, simple execute the following from the command line:\nconda create --name gt -c conda-forge graph-tool\nWhen conda prompts you for permission to download and install the required packages, agree. When it’s finished, activate the environment with\nconda activate gt\nWhen you do so, you should see your command prompt change; it will now start with (gt) (as opposed to dcss if you’ve been using the conda environment for this book). If you are using Jupyter, note that you’ll have to launch your Jupyter Notebook server inside that environment to access the packages inside the environment.\n\n\n28.18.2 Understanding Property Maps\nThe most important graph-tool concept to understand is how its array-like “property maps” work. Rather than attaching information about a node (e.g., its ID / label or degree centrality) to the node itself, each node in the network is assigned a unique index. That same index is contained in a property map, and whenever we want to know some information about a node, we use the node index to find the relevant information in the property map. There’s a bit of extra friction here, though: because of the C++ backend, each property map object contains only one type of data, that you have to declare in advance. This is a pain, but it’s what allows us to enjoy some pretty remarkable performance improvements.\nBecause graph-tool makes such heavy use of these array-like property maps, it’s easiest to think of a network in graph-tool as a collection of associated arrays. For example, in a network with three nodes – ['Lebron James', 'Anthony Davis', 'Kentavious Caldwell-Pope'] – and an associated property map of colours – [Red, Green, Blue] – Lebron James would be Red, Antony Davis would be Green, and Kentavious Caldwell-Pope would be Blue. We can encode just about anything in a property map, including vectors of values. For example, the [Red, Green, Blue] property map could also be stored as RGB values, [[255,0,0], [0,128,0], [0,0,255]], which would associate [255,0,0] with Lebron James.\nIt’s also very important to note that\n\ngraph-tool does not automatically label nodes, and\nit is possible for multiple nodes can have the same label.\n\nThis can result in some unwelcome surprises. For example, if your edgelist contains strings as opposed to numbers – such as\n[\n    ('Karamo', 'Tan'), \n    ('Karamo', 'Tan')\n]\nthen graph-tool will create four different nodes and two edges rather than creating two nodes and aggregating the edges into a weight of 2 for the tie between Karamo and Tan.\nYou might recall from Chapter 14 that different disciplines tend to use different words to refer to nodes and edges. In graph-tool, nodes are referred to as vertices. They are exactly the same. When we create a new vertex in graph-tool – v = g.add_vertex() – v becomes a vertex class object, which we can refer to as a vertex descriptor. Vertex descriptors are alternative to node indices and can be used to access information about a node from a property map. If we assigned our [Red, Green, Blue] property map to an object called colour_map, we could retrieve the information for node v with colour_map[v].\nEdge property maps, which can contain useful information such as edge weight, behave somewhat differently. They are accessed using edge descriptors, which can be obtained from the source and target nodes. For example, we might obtain and store an edge descriptor between nodes Karamo and Tan with e = g.edge('Karamo','Tan') or e = g.edge(1, 2) if you’ve assigned Karamo and Tan integer IDs to benefit from faster compute times.\nFinally, entire networks can themselves can have property maps. These network-level property maps can be accessed by passing the graph object itself. For example, if we have a network object called g and a property map called graph_property_map, we could access the properties with graph_property_map[g].\nThis might sound like a lot of additional overhead to worry about when conducting a network analysis, but you’ll likely find the impact fairly minimal once you get used to things. As with other network analysis packages, it makes it relatively easy to do a large amount of data processing outside of the package itself. For example, you can do a lot of work with the data that will eventually be stored as property maps using Pandas and Numpy. My main advice here is take great care that all of the data in your lists and arrays are in the same order, and of equal lengths.\nNow, let’s model.\n\n\n28.18.3 Imports\nfrom graph_tool.all import *\nimport pandas as pd\npd.set_option(\"display.notebook_repr_html\", False)\nimport matplotlib\nimport numpy as np\nimport math\nimport pickle\nfrom dcss.networks import label_radial_blockmodel, get_block_membership\n\n\n28.18.4 Data\nAs usual, I suggest refreshing yourself on the data we are using here by returning to the overview of datasets from Chapter 1. In brief, the Enron email data is provided as two CSV files, one with the edges between employees who have exchanged emails with one another, and one with the organizational position of Enron employees.\nWhen developing a blockmodel, we typically do so without having some external set of positions or roles that we want to approximate; the goal here is not supervised learning. However, for learning purposes, our goal will be to develop a blockmodel using relational data that mirrors job titles. The purpose of doing things this way is to illustrate the power of this approach to network analysis, as well as make the discussion of “positions” a bit less abstract. So, remember that when we talk about “positions” and “roles,” we don’t always (or even often) mean official positions or roles such as job titles.\nThe two datasets below contain the relational data from employee email communications and information about the job title each employees held in the organization.\nedges_df = pd.read_csv('data/enron/enron_full_edge_list.csv')\nedges_df.head()\n                      source                             target\n0    press.release@enron.com            all.worldwide@enron.com\n1  office.chairman@enron.com             all.downtown@enron.com\n2  office.chairman@enron.com      all.enron-worldwide@enron.com\n3    press.release@enron.com            all.worldwide@enron.com\n4  office.chairman@enron.com  all_enron_north.america@enron.com\nAs you can see, our edgelist has two columns, source and target. We don’t have any edge weights (though we will compute them below) or other edge attributes.\nemployee_df = pd.read_csv('data/enron/enron_employees_updated.csv')\nemployee_df.head()\n                           id                  position\n0        liz.taylor@enron.com  Administrative Assistant\n1    michelle.lokay@enron.com  Administrative Assistant\n2  holden.salisbury@enron.com                   Analyst\n3        kam.keiser@enron.com                   Analyst\n4   matthew.lenhart@enron.com                   Analyst\nThe information about each employee’s official position in the organization is provided in a column called 'position'. Let’s count the number of employees in each role.\nemployee_df['position'].value_counts()\nTrader                      35\nVice President              26\nDirector                    17\nManager                     15\nIn House Lawyer             11\nSenior Specialist            8\nSpecialist                   6\nManaging Director            6\nAnalyst                      5\nEmployee                     5\nPresident                    4\nCEO                          4\nAdministrative Assistant     2\nAssociate                    2\nSenior Manager               1\nCOO                          1\nCFO                          1\nName: position, dtype: int64\n\n28.18.4.1 Constructing the Communication Network\nTo create our network, let’s construct a weighted communication network between core employees using the edgelist and node attribute files above. First, we’ll aggregate and count edges to compute a weight. We’ll ignore any nodes that are not in the employee_df dataframe, narrowing our focus to core employees only. The “core employees” are those who were involved the legal proceedings following the Enron scandal.\nSince this is a directed communication network, i,j ties are different than j,i ties, so we can simply aggregate our edges dataframe by the combination of 'source' and 'target' columns and treat the count of their occurrences as our edge weight. We’ll also filter the resulting dataframe so that it only includes nodes that are part of the core employee subset.\nedges_df = edges_df.value_counts(['source', 'target']).reset_index(name='count').copy()\ncore_employees = set(employee_df['id'].tolist())\n\ncore_edges_df = edges_df[edges_df['source'].isin(core_employees) & \n                         edges_df['target'].isin(core_employees)]\nWith our weighted directed edgelist created, we can initialize a directed network.\neG = Graph(directed = True)\nWe can add the core employees to this network as nodes, add their job titles to a property map, and add the edge data (weights) to a property map. We’ll do that in three steps:\n\nget the information into lists,\ninitialize the property maps and tell graph-tool what type of data they we are going to provide, and\nloop over our two lists to add the employees to the networks and their node and edge attributes (job titles, edge weights) to property maps.\n\nFirst, create the lists!\nemployee_list = employee_df['id'].tolist()\ntitle_list = employee_df['position'].tolist()\nSecond, initialize the property maps! Note that in addition to the property maps themselves, we are creating a dictionary called vertex_lookup. As mentioned earlier in the chapter, we can use this to dictionary to simplify the ‘lookup’ process to select nodes using string values that carry some meaning about the node, rather than the integer identifier used by graph-tool.\nSince we are going to use email addresses as node labels, we’ll initialize a property map called labels and tell graph-tool to expect strings (because email addresses are strings). Similarly we will initialize a property map for job titles, called titles, and also containing strings. Finally, we will create an edge_weight property map. Since edge weights are integers in this case, we will tell graph-tool to expect integers.\nvertex_lookup = {}\n\nlabel = eG.new_vertex_property('string')\ntitle = eG.new_vertex_property('string')\nedge_weight = eG.new_edge_property('int')\nNow we’re ready to add information to the property maps! Let’s zip up our employee_list and title_list and then iterate over it. For each pairing of elements from the two lists, we’ll add the core employees to the network as nodes, their email addresses to the labels property map, and their job titles to the titles property map. Finally, we will add the information about the node index to the vertex_lookup dict we created above.\nfor vertex in zip(employee_list, title_list):\n    # create a new vertex instance\n    v = eG.add_vertex()\n\n    # add attributes to the property maps in the index position of the vertex\n    label[v] = vertex[0]\n    title[v] = vertex[1]\n\n    # add the vertex to the lookup dictionary, converting it to an integer \n    vertex_lookup[vertex[0]] = int(v)\nAs you probably anticipated, the next thing we need to do is process the edges between nodes. We can do that by using lists pulled from the edges dataframe, but remember we also need to consult vertex_lookup to ensure we are assigning the right edges between the right nodes!\nsource_list = core_edges_df['source'].tolist()\ntarget_list = core_edges_df['target'].tolist()\nweight_list = core_edges_df['count'].tolist()\n\nfor nodes in zip(source_list, target_list, weight_list):\n    from_idx = vertex_lookup[nodes[0]]\n    to_idx = vertex_lookup[nodes[1]]\n\n    # Let's ignore self-loops\n    if from_idx != to_idx:\n        edge = eG.add_edge(from_idx, to_idx)\n        edge_weight[edge] = nodes[2]\nWe’ve now reached the very final bit of preparation. We’ll make each of the property maps we’ve just initialized and populated with information internal to the graph and save the graph in graph-tool’s own format. That way we don’t need to recreate the network again later, we can just load up the network with all the relevant property maps already defined.\neG.vertex_properties['label'] = label\neG.vertex_properties['title'] = title\neG.edge_properties['edge_weight'] = edge_weight\n\nlookup = eG.new_graph_property('object')\nlookup[eG] = vertex_lookup\neG.graph_properties['vertex_lookup'] = lookup\nAnd with that, we’re ready to start developing stochastic blockmodels!\n\n\n\n28.18.5 Developing Stochastic Blockmodels\nIn the introduction, we discussed how there are some properties that stochastic blockmodels share with LDA. One of those properties is the process for developing, critiquing, improving, and eventually selecting the best model in an iterative fashion: Box’s loop. For example, in this case, after approximating the posterior distribution of the latent variables, we can test the fit of that posterior on the data, and repeat the process using the insight gained about what is and isn’t working in the model. In theory, enough iterations would produce the best model possible in terms of representing the data (not in terms of the usefulness of the results). In practice, we have to make a choice about when we’re satisfied with the results, because there’s no good way to know how many iterations it would take to produce the best model you can given the data you have.\nAs I mentioned earlier, our goal here is to develop a blockmodel that will partition our network into a set of positions that mirror the job titles that the core employees held within Enron. The catch, of course, is that we want to do this using only information from the relational data itself.\ngraph-tool has a very handy function, minimize_nested_blockmodel_dl(), that takes care of all the hard work for us. It’s fast to run, and tends to produce good results right out of the box. minimize_nested_blockmodel_dl() attempts to minimize something called the “description length” of a nested blockmodel. Let’s break this down, starting with the nested part. As you hopefully recall from earlier in this chapter, a nested stochastic blockmodel is a hierarchical Bayesian model. In other words, it embeds blocks inside other blocks in a multi-level hierarchy. Doing things this way makes it easier to find small blocks in a network that may contain a small number of nodes.\nThe minimize and dl parts of minimize_nested_blockmodel_dl() are a shorthand for minimize the description length. Minimum description length is an operationalization of Occam’s razor; it suggests that the best model is one that can represent all of the data with the least amount of information required. It helps us select a model that fully explains the data but is as simple as possible given the observed data.\nFinally, the blockmodel we will fit here is also degree-corrected (Karrer and Newman 2011). A standard baseline SBM assumes that nodes within any given block tend to have very similar, if not identical, degrees. Since this is extremely unrealistic in real world networks, it is almost always better to use the degree-corrected implementation.\nstate = minimize_nested_blockmodel_dl(eG, deg_corr = True)\nWith that one line of code, we’ve executed our 3-level Hierarchical Bayesian Stochastic Blockmodel!\nThe function we just executed created something called a blockstate, which is an object containing the results of partitioning the network running our blockmodel. We can print a summary of the blockstate for our nested degree-corrected description-length-minimized blockmodel to find out\n\nthe number of blocks that nodes were assigned to,\nthe number of levels in the nested hierarchy, and\nthe number of “meta-blocks” at each of those levels (blocks within blocks in the nested hierarchy).\n\nstate.print_summary()\nl: 0, N: 149, B: 13\nl: 1, N: 13, B: 4\nl: 2, N: 4, B: 1\nRemember that the model we just ran is a stochastic generative model, so the number of blocks will vary for each run of the model, but it typically finds 12-14 blocks at the bottom level. Remember, this is a nested variant where the “bottom level” consists of all the individual nodes, while the upper levels of the hierarchy are aggregate blocks, found by creating a new network where each block is a node and estimating a blockmodel based on that network. After some consideration, 12-14 blocks seems fairly reasonable. We have 17 job titles in the data but if we combined “Manager + Senior Manager”, “Senior Specialist + Specialist”, “Administrative Assistant + Employee”, and “CEO + CFO + COO”, we’d have 12 titles. This kind of combination would not impact the computation of the model at all and can be left until it’s time for interpretation.\nFinally, we can get a quick sense of how things went by visualizing the blockmodel (?fig-31_01). I’m limited to a narrow colour palette in print, but you can access a full resolution colour version of the image (and others like it) in the supplementary online materials. I recommend looking at the color versions of these images, as colour is used very effectively in these blockmodel visualizations.\nstate.draw(\n    layout = \"sfdp\", \n    vertex_text = eG.vertex_properties['title'], \n    eorder = eG.edge_properties['edge_weight'],\n    vertex_text_position = 315,\n    bg_color=[255,255,255,1],\n    output_size=[4024,4024],\n    output='figures/core_enron_blockmodel_sfdp.pdf'\n    )\n\n\n\nCap\n\n\nIn this figure, each node is represented by an individual point (as in other network visualiztions), only the nodes are organized into blocks. The squares are points where blocks converge up the hierarchy to form the nested structure - the structure of email exchanges between blocks will decide whether a block should be grouped with another one. For example, if you look at the group of 6 blocks in the top left of the image, you might notice that there are only two traders present, but there are a lot of lawyers and vice presidents, as well as a CEO.\nThis first attempt is already looking pretty good. We have 3 of the 4 CEOs in the same block near the right-hand side, along with three presidents. Note for later: the remaining CEO isn’t in the same meta-block - one level up the hierarchy - as the other CEOs.\nAs with other generative models, we need to think through generative mechanisms here. If you recall from Chapter 25, all this really means is that we need to think through simple social and interactional processes that may have resulted in (i.e., generated) the patterns we see in our data. What’s a plausible story of how this data was generated?\nRemember that we are detail with email communication between employees in an organization here. There are many ways to imagine the social mechanisms that best predict structure in a network like this. In this case, it could be that emails between the core employees predicts the relationship between those employees, or it could be that the emails they send to other non-core employee Enron email addresses are more predictive. This is an exploratory process that can’t fit reasonably in this chapter, but you can see a bit of it in the online supplement.\nLet’s see what the outcome is with different blockmodel estimation criteria. Stochastic blockmodels in graph-tool are able to incorporate edge weights into the estimation.\nstate_w = minimize_nested_blockmodel_dl(eG, deg_corr = True, \n                                              state_args=dict(\n                                                  recs=[eG.edge_properties['edge_weight']],\n                                                  rec_types=[\"discrete-binomial\"]))\nstate_w.print_summary()\nl: 0, N: 149, B: 67\nl: 1, N: 67, B: 10\nl: 2, N: 10, B: 2\nl: 3, N: 2, B: 1\nWe can see already that we end up with far too many blocks to be useful here! There’s no need to visualize this graph, but we have another option - let’s try setting the number of blocks to be the same as it was for the unweighted model, then see what the weights do for the results.\nstate_w2 = minimize_nested_blockmodel_dl(eG, deg_corr = True, B_min=12, B_max=12,\n                                              state_args=dict(\n                                                  recs=[eG.edge_properties['edge_weight']],\n                                                  rec_types=[\"discrete-binomial\"]))\nstate_w2.print_summary()\nl: 0, N: 149, B: 12\nl: 1, N: 12, B: 3\nl: 2, N: 3, B: 2\nl: 3, N: 2, B: 1\nAt first glance (?fig-31_02), incorporating edge weight seems as though it produces more tightly-knit, smaller blocks, and only two distinct groups of blocks one level up the hierarchy where we had four with the first model. The larger blocks are also more heterogenous, with CEO’s grouped alongside many traders and even “employees”.\nstate_w2.draw(\n    layout = \"sfdp\", \n    vertex_text = eG.vertex_properties['title'], \n    eorder = eG.edge_properties['edge_weight'],\n    vertex_text_position = 315,\n    bg_color=[255,255,255,1],\n    output_size=[4024,4024],\n    output='figures/core_enron_blockmodel_sfdpw.pdf'\n    )\n\n\n\nCap\n\n\nThe use of edge weights in a blockmodel is a theoretical consideration more than it is a technical one, so it takes some careful thought and experimenting to see what the impact is. In our case, we have people with quite different roles in the company, so their email volume will be quite different. If we don’t use edge weights, we stick to a stricter definition of equivalence, closer to structural, and here this produces the most intuitive results. Nonetheless, we should have a way to compare the results beyond just looking at a graph - these graphs won’t be very helpful for huge networks. We can use the get_block_membership utility from the dcss package to add block assignment information to the employee dataframe.\nemployee_blocks_df = get_block_membership(state, eG, employee_df,\n                                         'model_uw_1')\nemployee_blocks_df = get_block_membership(state_w2, eG, employee_blocks_df,\n                                         'model_w_2')\nLet’s take a look at some of the job titles that one would expect to be more well-defined.\ndf_by_position = employee_blocks_df.groupby('position').agg(list)\ndf_by_position[df_by_position.index.isin(['CEO','President', 'In House Lawyer'])].head()\n                                                                id  \\\nposition                                                             \nCEO              [david.w.delainey@enron.com, jeff.skilling@enr...   \nIn House Lawyer  [bill.rapp@enron.com, carol.clair@enron.com, d...   \nPresident        [greg.whalley@enron.com, jeffrey.a.shankman@en...   \n\n                                 model_uw_1_block_id  \\\nposition                                               \nCEO                                     [5, 5, 0, 0]   \nIn House Lawyer  [1, 9, 10, 10, 9, 7, 5, 3, 9, 9, 3]   \nPresident                              [5, 5, 0, 12]   \n\n                                model_w_2_block_id  \nposition                                            \nCEO                                   [0, 5, 1, 5]  \nIn House Lawyer  [9, 9, 0, 8, 2, 8, 3, 5, 6, 1, 6]  \nPresident                             [2, 2, 1, 5]  \nYou might be able to get a sense of things from some of the smaller lists here. For example, in the model_uw_1_block_id column, we can see that one block has 3 of the 4 CEOs, as well as 3 of the 4 Presidents, while another has the remaining CEO and President. 6 of the lawyers also tend to end up in the same block on this run (again, this is stochastic so results might vary a little bit). With the weighted model, only two of the CEOs end up in the same block, although they are joined by a President and a lawyer.\nAlternatively, we can count the number of unique block assignments by role (job title) and calculate the average, based on the number of people with those roles. A lower value here would be a loose indicator of accuracy, with two caveats: a 0.5 value for CEO would be the same if the 4 CEOs were divided equally into two blocks, rather than 3 in one block and 1 in another. This block assignment difference is conceptually significant, so a more robust metric might be desirable. Job titles that apply to only 1 employee will also, necessarily, have a perfectly poor score of 1.0 every time.\nemployee_blocks_df.groupby(['position'])['model_uw_1_block_id'].agg(lambda x: x.nunique()/x.count())\nposition\nAdministrative Assistant    1.000000\nAnalyst                     0.400000\nAssociate                   1.000000\nCEO                         0.500000\nCFO                         1.000000\nCOO                         1.000000\nDirector                    0.411765\nEmployee                    0.600000\nIn House Lawyer             0.545455\nManager                     0.466667\nManaging Director           0.666667\nPresident                   0.750000\nSenior Manager              1.000000\nSenior Specialist           0.875000\nSpecialist                  0.500000\nTrader                      0.200000\nVice President              0.423077\nName: model_uw_1_block_id, dtype: float64\nprint(employee_blocks_df.groupby(['position'])['model_uw_1_block_id'].agg(lambda x: x.nunique()/x.count()).sum())\nprint(employee_blocks_df.groupby(['position'])['model_w_2_block_id'].agg(lambda x: x.nunique()/x.count()).sum())\n11.338629507747154\n11.916386064915477\nWe can do the exact inverse to roughly assess the homogeneity of the blocks, by reversing the columns in the groupby operation.\nemployee_blocks_df.groupby(['model_uw_1_block_id'])['position'].agg(lambda x: x.nunique()/x.count())\nmodel_uw_1_block_id\n0     0.750000\n1     0.583333\n2     0.277778\n3     0.476190\n4     0.555556\n5     0.416667\n6     0.230769\n7     0.625000\n8     0.714286\n9     0.400000\n10    0.666667\n11    0.500000\n12    0.666667\nName: position, dtype: float64\nprint(employee_blocks_df.groupby(['model_uw_1_block_id'])['position'].agg(lambda x: x.nunique()/x.count()).sum())\nprint(employee_blocks_df.groupby(['model_w_2_block_id'])['position'].agg(lambda x: x.nunique()/x.count()).sum())\n6.862912087912089\n7.970732305329079\nThis loose evaluation suggests that the unweighted model might be preferred, but we can do better with this evaluation. Sci-kit learn provides many classification evaluation metrics and the problem we’re solving here is essentially a clustering classification. There are metrics within sklearn’s clustering section that provide the above evaluations but with more nuance (remember the equivalent 0.5 score if the CEOs were clustered with different proportions but the same number of blocks). A homogeneity_score evaluates, you guessed it, the homogeneity of the detected clusters, so if clusters contain more of the same type of job title, the results will score higher. Scores here are on a scale from 0 to 1, with 1 being the best.\nfrom sklearn.metrics import homogeneity_score, completeness_score, adjusted_mutual_info_score\nLet’s compare homogeneity scores for the unweighted network and then the weighted one. As with the rough evaluation above, the unweighted model has a better score.\nhomogeneity_score(employee_blocks_df['position'], employee_blocks_df['model_uw_1_block_id'])\n0.353428152904928\nhomogeneity_score(employee_blocks_df['position'], employee_blocks_df['model_w_2_block_id'])\n0.25528558562493037\nThe completeness_score inverts the previous score, instead assessing the homogeneity of block assignments for each job titles, so the degree to which nodes are assigned to blocks with other nodes that have the same title. The result is actually very similar in this case!\ncompleteness_score(employee_blocks_df['position'], employee_blocks_df['model_uw_1_block_id'])\n0.3435558493343224\ncompleteness_score(employee_blocks_df['position'], employee_blocks_df['model_w_2_block_id'])\n0.2771316517440044\nFinally, we can also do both of the above in a unified score, adjusted_mutual_info_score, where homogeneity and completeness are considered together and the position of the ground-truth and predicted labels doesn’t matter. This can also be used to calculate agreement between two labelling methods, when there is no known ground-truth, but unfortunately our block assignment classifications will not be the same between models - block 1 in one model is not necessarily the same as block 1 in the next, or even in repeat runs of the same model. Note that this method is a version of normalized_mutual_info_score that is adjusted to account for chance, because the standard mutual information score tends to overestimate the shared information between models that have a larger number of clusters.\nFor this score, the maximum is 1 but it is possible to have a negative score if the predicted clusters are nonsensical enough. We can see that the adjusted mutual info score below is roughly half of the individual scores above, for the unweighted network. For the weighted network, the score is much lower. If we compare the two block assignments together, they actually have more agreement with each other than the weighted model has with the ground truth job titles.\nadjusted_mutual_info_score(employee_blocks_df['position'], employee_blocks_df['model_uw_1_block_id'])\n0.15309516996415473\nadjusted_mutual_info_score(employee_blocks_df['position'], employee_blocks_df['model_w_2_block_id'])\n0.0756412457785869\nadjusted_mutual_info_score(employee_blocks_df['model_w_2_block_id'], employee_blocks_df['model_uw_1_block_id'])\n0.15563023649762936\nWith this information in mind, let’s continue on with the unweighted network to see if we can optimize it more, then examine the end result.\n\n\n28.18.6 Model Selection and Optimization\nGiven the stochastic nature of these models, it is always advisable to run them a number of times and then select the model with the least entropy. Higher entropy is not inherently bad. Properly discuss entropy here, and why we care. For example, a compressed JPEG image with only two colours will have a lot less entropy than one with a thousand colours.\nIn the case of stochastic block models, entropy returns the minimum description length, which is the amount of information the model needs to recreate the entire network. The goal of reducing entropy is fundamental to these models, with the assumption that minimizing entropy results in simpler models that do a better job of uncovering latent similarities in the data without overfitting. Below, we’ll execute 10 runs of minimize_nested_blockmodel_dl and print the entropy for each.\nstates = [minimize_nested_blockmodel_dl(eG, deg_corr=True) \n          for n in range(10)]\n\nfor s in states:\n    print(s.entropy())\n6162.281933127059\n6187.135324492942\n6168.918484063684\n6161.190122173799\n6163.517013260514\n6162.876759036053\n6178.052196472743\n6154.1481501809185\n6166.798460034726\n6154.869718381805\nWe can automatically grab the lowest entropy state using np.argmin.\nstate = states[np.argmin([s.entropy() for s in states])]\n\n\n28.18.7 More MCMC\nAt the expense of increased runtime, we can also follow-up the above model selection process by sampling from the posterior distribution and running mcmc_equilibrate, which performs random changes in the block assignments of the nodes, automatically handles the entropy calculations, and chooses the optimum values at the end. This step is also required to collect the block assignment posterior marginals, which tell us the likelihood (if any) that a node belongs to each block, based on the assignments it was given during the iterations. More iterations here will always improve the model, but with decreasing improvement/run-time payoffs.\nFirst, we will use the object S1, defined below, to keep track of the original entropy score to see how much we improved the model.\nS1 = state.entropy()\nS1\n6154.1481501809185\nTo collect marginal probabilities with MCMC, the blockstate needs to have been prepared for sampling, rather than for minimizing description length, which we can achieve by copying the blockstate and setting sampling to True. At the same time, we will add an additional 4 empty levels to the nested hierarchy so that the model has a chance to assign more levels. If these hierarchy levels don’t improve the model, the equilibration method will collapse them.\nstate = state.copy(bs=state.get_bs() + [np.zeros(1)] * 4,sampling = True)\nWe’re going to perform many iterations of the mcmc_equilibrate function, where nodes are moved between different blocks. Importantly, the MCMC method used in graph-tool doesn’t perform fully random moves, which would be a fairly typical MCMC approach. By taking advantage of the assumption that networks are made up of heavily interdependant observations, the MCMC estimation only has to randomly sample from probable block assignment moves - to the blocks that a node’s alters are members of.\nWe create a callback function to pass to mcmc_equilibrate so that we can collect a set of block assignment choices from each iteration. The bs values can be thought of as votes for block re-assignment, and constitute the posterior marginal probability of each node’s assignment to each block.\nbs = []\n\n## OUR CALLBACK FUNCTION THAT APPENDS EACH ESTIMATED BLOCKSTATE TO THE ARRAY\ndef collect_partitions(s):\n    global bs\n    bs.append(s.get_bs())\n        \nmcmc_equilibrate(state, force_niter=10000, mcmc_args=dict(niter=10), callback=collect_partitions)\n(6159.927069603201, 37378153, 4808499)\nNote that this will sometimes result in higher entropy for the block model solution! That’s because we need to select the best partition from the ones added to the bs list by the callback function.\nstate.entropy() - S1\n5.778919422760737\nThe PartitionModeState function takes our set of labeled partitions and tries to align them into a single set of common group labels. We can then use the get_marginal() method of the returned object to create a vertex property map of marginal probabilities for our original network graph. This property map can be used for calculations as well as for visualization of probable block memberships.\npmode = PartitionModeState(bs, nested=True, converge=True)\n\npv = pmode.get_marginal(eG)\neG.vertex_properties['pv'] = pv\nFinally, the convenience function get_max_nested() returns the most likely block assignment for each node as a single final blockstate, which will group nodes in proximity to each other in our visualization, based on their most likely membership. We apply this result back to our original blockstate object by providing it to the copy() method of the state object. Note that our entropy has improved a bit more here!\nbs = pmode.get_max_nested()\nstate = state.copy(bs=bs)\nstate.entropy()\n6153.278269237107\nLet’s re-calculate the same mutual information scores we used earlier to see if things have improved on those criteria.\nemployee_blocks_df = get_block_membership(state, eG, employee_blocks_df, 'model_uw_mcmc')\nhomogeneity_score(employee_blocks_df['position'], employee_blocks_df['model_uw_mcmc_block_id'])\n0.38131989351325507\ncompleteness_score(employee_blocks_df['position'], employee_blocks_df['model_uw_mcmc_block_id'])\n0.3526819549124348\nHomogeneity improves from 0.35 to almost 0.39, while completeness only improves a small amount.\nadjusted_mutual_info_score(employee_blocks_df['position'], employee_blocks_df['model_uw_mcmc_block_id'])\n0.1561547346951431\nBut the adjusted mutual info score below is actually slightly worse than it was before! This doesn’t necessarily mean the results are worse, though. We’ll take a look at a different layout for the blockmodel below and discuss some potential explanations for this.\n\n\n28.18.8 Visualizing Block Connections as a Radial Tree\nWhile the sfdp layout does a nice job of positioning nodes (and blocks) in spatial relation to each other, the radial tree layout can be very helpful for getting a sense of the connection patterns between the blocks and also keeps nodes together in a way that makes individual blocks very easy to distinguish. Since it is the default layout for printing a block state, we can easily obtain a simple representation using the .draw() method (see ?fig-31_03).\nstate.draw()\n\n\n\npng\n\n\n(&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x7f74c8ac3670, at 0x7f74c43f4a00&gt;,\n &lt;Graph object, directed, with 172 vertices and 171 edges, at 0x7f74c433b9d0&gt;,\n &lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x7f74c433b9d0, at 0x7f74c431ea90&gt;)\nAs is often the case, there are a few preparation steps we can do to improve the visualization of edges, as well as to add node labels to our figure. This process is a bit complex and is an adaptation of one that was devised by the author of graph-tool. The details aren’t particularly important, so we can use the utility function label_radial_blockmodel from the dcss package to take care of most of it.\neG = label_radial_blockmodel(eG, state)\nThe resulting figure is much improved (?fig-31_04), and clearly shows the relations between blocks, while also making it easier to examine which job titles were assigned to each block.\nstate.draw(\n    vertex_text = eG.vertex_properties['title'], \n    eorder = eG.edge_properties['edge_weight'],\n    vertex_shape='pie',\n    vertex_pie_fractions=eG.vertex_properties['pv'],\n    edge_control_points = eG.edge_properties['cts'],\n    pos=eG.vertex_properties['pos'], \n    vertex_size=10, \n    edge_pen_width = 0.2,\n    bg_color=[255,255,255,1],\n    vertex_text_rotation=eG.vertex_properties['text_rot'],\n    vertex_text_position=0,\n    output='figures/core_state_radial_tree_labels.pdf'\n    )\n(&lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x7f74c8ac3670, at 0x7f7550a574f0&gt;,\n &lt;Graph object, directed, with 172 vertices and 171 edges, at 0x7f7550a4d700&gt;,\n &lt;VertexPropertyMap object with value type 'vector&lt;double&gt;', for Graph 0x7f7550a4d700, at 0x7f7550a55970&gt;)\n\n\n\nCap\n\n\nYou’ll notice that some of the nodes are broken up into pie fractions - these indicate their probability of being assigned to a different block. In the full colour version, these fractions are coloured the same as the alternative block that the node might have been assigned to. You’ll also notice that the blocks have become significantly more heterogenous! Traders are in blocks with other traders, most lawyers are in a block that two other lawyers had some probability of being assigned to, and the CEOs are in fairly exclusive blocks. Although we no longer have 3 CEOs in one block with the COO, the block that one of the CEOs was moved to contains the other CEO, and their two respective blocks form a single block one level up the hierarchy! Earlier I mentioned that there are possible explanations for a decreased adjusted mutual information score and this is one example - that score doesn’t incorporate the higher levels of the hierarchy. Even though it’s probably actually a better model to have the four CEOs split evenly among two blocks, then put those two blocks together at the next hierarchy level, this would still negatively impact the mutual info score compared to the model where 3 CEOs were in one block.\nIt’s quite clear from the results of these stochastic blockmodels that there’s some very powerful estimation going on, and that the Bayesian aspects of it allow a great deal of nuance. The versatility of the modeling that drives graph-tool has led to a collaborative extension for topic modeling. Given the relational nature of words in text, which is often analyzed in the same way as social relations, topics can be blockmodelled from text documents to great effect. We’ll explore this method in the section that follows.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Latent structure in networks</span>"
    ]
  },
  {
    "objectID": "modeling-networks.html#conclusion",
    "href": "modeling-networks.html#conclusion",
    "title": "28  Latent structure in networks",
    "section": "28.19 CONCLUSION",
    "text": "28.19 CONCLUSION\n\n28.19.1 Key Points\n\nHierarchical Stochastic Blockmodels are remarkably powerful models that provide a nearly unparalleled degree of insight into the structure of a network and nodes’ roles within it\nSBMs build on the Bayesian intuitions established earlier in this book; they employ a similar approach of using latent variables and prior distributions to model unknown/unobserved\nTopSBM is really cool. Once you’re comfortable with the material in this chapter and the previous one, you should explore TopSBM on your own, or using the supplementary material online. –&gt;\n\n\n\n\n\nKarrer, Brian, and Mark Newman. 2011. “Stochastic Blockmodels and Community Structure in Networks.” Physical Review E 83 (1): 016107.\n\n\nPeixoto, Tiago. 2014. “Hierarchical Block Structures and High-Resolution Model Selection in Large Networks.” Physical Review X 4 (1): 011047.\n\n\nPeixoto, Tiago P. 2019. “Bayesian Stochastic Blockmodeling.” Advances in Network Clustering and Blockmodeling, 289–332.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Latent structure in networks</span>"
    ]
  },
  {
    "objectID": "modeling-networks.html#footnotes",
    "href": "modeling-networks.html#footnotes",
    "title": "28  Latent structure in networks",
    "section": "",
    "text": "For example, we could approximate the Negative Binomial distribution by using the Poisson distribution with a Gamma-distributed rate parameter. One way to do this would be to sample a rate parameter \\(\\lambda\\) from a Gamma distribution for each edge, use that parameter to sample the edge weight from the Poisson distribution, and then fit the SBM using the Poisson-distributed edge weights.↩︎\nWell, unless you count the partition in the next model as a single partition, in which case this would probably be the second best guess!↩︎",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Latent structure in networks</span>"
    ]
  },
  {
    "objectID": "diffusion-opinion-cultural-cognition.html",
    "href": "diffusion-opinion-cultural-cognition.html",
    "title": "30  Diffusion, opinion dynamics, and cultural cognition",
    "section": "",
    "text": "Full rewrite comining in fall 2024.",
    "crumbs": [
      "**MODELING**",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Diffusion, opinion dynamics, and cultural cognition</span>"
    ]
  },
  {
    "objectID": "artificial-neural-networks-fnn-rnn-cnn.html",
    "href": "artificial-neural-networks-fnn-rnn-cnn.html",
    "title": "32  Artificial neural networks",
    "section": "",
    "text": "32.1 LEARNING OBJECTIVES",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Artificial neural networks</span>"
    ]
  },
  {
    "objectID": "artificial-neural-networks-fnn-rnn-cnn.html#learning-objectives",
    "href": "artificial-neural-networks-fnn-rnn-cnn.html#learning-objectives",
    "title": "32  Artificial neural networks",
    "section": "",
    "text": "Describe the basic operation of early neural network models, the Perceptron and the Boltzmann machine.\nExplain the basic components of a neural network and how they work together to learn from data and make predictions.\nExplain how ‘forward propagation,’ ‘backward propagation,’ ‘gradient descent,’ and ‘autoencoders’ improve the performance of neural networks",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Artificial neural networks</span>"
    ]
  },
  {
    "objectID": "artificial-neural-networks-fnn-rnn-cnn.html#learning-materials",
    "href": "artificial-neural-networks-fnn-rnn-cnn.html#learning-materials",
    "title": "32  Artificial neural networks",
    "section": "32.2 LEARNING MATERIALS",
    "text": "32.2 LEARNING MATERIALS\nYou can find the online learning materials for this chapter in doing_computational_social_science/Chapter_23. cd into the directory and launch your Jupyter Server.",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Artificial neural networks</span>"
    ]
  },
  {
    "objectID": "artificial-neural-networks-fnn-rnn-cnn.html#introduction",
    "href": "artificial-neural-networks-fnn-rnn-cnn.html#introduction",
    "title": "32  Artificial neural networks",
    "section": "32.3 INTRODUCTION",
    "text": "32.3 INTRODUCTION\nThis chapter introduces artificial neural network models and deep learning. We will build on the distinctions we drew between the symbolic and connectionist paradigms in Chapter 20. We’ll start by introducing the Perceptron, which was one of the first artificial simulations of a biological neron. We will use the Perceptron as a relatively simple entry point into the more complex world of contemporary neural network modelling.\nOnce we discussed how neural network models work at the level of individual artificial neurons, we will shift our focus to the basic components and algorithms involved in contemporary neural network modelling. We will emphasize the basic components of a multilayer Perceptron model, as well as the algorithms involved in training these models. More specifically, we will learn how neural network models are organized into layers, with information about our data feeding forward through those layers and information about errors flowing backwards. We will learn about activation functions, backpropagation, gradient descent, and learning curves. We will conclude with a high-level discussion of more advanced “deep learning” neural network architectures and some ethical and political challenges that we need to consider when using them, or when evaluating other research that uses them.\nBy the end of this chapter, you will have a solid conceptual foundation in neural network modelling, and a sense of what makes deep learning so challenging.",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Artificial neural networks</span>"
    ]
  },
  {
    "objectID": "artificial-neural-networks-fnn-rnn-cnn.html#the-perceptron",
    "href": "artificial-neural-networks-fnn-rnn-cnn.html#the-perceptron",
    "title": "32  Artificial neural networks",
    "section": "32.4 THE PERCEPTRON",
    "text": "32.4 THE PERCEPTRON\nTo really understand how neural networks work, it’s necessary to understand what happens at the level of individual artificial neurons. For that reason, we’ll start our introduction to neural networks by discussing the first successful attempt to simulate a biological neuron. Recall from Chapter 20 that neural networks are the model family of choice within the connectionist paradigm, which is loosely inspired by biological cognition.\nThe Perceptron was a highly simplified model of a biological neuron, proposed by psychologist Frank Rosenblatt (1958). The simulated neuron would receive numerical inputs from multiple sources (i.e., other neurons). To simulate differences in the strength of each of those incoming signals, the Perceptron would multiply each by a weight. Then it would sum all of the weighted inputs and, if the sum exceeded a specific threshold, then the Perceptron would output 1 (it “fires”); if not, 0 (it does not fire). Recall the threshold model of complex contagion from Chapter 18 as an analogy. If enough neighbours are activated and sending strong enough signals, ego is likely to activate as well.\nFor a relatively simple model like this, then, the main questions are:\n\nHow do you come up with the weights that each simulated neuron uses to multiply incoming signals, and\nwhat thresholds should you place on the simulated neurons to determine whether the sum of the weighted inputs is enough to cause it to “fire” (output 1) or not (output 0).\n\nRosenblatt’s solution to these problems was influenced by behaviouralist notions of operant conditioning that were dominant in psychology at the time. In brief, he proposed teaching the Perceptron to learn the connection weights itself using a process that we would now call supervised learning.\nTo illustrate the process, imagine you are training a Perceptron to differentiate between black and white photos of cats and dogs (which is a pretty tall order for the Perceptron, but we’ll proceed anyway). In this scenario, your input features are individual pixel values. The number of initial inputs would be equal to however many pixels are in the original image. If each image was 28 pixels by 28 pixels (much less than the images we make these days), it would be represented by a total of 784 numerical input features.\nTo keep things really simple, in this example we will work with just four input features. Figure 32.1 illustrates the basic model. First, we start by assigning weights using a random number generator. It doesn’t matter what our initial weights are as long as they are not all the same. Positive weights are “excitatory” and negative weights are “inhibitory.” The Perceptron makes an initial prediction by multiplying each input value with its randomly assigned weight and then summing all of these weighted inputs. For example, in this case, it would perform the following calculation:\n\\[(0.2 \\cdot 0.9) + (0.1 \\cdot 0.1) + (0.08 \\cdot 0.7) + (0.92 \\cdot 0.2) = 0.43\\]\nIf \\(0.43\\) is greater than the simulated neuron’s fixed threshold, it fires (outputs 1, predicts cat). If not, it does not fire (outputs 0, predicts dog).\n\n\n\n\n\n\nFigure 32.1: The Perceptron received numerical inputs, multiplied them by weights to simulate differences in signal strength, and then “fired” if the sum of weighted inputs was greater than a fixed threshold.\n\n\n\nDrawing on operant conditioning, if the Perceptron makes an incorrect prediction – the image was a dog (output = 0) but the Percetron guessed cat (output = 1) – then it makes a minor adjustment to the weights, raising some and lowering others (giving us an early version of supervised learning). Then it makes another prediction on another image using these adjusted weights. It makes further minor adjustments to the weights whenever it makes an incorrect prediction and leaves them as they are when it makes correct predictions.\nThis is a dramatically simplified version of the biological networks in our brain. Although we will see how these models can be made more considerably more complex, even the more complex models are simple relative to the biological networks that inspire them.\nIt should now be clear what we mean when we say that Symbolic AI and Connectionist AI have very different ways of modelling cognition and “learning” from data. In Symbolic AI, declarative knowledge is represented as symbols and rules, and “cognition” is about performing operations like logical inference on those symbols, as we might when engaging in slow and controlled cognition. Conversely the connectionist paradigm starts below the level of symbols, with perception itself. Cognition is modelled in the form of massive and dense networks of relatively simple neurons, with “learning” being a process of adjusting the weights between neurons. Higher-level mental representations are the products of many neurons firing together.\nBrains are “black boxes” because, as I mentioned in the context of comparing Symbolic AI and Connectionist AI in Chapter 20, you can’t peek inside them and “see” concepts like cats, Russian Banyas, hardwood flooring, or toxic masculinity. You won’t see them if you look inside a Perceptron (which is far too simple for anything like this) or a contemporary ANN either (you would just see a massive densely-connected network of simulated neurons sending numbers back and forth to one another another). These processes are considered sub-symbolic because they underlie or give rise to the higher-level symbols that we can communicate about, for example with natural language. That said, in simpler ANN models, it is possible to see individual neurons influencing one another. You can find an excellent interactive example of this can be found at playground.tensorflow.org.\nThe early Perceptron was a simple machine, but it generated an enormous amount of intellectual hype when Rosenblatt first demonstrated it in the 1950s. Despite the obvious limitations of the model, Rosenblatt and others envisioned networks of these Perceptrons processing low-level input signals to perform increasingly difficult high-level tasks, including the kind of computer vision task in our hypothetical example of differentiating between images of cats and dogs. Let’s now consider some of the ways that this initial example developed and became more complex, eventually leading to the development of the kinds of neural network models that are dominant today.",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Artificial neural networks</span>"
    ]
  },
  {
    "objectID": "artificial-neural-networks-fnn-rnn-cnn.html#multilayer-perceptrons",
    "href": "artificial-neural-networks-fnn-rnn-cnn.html#multilayer-perceptrons",
    "title": "32  Artificial neural networks",
    "section": "32.5 MULTILAYER PERCEPTRONS",
    "text": "32.5 MULTILAYER PERCEPTRONS\nThe Perceptron provided an important foundation for contemporary ANN models. Let’s take a closer look at a powerful extension that is currently in wide use – the Multilayer Perceptron (MLP).\n\n32.5.1 Making Sense of Layers\nAs you might expect, MLPs organize networks of simulated neurons into multiple layers: an input layer, one or more hidden layers, and an output layer. This layered organization is illustrated in Figure 32.2.\n\n\n\n\n\n\nFigure 32.2: Simulated neurons in a Multilayer Perceptron are assigned to specific layers: an input layer, one or more hidden layers, and an output layer. Information propogates forward (left to right) through the network from the raw data in the input layer to the predictions made in the output later. In training, information about errors are propagated backward (right to left) from the output later to the input layer.\n\n\n\nOur data enter the MLP via the neurons in the input layer. Just like in the Perceptron, the input values are numerical. Every neuron in the input layer is connected to every neuron in the hidden layer, and every neuron in the hidden layer is connected to every neuron in the output layer. Therefore, the hidden layer in this MLP is dense. Later we will learn that there are other ways of organizing the connections between neurons. This model architecture is also sequential because information has to pass through each layer one at a time to go from one end of the network to another. Non-sequential model architectures are also possible, but we will not discuss them here.\nAnalogous to a biological neural network, the neurons in the input layer send their output values to the nodes in the hidden layers. The neurons in the hidden layer multiplies each incoming input value by its weight, which simulates differences in the connections weight between neurons in a biological network. The simulated neuron in the hidden layer then sums all of these weighted inputs to determine what it will output.\n\n\n32.5.2 Activation Functions\nAt this point, we encounter another importance difference between MLPs and their predecessor, the Perceptron. Whereas the Perceptron had a fixed threshold and would make deterministic binary decisions about whether to fire (1 for yes, 0 for no), the MLP applies an activation function to the sum of weighted inputs and outputs a continuous number. The continuous number is then passed along to all connected neurons in the next layer, which in this case are the neurons in the output layer. There are many possible activation functions one might use, but the most common three are sigmoid, tanh, and ReLU. We will learn about each in a moment, but first let’s clarify what activation functions are simulating in general.\nRecall from Chapter 20, during our brief introduction to biological neural networks, we learned that action potentials are triggered in postsynaptic neurons when they receive a level of stimulation that exceeds some threshold. The Perceptron simulated this with an actual fixed threshold, and then the neuron would deterministically “fire” if the weighted sum of inputs exceeds the neurons threshold. In other words, this is a binary step function that outputs 0s for every number before a threshold and 1s for every number above the threshold. But what if the sum of weighted inputs is just a wee bit above or below the threshold?\nIn a biological neural network, relatively low levels of stimulation are insufficient to trigger an action potential, but once the level of stimulation reaches a certain point the probability of an action potential jumps way up. Once that point has been reached, additional stimulation has little additional effect. In the 1980s, researchers realized the many benefits of making this simulated process a bit more like the biological process using activation functions. Figure 32.3 illustrates the differences between four different types of activation functions. For each, the x-axis represents the total input from other neurons, and the y-axis represents the neuron’s output.\n\n\n\n\n\n\nFigure 32.3: Four common activation functions for hidden layer neurons: a binary step function with a hard threshold (A), the sigmoid function (B), the hyperbolic / tanh function (C), and the rectified linear unit function / ReLU (D).\n\n\n\nThe first activation function (Figure 32.3, Subplot A) is the original binary step function used in the Perceptron: if the sum of weighted inputs exceeds some threshold (in this hypothetical example, the threshold is set to 2), then the neuron output 1 (i.e., it fires), else it outputs 0 (i.e., does not fire). In a famous paper from 1986, Rumelhart, Hinton, and Williams (1986) replaced the binary step function with a sigmoid activation function. (You may recognize the sigmoid function, Figure 32.3 Subplot B, from the context of logistic regression.) One of the many advantages of their revised approach is that it eliminates entirely the need to use hard thresholds and binary outputs. Instead, the sigmoid function returns a real number between 0 and 1, which can be interpreted as a probability. Similarly, the hyperbolic tangent activation function (Figure 32.3, Subplot C) is continuous and S-shaped, but outputs values between -1 and 1.\nThe fourth activation function shown in Figure 32.3 is the Rectified Linear Unit Function, or ReLU, shown in Subplot D. ReLU simply accepts the sum of weight inputs and if the sum is equal to or less than 0, it output a 0. If the sum of weighted inputs is greater than 0, it outputs whatever that positive value is. In other words, it also output continuous values that have a lower bound of 0, but no upper bound.\nThe sigmoid, hyperbolic tangent, and ReLU activation functions are all vastly superior to the binary step / hard threshold activation function. Aside from making ANNs slightly closer to their biological counterparts, these functions all output continuous values, which turns out to be very useful when training ANNs.\n\n\n32.5.3 What About the Output Layer?\nOnce we’ve added as many layers consisting of as many neurons as we care to include, we’re ready to create an output layer. Neural networks provide you with a remarkable degree of latitude when constructing them; the output layer is no exception. It’s up to you to decide what the best output later for your ANN will be; that decision will rest, in large part, on whether you’re interested in performing, for example, a Regression task (numerical approximation of a continuous variable) or a Classification task (probabilistically sorting observations/cases into two or more different classes). If you’re performing a Regression task, you can get away with using a simple single-neuron output layer with a fully linear activation function; this will effectively end up producing a (scaled) numerical output representing the summed-up weighted signals from each of the neurons in the preceding layer. If you’re performing a classification task with 2 labels/classes, you can use an output layer that replicates the functionality of a logistic regression. For classification tasks with more than 2 labels/classes, you can use a ‘softmax’ activation function, which is a generalization of logistic regression into larger numbers of categories. An intuitive interpretation of a softmax-powered output layer might view it as allowing each part of the preceding layer to ‘vote’ on which of the categories that part thinks the item belongs to. The softmax function then normalizes each of the ‘scores’ for each category into a series of probabilities that sum to 1; the item is classified as belonging to the class with the highest probability.",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Artificial neural networks</span>"
    ]
  },
  {
    "objectID": "artificial-neural-networks-fnn-rnn-cnn.html#training-anns-with-backpropagation-and-gradient-descent",
    "href": "artificial-neural-networks-fnn-rnn-cnn.html#training-anns-with-backpropagation-and-gradient-descent",
    "title": "32  Artificial neural networks",
    "section": "32.6 TRAINING ANNS WITH BACKPROPAGATION AND GRADIENT DESCENT",
    "text": "32.6 TRAINING ANNS WITH BACKPROPAGATION AND GRADIENT DESCENT\nOne of the major challenges that held back advancing this model for quite some time was that training a multi-layered ANN is far more complex than a single neuron (e.g., a Perceptron). When ANNs make incorrect predictions, how do we know which weights in the hidden layer of the network to adjust? The possible paths from input through the hidden layer to the output is very complex, and every mistaken prediction could be the result of any number of thousands or millions of weights and thresholds. Which ones should change, in what direction, and by how much? Backpropagation is an algorithm that enables us to answer those questions and better train our ANNs.\n\n32.6.1 Backpropagation\nWhen neurons in the hidden layers of an ANN output continuous values, rather than a binary decision, we can quantify the extent to which each individual weighted connection contributed to an overall prediction error. Just as information flows forward from an input to a final prediction, it is possible to send information about errors backwards from the final prediction to the input layers. This is called backpropagation. The algorithm itself was developed in the 1970s, but Rumelhart, Hinton, and Williams (1986) famously showed its usefulness in the context of neural network modelling, and it has greatly improved the ANN training process. In fact, backpropagation may be the most important algorithm in ANN modelling.\nWorking backwards from a prediction, the backpropagation algorithm starts by using a loss function to compute an overall measure of prediction error. The specific loss function used depends on context. If you are training a neural network on a regression problem, then Mean Squared Error (MSE) is a good choice. If you are training a neural network for a multi-class classification problem, Categorical Cross Entropy is a better choice. In short, the loss function you use depends on the type of model you are developing. Once the overall error has been computed, the next step is to calculate how much each individual weight in the ANN contributed to that error. Each weight can then be adjusted in the direction that would best minimize the overall prediction error. These adjustments are minor and local. Very small adjustments are made to weights based on the impact those changes would have only on connected neurons.\nTraining an ANN with backpropagation involves two processes for each example provided in training: a forward pass and a backward pass. During the forward pass, information is sent through the network from the input layer, through the hidden layer, and out the output layer. As in Figure XXX, you can picture this as information moving from left to right. The neurons in the hidden layer output continuous values that result from applying an activation function to the inputs they receive from the neurons in the layer below them. The neurons in the output layer apply a different activation function, but we will discuss that later.\nThe backward pass starts after a prediction has been made. First, an overall prediction error is computed using a loss function such as Mean Squared Error or Categorical Cross Entropy. Then the contribution that each connection weight makes to that overall error is computed, and then small adjustments are made to the weights such that the overall error is minimized. Once the adjustments are made, training can proceed with the next example.\n\n\n32.6.2 Gradient Descent\nThe changes made to connection weights are governed by an optimization algorithm called Gradient Descent. In general, Gradient Descent is used to find parameter values for some function that minimizes the loss, or cost, as much as possible. In the case of ANN modelling, our goal is to find the optimal values for all of the connection weights across all layers in our ANN. With Gradient Descent, we do that by making small modifications to the connection weights over many iterations. We start with our randomized weights and adjust them iteratively during training, example after example, until we have values that minimize the overall loss measured by our loss function.\nIn Figure 32.4, loss is represented on the Y axis, and the range of possible weight values is represented on the X axis. The connection weights range from -1 to 1, as they might if we were using the hyperbolic tangent activation function (tanh). Let’s say that our randomly selected starting point is the point S1 (for “Step 1”). If this was the value used for the connection weight, the loss for that weight would be the corresponding y value (which in this hypothetical example is not actually shown). If the connection weight was a little higher (e.g., shifted to the right a bit), the loss would decrease. Therefore, we increase our connection weight a bit in the next step, from -0.79 to -0.59 and reduce our loss a little bit.\nAt each step, the algorithm calculates how much the loss would change if the weight value was slightly different. Each potential new weight value has a gradient, or slope. Since we want to minimize loss, gradient descent will select a weight value that lets it descend (the opposite would be gradient ascent).\nWe continue these relatively small steps for S3, S4, S5, and S6, gradually increasing the connection weight and reducing the loss. By S7, we reach a connection weight that minimizes the loss. We have converged to the minimum loss. From this position, any further adjustments to the connection weight regardless of the direction would increase the loss (and would be gradient ascent). We can’t minimize the loss any further.\n\n\n\n\n\n\nFigure 32.4: An illustration of Gradient Descent to iteratively find connection weight values that minimize loss. In this example the optimal weight is 0, but it could have been any value between -1 and 1.\n\n\n\nEach step in Figure 32.4 is a learning step. The size of those steps is determined by a learning rate learning parameter in our ANN. In general, small steps are better because they reduce the chance of accidentally stepping over the optimal weight value. In our example, it might mean stepping over the optimal value at the bottom of the curve (0), climbing back up the curve on the other side, and then bouncing back and forth without ever stopping on the optimal value. The downside of using small learning rates is that it takes longer, and more iterations, to find the value that minimizes loss.\nThere is no reason to assume that the loss function has a nice single valley shape like the one in Figure 32.4. It could a wide variety of shapes, such as the one shown in Figure 32.5. This illustrates another issue that we might encounter when using Gradient Descent to find the optimal value for the connection weights in our ANN. First, imagine you have a random starting point: in this case, the grey S1 point. From here, Gradient Descent makes adjustments, each time reducing the loss by gradually increasing the weights with steps 2 - 6 (all represented with the grey points). After the 6th step, any further increases in the connection weight start increasing the loss. Gradient Descent thinks it has found an optimal weight, but it hasn’t. It’s stuck on a local minimum and doesn’t know about the global minimum that it might have found if the random starting point had been the black S1 point further to the right of the Figure.\nClearly the black S6 point in Figure 32.5 is better than the grey S6 point because it has a lower loss value. The more complex the terrain produced by the loss function, the more challenging it is to converge on the global minimum. There are variations on the basic Gradient Descent algorithm that can help with this problem. Stochastic Gradient Descent (SGD), for example, computes the gradients based on a randomly selected subset of the training data. This causes the loss values to jump around a lot more at each step, but over many iterations, it greatly reduces the risk of getting stuck in local minimums and increases the chance we will get at least very close to the global minimum. We will see an example of using SGD in an ANN later in this chapter.\n\n\n\n\n\n\nFigure 32.5: An illustration of Gradient Descent to iteratively find connection weight values that minimize loss.\n\n\n\n\n\n32.6.3 Learning Curves\nAs an ANN trains, it uses backpropagation and gradient descent to iteratively tune the weights until it converges on an locally optimal solution (which isn’t always the globally optimal solution). It goes through data repeatedly to do this. Each pass through the data is a single epoch of training. As we will see in the next chapter, we can calculate various performance measures during each epoch of training; it is often helpful to plot these measures. Figure 32.6 illustrates this for a hypothetical ANN. In this example, we can see that the accuracy rates for both the training and validation data increases with each epoch, and that the two rates are very close together (which suggests the model is not overfitting).\n\n\n\n\n\n\nFigure 32.6: Accuracy and loss metrics for the test and validation data at each epoch of training.",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Artificial neural networks</span>"
    ]
  },
  {
    "objectID": "artificial-neural-networks-fnn-rnn-cnn.html#more-complex-ann-architectures",
    "href": "artificial-neural-networks-fnn-rnn-cnn.html#more-complex-ann-architectures",
    "title": "32  Artificial neural networks",
    "section": "32.7 MORE COMPLEX ANN ARCHITECTURES",
    "text": "32.7 MORE COMPLEX ANN ARCHITECTURES\nNow that we’ve discussed some fairly simple neural network models and I’ve explained how some key algorithms (such as Gradient Descent) work, let’s turn our attention to some more complex ANN architectures. Focus on the big picture here. The goal here is still conceptual; we want to understand, at a high-level, what these types of models do and how they work. I will refer back to these models in later chapters (at which point you might want to flip back to these pages to remind yourself of the big picture).\n\n32.7.1 Stacked Autoencoders\nAutoencoders are a type of ANN that attempt to produce an output identical to whatever input was received, which is not as pointless as it might sound. Autoencoders have hidden layers that are smaller than their input and output layers. By trying to produce an output that is identical to their inputs, they to learn how to create a high-quality representation with a smaller number of bits. (You can think of this as analogous to file compression; when you zip a file, the same file contents are represented using fewer bits.) In practice this introduces a lot of computational problems, so instead we can use a clever trick. We make the hidden layer bigger than the input and output layers, but at any given moment only a small portion of those neurons are allowed to be active, meaning the autoencoder is still forced to learn a more compact representation, but the math is easier.\nIncreasingly sophisticated representations can be learned when autoencoders are stacked together, with the outputs of one becoming the inputs for another. With each autoencoder, the representations are less like low-level perceptual patterns and more like higher-level mental representations analogous to the types of symbols that feature in cognitive science and Symbolic AI, such as rules, frames, schemas, and scripts. In this way, we can use Autoencoders as a form of neural network-based dimensionality reduction; their low-dimensional representations of high-dimensional objects can be very useful! We will see some examples of this later in the book.",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Artificial neural networks</span>"
    ]
  },
  {
    "objectID": "artificial-neural-networks-fnn-rnn-cnn.html#convolutional-neural-networks",
    "href": "artificial-neural-networks-fnn-rnn-cnn.html#convolutional-neural-networks",
    "title": "32  Artificial neural networks",
    "section": "32.8 CONVOLUTIONAL NEURAL NETWORKS",
    "text": "32.8 CONVOLUTIONAL NEURAL NETWORKS\nConvolutional Neural Networks (CNNs, or ConvNets) are ubiquitous in computer vision – working with image data – and are increasingly also used in natural language processing. They were first introduced in the 1980s by Yann LeCun, who was inspired by the work of Kunihiko Fukushima on some of the first deep neural networks developed for computer vision tasks, in this case recognizing hand written digits. Two of the most common computer vision tasks CNNs are currently used for are:\n\nImage classification\nObject detection / recognition\n\nLike neural networks more generally, CNNs are biologically inspired. In particular, they are inspired primarily by our brain’s vision system. Let’s take a moment to understand some basics of the biology to better understand how the artificial version works. It is important to remember that the artificial versions depart from their biological inspirations in important ways; CNNs don’t need to work like their biological counterparts anymore than a plane should fly the same way birds do, but the biology helps provide a framework for making sense of CNNs.\n\n32.8.1 From Biological Vision to Artificial Vision\nHow do we see? The eyes play a very important role, of course, but the brain doesn’t simply receive images provided by the eyes. It actively constructs those representations. Our retinas convert patterns of light into neural signals using photoreceptors (consisting of rod and cone cells). Special neurons in the retina (bipolar cells) detect light areas on dark background and dark areas on bright backgrounds. Then, at a higher level of neural processing, retinal ganglion cells respond to differences in light within their own receptive field. These cells may activate depending on whether there is light in the centre or the edges of their receptive field, or in some particular orientation (e.g. horizontal), to which they are especially sensitive. Together they enable the detection of edges and other low-level building blocks of vision (Ward 2020).\nAfter this fairly minimal amount of processing, a neural signal is sent from the eyes to the brain via the optic nerve, where it splits into several different pathways. The most important, for our purposes, is the pathway to the primary visual cortex, located in the back of our brain. Starting in the late 1950s, David Hubel and Torsten Wiesel made a series of discoveries that revealed the hierarchical organization our visual system, in which complex abstract representations are built from the bottom up by combining simple low-level representations (e.g. edges). In addition, there is a top down process where our brains are actively involved in the construction of those representations, for example by filling in missing bits of information and creating a 3-dimensional model of our environment from 2-dimensional inputs (Ward 2020). These backward connections are important – they outnumber the forward connections – but are not currently well-understood (Mitchell 2019).\nIn the primary visual cortex, then, neurons are organized into hierarchical layers, with those on the bottom detecting low-level features, like edges. The layers above it detect more complex features, from relatively simple shapes to more complex ones, eventually resulting in our conscious perception of faces, objects, and the rest of our environments (Ward 2020).\n\n32.8.1.1 How Convolutional Neural Networks Process Data: From Pixels to Predictions\nCNNs, like other artificial neural networks, are inspired by this biology, but they don’t mirror it exactly. Drawing on the hierarchical organization of the visual system, CNNs are made up of a sequence of layers of neurons, with each layer in the sequence sending its output to the layers of neurons that come next, where they are processed as inputs. As with the multilayer Perceptron and other neural networks, each of these artificial neurons has an activation value that is computed from an input value and a weight.\nLet’s say we have an image of a cat. We can represent that image as a matrix that encodes information about the brightness and colour of each individual pixel in the image. Each neuron in the first layer corresponds to one of those pixels in the image, so they must be the same size. In other words, if there are 12 million pixels (as there are in an image from an iPhone in 2021, for example), then there must be 12 million neurons in the CNN’s first layer.\nEach hidden layer in the CNN is itself made up of multiple activation maps (also called feature maps), directly inspired by the hierarchical nature of the vision system. The neurons in each of these activation maps is organized like a grid, with each neuron responding to specific features within specific regions of the image like retinal ganglion cells responding to specific patterns of light within their individual receptive fields. Each activation map is focused on different types of visual features. As in the brain, some activation maps are focused on very low-level features such as edges, detected by variations in the distribution and intensity of reflected light. In layers focused on edge-detection, the simulated neurons activate when they detect an edge within their narrow receptive field that matches some specific orientation: horizontal, vertical, or any other angle. Their receptive field is a specific pixel location in the input image and a small surrounding area that overlaps with the receptive fields of other simulated neurons in the same activation map.\nThis is the most important way in which CNNs differ from other types of neural networks that have dense layers. The convolutional layers in a CNN are designed to learn patterns that are local, in other words within a narrow receptive field. In contrast, the type of neural networks we have learned about before now had dense layers, where every neuron in one layer feeds into the every neuron in the next layer, learning more global patterns from the entire image. As Chollet (2018) notes, this means that CNNs only has to learn a pattern once; if it learns it in one part of an image, it will recognize it in other parts of the image without having to relearn it, as a densely connected layer would. As a result, CNNs are more efficient with image training data than networks with densely connected layers are.\nEach neuron has an activation value that represents the extent to which the input numbers in its receptive field match its expectation, such as for a horizontal edge. Let’s say that the receptive field of a given neuron is a grid of 3 by 3 pixels. The numbers for each of the 9 pixels represent how bright the pixel is, from 0 to 255. As with more basic neural network models, these pixel values are multiplied by a given weight. All of those weighted inputs are summed and the resulting activation value can be passed on.\nWithin any given activation map inside any given layer, the simulated neurons all use the same weights. In other words, they all multiply the inputs from the pixels within their receptive fields by the same weights before summing them to produce their activation value. Each of these processes of multiplying the inputs in a receptive field by the weights shared by neurons in the same activation map is called a convolution.\nEach layer of the CNN has its own set of activation maps, each of which is a grid of neurons looking for a particular pattern within its narrow receptive field. These layers are called convolutional layers. The activation values that result from summing each weighted input are passed from that layer into the next as a new set of input values. Inspired by our biological vision system, the initial layers are focused on very low-level features such as edges, and subsequent layers combine these low-level patterns into more complex shapes and objects. The number of activation maps in each layer, and the number of layers in any given CNN, vary and are controlled by the researcher.\nIf we are training our CNN to perform a classification task – like classifying whether images containing a patch of trees or not – then the activation values from the penultimate layer of the CNN are passed to a classification module. This module is itself a neural network that will predict the likelihood of a patch of trees given the input values from the final layer in the CNN, which encodes information about the most high-level features in the image (e.g. grass, leaves, tree branches). The classification model outputs the probability of the image containing a patch of trees. If it were an object detection model, it would output probabilities of that the image contains any of the types of objects it knows about.\nOther than the organization of layers into activation maps and the process of performing convolutions on the inputs of each neurons receptive field, CNNs operate like ANNs. The weights, for example, are learned using back propagation during a supervised learning training process. Each pass through the data is a training epoch, and typically many of these are required to train a CNN. When the network “converges” on a good set of learned weights, the error is diffused as much as possible via back propagation, training is complete, and if the model is a good one, you can start using it to make predictions on unseen data.\n\n\n\n32.8.2 Biased Training Data for Computer Vision: Ethical and Political Concerns\nEarlier, I mentioned that CNNs were initially developed by Yann LeCun in the 1980s, but only recently became ubiquitous in computer vision. The models have changed little since they were first introduced. Rather, the massive explosion of data and the rapid increases in computing power brought CNNs into the spotlight. A number of widely-used training datasets, such as ImageNet, have played an important role in this development, and more recently, social media platforms like Flickr and Facebook that serve up massive collections of labeled datasets harvested from users, such as people tagged in photos posted to Facebook.\nThere are some major ethical and political issues to consider here. First, there is the market for your data; it is entirely possible that your images (as in photos you upload to sites like Facebook or Flickr), uploaded and tagged, are in a training dataset somewhere, perhaps even a public one like ImageNet. But a less obvious, and even more consequential, set of issues concern biases embedded in these massive training datasets, and more importantly appaling misogyny and racism (Crawford and Paglen 2019; Gebru 2020; Buolamwini and Gebru 2018; Vries et al. 2019; Steed and Caliskan 2021). Crawford and Paglen (2019) state the problem plainly at the start of “Excavating AI: The Politics of Images in Machine Learning Training Sets:”\n\nYou open up a database of pictures used to train artificial intelligence systems. At first, things seem straightforward. You’re met with thousands of images: apples and oranges, birds, dogs, horses, mountains, clouds, houses, and street signs. But as you probe further into the dataset, people begin to appear: cheerleaders, scuba divers, welders, Boy Scouts, fire walkers, and flower girls. Things get strange: A photograph of a woman smiling in a bikini is labeled a “slattern, slut, slovenly woman, trollop.” A young man drinking beer is categorized as an “alcoholic, alky, dipsomaniac, boozer, lush, soaker, souse.” A child wearing sunglasses is classified as a “failure, loser, non-starter, unsuccessful person.” You’re looking at the “person” category in a dataset called ImageNet, one of the most widely used training sets for machine learning.\nSomething is wrong with this picture.\n\nSometimes the biases are in the relationship between the image and its label, as with the examples that Crawford and Paglen cite. In other situations it is due to asymmetries in who is represented in images and how they are represented (e.g., Gebru 2020; Buolamwini and Gebru 2018). White men, for example, are generally far more represented in these training datasets than other people, and as a result CNNs trained on these datasets tend to perform far better for white men than they do for women or racialized people. For example, a CNN could classify a person as a women because the person is standing in a kitchen, and the training data contains many more images of women in kitchens than men in kitchens.\nA good example of these issues is the 80 Million Tiny Images dataset that was created by and formerly hosted by researchers at MIT (see Prabhu and Birhane 2020). The dataset consisted of images scraped from the web and was annotated using crowd sourcing. However, after being in wide use for 14 years, it was discovered that the training data contained thousands of images annotated with racial slurs, not to mention labels such as “rape suspect.” The dataset was also found to include many deeply problematic images, such as pictures taken up skirts, that were clearly taken (and of course circulated) without consent.\nOne especially high-profile illustration of the implications of racially biased training data happened in 2015 when Google released a new feature in their photo app that would tag images with captions derived from a CNN trained to classify the primary contents of an image. Because of training data biased towards white people, the CNN tagged a selfie of two black people with “Gorillas” (Mitchell 2019). Obviously this is unacceptable, and any applications of CNNs on image data – including for research in computational social science, not just commercial applications – need to directly address the issues of training data with racial and other biases.\nAs you know from Chapter 19, there is an ongoing debate about what we should do, given that biases in training data – not just image training data – reflect real biases and inequalities in the real world. On the one hand, we can learn more about these biases and inequalities from the problems that arise from models trained on those biased data. While there may be some merit to this idea within a purely scientific context, datasets used to train these models are very difficult and expensive to collect and build. It’s not like we could easily separate out training datasets for commercial applications, in which we work hard to reduce biases, from those intended for scientific research on bias, where we let those biases remain. The same training data is used in both contexts. So at best, using biased training data to study bias is making the best of a bad situation. These biases are amplified by the models and feed back into society, and as we saw in Chapter 19 these negative feedback loops create and solidify inequalities, especially when they are part of facial recognition systems or are part of opaque automated decision-making processes.\nWhile CNNs have many positive benefits in the world, including healthcare applications assisting in diagnoses using medical imaging data, others are obviously deeply problematic and rightfully controversial. None moreso than facial recognition (or really anything that involves classifying people), which is used not just to help you find pictures of your friends and adorable cats in your personal digital photo library, but by police departments and many others. Privacy concerns, as well as the negative consequences of mistakes resulting from hidden biases that disproprtionately affect racialized people. Regulation is clearly needed, but we are in the early days of this political debate. Most of the action in Europe right now is focused on transparency and the “right to explanation” in the context of automated decision making, such as whether you get a loan or probation. As Mitchell (2019), Gebru (2018), and many others have pointed out, debates about the ethics, politics, and risks of deep learning has been far too concerned with the potential threats of intelligent machines and far too unconcerned with the very real and immediate threats of opaque errors rooted in deeply embedded racial and gender biases.\nData isn’t the only culprit here. Nor is it the only solution. As computational social scientists, it is incumbent upon us to actively interrogate the fairness of the models that we build. If we find them lacking, it is not enough to simply blame the data and move on; we must confront the problem head on. It is up to us to fix our data, proactively build fairness into what we do.\n\nFurther Reading\nChapter 10 from G{’e}ron’s (2019) Hands-on Machine Learning and the first several chapters of Chollet’s (2018) Deep Learning with Python offer a deeper dive into neural network modelling than the introduction in this book. Chollet is the original developer of Keras, the Python package we’ll use to develop a simple neural network model in the next chapter.",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Artificial neural networks</span>"
    ]
  },
  {
    "objectID": "artificial-neural-networks-fnn-rnn-cnn.html#conclusion",
    "href": "artificial-neural-networks-fnn-rnn-cnn.html#conclusion",
    "title": "32  Artificial neural networks",
    "section": "32.9 CONCLUSION",
    "text": "32.9 CONCLUSION\n\n32.9.1 Key Points\n\nThis chapter introduced neural network models, shallow and deep.\nExamined some early neural network models – the perceptron and Bolztmann machine –\nLearned how the ‘backward propagation’ and ‘gradient descent’ addressed critical limitations in their modelling frameworks\nExplored the difference between ‘shallow’ and ‘deep’ neural networks,\nLearned about an approach to deep learning that involves stacking together ‘autoencoders.’\n\n\n\n\n\nBuolamwini, Joy, and Timnit Gebru. 2018. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” In Conference on Fairness, Accountability and Transparency, 77–91. PMLR.\n\n\nChollet, Francois. 2018. Deep Learning with Python. Vol. 361. Manning New York.\n\n\nCrawford, Kate, and Trevor Paglen. 2019. “Excavating AI: The Politics of Images in Machine Learning Training Sets.” Excavating AI.\n\n\nGebru, Timnit. 2018. “How to Stop Artificial Intelligence from Marginalizing Communities?” https://www.youtube.com/watch?v=PWCtoVt1CJM.\n\n\n———. 2020. “Race and Gender.” The Oxford Handbook of Ethics of AI, 251–69.\n\n\nGéron, Aurélien. 2019. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O’Reilly Media.\n\n\nPrabhu, Vinay Uday, and Abeba Birhane. 2020. “Large Image Datasets: A Pyrrhic Win for Computer Vision?” arXiv Preprint arXiv:2006.16923.\n\n\nRosenblatt, Frank. 1958. “The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.” Psychological Review 65 (6): 386.\n\n\nRumelhart, David, Geoffrey Hinton, and Ronald Williams. 1986. “Learning Representations by Back-Propagating Errors.” Nature 323 (6088): 533–36.\n\n\nSteed, Ryan, and Aylin Caliskan. 2021. “Image Representations Learned with Unsupervised Pre-Training Contain Human-Like Biases.” In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 701–13.\n\n\nVries, Terrance de, Ishan Misra, Changhan Wang, and Laurens van der Maaten. 2019. “Does Object Recognition Work for Everyone?” In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 52–59.",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Artificial neural networks</span>"
    ]
  },
  {
    "objectID": "language-models-and-embeddings.html",
    "href": "language-models-and-embeddings.html",
    "title": "33  Language models and word embeddings",
    "section": "",
    "text": "33.1 LEARNING OBJECTIVES\nBy the end of this chapter, you should be able to:",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Language models and word embeddings</span>"
    ]
  },
  {
    "objectID": "language-models-and-embeddings.html#learning-objectives",
    "href": "language-models-and-embeddings.html#learning-objectives",
    "title": "33  Language models and word embeddings",
    "section": "",
    "text": "Describe the main components of SpaCy’s natural language processing pipeline\nEffectively use SpaCy’s doc, token, and span data structures for working with text data\nDescribe why normalizing text data can improve the quality of downstream analyses\nDescribe the difference between stemming and lemmatization\nUse part-of-speech labels to select and filter tokens from documents\nExamine noun chunks (ie. phrases) that are detected by SpaCy’s pipeline\nExamine Subject, Verb, Object Triplets",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Language models and word embeddings</span>"
    ]
  },
  {
    "objectID": "language-models-and-embeddings.html#learning-materials",
    "href": "language-models-and-embeddings.html#learning-materials",
    "title": "33  Language models and word embeddings",
    "section": "33.2 LEARNING MATERIALS",
    "text": "33.2 LEARNING MATERIALS\nYou can find the online learning materials for this chapter in doing_computational_social_science/Chapter_10. cd into the directory and launch your Jupyter Server.",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Language models and word embeddings</span>"
    ]
  },
  {
    "objectID": "language-models-and-embeddings.html#introduction",
    "href": "language-models-and-embeddings.html#introduction",
    "title": "33  Language models and word embeddings",
    "section": "33.3 INTRODUCTION",
    "text": "33.3 INTRODUCTION\nIn this chapter, we will shift our focus from working with structured quantitative data to natural language data stored in the form of unstructured text. We will begin by learning how to use the package SpaCy for common natural language processing (NLP) tasks, such as cleaning and normalizing text data, followed by a discussion of labeling words by their part-of-speech, manipulating syntactic dependencies betweens words, and using all of this to create a rough 3-word summary of the content in a sentence. Later, we will put this knowledge to use for custom text pre-processing functions to use for downstream tasks in other chapters of the book.\n\n33.3.1 Package Imports\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport spacy\nfrom spacy import displacy\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom dcss import set_style\nset_style()",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Language models and word embeddings</span>"
    ]
  },
  {
    "objectID": "language-models-and-embeddings.html#text-processing",
    "href": "language-models-and-embeddings.html#text-processing",
    "title": "33  Language models and word embeddings",
    "section": "33.4 TEXT PROCESSING",
    "text": "33.4 TEXT PROCESSING\nWith the exception of some recent neural network and embedding-based text methods that we will consider later in this book, the quality of most text analyses can be dramatically improved with careful text processing prior to any modelling. For data cleaning, common text processing tasks include removing punctuation, converting to lower case, normalizing words using techniques like stemming or lemmatization, and selecting some subset of terms to use in the analysis. When selecting the subset of terms, it is possible to use a vocabulary that you curate yourself (in which case it is referred to as a dictionary) or to select terms based on some sort of criteria, such as their frequency or part-of-speech (e.g., noun, adjectives).\nYou can process your text data any number of ways in Python, but my advice is that you use a package called SpaCy. SpaCy is, to put it plainly, head and shoulders above the rest when it comes to processing natural language data in Python, or in any other language for that matter. If you are interested in natural language processing, SpaCy alone is reason to do your work entirely in Python. In this first part of the chapter, we will introduce SpaCy with an emphasis on its built in data processing pipelines and data structures, and then we will practice using it to process a data set consisting of political speeches.\n\n33.4.1 Getting to Know SpaCy\nOne of the major benefits of using SpaCy is that it is tightly integrated with state-of-the-art statistical language models, trained using deep learning methods that you will start learning in later chapters.\nWe are not yet ready to get into the details of pre-trained statistical language models, but we will briefly touch on them here since knowing a bit about them is an important part of learning how to use SpaCy to process natural language data.\nSome recent advances have begun revolutionizing natural language processing. To grossly oversimplify things, transfer learning means that the output of a machine learning model that was trained in one context is reused in another context. In fields like computer vision and natural language processing, we are almost always talking about deep learning models that take an enormous amount of time and energy to train. In NLP, the basic idea is to train such a model on truly massive datasets (e.g., crawls of the entire open web). In doing so, the model learns a lot about language in general, but perhaps not much about any specific domain. The output from the pre-trained model can be made available to researchers, who can update it using annotated data from the specific domain they are interested in, such as news stories reporting on the Black Lives Matter movement. For most tasks, this transfer learning approach outperforms models that have been trained on a massive dataset but have not been updated with domain-specific data, or models trained the other way around.\nWhile we haven’t actually gotten into the machine learning (let alone deep neural networks and transfer learning), it is useful to keep this general idea of reusing models in a transfer learning framework in mind. In this chapter, for example, all of the methods you learn how to use are informed by a statistical language model that has been pre-trained on a massive general text corpus, including web data from commoncrawl.org and the OntoNotes 5 corpus, which contains data from telephone conversations, newswire, newsgroups, broadcast news, broadcast conversation, and weblogs. The pre-trained language models that SpaCy provides can be used as is, or they can be updated with domain-specific annotated data. In the rest of this chapter, we will not update the pre-trained models.\nSpaCy’s pre-trained models come in three sizes – small, medium, and large. Each is available in multiple languages1 and follows a simple naming convention: language + model name (which is the type of model + genre of text it was trained on + the model size). The medium core English model trained on news data is en_core_news_md, and the large English core model trained on web data (blogs, comments, and online news) is en_core_web_lg.\nThese models vary in what they do, how they do it, how fast they work, how much memory they require, and how accurate they are for various types of tasks. As we now know, it is important to pick the model that is best suited to the specific research application. The smaller models are of course faster and less memory-intensive but they tend to be a bit less accurate. For most general-purpose tasks they work fine, but your case is probably not “general purpose” – it is probably fairly domain specific, in which case you may want to work with a larger model, or a model that you can train and update yourself.\nModels are not installed with SpaCy, so you will need to download them to your machine. You can do this on the command line with the following command:\npython -m spacy download en_core_web_sm\npython -m spacy download en_core_web_md\npython -m spacy download en_core_web_lg\nOne they have been downloaded, we can use SpaCy’s pre-trained models by loading them into memory using the .load() method and assigning the model to a language object, which is SpaCy’s NLP “pipeline”. As we will see below, this object contains everything needed to process our raw text. You can call it whatever you want, but the convention is to call it nlp. Once we have imported SpaCy and loaded one of the “core” models, we are ready to start processing text. We don’t need the named entity recognition or syntactic dependency parser for this part, so we’ll disable those components of the pipeline.\n# nlp = spacy.load(\"en_core_web_sm\", disable=['ner', 'parser'])\n!python -m spacy download en_core_web_sm\nnlp = spacy.load(\"en_core_web_sm\")\nWe’ve now created an instance of spaCy’s text processing pipeline. Let’s put it to use!\n\n33.4.1.1 The SpaCy NLP Pipeline\nOnce the language model has been loaded (nlp), we can start processing our raw text by passing it through SpaCy’s default text processing pipeline, which is illustrated in ?fig-10_01. This is often the slowest part of a natural language processing workflow because SpaCy does a lot of heavy lifting right at the start. The result of this process will be something called a Doc object, which we will discuss momentarily; for now, let’s focus on the big picture and then circle back and fill in the details on each pipeline component and data structure later.\n\nAs shown in Figure XXX, as soon as our original text enters SpaCy’s pipeline, it encounters the tokenizer, which identifies the boundaries of words and sentences. Most of the time, punctuation makes it relatively simple for computers to detect sentence boundaries but periods in abbreviations and acronyms (e.g. U.K., U.S.A) can complicate this simple approach. Even tokenizing individual words can be tricky, as this process involves making decisions like whether to convert contractions to one token or two (e.g. it’s vs. it is), or whether to tokenize special characters like emoji. SpaCy tokenizes text using language-specific rules, differentiating between punctuation marking the end of a sentence and punctuation used in acronyms and abbreviations. It will also use pre-defined language-specific rules to split tokens like don't into do and n't. Although these rules are language-specific, if SpaCy doesn’t already have a tokenizer for a language you need, it is possible to add new languages. (Instructions on how to do this are available in the SpaCy documentation.)\nIn the second step of the pipeline, SpaCy assigns each a tag based on its part-of-speech using its pre-trained statistical models. In doing so, SpaCy combines rules-based expertise from linguistics with supervised machine learning models. The third step maps syntactic dependencies between words (e.g., which words in a sentence depend on or modify other words in a sentence) using its neural network model. At its most basic, dependency parsing is the basis for accurate sentence segmentation in SpaCy, but it also enables more complex analysis typical to the field of linguistics. The fourth step in the processing pipeline is to recognize named entities. This is a very useful and important task for computational social scientists but is relatively complex and tends to be highly-dependent on the data used to train the model. Therefore, we will set named entity recognition aside until later, where we can explore it in more depth and learn how to train models that are customized to work best for our specific research applications (see Chapter 33). Note that when we loaded the pre-trained language model and initialized the nlp pipeline, we disabled the ner component. Since we are not going to use it here, disabling it in the pipeline speeds up text processing a noticeable amount because it means SpaCy won’t spend time executing that part of the pipeline.\nThe general processing pipeline I have just described is summarized in the Figure below, which is reproduced from the spaCy documentation. Note the “Custom Pipeline Components” on the right side of the processing pipeline. This indicates the option of adding additional steps to the pipeline, such as categorizing texts based on some pre-defined set of labels, assigning customized attributes to the Doc, Token, and Span objects, merging noun chunks or named entities into single tokens, and so on. Technically you can add your own custom steps to any part of the SpaCy pipeline, not just the end. These custom steps are beyond the scope of this chapter, but now you know it’s possible to add them.\nNow that we understand how to download, load, and use pre-trained statistical models as part of SpaCy’s default text processing pipeline, it’s time to learn about SpaCy’s containers: Docs, tokens, and spans.\n\n\n33.4.1.2 The SpaCy Containers\nWe’ll use a simple example to illustrate SpaCy’s containers. We start by passing some raw input text into the processing pipeline and then demonstrate how to work with the containers that store the output of that pipeline.\nAs an example, let’s consider the abstract for Bart Bonikowski’s (2017) journal article “Ethno-nationalist populism and the mobilization of collective resentment” published in The British Journal of Sociology. Here is the raw text of the abstract:\n\nScholarly and journalistic accounts of the recent successes of radical-right politics in Europe and the United States, including the Brexit referendum and the Trump campaign, tend to conflate three phenomena: populism, ethno-nationalism and authoritarianism. While all three are important elements of the radical right, they are neither coterminous nor limited to the right. The resulting lack of analytical clarity has hindered accounts of the causes and consequences of ethno-nationalist populism. To address this problem, I bring together existing research on nationalism, populism and authoritarianism in contemporary democracies to precisely define these concepts and examine temporal patterns in their supply and demand, that is, politicians’ discursive strategies and the corresponding public attitudes. Based on the available evidence, I conclude that both the supply and demand sides of radical politics have been relatively stable over time, which suggests that in order to understand public support for radical politics, scholars should instead focus on the increased resonance between pre-existing attitudes and discursive frames. Drawing on recent research in cultural sociology, I argue that resonance is not only a function of the congruence between a frame and the beliefs of its audience, but also of shifting context. In the case of radical-right politics, a variety of social changes have engendered a sense of collective status threat among national ethnocultural majorities. Political and media discourse has channelled such threats into resentments toward elites, immigrants, and ethnic, racial and religious minorities, thereby activating previously latent attitudes and lending legitimacy to radical political campaigns that promise to return power and status to their aggrieved supporters. Not only does this form of politics threaten democratic institutions and inter-group relations, but it also has the potential to alter the contours of mainstream public discourse, thereby creating the conditions of possibility for future successes of populist, nationalist, and authoritarian politics.\n\nI have the abstract saved in a text file called “bonikowski_2017.txt”. To feed this abstract into the SpaCy pipeline we’ll read it into memory, assign it to a variable, and then call our nlp() object on it.\nwith open('data/bonikowski_2017.txt', 'r') as f:\n    abstract = f.read()\n\n33.4.1.2.1 Docs\nIn SpaCy, the first data structure to understand is the Doc object returned from the default processing pipeline indicated in the Figure above. The Doc object contains the linguistic annotations that we will use in our analyses, such as information about parts-of-speech. As indicated in the Figure above, we create the Doc object by running our data through the NLP pipeline. We’ll call the Doc object doc, but of course we could call it pretty much anything we want.\ndoc = nlp(abstract)\nprint(f'There are {len(doc)} tokens in this document.')\nSpaCy’s Doc object is designed to facilitate non-destructive workflows. It’s built around the principle of always being able to access the original input text. In SpaCy, no information is ever lost and the original text can always be reconstructed by accessing the .text attribute of a Doc, Sentence, or Token object. For example, doc.text recreates the exact text from the abstract object that we fed into the pipeline. Note that although we access .text as we would an attribute of an object, as though the text is stored plainly as a variable attached to it, .text is actually a class method that retrieves the original text from SpaCy’s underlying C storage structure.\nEach Doc object includes information about all of the individual sentences and tokens that are used in the raw text. For example, we can print each individual sentence in the Doc. In the code block below, we print each sentence from the abstract. I won’t print the full text here, but you will see it on your screen if you follow along with the code.\nfor sent in doc.sents:\n    print(sent, '\\n')\nSimilarly, we can iterate over the Doc object and print out each token. Iterating tokens is the default behaviour of a Doc object, so we don’t need to use .tokens to access them.\nfor token in doc:\n    print(token)\nThe ability to iterate over tokens greatly simplifies the process of cleaning and extracting relevant information from our text data. In the sections below, we’ll iterate over tokens for a variety of important text processing tasks, including normalizing text and extracting words based on their part-of-speech, two tasks we turn to shortly.\nThe Doc object itself can be stored on disk and reloaded later, which can be very useful when working with large collections of text that take non-trivial amounts of time to pass through the default processing pipeline. This can be done a few different ways, including the new DocBin class for serializing and holding the contents of multiple Doc objects, which can then be saved as a .spacy file using DocBin.to_disk(). The to_array() method exports an individual Doc object to an ndarray (from numpy), where each token occupies a row and each token attribute is a column. These arrays can also be saved to disk using numpy, but the DocBin method is the most convenient.\nfrom spacy.tokens import DocBin\n\ndoc_export = DocBin()\ndoc_export.add(doc)\ndoc_export.to_disk('data/bart_bonikowski_doc.spacy')\nOf course, it is possible to read these Docs back into memory using methods like DocBin.from_disk(), or loading the saved ndarray and using Doc.from_array(). Loading from DocBin is the most convenient, but keep in mind that you need a vocabulary from an nlp() object to recreate the Doc objects themselves.\ndoc_import = DocBin().from_disk('data/bart_bonikowski_doc.spacy')\ndocs = list(doc_import.get_docs(nlp.vocab))\ndoc = docs[0]\nprint(f'There are {len(doc)} tokens in this document.')\nThere are 346 tokens in this document.\n\n\n33.4.1.2.2 Token\nThe second type of object to know about is the Token. A token is each individual element of the raw text, such as words and punctuation. The Token object stores information about lexical types, adjacent whitespace, the parent Doc that a token belongs to, and “offsets” that index precisely where the token occurs within the parent Doc. As we will see in subsequent chapters, all of this Token metadata can be used to accomplish specific natural language processing tasks with a high-degree of accuracy, such as the information extraction tasks covered in later chapters.\nTokens are stored as hash values to save memory, but just as we can access the raw input text of a Doc object using .text, we can see the textual representation of a given token using .text. We can also access each token by specifying its index position in the Doc or by iterating over the Doc.\nfor token in doc:\n    print(token.text)\nAn enormous amount of information is stored about each Token, most of which can be retrieved using methods discussed extensively in the documentation. We’ll cover examples of some fairly important ones, including methods for accessing the normalized forms of the token such as a lemma, its part-of-speech, the dependency relations it’s embedded in, and in some cases, even an estimate of the token’s sentiment.\n\n33.4.1.2.2.1 Span\nThe final data structure to understand before moving on is the Span, which is a slice of a Doc object that consists of multiple tokens but is smaller than the full Doc. When you iterate of sentences in a document, each of those is actually a Span. Knowing how spans work can be very useful for data exploration, as well as programmatically gathering contextual words that are adjacent to a target type of token, such as a type of named entity. We can specify a span by using slice notation. For example, we could define a Span by providing the range of token indexes from 5 to 15. Note that this span will include token 5 but not token 15!\na_span = doc[5:15]\nGiven a span, we can use many of the same methods available for Docs and Tokens, as well as merging and splitting Spans, or copying them into their own Doc objects.\nNow that we have a solid foundational understanding of SpaCy’s statistical models, processing pipeline, and containers, we can take a closer look at two important components of the text processing pipeline that are very useful when pre-processing text data for the type of analyses we will perform in this chapter: (a) normalizing text via lemmatization, and (b) part-of-speech tagging.",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Language models and word embeddings</span>"
    ]
  },
  {
    "objectID": "language-models-and-embeddings.html#normalizing-text-via-lemmatization",
    "href": "language-models-and-embeddings.html#normalizing-text-via-lemmatization",
    "title": "33  Language models and word embeddings",
    "section": "33.5 NORMALIZING TEXT VIA LEMMATIZATION",
    "text": "33.5 NORMALIZING TEXT VIA LEMMATIZATION\nWhen we work with natural language data, we have to decide how to handle words that mean more or less the same thing but have different surface forms (e.g. compute, computing). On the one hand, leaving words as they appear preserves nuances in language that may be useful. However, those words are tokenized and counted separately, as if they had no semantic similarity. An alternative approach is to normalize the text by grouping together words that mean more or less the same thing and reducing them to the same token. The idea, in short, is to define classes of equivalent words and treat them as a single token. Doing so loses some of the nuance but can dramatically improve the results of most text analysis algorithms. The two most widely-used approaches to text normalization are stemming and lemmatization.\nStemming is a rule-based approach to normalizing words regardless of what role the word plays in a sentence (e.g. noun or verb), or of the surrounding context. For example, the Snowball stemmer takes in each individual word and follows rules about what parts of the word (e.g. “ing”) should be cut off. As you might imagine, the results you get back are usually not themselves valid words.\nRather than chopping off parts of tokens to get to a word stem, lemmatization normalizes words by reducing them to their dictionary form. As a result, it always returns valid words, which makes it considerably easier to interpret the results of almost any text analysis. In addition, lemmatization can be done either with a simple language-specific lookup table or in a rule-based way that considers a token’s part-of-speech (discussed below), which enables it to differentiate between ways of using the same word (e.g. “meeting” as a noun, “meeting” as a verb) and identical words that have different normalisation rules in different contexts. Lemmatization is extremely accurate and is almost always going to be a better choice than stemming. It is also more widely used.\nKeeping in mind that our most common goal with computational text analysis is to see the shape and structure of the forest, not any individual tree, you can probably see why this is useful in the context of analyzing natural language data. Although we lose some nuance by normalizing the text, we improve our analysis of the corpus (i.e. the “forest”) itself.\nAs mentioned earlier, SpaCy’s nlp() does most of the heavy computing up front. As a result, our Doc object already includes information about the lemmas of each token in our abstract. By default, the latest (3.0+) version of SpaCy uses the simpler lookup lemmatizer. To use the newer rule-based one that incorporates part-of-speech information, we’ll install the additional data and modify the pipeline component to use the rule-based one.\nYou can install the spacy-lookups-data package in a virtual environment with\npip install spacy-lookups-data\nAlternatively, if you are not using a virtual environment for some reason, you can run:\npip install --user spacy-lookups-data\nThis new lemmatizer needs to replace the existing one, but it also needs to come after the other default pipeline components that assign part-of-speech tags. Unfortunately, simply using nlp.replace(), puts the new lemmatizer after the parser but before the tags are mapped by the AttributeRuler part of the pipeline. It’s unclear whether this is intentional or a minor bug due to the fact that SpaCy is in the middle of a major transition to Version 3. The easiest approach currently is to exclude the default lemmatizer during loading, then add the new one back in at the end. The lemmatizer also needs to be initialized in order to load the data from spacy-lookups-data.\n# nlp = spacy.load('en_core_web_sm', disable=['ner'], exclude = ['lemmatizer'])\n# lemmatizer = nlp.add_pipe('lemmatizer', config = {'mode': 'rule'})\n# lemmatizer.initialize()\nnlp = spacy.load('en_core_web_sm')\nWe can iterate over each token in the Doc and add its lemma to a list. It’s worth noting that using .lemma_ on a token returns only the lemmatized text, not the original token, so the lemmas object we create here is a standard python list of strings. To do additional SpaCy-specific operations, we have to return to the original doc object.\ndoc = nlp(abstract)\nlemmatized = [(token.text, token.lemma_) for token in doc]\nThe list we just created contains all the tokens in our original document as well as their lemmas where appropriate. If not appropriate, the same token is added twice. To get a sense of the difference between the original tokens and their lemmas, and how minimal (and yet helpful) this normalization can be, let’s take a peek at the lemmas from the first 100 words of the abstract:\nfor each in lemmatized[:100]:\n    if each[0].lower() != each[1].lower():\n        print(f'{each[0]} ({each[1]})')\naccounts (account)\nsuccesses (success)\npolitics (politic)\nincluding (include)\nphenomena (phenomenon)\nare (be)\nelements (element)\nare (be)\nresulting (result)\nhas (have)\nhindered (hinder)\naccounts (account)\ncauses (cause)\nconsequences (consequence)\nexisting (exist)\nThis simple process of iterating over tokens and selecting some, but not all, is something we will do again and again in this chapter. There are more efficient ways to do this kind of pre-processing work – specifically by writing a custom function – but we will put that task on hold until we’ve covered each of the individual pieces.",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Language models and word embeddings</span>"
    ]
  },
  {
    "objectID": "language-models-and-embeddings.html#part-of-speech-tagging",
    "href": "language-models-and-embeddings.html#part-of-speech-tagging",
    "title": "33  Language models and word embeddings",
    "section": "33.6 PART-OF-SPEECH TAGGING",
    "text": "33.6 PART-OF-SPEECH TAGGING\nIn some research applications, you may want to restrict the subset of words that you include in your text analysis. For example, if you are primarily interested in understanding what people are writing or talking about (as opposed to how they are talking about something), then you may decide to include only nouns and proper nouns, or noun chunks (discussed below) in your analysis. In our example abstract, nouns and noun chunks like “Europe,” “radical-right politics,” “Brexit referendum,” “Trump campaign,” “causes and consequences,” “ethno-nationalist populism,” and so on tell us far more about the content of this abstract than words such as “and,” “has,” “recent,” or “available.” We can do this by filtering words based on their part-of-speech.\nIf you’re a little lost at this point, that’s a good thing; it means you’re paying attention, and are justifiably struggling to conceptualize the reconstitution of language we’ve covered in the last few paragraphs. At this point, an example might help show how these processes play out in action. Returning to our example abstract, we’ll start by examining each word and its part of speech.\nfor item in doc[:20]:\n    print(f'{item.text} ({item.pos_})')\nScholarly (ADJ)\nand (CCONJ)\njournalistic (ADJ)\naccounts (NOUN)\nof (ADP)\nthe (DET)\nrecent (ADJ)\nsuccesses (NOUN)\nof (ADP)\nradical (ADJ)\n- (PUNCT)\nright (NOUN)\npolitics (NOUN)\nin (ADP)\nEurope (PROPN)\nand (CCONJ)\nthe (DET)\nUnited (PROPN)\nStates (PROPN)\n, (PUNCT)\nSpaCy classifies each word into one of 19 different parts-of-speech, each of which is defined in the documentation. However, if you are uncertain about what a part-of-speech tag is, you can also ask SpaCy to explain() it to you. For example, spacy.explain('ADJ') will return adjective, and spacy.explain('ADP') will return adposition. Because the part-of-speech a word plays can vary depending on the sentence – ‘meeting’ can be a noun or a verb, depending on the context – SpaCy’s approach to part-of-speech tagging combines language-based rules and statistical knowledge from its trained models that can be used to estimate the best part-of-speech for a word given the words that appear before and after it.\nIf these 19 parts-of-speech are not sufficient for your purposes, it is possible to access fine-grained parts-of-speech that include additional information, including verb tenses and specific types of pronouns. These fine-grained parts-of-speech can be accessed using the .tag attribute rather than .pos_. As you likely expect, there are far more fine-grained parts-of-speech than coarse-grained. Their meanings can all be found online in the SpaCy documentation.\nBecause SpaCy assigns a part-of-speech to each token when we initially call nlp(), we can iterate over the tokens in our abstract and extract those that match the part-of-speech we are most interested in. For example, the following code will identify the nouns in our abstract.\nnouns = [item.text for item in doc if item.pos_ == 'NOUN']\nprint(nouns[:20])\n['accounts', 'successes', 'right', 'politics', 'referendum', 'campaign', 'phenomena', 'populism', 'ethno', 'nationalism', 'authoritarianism', 'elements', 'right', 'right', 'lack', 'clarity', 'accounts', 'causes', 'consequences', 'ethno']\nWe can do the same for other parts of speech, such as adjectives, or for multiple parts of speech.\nadjectives = [item.text for item in doc if item.pos_ == 'ADJ']\nadjectives[:20]\n['Scholarly',\n 'journalistic',\n 'recent',\n 'radical',\n 'important',\n 'radical',\n 'coterminous',\n 'limited',\n 'analytical',\n 'nationalist',\n 'contemporary',\n 'temporal',\n 'discursive',\n 'public',\n 'available',\n 'radical',\n 'stable',\n 'public',\n 'radical',\n 'pre']\nparts = ['NOUN', 'ADJ']\nwords = [item.text for item in doc if item.pos_ in parts]\nwords[:20]\n['Scholarly',\n 'journalistic',\n 'accounts',\n 'recent',\n 'successes',\n 'radical',\n 'right',\n 'politics',\n 'referendum',\n 'campaign',\n 'phenomena',\n 'populism',\n 'ethno',\n 'nationalism',\n 'authoritarianism',\n 'important',\n 'elements',\n 'radical',\n 'right',\n 'coterminous']\nThe accuracy of the part-of-speech tagger in version 3 of SpaCy is 97% for the small English core model and 97.4% for the large English core models, both of which are trained using convolutional neural networks. As mentioned earlier, you will only see modest gains in accuracy by switching to a larger statistical model. Ultimately, as you will soon learn, the accuracy of these kinds of models depends in large part on the data they’re trained on. The good news is that the accuracy rates for part-of-speech tagging are consistently high regardless of the corpus used for training, and for researchers like us who are more interested in applying these algorithms, rather than developing them, have nothing to gain from trying to beat 97% accuracy.",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Language models and word embeddings</span>"
    ]
  },
  {
    "objectID": "language-models-and-embeddings.html#syntactic-dependency-parsing",
    "href": "language-models-and-embeddings.html#syntactic-dependency-parsing",
    "title": "33  Language models and word embeddings",
    "section": "33.7 SYNTACTIC DEPENDENCY PARSING",
    "text": "33.7 SYNTACTIC DEPENDENCY PARSING\nThe third component of the SpaCy processing pipeline (see Figure 1) is the syntactic dependency parser. This rule-based parser rests on a solid foundation of linguistic research and, when combined with machine learning models, greatly increases the accuracy of a variety of important text processing tasks. It also makes it possible to extract meaningful sequences of words from texts, such as short phrases, or components of larger narratives and frames. We will consider the power of this approach by looking at how SpaCy extracts noun chunks from text, setting aside more complex manipulations of the dependency tree until later.\nWhen we communicate in natural languages such as English, we follow sets of commonly held rules that govern how we arrange words, clauses, and phrases in sentences. For the most part, we learn these rules – grammar – implicitly via socialization as children, and then more explicitly later in life. For non-linguists, some explicit forms of instruction about the “correct” and “incorrect” way of doing things in a language is what probably comes to mind when we think about grammar, but from a linguistic point of view grammatical rules should not be seen as proscriptive but rather as cultural and evolving in populations over time. Grammatical “rules” are about dominant patterns in usage in a population (linguists use the word ‘rule’ in the way sociologists and political scientists do, not the way physicists do). They are one of the best examples of shared culture and implicit cultural rules we have! Rather than proscription, linguists are focused on description and explanation of grammatical rules, and there is an enormous amount of formal linguistic theory and research on modelling grammar. In fact, Pãnini’s study of the grammatical structure of Sanskrit was written in the 4th-century and is still discussed today (Jurafsky and Hand 2009)!\nOne of the most enduring ways of modelling grammar is dependency parsing, which has its origins in ancient Greek and Indian linguistics (Jurafsky and Hand 2009). Dependency parsing is a rules-based approach that models the relationships between words in a sentence as a directed network. The edges in the network represent various kinds of grammatical relationships between pairs of words. You may already be able to think of some important grammatical relations, such as clausal argument relations (e.g. a word can be a nominal subject of another word, a direct or indirect object, or a clausal complement), modifier relations (e.g. adjectives that modify a noun, adverbs that modify a verb), or others such as coordinating conjunctions that connect phrases and clauses in sentences. Linguists have documented many important grammatical relations and have systematically compared how they operate across different languages (e.g. Nivre and Fang 2017). SpaCy combines this rules-based dependency parsing with machine learning models, which results in extremely high levels of accuracy for a broad range of NLP tasks, such as part-of-speech tagging, discussed earlier.\nThere are some rules around how these dependency-relation networks are constructed that are helpful to understand. First, every sentence has one root word (i.e. node) that is not dependent on any other words. It’s the starting point for our sentence from which all other words “grow”. Second, with the single exception of the root word, every word has one and only one dependency relationship with another word. Finally, there is a path that starts at the root word and connects to every other word in the tree. This directed acyclic network is usually represented with the text written horizontally left to right, with arcs connecting and labeling specific dependency relationships between words.\nThe syntactic dependency parser built into SpaCy is powerful, accurate, and relatively fast. SpaCy also simplifies the process of understanding these syntactic dependencies by using a visualization tool called displacy, which is especially useful for researchers with little background knowledge of formal linguistic theory. For example, let’s use displacy to visualize the syntactic dependencies in a short sentence. Below, we do this for a short and simple sentence. If you’re executing code from a script, you should use the .serve() method. If you’re in a Jupyter Notebook, you should use .render() instead.\nsentence = nlp(\"This book is a practical guide to computational social science\")\n\n\n\nA visualization of syntactic dependency relationships between words.\n\n\nThe dependency relations that SpaCy identified in this simple sentence are shown in the Table below and in ?fig-10_02 (produced using displacy). As you can see, SpaCy has mapped each word in our document to another word, based on a specific type of dependency relationship. Those dependency types are actually labeled on the arcs in the visualization. In Figure XXX and Table XXX, each word has a “head” (which sends a directed link to the word as a “child”) but only some have “children” (which receive an incoming link from a word if they depend on it).\n\nA table view of the syntactic dependencies shown in ?fig-10_02.\n\n\n\n\n\n\n\n\n\nTEXT\nDEP\nHEAD TEXT\nHEAD POS\nCHILDREN\n\n\n\n\nthis\ndet\nbook\nNOUN\n[]\n\n\nbook\nnsubj\nis\nAUX\n[this]\n\n\nis\nROOT\nis\nAUX\n[book, guide, .]\n\n\na\ndet\nguide\nNOUN\n[]\n\n\npractical\namod\nguide\nNOUN\n[]\n\n\nguide\nattr\nis\nAUX\n[a, practical, to]\n\n\nto\nprep\nguide\nNOUN\n[science]\n\n\ncomputational\namod\nscience\nNOUN\n[]\n\n\nsocial\namod\nscience\nNOUN\n[]\n\n\nscience\npobj\nto\nADP\n[computational, social]\n\n\n.\npunct\nis\nAUX\n[]\n\n\n\nFor now, what’s important to understand is that SpaCy does this dependency parsing as part of the default processing pipeline (and like other parts of the pipeline, it is possible to disable it if you don’t need it). However, we can extract information about these dependency relations directly from the syntactic tree, which in turn enables us to extract a variety of useful information from text with a very high degree of precision, and makes it possible to partially automate methods such as quantitative narrative analysis, briefly discussed below, which are otherwise very laborious and time consuming.\n\n33.7.1 Noun Chunks\nOne substantial benefit of dependency parsing is the ability to extract coherent phrases and other sub-sentence chunks of meaning from text. We will learn a bit about how to navigate the dependency tree shortly, but for now we can get a sense of the power of dependency parsing by looking at the example of noun phrases, which SpaCy calls “noun chunks.”\nNoun chunks consist of a single word (the noun) or a string of words including a noun and the words that modify that noun. These are usually “pre-modifiers,” meaning words (e.g. adjectives) that appear before the focal noun, not after. A base noun phrase is a phrase that has a noun as its head, and which does not itself contain another noun phrase.\nBelow, we iterate over the doc containing the text of Bonikowski’s article and print each noun chunk:\nfor item in list(doc.noun_chunks)[:10]:\n    print(item.text)\nRemember, the computer doesn’t actually know the meaning of any of these words or phrases. Given that, the results are surprisingly accurate; it should be clear how useful this kind of simplification could be for working with large volumes of text! In a later chapter, we will take a closer look at detecting noun chunks, using a machine learning approach designed specifically for this task.\n\n\n33.7.2 Extracting Words by Dependency Labels: Subject, Verb, Object Triplets\nEarlier, you learned how to process a large collection of Docs and extract Tokens from each based on several criteria, including their part-of-speech. We can also extract tokens from documents based on other criteria, such as their dependency relationships with other words. For example, if we wanted to extract a very small representation of an action-object narrative from a sentence (e.g., “Kat (subject) plays (verb) bass (object).”), we could extract the transitive verb (i.e., a verb that takes an object, “plays”) and the direct object of that transitive verb (i.e., “bass”). To do this, we simply check the .dep_ tags for each token rather than the .pos_ tags. For example, the loops below creates a list of tuples containing the transitive verbs and direct objects for each sentence in doc.\nfor sent in doc.sents:\n    tvdo = [(token.head.text, token.text) for token in sent if token.dep_ == 'dobj']\n    print(tvdo)\nWhen analyzing text in terms of these semantic dependencies, we are often looking to extract information in the form of a semantic triplet of subject-verb-object, also known as an SVO. In social scientific text analysis, these triplets are most closely associated with the quantitative narrative analysis framework developed by Roberto Fransozi (2004). The idea, in short, is that these SVOs contain crucial information about who did what to whom. We will see examples of working with this kind of data in later chapters, but let’s take a preliminary look at what the kind of think we can expect when extracting SVOs.\nWalking through the linguistic technicalities of a fully functional SVO workflow is outside the scope of this chapter, but we can use the subject_verb_object_triples() function included in the dcss package to see the results of a reasonably complex implementation of the basic idea, as outlined by researchers such as Fransozi.\nfrom dcss.text import subject_verb_object_triples\n\nlist(subject_verb_object_triples(doc))\nSome of these look pretty good, but others leave a little to be desired. As you can probably imagine, there are an enormous number of challenges involved in automating this kind of language processing. To get things just right, you have to consider how people write and speak in different contexts, how sentence construction varies (active, passive; formal, informal), how statements differ from questions, and so on. It is possible to get very high-quality results by building complex logic into the way you walk through the dependency trees, but in general you can expect to find that the signal-to-noise ratio in automated SVO analyses typically means you have to do a good amount of manual work to clean up the results.\n\nFurther Reading\nVasiliev (2020) provides a fairly deep dive into spaCy for a variety of natural language processing tasks. The spaCy documentation is itself also very good, although some parts of it might be a bit challenging to fully understand until you know a bit more about neural networks and large-scale pre-trained language models. Those topics are covered later in the book.",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Language models and word embeddings</span>"
    ]
  },
  {
    "objectID": "language-models-and-embeddings.html#conclusion",
    "href": "language-models-and-embeddings.html#conclusion",
    "title": "33  Language models and word embeddings",
    "section": "33.8 CONCLUSION",
    "text": "33.8 CONCLUSION\n\n33.8.1 Key Points\n\nWe discussed a variety of common text processing tasks and demonstrated how to use them on a small text dataset and a very large one.\nLearned about how SpaCy’s text processing pipeline is organized, and how to use its data structures\nWe used SpaCy’s pipeline and data structures to normalize text via lemmatization\nFiltered and selected words based on the part-of-speech and their syntactic dependencies\nLearned how to approximately identify the subject, verb, and object in a sentence",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Language models and word embeddings</span>"
    ]
  },
  {
    "objectID": "language-models-and-embeddings.html#learning-objectives-1",
    "href": "language-models-and-embeddings.html#learning-objectives-1",
    "title": "33  Language models and word embeddings",
    "section": "34.1 LEARNING OBJECTIVES",
    "text": "34.1 LEARNING OBJECTIVES\n\nLearn what word embeddings models are, and what they can be used for\nLearn what Word2Vec is and how the CBOW and Skip-gram architectures differ\nUnderstand why we should not trust intuitions about complex high-dimensional vector spaces",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Language models and word embeddings</span>"
    ]
  },
  {
    "objectID": "language-models-and-embeddings.html#learning-materials-1",
    "href": "language-models-and-embeddings.html#learning-materials-1",
    "title": "33  Language models and word embeddings",
    "section": "34.2 LEARNING MATERIALS",
    "text": "34.2 LEARNING MATERIALS\nYou can find the online learning materials for this chapter in doing_computational_social_science/Chapter_32. cd into the directory and launch your Jupyter Server.",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Language models and word embeddings</span>"
    ]
  },
  {
    "objectID": "language-models-and-embeddings.html#introduction-1",
    "href": "language-models-and-embeddings.html#introduction-1",
    "title": "33  Language models and word embeddings",
    "section": "34.3 INTRODUCTION",
    "text": "34.3 INTRODUCTION\nThe text analysis models we’ve been working with to this point in the book have primarly been focused on fairly traditional content analytic tasks, such as describing and comparing the thematic content contained in a collection of text documents. Nearly all of these models have been based on long and sparse vector representations of text data, otherwise known as a “bag-of-words.” In this chapter, we will learn how to represent text data with short dense vectors, otherwise known as word embeddings. Embeddings have interesting implications if used to understand how different words are used in similar contexts, giving us insights into patterns of language use. There is a tendency to think of embeddings are modelling meaning, but for reasons that will become clear in this chapter, we should be careful to avoid imputing meaning to embeddings.\nIn what follows, we’ll discuss some of the challenges involved with modelling meaning in general, followed by an introduction to using neural word embedding models. As always, we’ll break the models down to better understand how they work, and we’ll spend a bit of time working with pre-trained embeddings to deepen your understanding of embeddings, and to get a feel for vector space. We’ll emphasize fairly simple vector math operations with these embeddings to help you understand why we should not assume that embeddings are good proxies for meaning, and why we need to be very careful with how we interpret the results of analyses that bundle together many vector operations to construct larger “dimensions” of cultural meaning. Finally, I’ll close the chapter by showing you how to train your own word embedding models, including how to train multiple models in a way that facilitates valid cross-sectional comparisons and historical / temporal analysis.",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Language models and word embeddings</span>"
    ]
  },
  {
    "objectID": "language-models-and-embeddings.html#can-we-model-meaning",
    "href": "language-models-and-embeddings.html#can-we-model-meaning",
    "title": "33  Language models and word embeddings",
    "section": "34.4 CAN WE MODEL MEANING?",
    "text": "34.4 CAN WE MODEL MEANING?\nWord embeddings have received a lot of interest as quantitative representations of what words “mean.” It’s an astoundingly complex problem, and we need to tread very carefully. So, before we get into the specifics of the models, let’s take a moment to briefly consider some of the relevant theoretical background here.\nQuestions about meaning and its consequences for social scientific inquiry have been at the center of some of the biggest theoretical and methodological debates and divides in sociology and other social sciences since at least the early 20th century (see Mohr et al. 2020 for a fascinating discussion in the context of contemporary cultural sociology). Researchers primarily concerned with understanding meaning have tended to prefer more qualitative and interpretivist approaches. Historically, this complexity has led many quantitatively-minded researchers to concede serious efforts to understand meaning to interpretivists, and to focus instead on describing and comparing content.\nDespite this longstanding paradigmatic and methodological fault line, these have never been the only two options for social scientific text analysis. For example, relational sociologists working at the intersection of cultural sociology and social network analysis have developed a wide variety of formal and mathematical models of the cultural-cognitive dimensions of institutions, and for inductively exploring and modelling “meaning structures” (Mohr 1998; Mohr, Wagner-Pacifici, and Breiger 2015; Edelmann and Mohr 2018; Mohr and Bogdanov 2013). Much of the theoretical and methodological considerations guiding text-analytic work in “relational sociology” have evolved in lockstep with network analysis (see Emirbayer 1997; Crossley 2010; Mische 2011), and in particular with the evolution of network analytic methods that are focused on understanding relational identities, the cultural-cognitive dimensions of institutions, and the dynamics of socio-semantic networks (which combine network analysis with various kinds of natural language processing). These developments are interesting in part because much of 1970s and 80s-era network analysis energetically eschewed all questions of culture and meaning, considered intractable and unscientific, in pursuit of establishing a thoroughly structural paradigm. But from the 1990s onward, even the most fervent structuralists were taking culture and meaning seriously (e.g., White 1992), in search of a deeper understanding of the co-constitution of social structure (networks) and culture (meanings, practices, identities, etc.). Much has happened since then.\nAs part of this larger effort to integrate relational theory and methods, we’ve seen a proliferation of new methodological tools and approaches for text analysis – some developed “in-house,” others imported – that try to avoid counter-productive dichotomies (e.g., quantitative and qualitative, inductive and deductive, exploratory and confirmatory). The embedding methods I introduce in this chapter and the next can be seen as another contribution to efforts to measure and model meaning structures. They have opened up new discussions in computational research on culture, knowledge, and ideology (Kozlowski, Taddy, and Evans 2019; Linzhuo, Lingfei, and James 2020; Stoltz and Taylor 2019; Taylor and Stoltz 2020; Rheault and Cochrane 2020; McLevey et al. 2021), including deeply embedded cultural stereotypes and collective biases (Garg et al. 2018; Bolukbasi et al. 2016; Jones et al. 2020; Papakyriakopoulos et al. 2020). There are also ongoing efforts to develop new methodological tools for using word embeddings to conduct research, informed by intersectionality theory (Collins and Bilge 2020; Collins 2015; Crenshaw 1989), on the social categories and institutions that intersect to create and maintain social inequality and systems of oppression (e.g., Nelson 2021). We will briefly discuss these and other applications below. It is important to keep in mind my earlier statement: these issues are complex. We need to be careful to exercise caution when presented with “easy” answers that draw the connection between embeddings and meaning.\nOf course, sociologists and other social scientists are not the only ones who’ve struggled long and hard with the problem of measuring and modelling meaning. The dominant way of modelling meaning in the field of computational linguistics has deep affinities with social scientific paradigms. The branch of linguistics concerned with meaning is called semantics, and in many respects, its starting point is the failure of dictionary-based approaches for defining the meaning of words. Paul Elbourne’s (2011) book Meaning: A Slim Guide to Semantics starts with a thorough debunking of the dictionary approach to meaning, showing the limitations of everyday dictionary definitions when applied to the laborious work done by philosophers over thousands of years to define the meaning of specific words like “knowledge.” Many social scientists who gripe about the lack of broadly-shared definitions of core concepts in our field – e.g. culture, network, field, habitus, system, identity, class, gender, and so on – will be comforted to know that similar concerns are raised in other sciences and in engineering, like metallurgists being unable to reach a consensus on an acceptable definition of metal (Elbourne 2011, 9).\nWe are used to the idea that dictionaries are an authority on meaning, but of course dictionary definitions change over time in response to how language is used. For example, Merriam-Webster recently added ‘they’ as a personal pronoun, reflecting large-scale social changes in how we think and talk about gender identities. Other new words, phrases, and concepts from popular culture have also been added, such as the Bechdel test, swole, on point, page view, screen time, cybersafety, bottle episode, go cup, gig economy, and climate change denial. Culture and language evolve.\nElbourne (2011) provides many examples of the “mind-boggling complexity” involved in giving adequate definitions to the meanings of words. His larger point is that any definition-based approach to assigning meanings to words (including dictionaries) will always be unsatisfactory. A serious theory or approach to modelling meaning needs much more than definitions. His comparison of different cognitive and linguistic theories are well worth reading but are beyond the scope of this chapter, but one of the key take-aways is that meanings are not definitions and they are not determined by the characteristics of the things they refer to. Instead, meanings are concepts that reside in our heads and are generally attached to low-level units like words, which are strongly modified by the contexts in which they’re used and scale up to higher-level units like sentences. These meanings are shared but not universal. When it comes to any given thing – say the word “populist” – the concept in my head is not identical to the concept in your head, but communication does not break down because our concepts are qualitatively similar. We might not mean exactly the same thing by the word populist, but our concepts overlap sufficiently enough that we can have a meaningful conversation and our interactions don’t descend into conceptual chaos.\nThe core sociological idea here is grounded in a critique of two extremes. Traditional philosophical approaches to cognition and meaning have been overly-focused on individual thinking and meaning. Conversely, neuroscience primarily focuses on processes presumed to be more-or-less universal, such as understanding the biological and chemical mechanisms that enable thought in general rather than explaining specific thoughts. But as Karen Cerulo and many others have pointed out, even if the cognitive processes are universal, cognitive products are not (Lizardo et al. 2019; K. A. Cerulo 2010; K. Cerulo 2002; Ignatow 2009). There is variation in how groups of people – societies, subcultures, whatever – perceive the world, draw boundaries, employ metaphors and analogies, and so on (DiMaggio 1997; Brekhus and Ignatow 2019). These meaning structures are not reducible to concepts in individual people’s heads; they are embedded in different cultural systems that are external to any individual person, and are shared but not universal. Given this variability and the staggering complexity of meaning in general, we can best understand meaning by understanding how people use language in context.\nThe idea that we could best understand shared but not necessarily universal meanings by studying how groups of people use language was, surprisingly, a revolutionary idea as recently as the 1950s. It was most famously posited by the famed philosopher Ludwig Wittgenstein (1953), whose argument that ‘meaning resides in use’ was the inspiration behind the specific linguistic hypothesis that informs embedding models and is a common theme underlying many of the recent breakthroughs in natural language processing: the distributional hypothesis.\n\n34.4.1 The Distributional Hypothesis\nWittgenstein’s (1953) proposal that empirical observations of how people actually use language could reveal far more about meaning than formal rules derived through logical analysis was taken up by a group of linguists in the 1950s (especially Joos 1950; Harris 1954; Firth 1957) who first proposed the distributional hypothesis, which has informed approaches to measuring and modelling meaning in language data ever since.\nAccording to the distributional hypothesis, words that appear in similar semantic contexts, or “environments,” will tend to have similar meanings.2 In one of the foundational statements of the idea, Zellig Harris (1954) defined a word’s context in terms of the other words that it co-occurs with, given some boundary such as a phrase or a sentence. For example, we can infer that “physician” and “doctor” mean similar things if we see that they tend to be used interchangeably in sentences like “Alondra is looking for a [physician, doctor, ...] specializing in pain management.” Across many texts, we might also learn that “doctor” and “professor” are also more or less interchangeable but in different types of context. While the former pair of words might co-occur in contexts shared with words such as “pain”, “medicine”, “nurse”, and “injury”, the latter pair may co-occur in contexts shared with words like “university”, “students”, “research”, “teaching”, and “knowledge”. “Professor” and “physician” may also co-occur, but more rarely. In any instance, the meaning of the words depends on the other words surrounding it. Words that have identical, or nearly identical contexts are synonyms. In fact, the distributional hypothesis bears a striking resemblance to the idea of structural equivalence in social network analysis, which was introduced in Chapter 30. (Like synonyms, people that are structurally similar tend to be connected to the same alters.)\nDistributionalists like Harris and Firth believe that formal theories of language should be kept to a minimum and knowledge should be produced by rigorous analysis of empirical data on language use. Given enough data on natural language use (e.g. in everyday interactions, in email messages and social media posts, in news stories and scientific publications, etc.), we can learn an enormous amount about the contextual relationships between words as they are actually used. In practice, this idea is operationalized in terms of vector semantics, and is the foundation of all modern natural language processing that is concerned with understanding meaning (Jurafsky and Hand 2009).\nWith that briefest of context introduced, let’s turn our attention to word embeddings.",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Language models and word embeddings</span>"
    ]
  },
  {
    "objectID": "language-models-and-embeddings.html#what-are-neural-word-embeddings",
    "href": "language-models-and-embeddings.html#what-are-neural-word-embeddings",
    "title": "33  Language models and word embeddings",
    "section": "34.5 WHAT ARE NEURAL WORD EMBEDDINGS?",
    "text": "34.5 WHAT ARE NEURAL WORD EMBEDDINGS?\nIn previous chapters, we used bag-of-words models to represent individual documents as long and sparse vectors, and document collections as wide, sparse matrices (i.e., DTMs). These matrices are long because each feature represents a unique word in the vocabulary, and each cell represents something like presence / absence, frequency, or some sort of weight such as TF-IDF for each word in each document. They are sparse because most words do not appear in most documents, which means that most cells have values of 0. This approach can be very powerful for modelling latent distributions of topical content, but we actually gain more insight into what words mean by using shorter, denser vector representations, generally referred as word embeddings. Words are just the beginning, though. They provide a foundation we can build on to explore and model meaning and larger cultural systems in ways that were not possible just a short time ago.\nIn bag-of-word models, we represent documents with long sparse vectors indicating the presence or absence, frequency, or weight of a word in each document. Embeddings differ in that they represent words with short dense vectors that define the local semantic contexts within wich words are used. ?fig-32_01 illustrates this idea of local semantic contexts using a window of 5 words that slides over each word in sentence from Neblo, Esterling, and Lazer (2018). This sliding window approach gives us much deeper insight into how words relate to other words, but it comes at the cost of fine-grained information about how each word relates to the documents in which they appeared.\n\nIn addition to (1) assigning vectors to words instead of documents, (2) observing co-occurrences within small local contexts rather than entire documents, and (3) using short dense vectors instead of long spare vectors, embeddings are also different in that (4) the vector representation for any given word is learned by a neural network trained on positive and negative examples of co-occurrence data. (In fact, we could have extracted embeddings from the neural networks we trained in Chapter 24!) Words that tend to appear in the same contexts, but rarely with one another, tend to share meanings. The learned word embeddings put words with similar meanings close to one another in vector space.\n\n\n34.5.1 Learning Embeddings with Word2Vec\nNow that we have some context for understanding embeddings, let’s discuss one of the most important recent breakthroughs in learning word embeddings from text data – word2vec. As with previous chapters, the goal here is mainly to clarify the way the models work at a relatively high-level.\nThe development of word2vec by Tomas Mikolov and a team of researchers at Google (Tomas Mikolov, Sutskever, et al. 2013; Tomas Mikolov, Chen, et al. 2013) was a transformative development in natural language processing. As we’ve already discussed, word embedding models in general are focused on the local semantic contexts that words are used in rather than the documents they appear in; they learn these short dense representations from the data rather than relying on count-based features. Let’s break down the modelling process, as we have with previous models in the book.\nWord2vec has two different architectures: Continuous Bag-of-Words (CBOW) and Skip-gram. Both use word co-occurrence data generated from local semantic contexts, such as a moving window of 5 words around a focal word as shown in the example in Figure XXX. However, CBOW and Skip-gram use this co-occurrence data differently; CBOW uses the context words (within each thin crimson box) in a shallow neural network model trained to predict the target word (in each thick crimson box), whereas skip-gram uses the target word to predict the context words. The interesting thing about the neural network model used in these two architectures is that we don’t actually care about their predictions. What we care about are the feature weights that the neural networks learn in the context of figuring out how to make their predictions well. Those feature weights are our word embeddings! We only train the neural network models to obtain the embeddings. That’s their raison d’etre.\nThe shallow neural network models that word2vec use to learn the embeddings, illustrated in Figure XXX (which is adapted from Tomas Mikolov, Sutskever, et al. (2013)), have a few clever modifications. In CBOW, the non-linear hidden layer is replaced by a much simpler linear projection. For each token in the data, the feature vector of the underlying token is the target, while the input to the neural network is the average of the vectors for each of the individual context tokens (ie. the vectors of the surrounding words). It’s worth mentioning briefly that the Gensim implementation of word2vec gives the option to sum the context vectors rather than average them. After the neural network tries to predict the target word, the resulting probabilities are used to update the feature weights for both the target token and the vectors of the context tokens that were averaged. Once training is done, these updated weights provide each word with a single, dense vector of feature weights. For Skip-gram, rather than sending an averaged context vector to the neural network objective function, the input is the token under consideration and the output is error (probability) vectors for each context word that are then added together, before being used to update feature weights.\nSecond, the functions used for the prediction in the output layer are different than what we might typically use in such a neural network. The CBOW architecture replaces the traditional softmax (log-linear) classifier for the output layer with a binary logistic regression classifier, and the Skip-gram architecture replaces it with a much more efficient hierarchical softmax variant. In ?fig-32_02, \\(T\\) represents the target word, and the indices represent word position in relation to the target word.\n\nFor CBOW, the second innovation is especially valuable, as using a softmax classifier to make and evaluate predictions would require updating feature weights (i.g., embeddings) for every word in the vocabulary every time a prediction is made for a word in the corpus. Instead, word2vec uses a clever innovation called negative sampling, in which the target word is evaluated as either co-occurring with the context words from the moving window or not. This enables the use of binary logistic regression for the output layer.\nIf you’re thinking “hold up, won’t the context words all have a score of 1?,” you’re right! To deal with this problem, the model randomly selects the required number of negative samples from the rest of the corpus (i.e., not from the local semantic context) and assigns them all 0s for that particular batch. As a result, the weights (again, embeddings) only need to be slightly increased for the target and context words and slightly decreased for the words from the negative sample.\nThe CBOW architecture is a variant of bag-of-words in that word sequence within the local semantic context does not have an impact on the prediction task described above. The similarities end there. Rather than creating one large static matrix, the ‘continuous’ part of CBOW refers to how the sliding window moves through the whole corpus, creating and then discarding a bag-of-words for each target word. Since the embeddings for each of the context words are averaged for the prediction task, the semantic context is flattened to a single vector regardless of the number of words in the semantic context. For this reason, it’s better to keep the semantic contexts fairly small. Otherwise the summing of embeddings can result in a non-descript vector soup, with the subtleties of each word increasingly diminished by the inclusion of more distant words. The authors of word2vec report that a window size of 4 on each side of the target word produced the best results for them.\nIn the Skip-gram architecture, the input and prediction tasks are basically the inverse of CBOW. Rather than using the average of the embeddings of words around the target word to try to predict the target word, Skip-gram uses the target word to try to predict the co-occurring words in its semantic context. There is no averaging of vectors before training, and the training process focuses on the relationship between the target word and many different context words, so the embeddings learned by Skip-gram tend to be more subtle and lossless. Skip-gram has a much longer training time, though, because each word under consideration is used to predict multiple context words before the prediction vectors for each of those words are added together and then used to update the feature weight vectors for the context words. As with CBOW, we can improve the training runtime by using negative sampling.\nUnlike CBOW, where the summing of embeddings prior to prediction can result in a less informative vector soup if the semantic contexts are too large, Skip-gram actually benefits from larger window sizes (at the expense of increased runtime). One benefit is that the lack of summing means any updates to the weights are specific to that word, and are therefore more precise. Second, there are far more updates to the embeddings, as each word is used in far more model predictions than would be the case in CBOW. Finally, Skip-gram models do consider word ordering a bit, in that they weight relationships between the target word and context words based on how far away they are within the semantic context, so a window of 10, for example, is a pretty good balance.\nA discussion of the hierarchical softmax variant that word2vec uses is outside the scope of this chapter, but the simplified version is that words and their outputs are arranged in a tree-like pattern, such that many words (leaves) are often connected to the same output and their weights can all be updated from a single calculation. The more important thing to know is that hierarchical softmax tends to perform better on infrequent words whereas negative sampling performs better on frequent words. Either of these classifiers can be used for both the CBOW and Skip-gram options, and can actually be used at the same time.\nBoth model architectures, then, have their strengths and weaknesses. The CBOW architecture is a bit better at learning syntactic relationships, so is likely to produce embeddings where word pairs like ‘neuron’ and ‘neurons’ or ‘broken’ and ‘broke’ will be very similar. CBOW also tends to better represent frequently appearing words and is faster to train, so is well-suited to large corpuses. Skip-gram models produce more precise word embeddings in general, and especially for rare words. The embeddings it produces can be especially good at finding words that are near-synonyms. The cost of these improvements are increases in runtime, but in cases where that is less of a concern (e.g., working with smaller datasets), the improvements can certainly be worth the wait. The differences between these architectures are less significant given the specific model parameters used and given enough iterations.",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Language models and word embeddings</span>"
    ]
  },
  {
    "objectID": "language-models-and-embeddings.html#cultural-cartography-getting-a-feel-for-vector-space",
    "href": "language-models-and-embeddings.html#cultural-cartography-getting-a-feel-for-vector-space",
    "title": "33  Language models and word embeddings",
    "section": "34.6 CULTURAL CARTOGRAPHY: GETTING A FEEL FOR VECTOR SPACE",
    "text": "34.6 CULTURAL CARTOGRAPHY: GETTING A FEEL FOR VECTOR SPACE\nWord embeddings are very powerful for many applications and sometimes the results are astonishing. But there are some very important caveats to keep in mind when using word2vec-style embeddings. We will illustrate those caveats with perhaps the most iconic and oft-referenced example of word embedding “analogies”.\n\n34.6.1 King - Man + Woman \\(\\neq\\) Queen\nRecall that part of the CBOW training process is to average (or just sum) the context vectors. Tomáš Mikolov, Yih, and Zweig (2013) found that if you take the word embedding vector for “king”, add it to the vector for “woman”, and then subtract the vector for “man”, the resulting vector is “very close” to the vector for “queen”. This example has been referenced countless times, from package documentation to social science papers that aim to measure and compare complex cultural concepts.\nWe will use the very convenient whatlies package to plot the iconic word embedding example.\nfrom whatlies import Embedding\nfrom whatlies.embeddingset import EmbeddingSet\nfrom whatlies.language import SpacyLanguage\nlang = SpacyLanguage('en_core_web_md')\n\nimport pandas as pd\npd.set_option(\"display.notebook_repr_html\", False)\nfrom dcss.utils import list_files, IterSents, mp_disk\nfrom dcss.text import bigram_process\n\nimport gensim\nfrom multiprocessing import Process, Manager\nfrom gensim.utils import simple_preprocess\n\nimport matplotlib.pyplot as plt\nfrom dcss import set_style\nset_style()\nUsing the plot() function, we can plot either a single word vector, or some mathematical combination of vectors enclosed in brackets (as shown in ?fig-32_03). If you call plot() multiple times in the same cell, all of the requested vectors will show up in the figure.\n(lang['queen'] - lang['king']).plot(kind='arrow', color='lightgray', show_ops=True)\n(lang['king'] + lang['woman'] - lang['man']).plot(kind='arrow', color='lightgray', show_ops=True)\n\nlang['man'].plot(kind='arrow', color='crimson')\nlang['woman'].plot(kind='arrow', color='crimson')\n\nlang['king'].plot(kind='arrow', color='black')\nlang['queen'].plot(kind='arrow', color='black')\n\nplt.axis('off');\nplt.show()\n\n\n\npng\n\n\nThe combination vector appears to be virtually identical to the vector for “queen”. But there is more to this than meets the eye. Let’s look at a few comparisons between the vectors with some useful vector combination and comparison functions built-in to whatlies.\nprint(\"Queen and King: \" + str(lang['queen'].distance(lang['king'])))\nprint(\"Man and Woman: \" + str(lang['man'].distance(lang['woman'])))\nprint(\"Man and King: \" + str(lang['man'].distance(lang['king'])))\nprint(\"Woman and King: \" + str(lang['woman'].distance(lang['king'])))\nQueen and King: 0.27473903\nMan and Woman: 0.2598256\nMan and King: 0.59115386\nWoman and King: 0.7344341\nTake note that “queen” and “king” aren’t very distant from each other (this is cosine distance). Neither are “man” and “woman”. This is because they actually share a lot of the same semantic contexts; that is, they are used, conversationally, in very similar ways. With that said, “man” is definitely a bit closer to “king” than “woman” is. Let’s do the vector math.\nking_woman_no_man = lang['king'] + lang['woman'] - lang['man']\nprint(\"King and combo-vector:\" + str(lang['king'].distance(king_woman_no_man)))\nprint(\"Queen and combo-vector: \" + str(lang['queen'].distance(king_woman_no_man)))\nKing and combo-vector:0.19757414\nQueen and combo-vector: 0.21191555\nThe combined vector that should be almost the same as “queen” is actually still closer to the vector for “king”. Given the plot above, how is this possible? This is the first caveat: word embedding vectors are multi-dimensional space - in this case, 300 dimensions. The best we can really plot is 3-dimensional space and the plot above is 2-dimensional. In either case, there is a LOT of data-reduction happening.\nLet’s get a different perspective on things by using plot_interactive() (a screenshot of which is shown in ?fig-32_04). First, add the vectors to an EmbeddingSet() class instance. Then it’s as simple as adding .plot_interactive() to that object, along with a few parameters, including the distance metric to use for the axes (cosine distance).\n## RENAME THE COMBINATION VECTOR BECAUSE THE ORIGINAL ('MAN') WOULD BE USED FOR THE PLOT\nking_woman_no_man.orig = king_woman_no_man.name \n\nking_queen_man_woman_plus = EmbeddingSet(lang['king'], lang['queen'], \n                                         lang['man'], lang['woman'], king_woman_no_man)\n\nking_queen_man_woman_plus.plot_interactive(x_axis=lang[\"king\"], \n                                           y_axis=lang[\"queen\"], \n                                           axis_metric = 'cosine_similarity')\n\nThis helps put things into perspective. The combination vector is clearly a shift towards queen and away from king, but not dramatically considering that it’s been influenced by two vectors, so the ‘king’ vector is actually only 1/3rd of the combination one. Recall how these words, which you might be tempted to consider opposites, actually share a lot of contexts. Their embeddings are all wrapped up with each other. When you remove the vector for ‘man’ from the vector for ‘woman’, you are actually taking some defining details away from the vector for ‘woman’ because you’ve removed parts of the contexts that they share! Here’s an illustrative example.\nprint(\"Woman and Queen: \" + str(lang['woman'].distance(lang['queen'])))\nprint(\"Woman and Queen without man: \" + str((lang['woman']-lang['man']).distance(lang['queen'])))\nWoman and Queen: 0.5933935\nWoman and Queen without man: 0.7745669\nThe distance between “woman” and “queen” actually increases by about 18% if you subtract the vector for “man”! You can see why we need to be extremely careful and methodical in any research that relies on complex combinations of vectors, and in fact this may be a research path to avoid entirely. If you find yourself in a situation where one term is more central to the concept you’re examining than others, for example, the other terms will outweigh the important one. However, remember that these vectors are just arrays – you can weight the entire array if you want to change its contribution to the combined vector.\nIn the next cell, we access the raw vectors for ‘woman’ and ‘man’, multiplying the latter by 0.5, before making them Embedding class objects again.\nprint(\"Woman and Queen without man: \" + str(Embedding('halfway', lang['woman'].vector-lang['man'].vector*0.5).distance(lang['queen'])))\nWoman and Queen without man: 0.61308193\nAs you can see, removing only half of the vector for ‘man’ dramatically reduces the amount of extra distance between ‘woman’ and ‘queen’. In the section that follows, we’ll take a bit of time to look at groups of embeddings for words that are not quite so universally used in most life contexts.\nYou might also be wondering why the king+woman-man example has been used so frequently in the literature, given this issue. You will find that, in some implementations of word2vec, ‘queen’ will be returned as the “most similar” word to the combined vector. Typically, when you use an in-built function to combine words and then return the most similar words, the results returned will not include the constituent words! If they didn’t, those functions would always return the word itself as the top similar word! This is understandable for a convenience function, but also important to be aware of when using embeddings for research. This issue has been noted and discussed in more detail previously, with some heavy caution about the use of word analogy tasks for any serious purposes (Nissim, Noord, and Goot 2020). The authors also reference the introductory paper for transformer models, which we’ll cover in detail in the next chapter, noting that they’ve completely eliminated the concept of analogy as either a training task, or model evaluation method.\nWith that said, if we’re careful about how we use embeddings and the claims we make about them, it’s hard to argue against the results of the embeddings, without manipulation, as indicators of patterns of text use.",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Language models and word embeddings</span>"
    ]
  },
  {
    "objectID": "language-models-and-embeddings.html#learning-embeddings-with-gensim",
    "href": "language-models-and-embeddings.html#learning-embeddings-with-gensim",
    "title": "33  Language models and word embeddings",
    "section": "34.7 LEARNING EMBEDDINGS WITH GENSIM",
    "text": "34.7 LEARNING EMBEDDINGS WITH GENSIM\nNow that you have some understanding of what word embeddings are, what they can be used for, and how the models that learn them work, let’s get our hands dirty by actually training some models with Gensim.\n\n34.7.1 Data\nWe’ll use the Canadian Hansard dataset for the rest of this chapter (and the next).\ndatasets = list_files(\"data/canadian_hansards/lipad/\", 'csv')\nlen(datasets)\n3401\nTraining good word embeddings requires a lot of text, and we want to avoid loading all that text into memory at once. Gensim’s algorithm expects only a single sentence at a time, so a clever way to avoid consuming a lot of memory is to store each sentence from the data on its own line in a text file, and then read that enormous text file into memory one line at a time, passing just the sentence to Gensim. That way, we never have to hold all of our data in memory at the same time.\nThis requires some pre-processing. The Canadian Hansard data is provided as a large collection of CSV files, each containing a single Series with full text for a given speech. We want to get each sentence from each speech in each dataset, while working as efficiently as possible and minimizing the amount of data held in memory.\nThe function below is one way to do this. It will take some time to run, but perhaps not as long as you would think, given how much data we are working with here, and given that we can use the mp_disk utility for multiprocessing to take advantage of available CPU cores. A less general version of the mp utility, mp_disk, accepts an iterable (e.g. a list) of the data that needs processing, the function you’ll use to process it, a filename to write the results to, and any other arguments that the processing function needs.\nYou may notice the unexplained q object at the end of this function call. Although a full discussion of the ins and outs of multiprocessing is beyond the scope of this chapter, it is useful to understand what is going on here. The q and the m objects are specific instances of general classes in python’s multiprocessing module that allow us to write to a text file from multiple parallel processes without having to worry about file access locks or file corruption. The iterable with the data in it will also be divided into multiple lists, so that each CPU core can work on its own subset, so it’s important that the function is prepared to deal with a list of data and also return that data in a list.\nThe next block of text iterates over each of the dataframes in the batch, adding the speeches from each to a list. The batch of speeches is sent to the bigram_process function, which is a convenience wrapper for Gensim’s n-gram pipeline and some text pre-processing using SpaCy. This function expects a flat list of documents, then handles breaking each document into sentences and creating the flat list of sentences that Gensim expects for bigram model training. The utility function returns a list of untokenized sentences, with bigram pairs of words joined by _.\nTo cap off the process, we send each batch of results to the multiprocessing Queue object so that each sentence can be written onto a new line of the file speeches.txt. Before sending the sentences to the file writing queue, we join them into a single string with a new line character in between, because this is much faster than having the multiprocessing queue write each line to the output file individually.\nWhew. Let’s do it.\ndef get_sentences(dataset):\n    \n    dfs = [pd.read_csv(df) for df in dataset]  \n    speeches = []\n    \n    for df in dfs:\n        speeches.extend(df['speechtext'].tolist())\n    speeches = [str(s).replace('\\n|\\r', ' ') for s in speeches]     \n    _, sentences = bigram_process(speeches, n_process = 1)    \n    sentences = '\\n'.join(sentences)  \n    \n    q.put(sentences)\nBelow, we use the above get_sentences() function to process the data in our datasets object, writing the results out to speeches.txt, with each sentence from each speech getting it’s own line in the file. It will take some time to run, but perhaps not as long as you would think given how much data we are working with here.\nm = Manager()\nq = m.Queue()\nmp_disk(datasets, get_sentences, 'data/txt_files/can_hansard_speeches.txt', q)\nLet’s do a quick count to see how many words our dataset contains.\nwith open('data/txt_files/can_hansard_speeches.txt') as file:\n    data = file.read()\n    words = data.split()\n    print(len(words))\nThis file has roughly 180 million words after processing.\nWith our data re-organized in speeches.txt, we can iterate over the file to train a CBOW or Skipgram classification model, while using as little memory as possible. We will use a custom class that does the iteration for us, yielding one sentence at a time, which we can pass into gensim.models.Word2Vec(). Once again, you can expect this process to take some time but it’ll be sped up by setting the workers parameter to the number of CPU cores you have.\nsentences = IterSents('data/txt_files/can_hansard_speeches.txt')\n\nmodel = gensim.models.Word2Vec(sentences, size = 300, window = 4, iter = 5, \n                               sg = 0, min_count = 10, negative = 5, workers = 4)\nAnd with that, we’ve learned our embeddings from a dataset of roughly 180 million words! We don’t want to have to relearn these embeddings needlessly (who has time for that?), so we’ll write the model vocabulary to a text file called model_vocabulary.txt and then save the model itself to disk. That way, we can reload our trained model, rather than wasting time and energy re-training it.\nvocabulary = sorted(list(model.wv.vocab))\n\nwith open('../models/model_vocabulary.txt', 'w') as f:\n    for v in vocabulary:\n        f.write(v)\n        f.write('\\n')\n\nmodel.save('../models/word2vec.model')\nThe model can be reloaded anytime, and if we don’t have to update it anymore, we can keep just the word vectors themselves, which is a leaner object.\nmodel = gensim.models.Word2Vec.load('../models/word2vec.model')\nmodel = model.wv",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Language models and word embeddings</span>"
    ]
  },
  {
    "objectID": "language-models-and-embeddings.html#comparing-embeddings",
    "href": "language-models-and-embeddings.html#comparing-embeddings",
    "title": "33  Language models and word embeddings",
    "section": "34.8 COMPARING EMBEDDINGS",
    "text": "34.8 COMPARING EMBEDDINGS\nEverything we’ve done so far can also be done comparatively, which makes things much more interesting from a social scientific perspective. The trouble with these sorts of extensions is that the word embedding training process is stochastic, so we can’t just learn embeddings for various different datasets and directly compare them. In fact, there’s no guarantee that two models trained on the exact same data will end up looking even remotely similar! While the relations between the words in vector space may be more or less consistent in the two models (in the sense that the angle between them will be similar) the random starting positions of those words in that vector space can produce wildly differing final states. To do anything comparative, cross-sectional or over time, we need our vector spaces to be aligned.\nThere have been a number of solutions proposed to solve this problem (e.g., Ruder, Vulić, and Søgaard 2019; Artetxe, Labaka, and Agirre 2016; Mogadala and Rettinger 2016; Di Carlo, Bianchi, and Palmonari 2019), but we will focus on the “compass” approach developed by Di Carlo, Bianchi, and Palmonari (2019) because it’s well-implemented, efficient, and has Gensim at its core. It’s designed with temporal data in mind, but we handle cross-sectional comparisons in the exact same way. Below, I’ll walk you through training a word embedding model “anchor” (the compass) as a basis for comparison, and then we’ll spend a bit of time working through a few temporal and cross-sectional comparisons.\n\nThe compass functionality is available in the python package TWEC, which must be installed manually from the source code provided on GitHub. As the authors of the package note, TWEC requires a customized version of Gensim, so it’s advisable to make a virtual environment specifically for working with this package. As a reminder, you can do so with the following steps, all from the command line:\n\nClone the GitHub repository at https://github.com/valedica/twec.git\nCreate a new conda vitual environment with conda create -n twec_training\nActivate your new conda environment with conda activate twec_training\npip install cython\nThe author’s custom version of Gensim, pip install git+https://github.com/valedica/gensim.git\ncd into the twec repository\npip install –user .\n\nIf you end up having a lot of trouble getting TWEC up and running, you can use any version of Gensim to load the models that have been pre-trained for this chapter. You can read more about our pre-trained models in the online supplementary materials.\n\n\n34.8.1 Imports\nSince we are working in a new virtual environment (details provided in Box XXX) with a fresh new Python kernel. We’ll continue to work with the Canadian Hansard data.\nfrom twec.twec import TWEC\nfrom gensim.models.word2vec import Word2Vec\nimport pandas as pd\nfrom dcss.utils import list_files, mp_disk\n\nfrom tok import Tokenizer\nfrom gensim.utils import simple_preprocess\nfrom multiprocessing import Process, Manager\nimport re\n\n\n34.8.2 Aligning Your Vector Spaces!\nThe general process of using the TWEC approach to train a series of embedding models that are aligned from the start is as follows:\n\nTrain a word2vec model on the entire dataset in one go, retaining the position layer of the neural network model. This layer is called the compass. It computes a set of baseline embeddings that a series of embedding models (trained in Step 2) trained on every subset of the data (temporal slices, for example) can use as a common starting point, like a kind of “reference model.”\nTrain a word2vec model for each subset of the data using the compass layer from Step 1 as the starting point. This ensures the vector spaces are properly aligned and lets the vector coordinates move around according to the embeddings of words in that subset of data.\n\nOnce the reference model has been trained, the series of contextual models trained in the next step call all be trained with a common starting point (as opposed to a random one). Then the embeddings for each subset diverge from that common origin as appropriate and the differences and similarities between their vectors can be interpreted as meaningful differences. Di Carlo, Bianchi, and Palmonari (2019) provide plenty of technical details on how TWEC works, if you are interested in going beyond what I introduce here.\nLet’s perform both steps. We’ll use the compass trained in Step 1 for a series of temporal and cross-sectional comparsons later in the chapter.\n\n\n34.8.3 Step 1: Train the Compass\nTo train the compass, TWEC expects a text file where each sentence in our dataset is provided on a new line. Since we prepared this exact file in the previous chapter, we’ll reuse it here. It’s stored in speeches.txt.\ncompass_path = 'data/txt_files/can_hansard_speeches.txt'\nBecause TWEC uses a custom version of Gensim, it doesn’t automatically receive the many updates that Gensim has had in recent years. One of the package dependencies has been updated since the Di Carlo, Bianchi, and Palmonari (2019) paper was published and now produces a warning about a function that will eventually be deprecated. To keep things a bit cleaner, we’ll tell Python to suppress those warnings.\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nNow we can inialize a TWEC class object, providing the parameters to pass to Gensim for training (note that the negative= argument for negative sampling is replaced by ns= here). We’ll use this object to create the compass and when training the aligned temporal slices.\nThe results are automatically saved to a model/ folder in the current working directory. This process will take the same amount of time as it took to train the Word2Vec model above, so it’s best to set “overwrite” to False so we don’t accidentally lose all of that processing time. Remember to set the number of workers to the number of cores you want to use - most personal computers have 4 cores. If you ever need to pick things back up after a restart (or a kernel crash) running the cell again will simply reload the trained compass.\naligner = TWEC(size = 300, siter = 5, diter = 5, window = 10, sg = 0, min_count = 10, ns = 5, workers = 4)\naligner.train_compass(compass_path, overwrite=False)\n\n\n34.8.4 Step 2: Train a Series of Aligned Embedding Models\nNow that our reference model has been trained and stored in the aligner object, we can proceed with training a series of embedding models on various subsets of our data. In the examples that follow, we will train a series of models to show change over time, followed by a series of models to compare speeches by different political parties. We will use the same aligner object as the reference model for both.\n\n34.8.4.1 Research on Cultural Change with Temporal Embeddings\nRegardless of whether our comparison is cross-sectional or temporal, we need to subset our data prior to training any additional models. Since we are starting using embeddings to compare change over time, let’s divide our data into different temporal slices. We’ll be training a gemsim word2vec model with each subset, so we will prepare the data with one sentence-per-line file for model training.\nIn this case, the CSV files in the Canadian Hansard dataset are organized into folders by year. We can use that to our advantage here. First, we’ll load up the CSV files and create some lists to store the file paths for each decade.\ndatasets = list_files(\"data/canadian_hansards/lipad/\", 'csv')\nlen(datasets)\ncanadian_1990s = []\ncanadian_2000s = []\ncanadian_2010s = []\n\nfor i in range(1990,1999):\n    year_data = 'data/canadian_hansards/lipad/' + str(i) + '/'\n    datasets_1990s = list_files(year_data, 'csv')\n    canadian_1990s.extend(datasets_1990s)\n    \nfor i in range(2000,2009):\n    year_data = 'data/canadian_hansards/lipad/' + str(i) + '/'\n    datasets_2000s = list_files(year_data, 'csv')\n    canadian_2000s.extend(datasets_2000s)\n    \nfor i in range(2010,2019):\n    year_data = 'data/canadian_hansards/lipad/' + str(i) + '/'\n    datasets_2010s = list_files(year_data, 'csv')\n    canadian_2010s.extend(datasets_2010s)\nNow that we have our data organized into temporal slices, we need to create our sentence-per-line files. To do that with multiprocessing, we’ll re-use the get_sentences() function we used in the previous chapter.\nm = Manager()\nq = m.Queue()\nmp_disk(canadian_1990s, get_sentences, 'data/txt_files/1990s_speeches.txt', q)\nm = Manager()\nq = m.Queue()\nmp_disk(canadian_2000s, get_sentences, 'data/txt_files/2000s_speeches.txt', q)\nm = Manager()\nq = m.Queue()\nmp_disk(canadian_2010s, get_sentences, 'data/txt_files/2010s_speeches.txt', q)\nFinally, we can train individual models on the slices using the aligner object. As you may have guessed, this can take a bit of time and you probably want to process each in its own cell, setting “save” to True so that the model will be output to the model/ directory, with a filename matching the name of the text file provided.\nmodel_1990s = aligner.train_slice('data/txt_files/1990s_speeches.txt', save=True)\nmodel_2000s = aligner.train_slice('data/txt_files/2000s_speeches.txt', save=True)\nmodel_2010s = aligner.train_slice('data/txt_files/2010s_speeches.txt', save=True)\nAt this point we don’t need the compass model anymore, but it’s a good idea to keep it around. The contextual models we’ve trained for each temporal slice are good to go, and unlike the compass model, can simply be loaded into Gensim for analysis. Note that although we used sg=0 above because Skip-gram takes a long time to train compared to CBOW, the models you can load below were trained with Skip-gram.\nmodel_1990s = Word2Vec.load('../models/1990s_speeches.model')\nmodel_2000s = Word2Vec.load('../models/2000s_speeches.model')\nmodel_2010s = Word2Vec.load('../models/2010s_speeches.model')\nNow that we’ve trained our aligned temporal embedding models, we can do all kinds of interesting and useful things, such as comparing the embeddings of terms in different decades. As a simple example, let’s look at the most similar words to ‘climate_change’ across each decade. We should expect to see tokens such as ‘global_warming’ showing up, but that’s what we want; our model (which doesn’t actually know what words mean) is doing what it’s supposed to do. Below we can see that the similarity between these terms starts to decline a bit in the 2010s, when ‘climate_change’ became the preferred term.\nmodel_1990s.wv.most_similar(positive = 'climate_change', topn = 10)\nmodel_2000s.wv.most_similar(positive = 'climate_change', topn = 10)\nmodel_2010s.wv.most_similar(positive = 'climate_change', topn = 10)\n\n\n34.8.4.2 Cross-sectional Comparisons: Political Parties on Climate Change\nSometimes our research goals are to compare culture and meaning across subgroups in a population, rather than change over time. For example, continuing with the examples we’ve used in this chapter so far, we might be more interested in comparing how different political parties talk about climate change than how political discussions of climate change have evolved over time.\nTo make those comparisons, we need to organize our data by political party rather than by decade. To keep things relatively simple, we’ll focus on the three major political parties: the Liberals, the New Democratic Party, and the Conservatives, keeping in mind that the latter is a relatively recent merger of the former Canadian Alliance, Progressive Conservative, and Reform parties. In this case, slicing the data isn’t quite as straightforward, so we’ll create a modified version of get_sentences() that will accept lists of terms to mask (filter) the dataframes with.\nliberal = ['Liberal']\nconservative = ['Conservative', 'Canadian Alliance', 'Progressive Conservative', 'Reform']\nndp = ['New Democratic Party']\ndef get_sentences_by_party(dataset, filter_terms):\n    \n    dfs_unfiltered = [pd.read_csv(df) for df in dataset]\n    dfs = []  \n    \n    for df in dfs_unfiltered:\n        temp_df = df.dropna(subset = ['speakerparty'])\n        mask = temp_df['speakerparty'].apply(lambda x: any(party for party in filter_terms if party in x))\n        temp_df2 = temp_df[mask]\n        if len(temp_df2) &gt; 0:\n            dfs.append(temp_df2)\n        \n    speeches = []\n    \n    for df in dfs:\n        speeches.extend(df['speechtext'].tolist())\n    speeches = [str(s).replace('\\n|\\r', ' ') for s in speeches]   # make sure everything is a lowercase string, remove newlines    \n    _, sentences = u.bigram_process(speeches)    \n    sentences = '\\n'.join(sentences)  # join the batch of sentences with newlines into 1 string\n    \n    q.put(sentences)\nm = Manager()\nq = m.Queue()\n\nmp_disk(datasets, get_sentences_by_party, 'data/txt_files/liberal_speeches.txt', q, liberal)\nm = Manager()\nq = m.Queue()\n\nmp_disk(datasets, get_sentences_by_party, 'data/txt_files/conservative_speeches.txt', q, conservative)\nm = Manager()\nq = m.Queue()\n\nmp_disk(datasets, get_sentences_by_party, 'data/txt_files/ndp_speeches.txt', q, ndp)\nNow we can train an aligned model for each of the three parties, using the same aligner object we used earlier (trained on the full corpus).\nmodel_liberal = aligner.train_slice('data/txt_files/liberal_speeches.txt', save=True)\nmodel_conservative = aligner.train_slice('data/txt_files/conservative_speeches.txt', save=True)\nmodel_ndp = aligner.train_slice('data/txt_files/ndp_speeches.txt', save=True)\nWith our three aligned models, we can now compare how each of the three major parties talk about climate change. Remember that this is for all party-specific talk from 1990 onwards. We could train more models to disaggregate things even further (e.g., each party in each decade), but we’ll keep things simple here.\nmodel_liberal = Word2Vec.load('../models/liberal_speeches.model')\nmodel_conservative = Word2Vec.load('../models/conservative_speeches.model')\nmodel_ndp = Word2Vec.load('../models/ndp_speeches.model')\nmodel_liberal.wv.most_similar(positive = 'climate_change', topn = 10)\nmodel_conservative.wv.most_similar(positive = 'climate_change', topn = 10)\nmodel_ndp.wv.most_similar(positive = 'climate_change', topn = 10)\nOf course, everything we did previously with the pre-trained embeddings can be applied and generalized with the models we’ve trained here. Give it a shot!\n\nFurther Reading\nAdji Dieng, Francisco Ruiz, and David Blei (2020) have developed a really interesting probabilistic topic model that uses embeddings to represent text rather than the DTM representations used in LDA topic models. They also generalize this model for dynamic data in (A. Dieng, Ruiz, and Blei 2019). If you are interested in the relationship between topic models and word embeddings, I recommend reading their articles.",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Language models and word embeddings</span>"
    ]
  },
  {
    "objectID": "language-models-and-embeddings.html#conclusion-1",
    "href": "language-models-and-embeddings.html#conclusion-1",
    "title": "33  Language models and word embeddings",
    "section": "34.9 CONCLUSION",
    "text": "34.9 CONCLUSION\n\n34.9.1 Key Points\n\nWord embeddings represent words with short dense vectors that describe the word’s local semantic contexts\nEmbeddings as a whole depict patterns of word usage and language structure\nThey are NOT “meaning,” and we should not trust intuitions built on low-dimensional representations\nConstructed embeddings and aligned embeddings using Gensim\n\n\n\n\n\nArtetxe, Mikel, Gorka Labaka, and Eneko Agirre. 2016. “Learning Principled Bilingual Mappings of Word Embeddings While Preserving Monolingual Invariance.” In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 2289–94.\n\n\nBolukbasi, Tolga, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. 2016. “Man Is to Computer Programmer as Woman Is to Homemaker? Debiasing Word Embeddings.” arXiv Preprint arXiv:1607.06520.\n\n\nBonikowski, Bart. 2017. “Ethno-Nationalist Populism and the Mobilization of Collective Resentment.” The British Journal of Sociology 68: S181–213.\n\n\nBrekhus, Wayne, and Gabe Ignatow. 2019. The Oxford Handbook of Cognitive Sociology. Oxford University Press.\n\n\nCerulo, Karen. 2002. Culture in Mind: Toward a Sociology of Culture and Cognition. Psychology Press.\n\n\nCerulo, Karen A. 2010. “Mining the Intersections of Cognitive Sociology and Neuroscience.” Poetics 38 (2): 115–32.\n\n\nCollins, Patricia. 2015. “Intersectionality’s Definitional Dilemmas.” Annual Review of Sociology 41: 1–20.\n\n\nCollins, Patricia, and Sirma Bilge. 2020. Intersectionality. John Wiley & Sons.\n\n\nCrenshaw, Kimberlé. 1989. “Demarginalizing the Intersection of Race and Sex: A Black Feminist Critique of Antidiscrimination Doctrine, Feminist Theory and Antiracist Politics.” U. Chi. Legal f., 139.\n\n\nCrossley, Nick. 2010. Towards Relational Sociology. Routledge.\n\n\nDi Carlo, Valerio, Federico Bianchi, and Matteo Palmonari. 2019. “Training Temporal Word Embeddings with a Compass.” In Proceedings of the AAAI Conference on Artificial Intelligence, 33:6326–34. 01.\n\n\nDieng, Adji B, Francisco JR Ruiz, and David M Blei. 2020. “Topic Modeling in Embedding Spaces.” Transactions of the Association for Computational Linguistics 8: 439–53.\n\n\nDieng, Adji, Francisco Ruiz, and David Blei. 2019. “The Dynamic Embedded Topic Model.” arXiv Preprint arXiv:1907.05545.\n\n\nDiMaggio, Paul. 1997. “Culture and Cognition.” Annual Review of Sociology 23 (1): 263–87.\n\n\nEdelmann, Achim, and John Mohr. 2018. “Formal Studies of Culture: Issues, Challenges, and Current Trends.” Poetics 68: 1–9.\n\n\nElbourne, Paul. 2011. Meaning: A Slim Guide to Semantics. Oxford University Press.\n\n\nEmirbayer, Mustafa. 1997. “Manifesto for a Relational Sociology.” American Journal of Sociology 103 (2): 281–317.\n\n\nFirth, John. 1957. “A Synopsis of Linguistic Theory, 1930-1955.” Studies in Linguistic Analysis.\n\n\nFranzosi, Roberto. 2004. From Words to Numbers: Narrative, Data, and Social Science. Cambridge University Press.\n\n\nGarg, Nikhil, Londa Schiebinger, Dan Jurafsky, and James Zou. 2018. “Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes.” Proceedings of the National Academy of Sciences 115 (16): E3635–44.\n\n\nHarris, Zellig S. 1954. “Distributional Structure.” Word 10 (2-3): 146–62.\n\n\nIgnatow, Gabriel. 2009. “Culture and Embodied Cognition: Moral Discourses in Internet Support Groups for Overeaters.” Social Forces 88 (2): 643–69.\n\n\nJones, Jason, Mohammad Ruhul Amin, Jessica Kim, and Steven Skiena. 2020. “Stereotypical Gender Associations in Language Have Decreased over Time.” Sociological Science 7: 1–35.\n\n\nJoos, Martin. 1950. “Description of Language Design.” The Journal of the Acoustical Society of America 22 (6): 701–7.\n\n\nJurafsky, Dan, and Martin Hand. 2009. Speech & Language Processing. 2nd ed. Pearson Prentice Hall.\n\n\nKozlowski, Austin, Matt Taddy, and James Evans. 2019. “The Geometry of Culture: Analyzing the Meanings of Class Through Word Embeddings.” American Sociological Review 84 (5): 905–49.\n\n\nLinzhuo, Li, Wu Lingfei, and Evans James. 2020. “Social Centralization and Semantic Collapse: Hyperbolic Embeddings of Networks and Text.” Poetics 78: 101428.\n\n\nLizardo, Omar, Brandon Sepulvado, Dustin S Stoltz, and Marshall A Taylor. 2019. “What Can Cognitive Neuroscience Do for Cultural Sociology?” American Journal of Cultural Sociology, 1–26.\n\n\nMcLevey, John, Tyler Crick, Browne Pierson, and Darrin Durant. 2021. “Word Embeddings and the Structural and Cultural Dimensions of Democracy and Autocracy, 1900-2020.” Canadian Review of Sociology 7: 544–69.\n\n\nMikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of Word Representations in Vector Space.” arXiv Preprint arXiv:1301.3781.\n\n\nMikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. “Distributed Representations of Words and Phrases and Their Compositionality.” In Advances in Neural Information Processing Systems, 3111–19.\n\n\nMikolov, Tomáš, Wen-tau Yih, and Geoffrey Zweig. 2013. “Linguistic Regularities in Continuous Space Word Representations.” In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 746–51.\n\n\nMische, Ann. 2011. “Relational Sociology, Culture, and Agency.” The SAGE Handbook of Social Network Analysis, 80–97.\n\n\nMogadala, Aditya, and Achim Rettinger. 2016. “Bilingual Word Embeddings from Parallel and Non-Parallel Corpora for Cross-Language Text Classification.” In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 692–702.\n\n\nMohr, John. 1998. “Measuring Meaning Structures.” Annual Review of Sociology 24 (1): 345–70.\n\n\nMohr, John, Christopher Bail, Margaret Frye, Jennifer Lena, Omar Lizardo, Terence McDonnell, Ann Mische, Iddo Tavory, and Frederick Wherry. 2020. Measuring Culture. Columbia University Press.\n\n\nMohr, John, and Petko Bogdanov. 2013. “Introduction—Topic Models: What They Are and Why They Matter.” Elsevier.\n\n\nMohr, John, Robin Wagner-Pacifici, and Ronald Breiger. 2015. “Toward a Computational Hermeneutics.” Big Data & Society 2 (2): 2053951715613809.\n\n\nNeblo, Michael, Kevin Esterling, and David Lazer. 2018. Politics with the People: Building a Directly Representative Democracy. Vol. 555. Cambridge University Press.\n\n\nNelson, Laura. 2021. “Leveraging the Alignment Between Machine Learning and Intersectionality: Using Word Embeddings to Measure Intersectional Experiences of the Nineteenth Century US South.” Poetics, 101539.\n\n\nNissim, Malvina, Rik van Noord, and Rob van der Goot. 2020. “Fair Is Better Than Sensational: Man Is to Doctor as Woman Is to Doctor.” Computational Linguistics 46 (2): 487–97.\n\n\nNivre, Joakim, and Chiao-Ting Fang. 2017. “Universal Dependency Evaluation.” In Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017), 86–95.\n\n\nPapakyriakopoulos, Orestis, Simon Hegelich, Juan Carlos Medina Serrano, and Fabienne Marco. 2020. “Bias in Word Embeddings.” In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 446–57.\n\n\nRheault, Ludovic, and Christopher Cochrane. 2020. “Word Embeddings for the Analysis of Ideological Placement in Parliamentary Corpora.” Political Analysis 28 (1): 112–33.\n\n\nRuder, Sebastian, Ivan Vulić, and Anders Søgaard. 2019. “A Survey of Cross-Lingual Word Embedding Models.” Journal of Artificial Intelligence Research 65: 569–631.\n\n\nStoltz, Dustin, and Marshall Taylor. 2019. “Concept Mover’s Distance: Measuring Concept Engagement via Word Embeddings in Texts.” Journal of Computational Social Science 2 (2): 293–313.\n\n\nTaylor, Marshall, and Dustin Stoltz. 2020. “Concept Class Analysis: A Method for Identifying Cultural Schemas in Texts.” Sociological Science 7: 544–69.\n\n\nVasiliev, Yuli. 2020. Natural Language Processing with Python and SpaCy: A Practical Introduction. No Starch Press.\n\n\nWhite, Harrison. 1992. Identity and Control: How Social Formations Emerge. Princeton university press.\n\n\nWittgenstein, Ludwig. 1953. “Philosophical Investigations: The English Text of the Third Edition.”",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Language models and word embeddings</span>"
    ]
  },
  {
    "objectID": "language-models-and-embeddings.html#footnotes",
    "href": "language-models-and-embeddings.html#footnotes",
    "title": "33  Language models and word embeddings",
    "section": "",
    "text": "At the time of writing, SpaCy provides these models for English, German, Spanish, Portuguese, French, Italian, Dutch, Norwegian, and Lithuanian. It is also capable of processing multilingual documents and tokenization for over 50 languages to allow model training. In the rest of this chapter and those that follow, we will use English-language models.↩︎\nContext and environment can be used interchangeably in this case. (See what I did there?) For the sake of consistency, I will use the word context.↩︎",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Language models and word embeddings</span>"
    ]
  },
  {
    "objectID": "transformer-revolution.html",
    "href": "transformer-revolution.html",
    "title": "33  The transformer revolution",
    "section": "",
    "text": "Full rewrite coming in fall 2024.",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>The transformer revolution</span>"
    ]
  },
  {
    "objectID": "modeling-text-transformer-topic-models.html",
    "href": "modeling-text-transformer-topic-models.html",
    "title": "34  Modeling text: transformer topic models",
    "section": "",
    "text": "34.1 Learning Objectives\nBy the end of this tutorial, you should be able to:",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Modeling text: transformer topic models</span>"
    ]
  },
  {
    "objectID": "modeling-text-transformer-topic-models.html#learning-objectives",
    "href": "modeling-text-transformer-topic-models.html#learning-objectives",
    "title": "34  Modeling text: transformer topic models",
    "section": "",
    "text": "Understand the Evolution of Topic Modeling\n\nDescribe the progression from simple count-based methods to advanced transformer-based models.\nExplain how each wave of topic modeling improves upon the previous techniques.\n\nPreprocess Text Data\n\nImplement text preprocessing steps including tokenization, stopword removal, lemmatization, and bigram detection.\nConstruct document-term matrices and understand their role in topic modeling.\n\nDevelop Topic Models\n\nPerform basic count-based analysis and difference of proportions analysis to explore manifest thematic differences across document groups.\nConduct a Latent Semantic Analysis (LSA) using Truncated Singular Value Decomposition (SVD) to discover latent topics in text data.\nDevelop probabilistic topic models, specifically Latent Dirichlet Allocation (LDA) models, and interpret the results.\nFit and interpret transformer-based topic models using BERTopic.\n\nVisualize and Interpret Topic Models\n\nCreate meaningful visualizations to compare and interpret topics across different documents.\nEvaluate the strengths and weaknesses of different topic modeling approaches.",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Modeling text: transformer topic models</span>"
    ]
  },
  {
    "objectID": "modeling-text-transformer-topic-models.html#data-processing",
    "href": "modeling-text-transformer-topic-models.html#data-processing",
    "title": "34  Modeling text: transformer topic models",
    "section": "36.1 Data Processing",
    "text": "36.1 Data Processing\nLet’s confirm that we have the correct years.\nbh1620['year'].min(), bh1620['year'].max()\nLet’s do some initial processing by selecting the subset of columns we are interested in and dropping rows that are missing data from crucial columns.\nbh1620 = bh1620[['speech', 'speakername', 'party', 'constituency', 'year']]\nbh1620.dropna(subset=['party', 'speakername', 'speech'], inplace=True)\nbh1620.info()\nDropping missing data leaves us with {python} len(bh1620) speeches.",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Modeling text: transformer topic models</span>"
    ]
  },
  {
    "objectID": "modeling-text-transformer-topic-models.html#party-breakdown",
    "href": "modeling-text-transformer-topic-models.html#party-breakdown",
    "title": "34  Modeling text: transformer topic models",
    "section": "36.2 Party Breakdown",
    "text": "36.2 Party Breakdown\nLet’s take a look at the breakdown of speeches by party.\nbh1620['party'].value_counts().reset_index()\nWe’ll focus only on speeches made by MPs from parties that collectively gave more than 400 speeches within our four-year window.\nparties_keep = [\n    'Conservative',\n    'Labour',\n    'Scottish National Party',\n    'Labour (Co-op)',\n    'Liberal Democrat',\n    'Democratic Unionist Party',\n    'Plaid Cymru',\n    'Green Party'\n]\n\nparty_subset = bh1620[bh1620['party'].isin(parties_keep)].copy()\nparty_subset.reset_index(drop=True, inplace=True)\n\ntotal_speech_counts = party_subset['party'].value_counts()\ntotal_speech_counts\n\n36.2.1 Stratified Random Sample of Political Speeches\nGiven the size of the dataset, you may want to work with a smaller sample. We can draw a stratified random sample where the strata are political parties so that that each party is proportionally represented in our sample.\nsample_size_fraction = 0.1  # let's just take 10% for now\n\nsampled_speeches = party_subset.groupby('party').sample(\n    replace=False,\n    frac=sample_size_fraction,\n    random_state=23\n)\n\nlen(sampled_speeches)\nOur stratified random sample contains {python} len(sampled_speeches) speeches. Let’s examine the party breakdown in our sample.\nsampled_speech_counts = sampled_speeches['party'].value_counts()\n\nsample_sizes = pd.DataFrame(\n    zip(total_speech_counts, sampled_speech_counts),\n    columns=['Total', 'Sample'],\n    index=parties_keep\n)\n\nsample_sizes\n\n\n36.2.2 Speech Length by Party\nLet’s compare the length of speeches across political parties by computing the number of tokens in each speech.\nsampled_speeches['speech_len'] = sampled_speeches['speech'].apply(\n  lambda x: len(x.split(\" \"))\n)\n\nparties = sampled_speeches.groupby('party')\nparties['speech_len'].median()\nWe can visualize party differences by plotting the kernel density estimate for speech length within each party.\ndef party_subplot(subgroup, title, position):\n    sns.kdeplot(ax=position, data=subgroup, x='speech_len',\n                log_scale=True, fill=False, alpha=1, linewidth=3, color='C0')\n    position.set(xlabel='Number of tokens (log scale)', title=title)\n\nfig, ax = plt.subplots(2, 4, sharex=True, sharey=True, figsize=(20, 6))\nparty_subplot(parties.get_group('Conservative'), 'Conservative', ax[0, 0])\nparty_subplot(parties.get_group('Labour'), 'Labour', ax[0, 1])\nparty_subplot(parties.get_group('Scottish National Party'), 'Scottish National Party', ax[0, 2])\nparty_subplot(parties.get_group('Labour (Co-op)'), 'Labour (Co-op)', ax[0, 3])\nparty_subplot(parties.get_group('Liberal Democrat'), 'Liberal Democrat', ax[1, 0])\nparty_subplot(parties.get_group('Democratic Unionist Party'), 'Democratic Unionist Party', ax[1, 1])\nparty_subplot(parties.get_group('Plaid Cymru'), 'Plaid Cymru', ax[1, 2])\nparty_subplot(parties.get_group('Green Party'), 'Green Party', ax[1, 3])\n\nplt.tight_layout()\nplt.savefig('output/speech_length_by_party.png', dpi=300)",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Modeling text: transformer topic models</span>"
    ]
  },
  {
    "objectID": "modeling-text-transformer-topic-models.html#text-processing",
    "href": "modeling-text-transformer-topic-models.html#text-processing",
    "title": "34  Modeling text: transformer topic models",
    "section": "36.3 Text Processing",
    "text": "36.3 Text Processing\nText processing is a crucial step before applying the kinds of text analysis techniques we will apply here.2 Common steps include removing stopwords, converting to lowercase, lemmatization, and detecting bigrams. We will use the preprocess() function from the course package to perform these operations.\nNote that the code block below will take some time to run, primarily because of the bigram detection. You’ll see several progress bars appear (one at a time) to give you a sense of how long this code needs to run on your machine.\ndocs = sampled_speeches['speech']\n\nbigram_model, processed_texts = preprocess(\n    docs,\n    nlp=nlp,\n    bigrams=True,\n    detokenize=True,\n    n_process=1\n)\nWe’ve now detected bigrams, filtered out stopwords, selected relevant tokens (nouns, proper nouns, and adjectives) using part-of-speech tagging, and lemmatized them. Let’s add the preprocessed speech data to our dataframe.\n\n\n\n\n\n\nNote\n\n\n\nHeads up, the code below will take a while to run! You’ll see three progress bars appear (one at a time) to give you a sense of how it will take to run.\n\n\nsampled_speeches['processed_text'] = processed_texts\nWe now have two Series with text data: the original full speech text and the processed version. I know the processed text doesn’t look better from a human perspective, but it will help out these initial models quite a bit.",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Modeling text: transformer topic models</span>"
    ]
  },
  {
    "objectID": "modeling-text-transformer-topic-models.html#creating-a-document-term-matrix",
    "href": "modeling-text-transformer-topic-models.html#creating-a-document-term-matrix",
    "title": "34  Modeling text: transformer topic models",
    "section": "36.4 Creating a Document-Term Matrix",
    "text": "36.4 Creating a Document-Term Matrix\nCreating a document-term matrix (DTM) is the first step in many text analysis workflows. It allows us to represent our text data in a structured format where each document is represented as a vector of word counts or TF-IDF scores.\ncount_vectorizer = CountVectorizer(\n    max_df=0.1,\n    min_df=3,\n    strip_accents='ascii',\n)\n\ncount_matrix = count_vectorizer.fit_transform(sampled_speeches['processed_text'])\nvocabulary = count_vectorizer.get_feature_names_out()\n\ncount_matrix.shape\nNow that we’ve processed our data, let’s start digging into topic modeling, starting with the early deterministic methods and progressing to the latest transformer-based models!",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Modeling text: transformer topic models</span>"
    ]
  },
  {
    "objectID": "modeling-text-transformer-topic-models.html#counting-and-comparing-manifest-content",
    "href": "modeling-text-transformer-topic-models.html#counting-and-comparing-manifest-content",
    "title": "34  Modeling text: transformer topic models",
    "section": "37.1 Counting and Comparing Manifest Content",
    "text": "37.1 Counting and Comparing Manifest Content\nWe can start by exploring simple count-based methods and difference of proportions analysis. This provides a foundation for understanding more complex topic models.\ncount_data = pd.DataFrame.sparse.from_spmatrix(count_matrix)\ncount_data.columns = vocabulary\n\ncount_data.index = sampled_speeches['party']\ncount_data.shape\nLet’s look at a random sample of our data (yes, another).\nparty_counts = sparse_groupby(sampled_speeches['party'], count_matrix, vocabulary)\nresults = party_counts.div(party_counts.sum(axis=1), axis=0)\nresults_t = results.T\nresults_t.sample(20, random_state=10061986)\nWith this dataframe, we can compare the proportions of specific tokens across each party.\nsearch_term = 'scotland'\nresults_t.loc[search_term].sort_values(ascending=False)\nWe can also compute the difference of proportions between any pair of document groups, revealing which tokens are more associated with one group over another.\ndiff_con_snp = results_t['Conservative'] - results_t['Scottish National Party']\ndiff_con_snp.sort_values(ascending=False, inplace=True)\n\ncon_not_snp = diff_con_snp.head(20)  # Conservatives but not SNP\nlab_not_snp = diff_con_snp.tail(20)  # SNP but not Conservatives\n\ndop = pd.concat([con_not_snp, lab_not_snp])\nLet’s visualize these differences.\nfig, ax = plt.subplots(figsize=(12, 12))\nsns.swarmplot(x=dop, y=dop.index, color='black', size=8)\nax.axvline(0)  # add a vertical line at 0\nplt.grid()  # add a grid to the plot to make it easier to interpret\nax.set(xlabel=r'($\\longleftarrow$ Scottish National Party)        (Conservative Party $\\longrightarrow$)', ylabel='', title='Difference of Proportions')\nplt.tight_layout()\nplt.show()\nPlots like this one highlight differences in how groups of people talk, or groups of documents are written, based on relative differences in the words they use. In this specific case, our plot compares the difference in proportions of word usage between the SNP and the Conservatives.\nThe words we are comparing are on the y-axis. The x-axis shows the difference in proportions between the two parties, with a vertical line indicating 0. Negative values indicate words that are used more frequently by the SNP, whereas positive values indicate words more frequently used by the Conservatives.\nEach dot represents a word, and its position on the x-axis indicates the extent to which that word is used by one party relative to the other. For example, words like “scotland”, “scottish”, and “brexit” are on the far left, indicating that these terms are more commonly used by the SNP than by the Conservative Party.3 On the other hand, words like “eu”, “policy”, and “impact” are on the right side, suggesting these are more commonly used by the Conservative Party. The further a dot is from the red line (0), the larger the difference in usage between the two parties. Words closer to the red line (0) have less different usage and could be used similarly by both parties.\nTry running this analysis this with other search terms!",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Modeling text: transformer topic models</span>"
    ]
  },
  {
    "objectID": "modeling-text-transformer-topic-models.html#topic-assignments-and-probabilities",
    "href": "modeling-text-transformer-topic-models.html#topic-assignments-and-probabilities",
    "title": "34  Modeling text: transformer topic models",
    "section": "40.1 Topic Assignments and Probabilities",
    "text": "40.1 Topic Assignments and Probabilities\nWe can explore the topic assignments and their associated probabilities for each document.\nsampled_speeches['topic'] = topics\nsampled_speeches['p(topic)'] = probs\nsampled_speeches[['speech', 'topic', 'p(topic)']]\nsampled_speeches",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Modeling text: transformer topic models</span>"
    ]
  },
  {
    "objectID": "modeling-text-transformer-topic-models.html#topic-lookup",
    "href": "modeling-text-transformer-topic-models.html#topic-lookup",
    "title": "34  Modeling text: transformer topic models",
    "section": "40.2 Topic Lookup",
    "text": "40.2 Topic Lookup\nWe can quickly look up the top words associated with any given topic.\ntopic_model.get_topic(0)",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Modeling text: transformer topic models</span>"
    ]
  },
  {
    "objectID": "modeling-text-transformer-topic-models.html#topic-information",
    "href": "modeling-text-transformer-topic-models.html#topic-information",
    "title": "34  Modeling text: transformer topic models",
    "section": "40.3 Topic Information",
    "text": "40.3 Topic Information\nLet’s retrieve and explore information about the topics identified by the model.\ntopic_info = topic_model.get_topic_info()\ntopic_info\n\nprint(len(topic_info))\n\nfor i in range(len(topic_info)):\n    print(f'Topic {i}: {topic_model.get_topic(i)}')\ntopic_info",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Modeling text: transformer topic models</span>"
    ]
  },
  {
    "objectID": "modeling-text-transformer-topic-models.html#document-info",
    "href": "modeling-text-transformer-topic-models.html#document-info",
    "title": "34  Modeling text: transformer topic models",
    "section": "40.4 Document Info",
    "text": "40.4 Document Info\nFinally, we can get information about how documents are classified into topics.\ntopic_model.get_document_info(docs)\nWe can also visualize the distribution of topics per document. BERTopic has a function to do this, visualize_distribution(). To see the topic distribution for the first document in our corpus,\ntopic_distr, _ = topic_model.approximate_distribution(sampled_speeches[\"speech\"])\ntopic_model.visualize_distribution(topic_distr[0])\nWhile this is very handy, I’m a bit fastidious when it comes to plotting and… I don’t love it. I developed an alternative function, plot_topic_distribution() that you might prefer.\nplot_topic_distribution(topic_distr, topic_model, 0, filename='output/topic_distribution_document_0.png')",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Modeling text: transformer topic models</span>"
    ]
  },
  {
    "objectID": "modeling-text-transformer-topic-models.html#footnotes",
    "href": "modeling-text-transformer-topic-models.html#footnotes",
    "title": "34  Modeling text: transformer topic models",
    "section": "",
    "text": "This historical framing accurately represents the development of topic models, but the history of applications is a little messier. The trajectory is more or less the same, but earlier models are still useful and continue to be used in many fields. The heuristic framing of first, second, and third wave models is my own.↩︎\nThis is not the case for the transformer-based approaches we will consider later.↩︎\nObviously, the positive and negative signs are arbitrary.↩︎\nWhy TF-IDF and normalization for LSA? LSA works by decomposing the term-document matrix (often a TF-IDF matrix) into a set of orthogonal factors, each of which represents a latent semantic dimension. By normalizing the TF-IDF matrix (e.g., applying L2 normalization to each document vector, as we have here), we make sure that each document contributes equally to the latent space, which is useful when there is variation in document lengths (which we saw in our data processing step). This improves the interpretability of the resulting semantic dimensions.↩︎",
    "crumbs": [
      "**DEEP LEARNING**",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Modeling text: transformer topic models</span>"
    ]
  },
  {
    "objectID": "ethical-css.html",
    "href": "ethical-css.html",
    "title": "35  Ethical CSS",
    "section": "",
    "text": "35.1 LEARNING OBJECTIVES",
    "crumbs": [
      "**PROFESSIONAL RESPONSIBILITIES**",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Ethical CSS</span>"
    ]
  },
  {
    "objectID": "ethical-css.html#learning-objectives",
    "href": "ethical-css.html#learning-objectives",
    "title": "35  Ethical CSS",
    "section": "",
    "text": "Explain the challenges with informed consent in computational social science\nDescribe the tensions between the competing ethical principles of privacy and transparency\nExplain how algorithmic biases and biased training datasets can amplify and exacerbate inequalities\nExplicitly articulate the normative and political values that underlie your research\nIdentify the types of computational social science that you will and will not do",
    "crumbs": [
      "**PROFESSIONAL RESPONSIBILITIES**",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Ethical CSS</span>"
    ]
  },
  {
    "objectID": "ethical-css.html#learning-materials",
    "href": "ethical-css.html#learning-materials",
    "title": "35  Ethical CSS",
    "section": "35.2 LEARNING MATERIALS",
    "text": "35.2 LEARNING MATERIALS\nYou can find the online learning materials for this chapter in doing_computational_social_science/Chapter_19. cd into the directory and launch your Jupyter Server.",
    "crumbs": [
      "**PROFESSIONAL RESPONSIBILITIES**",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Ethical CSS</span>"
    ]
  },
  {
    "objectID": "ethical-css.html#introduction",
    "href": "ethical-css.html#introduction",
    "title": "35  Ethical CSS",
    "section": "35.3 INTRODUCTION",
    "text": "35.3 INTRODUCTION\nThe chapters following this one will introduce a variety of machine learning models. Before we get there, we’re going to consider some of the ethical and political challenges that arise in the context of computational social science. One of the many themes in this chapter is that we are working in unsettled times when it comes to research ethics in computational social science and data science. Many of the methods and models in this book provide access to power that we are not accustomed to dealing with, and for which there are few guidelines and standards. The recent advances in computational methods have far outstripped what we, as social scientists, have been historically capable of, and our ethical standards and practices have not yet caught up. As professional researchers, we need to make ethical decisions in our work. That means doing more than making sure we don’t violate currently established ethical principles. Throughout this chapter, I will emphasize that current ethical standards are not adequate for much of what is introduced in this book (e.g., machine learning).\nRather than being reactive (e.g., changing practices and establishing standards after people have already been harmed), we should be proactive (e.g., anticipating and mitigating potential harms). We must adopt practices that help ensure we are doing our work in ways that enable us to be transparent and accountable to the right people at the right times. It means asking ourselves hard questions about the types of things we will and won’t do and making a serious effort to anticipate the potential unintended negative consequences of the work we do. There is no avoiding constant reflexive practice. Nor can we avoid the politics of research. We must confront difficult political issues head on and make our normative values explicit and visible in our work. We do this not only to protect ourselves, our participants, and anyone who might be affected by our work once it leaves our hands, but because it also produces better science: science that is transparent, accountable, and reproducible.\nWe’ll start by considering the context of social network analysis, which we covered in the preceding chapters, followed by matching issues we have to negotiate as we work with machine learning in following chapters.",
    "crumbs": [
      "**PROFESSIONAL RESPONSIBILITIES**",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Ethical CSS</span>"
    ]
  },
  {
    "objectID": "ethical-css.html#research-ethics-and-social-network-analysis",
    "href": "ethical-css.html#research-ethics-and-social-network-analysis",
    "title": "35  Ethical CSS",
    "section": "35.4 RESEARCH ETHICS AND SOCIAL NETWORK ANALYSIS",
    "text": "35.4 RESEARCH ETHICS AND SOCIAL NETWORK ANALYSIS\nAs researchers, we are not detached from the social and political world we study, and we need to remember that our position as researchers puts us in unique positions of power. In network analysis, knowledge of a network imparts power over it in concrete and tangible ways. Most of us have limited understanding of the structure of the networks we live our daily lives in, and whatever understanding we do have diminishes rapidly as we move beyond our immediate connections. As researchers, we have privileged access to intimate details of the lives of real people and the unique relational contexts that shape their lives, for better and for worse. This information is often sensitive and has the potential to cause harm if not handled properly.\nAt some point in your research career you will gain information that is very important, valuable, or compromising to the people participating in your study, and in network analysis that can happen without any one individual realizing it. Part of the value of studying networks comes from the ways that micro-level interactions (e.g. friendships, advice, communication) combine to produce highly consequential network structures that are not immediately obvious. When we collect relational data, we gain access to information about an emergent network structure that, though lacking in details, can reveal a picture that’s very difficult to see from the inside.\nThe decisions we make when we collect relational data, construct and analyze networks, and present our findings all have important ethical dimensions. For example, in a commentary from a 2021 special issue of Social Networks on ethical challenges in network analysis, Bernie Hogan (2021) recounts several experiences where presentations of simple network visualizations caused unintentional harm. In one case, a student gave a presentation that included a visualization of a network of their classmates, revealing that everyone was connected in one large group except a single isolated student. Similarly, after presenting visualizations of an academic network, Hogan describes being contacted by disconcerted academics who were located on the periphery of the network (implying they were marginal), but who felt this unfairly painted them in a poor light as they were primarily active in other networks that didn’t fall within the presented network’s boundaries. These were not necessarily “marginal” academics, but the definition of network boundaries portrayed them as marginal. We don’t just reveal networks as they really exist, we construct them, and in ways that feed back into the world.\n\nFurther Reading\nTo learn more about some salient ethical issues in comtemporary network analysis, I recommend readind the 2021 special issue of Social Networks edited by Tubaro et al. (2020).\n\nCases such as these are a reminder that unavoidable and seemingly benign measurement decisions play a significant role in determining who is portrayed as central or marginal within the boundaries of a network as we define it; we have a certain amount of control over influential representations of the world that cast some people as more central (and therefore more powerful, influential, and high-status) than others. This is what I meant when I said we construct networks, we don’t just reveal them. Since it is possible to cause harm with our constructions, we should consider the important ethical dimensions of the decisions involved, such as which ties we measure among which people. And since many harms can come from portraying specific people as central or marginal, we should also consider the ethical implications of how we share information about networks, whether we are sharing data or presenting results in some other form. All of this is especially problematic for people who are already marginalized. Cases like these are likely more common than we realize.\nThere are concrete things we can do to help mitigate the negative effects of situations such as those described above, but many problems persist. For example, properly anonymizing network data can go a pretty long way. However, this is not a just a matter of “give everyone numeric IDs” because people are often able to make pretty good inferences about who’s who in a network they are involved in even if they don’t have all the information needed to construct the network in the first place. If someone showed you a visualization of a friendship network that you’re part of, I’d wager that with some time and thought you could make very good guesses as to who was where in the network. The ability to use extremely surface-level data to know, with relative certainty, information about individuals is powerful.\nSo how can we present data while protecting anonymity? There are a variety of options. Consider the Faux Magnolia High network data available in the statnet R library (Handcock et al. 2003), for example. It describes a fake high school with 1461 students with attribute data for grade, sex, and race. While it was based on real data, and those variables could potentially have been used to identify individual students, an exponential random graph model was used to infer the broad patterns between these variables and the network structure. Those patterns were then used to create a randomly generated network that became the dataset provided to the public. (Unfortunately, I couldn’t make space for exponential random graph models (ERGMs) in the networks chapters, but Lusher, Koskinen, and Robins (2013) provide a good starting point if you are interested in delving further into ERGMs.) Unfortunately, this won’t work for all network data, nor for all data in general; the Faux Magnolia High data is primarily used for learning and testing network models. It poses little value for further network research because it is so far divorced from the original data. It makes no claims to represent any relationship between the original data and network structure beyond that captured in the model used to generate it.\nThis raises difficult questions about the tension between privacy and transparency. We’ll turn to these issues directly in a moment, but for now, I want to emphasize that network data collection can sometimes result in information about people who have not provided consent, or specifically informed consent. For example, if you collect data on an organization’s management team and ask employees to name the people they give advice to and receive advice from, you will likely end up with information about someone who simply wasn’t in the office that day, and all the little bits of relational information from many different people add up to paint a picture of that person’s position in the advice-sharing network.\nAs with other ethical challenges we will discuss below, do not assume that you are in the clear because your research passes an ethics review. As I’ve mentioned, current ethical standards are lagging behind advancing methods, and they are not well suited to judging how cutting-edge work might be used by others. One of the driving forces for the recent explosion of network analysis derives from the generalizability of methods, measures, and models. At their heart, networks are mathematical constructs. Anything that can be reasonably conceptualized as a collection of things connected to other things is within its purview. A measure that can be used to describe “popularity” or “influence” in sociology can be used for “risk of exposure” in an epidemiological model or “importance” in a criminal or terrorist network. Knowledge about networks in general is powerful because network analysis itself is so generalizable. You shouldn’t assume that your work will only be used in the way you intended it to be used.\nWhile I have focused on how researchers need to consider the ethics of working with networks, we aren’t the only ones working on them. Google built one of the most valuable tech companies in the world on the foundation of PageRank (a centrality-like algorithm drawing on network analysis to estimate the relative “quality” of a website based on the links leading to and from it). Similarly police forces and intelligence agencies profit from information about the structure and dynamics of our social networks, and it doesn’t especially matter if they have any information about the explicit content of those ties. You can make powerful inferences using only your knowledge of the structure of the network as Kieran Healy (2013) cleverly showed in a blog post following revelations in 2012 about the extent of NSA metadata-based surveillance (e.g., Upsahl 2013). These non-academic groups do not have the same standards we hold ourselves to, but they have access to everything we publish, more data, and far more money and computing power. When we develop new network tools or data, we need to consider what others with more resources might be able to do with it.\nIn the following section, I move from network data to discussing data more generally, and I will focus more closely on issues of informed consent and balancing the principles of privacy and transparency.",
    "crumbs": [
      "**PROFESSIONAL RESPONSIBILITIES**",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Ethical CSS</span>"
    ]
  },
  {
    "objectID": "ethical-css.html#informed-consent-privacy-and-transparency",
    "href": "ethical-css.html#informed-consent-privacy-and-transparency",
    "title": "35  Ethical CSS",
    "section": "35.5 INFORMED CONSENT, PRIVACY, AND TRANSPARENCY",
    "text": "35.5 INFORMED CONSENT, PRIVACY, AND TRANSPARENCY\nDigital data collection (including the collection methods we discussed in Chapters 5 and 6) poses greater ethical challenges than more traditional data collection methods, and issues with voluntary informed consent are especially salient. This is largely because we can observe (and interfere) from a great distance, and without the knowledge of the people and groups we are observing (and potentially interfering with, for example in online experiments). The massive availability of data online also poses new challenges for privacy, as information that is anonymous in one dataset can quickly become uniquely identifiable when combined with other data. This necessitates a considerable amount of careful ethical thinking and decision-making for individual researchers and teams (Salganik 2019; Beninger 2017), as there are no pre-established rules or ethical checklists to rely on in these and many other situations we might find ourselves in when collecting digital data. In research with social media data, where issues around informed consent are ubiquitous (Sloan and Quan-Haase 2017), some have argued for increased ethical standards (Goel 2014) while others have argued that this is unnecessary for minimal risk research on data in the public domain (e.g., Grimmelmann 2015).\nOne of the reasons why these debates rage on is because the boundaries between public and private are much more porous with data collected from social media platforms and the open web (see Sloan and Quan-Haase 2017). And while people may realize that much of what they do and say online can be read by anyone, they are not necessarily thinking about the fact their words and actions are being recorded, stored, and used for something other than their own intended purpose. And even if they are thinking about that, people may not anticipate how the data collected about them from social media platforms and the open web may be linked up with other data, just like they may not anticipate the richness of the network knowledge that can be gleaned from lots of seemingly trivial details, like the name of the person you call when you need to vent about your insufferable coworker.\nFor example, from 2006 to 2009, Lewis et al. (2008) collected a huge volume of Facebook data from a cohort of students over four years. With this, they created network datasets with information about the students’ home states, cultural tastes such as preferred books and musical genres, political affiliations, the structure of their friendship networks, photos, and so on. All of the Facebook data they collected was from public profiles, but it was not collected with informed consent. The researchers linked the Facebook data with data from the college (e.g., on academic major). That’s quite the collection of intimate portraits of some 1,700 unaware people.\nAs part of the terms of funds they received from the National Science Foundation, Lewis et al. (2008) made an anonymized version of their data publicly available via Dataverse; they did not identify the institution by name, used identification numbers instead of names, and they delayed releasing personal information like interests in movies, books, and so on. Within days of the first wave of release, Zimmer (2010) and others were able to identify Harvard as the source of the data and show that enough unique information was available to identify individual students.\nThere is nothing inherently wrong with linking datasets. Researchers do it all the time, and for good reason. But where there is a lack of consent, the data is extensive and sensitive, and there is a lack of agreed-upon ethical standards, the risks should be readily apparent. While people know their actions are public, they can’t reasonably be expected to anticipate all the things that researchers (or government or industry) will do with that data, what they will link it to, and what the resulting picture of them will look like. So, while they may have consented to publicly releasing certain data on certain platforms, they have not consented to the various ways that we might recombine those data in ways they never considered, and which they may not fully realize is even possible. Common privacy protection methods are little defense against dedicated research methods, and we may easily de-anonymize individuals without realizing it in our pursuit of more robust data.\nAs with network data, anonymized names are not enough to protect people. In the 1990s, a government agency called the Group Insurance Commission collected state employees’ health records for the purposes of purchasing health insurance, and released an anonymized dataset to researchers (Salganik 2019). This data included things like medical records, but also information like zip code, birth date, and sex. By combining this data with voting records (that also had zip code, birthdate, and sex) purchased for $20, Latanya Sweeney, a grad student, was able to attach the name of the governor of Massachusetts to specific medical records, and then mailed him a copy. By linking records, data that is internally anonymous can be used to identify personal information that no one source intended to allow. Whenever you release anonymized data, you need to think very carefully about not just your own data, but what other kinds of data might exist that could be used in harmful ways.\nMedical records are an obvious example of informational risk: the potential for harm from the disclosure of information, but this is far from the only example. Latanya Sweeney (2002) has shown, for example, that 87% of the US population could be reasonably identified with just their 5-digit ZIP, gender, and date of birth. The risk posed by record linkage means that even seemingly innocuous data can be used to unlock much riskier data elsewhere. Even attempts to perturb the data, by switching some values around, may not be enough if enough unchanged data is still available. Given the power of machine learning to make inferences about unseen data, which we will cover later in this book, I will echo Salganik (2019) and stress that you should start with the assumption that any data you make available is potentially identifiable, and potentially serious.\nAs researchers, we tend to hyper-focus on the aspects of our data that pertain to our specific research projects, as if we were only responsible for what we ourselves do with the data we collect. After all, we collected the data for a particular purpose, and that purpose can define how we perceive its uses. We should also consider what other questions might be answerable with our data, both as a matter of good research and as a matter of protecting the data we have direct responsibility over, and the indirect data that it might unlock.\nOne response to this type of problem is to simply share nothing; lock down all the data. But this collides with another very important ethical principle and scientific norm: transparency, which is a necessary but insufficient condition for accountability. We don’t want black box science that nobody can question, challenge, or critique. We will later discuss how datasets can contain racist and sexist data that are learned by models, put into production, and further propagated, for example. Privacy and transparency are in direct contradiction with one another. So where on the scale should the needle to be? There is no perfect solution for completely transparent research and completely protected privacy, so we consider the importance of both according to the situation. There is no avoiding difficult decision-making and constant ethical reflection and reflexive practice.\n“According to the situation” is key here. As Diakopoulos (2020) sums up the key idea about the ethical importance of transparency:\n\n“Transparency can be defined as ‘the availability of information about an actor allowing other actors to monitor the workings of performance of this actor.’ In other words, transparency is about information, related both to outcomes and procedures used by an actor, and it is relational, involving the exchange of information between actors. Transparency therefore provides the informational substrate for ethical deliberation of a system’s behavior by external actors. It is hard to imagine a robust debate around an algorithmic system without providing the relevant stakeholders the information detailing what that system does and how it operates. Yet it’s important to emphasize that transparency is not sufficient to ensure algorithmic accountability.” (Page 198)\n\nBut as Diakopoulos (2020) points out, we can’t understand algorithmic transparency in a binary – transparent or not – as there are many different types of transparency, including what types of information and how much is provided, to whom, and for what purposes. The nature of disclosure can also matter, as self-disclosures are self-interested and present things in a certain light. Not all transparency is good transparency, and not all types of transparency lend themselves to accountability. He identifies a number of things we need to consider when trying to strike this delicate balance between privacy and transparency:\n\nHuman Involvement: Some machine learning algorithms require human input. Supervised machine learning may require data that have been annotated by humans, while others require humans to provide feedback on results, or during operation. Wherever humans have non-trivial input into the process, their decisions should be made open and available.\nThe Data: Machine learning often involves “training” an algorithm on some set of data. This data, and how it was produced, can have significant impacts how the algorithm functions. If photo data has been used to train a facial recognition algorithm, biases in the original data, like a disproportionate number of white men on some social media sites, can taint any subsequent work that doesn’t match the training data. If we don’t know the training data, we can’t examine it for biases.\nThe Model and Code: While algorithms are executed by computers, humans wrote them. They were written to solve specific problems, sometimes with specific data and goals in mind. Decisions were made about what variables to optimize, and much more. Researchers decide the values of parameters, or decide not to decide and use default values. These decisions should be open and available for review.\n\nIn an ideal world, no important decisions about our data or models would need to be hidden to protect privacy or confidentiality. In practice, that is often not the case, and we must navigate as best we can our obligations to safeguard our data while making our work as open and transparent as possible. Both are essential; we cannot completely abandon one for the other while still meeting a high standard for ethical research. The answer is not to make all information available; there are too many factors to balance, risks to assess, privacy to protect, and so on. Nor is the answer full transparency, which is not good for anyone. It’s contextually-appropriate transparency, where decisions are made close to the specific cases with the relevant stakeholders. These are the kinds of transparency that are most important to ensuring algorithmic accountability.\nIn addition to contextual ethical considerations, we can look for ways to build fairness into our practices more deeply (Nielse 2021), and adopt develop new privacy-oriented practices such as Sweeney’s (2002) proposed \\(k\\)-anonymity. This notation should be familiar based on our discussion of \\(k\\)-cliques in the networks chapters. The idea behind \\(k\\)-anonymity is that no one individual in a dataset can be distinguished from at least \\(k\\) other individuals in the same data using a combination of unique “quasi-identifiers” (e.g. 5-digit ZIP, gender, and date of birth). The goal here, like in Faux Magnolia High, is to protect privacy by hiding needles in identical needle stacks, but we manage how transparent/anonymous our data is with the value of \\(k\\). With especially sensitive data, we may choose higher values, while lower values may be more appropriate for low-risk stakes. This may mean generalizing some data to make it less specific: if only one person is from Glasgow in your dataset, that might mean replacing their location data with Scotland, or you could remove their location data, or remove them from the data altogether. In every case, we make our data less transparent, but we try to preserve the contextually appropriate transparency of the data while also protecting individual privacy and anonymity.\nAs computational scientists, we must wield our power responsibly. That means doing our work in ways that are transparent and facilitate accountability while also ensuring privacy and respecting the people represented in our datasets. It also means doing our work in ways that are auditable and which which enable us to be accountable for the work we do and the impacts it has. That may manifest in any number of ways, the most obvious of which are to use tools that record every decision, every step that takes and input and produces an output, are recorded and can be understood. There are systems that enable this, and using them is the cost of entry.\nHowever, being aware of the political power we wield and adopting tools and workflows that attempt to make our work as transparent and accountable as possible are, as I mentioned earlier, necessary but insufficient. To wield power responsibly, it is necessary to go beyond abstract ethical principles to think more deeply about how and why we do science, and what kinds of science we want to contribute to and advance, and which we want no part of. In the next section, we’ll discuss bias and algorithmic decision-making as examples of why it is so important to ask ourselves these kinds of questions.\n\nIn addition to Diakopoulos (2020), I suggest looking into other articles on transparency and accountability by Diakopoulos and others, such as Diakopoulos (2017) and Ananny and Crawford (2018).",
    "crumbs": [
      "**PROFESSIONAL RESPONSIBILITIES**",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Ethical CSS</span>"
    ]
  },
  {
    "objectID": "ethical-css.html#bias-and-algorithmic-decision-making",
    "href": "ethical-css.html#bias-and-algorithmic-decision-making",
    "title": "35  Ethical CSS",
    "section": "35.6 BIAS AND ALGORITHMIC DECISION-MAKING",
    "text": "35.6 BIAS AND ALGORITHMIC DECISION-MAKING\nIn a widely-viewed talk “How To Stop Artificial Intelligence From Marginalizing Communities,” Timnit Gebru (2018) raises two very important questions about the many machine learning algorithms that are invisibly woven into virtually every aspect of our lives. For any given algorithm:\n\nShould it exist at all?\nIf it is to exist, is it robust enough to use in high-stakes contexts (e.g., in the criminal justice system, healthcare, education, etc.)?\n\nGebru’s questions take aim directly at high-stakes algorithmic decision-making (ADM); rightfully so, as ADM is one of the most insidious mechanisms through which systemic inequality is perpetuated. But more importantly, these questions are especially relevant to us as researchers; you will likely have opportunities to contribute to technologies such as these, or others that are similar in one way or another. Given that you could easily find yourself in a situation where that’s a possible outcome, it’s important for us to ask ourselves these questions early and often so we can better understand what kinds of technologies we are uncomfortable contributing to, whether because we think they are inherently dangerous or simply too prone to abuse to be worth the risk.\nIf you don’t spend a lot of time thinking about, critiquing, or developing algorithms, it might seem like incorporating algorithms into decision-making is reasonable and perhaps even more impartial than the alternative. After all, algorithms are just a series of steps consistently carried out by computers, following mathematical rules and precision. And a computer is incapable of thought, let alone bigotry.\nThis is a complete fantasy; algorithms don’t spring into existence fully formed out of nowhere. They’re written by humans to enforce human rules, and I doubt anyone would say that the rules we make are always fair. When our biases are encoded into algorithms, those biases are perpetuated and amplified, often with very serious consequences. These biases disproportionately affect people who are already marginalized. There is a rapidly growing literature (e.g., West, Whittaker, and Crawford 2019; Gebru 2020; Angwin et al. 2016; O’Neil 2016; Eubanks 2018; Benjamin 2019; Nelson 2021; Noble 2018; Vries et al. 2019; Buolamwini and Gebru 2018; Hamidi, Scheuerman, and Branham 2018) and there is no excuse for ignorance.\nWho can we turn to when an algorithm discriminates? Rarely ever one person. ADM technologies are thought up, planned, developed, and implemented by many people, diffusing any direct responsibility and allowing any one person or group to somewhat reasonably claim that they cannot be held personally responsible for specific negative outcomes. If you think something is wrong, you can always try to get the organization to change the rules, right?\nThis is one small part of Virginia Eubanks’ (2018) description of the evolution of what she calls the “Digital Poorhouse:” technological systems born from conservative hysteria over welfare costs, fraud, and inefficiency as the 1973 recession hit. With recent legal protections put in place to protect people needing welfare from discriminatory eligibility rules, politicians and state bureaucrats were caught between a desire to cut public assistance spending and the law. So, they found a way to cut spending, and gave it a spin that was hard to dispute at face value. They commissioned new technologies to save money by “distributing aid more efficiently.” After all, computers could ensure that every rule was being followed, welfare fraudsters couldn’t sneak through the cracks in the algorithms, and everyone would be getting equal treatment. Welfare assistance had rules, and computers would simply enforce the rules that were already there. By the 1980s, computers were collecting, analyzing, and storing incredibly detailed data on families receiving public assistance. And they were sharing this data with agencies across the US government, including the Department of Defence, state governments, federal employers, civil and criminal courts, local welfare agencies, and the Department of Justice.\nAlgorithms trawled these data for indications of fraud, criminal activity, or other inconsistencies. Through a combination of new rules and technologies, Republican legislators in New York state set about solving the problem of “cheats, frauds, and abusers” of the welfare system (Eubanks 2018). In 1972, almost 50% of citizens living under the poverty line were on public assistance; as of 2018, it was less than 10%. Every new set of rules could be justified if they found a few examples of misuse, which could then be amplified and used to justify the next round of rules. When failure to be on time for an appointment or otherwise missing any caseworker-prescribed therapeutic or job-training activity can be met with sanctions that result in temporary or permanent loss of benefits, this feeds into a cycle of poverty. People in need of assistance are then punished for illness, taking care of dependents, or occupational obligations, which in turn produces greater pressures on health, family, and finances. In protecting against people becoming “dependent” on the government, algorithms become the walls of the Digital Poorhouse, actively hindering people from escaping privation and perpetuating the cycle of poverty.\nThink back to Gebru’s questions. Should these algorithms exist? Are they robust enough to handle high-stakes contexts? The first question is always difficult, in part because the same algorithms can be used in so many different contexts and to so many different ends. The second question is easier to answer: no, they are not good enough to rely on in these high-stakes contexts. These are questions that we should always be thinking about when we produce algorithms that make decisions where humans would otherwise. We need to ask these questions because we are working in areas with important unsettled ethical dimensions where the decisions we make have material consequences on people lives. These questions should help us determine what kinds of work we will do, and what kinds we will not.\nIn addition to consent, informational risk, the tensions between competing principles such as privacy and transparency, and the highly consequential risks of algorithmic bias and decision paired with algorithmic decisions making, we have to be deeply concerned with the data we train our models with, and whether those data contain biases that would be perpetuated if used in an applied context. We’ll discuss the details in the next chapter and many that follow, but for now what you need to know is that machines only “learn” what we teach them via many examples. Certain kinds of machine learning make it very hard to understand what exactly the machine has learned, which contributes to a lack of accountability in a context where what the model learned has very significant consequences for the lives of real people. Here’s the problem, having been collected from the real world, they reflect the biases that exist wherever they were first collected. And of course any biases of the people who collected them, which is a problem given the extent to which marginalized people are underrepresented in fields like machine learning and artificial intelligence research (e.g., West, Whittaker, and Crawford 2019; Gebru 2020). Many of these models learn, or are explicitly trained to learn [e.g., classification models for social categories such as race, gender, and sexuality], those biases, which are then amplified and further propogated. Sometimes these biases are blatently obvious once you know to look for them (Buolamwini and Gebru 2018). Othertimes they can be much more illusive, even though there are plenty of good reasons to suspect they are there in some form (Bolukbasi et al. 2016; Gonen and Goldberg 2019; Nissim, Noord, and Goot 2020).\n\nFurther Reading\nThere is a lot of excellent work on ethics and politics of machine learning and artificial intelligence that is important to know. I strongly recommend O’Neil (2016), Eubanks (2018), and Angwin et al. (2016) for general introductions to issues related to systemic social inequality and algorithmic decision making. Timnit Gebru (2020) provides a good overview of questions related to race and gender in machine learning and ethics. West, Whittaker, and Crawford (2019) provide a close look at issues related to diversity and representation issues in machine learning and artificial intelligence that includes a critique of “pipeline” research on diversity in STEM fields.\nAbeba Birhane and Fred Cummins (2019) “Algorithmic injustices” offers a perspective grounded in philosophical work on relational ethics, and Hanna et al. (2020) offers a guidelines for work on algorithmic fairness that is grounded in critical race theory and sociological and historical work on the social construction of race and systemic social inequality. Denton et al. (2020) tackle of issues of algorithmic unfairness in benchmark machine learning datasets, which are biased towards white, cisgender, male, and Western people.",
    "crumbs": [
      "**PROFESSIONAL RESPONSIBILITIES**",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Ethical CSS</span>"
    ]
  },
  {
    "objectID": "ethical-css.html#ditching-the-value-free-ideal-for-ethics-politics-and-science",
    "href": "ethical-css.html#ditching-the-value-free-ideal-for-ethics-politics-and-science",
    "title": "35  Ethical CSS",
    "section": "35.7 DITCHING THE VALUE-FREE IDEAL FOR ETHICS, POLITICS, AND SCIENCE",
    "text": "35.7 DITCHING THE VALUE-FREE IDEAL FOR ETHICS, POLITICS, AND SCIENCE\nWe’ve discussed a lot of major challenges in this chapter so far, but we’ve barely scratched the surface. One thing I hope has been clear so far is that data are not inherently objective descriptions of reality that reveal the truth to us, like some sort of mythical view from nowhere; they are things that we construct. It’s not a matter of collecting and drawing insights from “raw data;” it’s models all the way down. Deciding to collect data in any way, is in effect a modelling decision that is propagated forward into other models (like univariate distributions), which in turn is propagated forward into more complex models (like machine learning models). At the end of all this, we design digital infrastructure that further entrenches our models in the world, whether it’s in the algorithms that recommend friends and news articles, or predictive models that we come to understand and game over time, further re-structuring and re-imagining our societies.\nWhile we should reflect on whether the data we collect and encode represents the world in some statistical sense, this is only the most obvious dimension of the problem of fair representation. It is also crucial to think about how the data we collect, and how we encode it, works back on the world. In other words, we need to think about how the ways we collect and encode data represent people, and whether the potential impacts from our work are fair and just. If the idea of doing computational social science with justice in mind is a bit too much for you, then I recommend, at the very least, starting with a commitment not to do computational social science in ways that contribute to injustices, which, as the algorithmic injustice literature makes patently clear, is very easy to do. In the end, the decision about what kind of work you will or will not do is up to you and any ethics board/stakeholders you must answer to, but this decision should be intentional. Refusing to make a decision is a decision, so it’s better to know what you’re comfortable contributing to so you don’t get a nasty surprise later on.\nI hope this resonates, but even if it does, it may not sit very well with everyone’s understanding of how science is supposed to be done. Shouldn’t we strive for impartiality? Shouldn’t we be pursuing the “value-free ideal?” This debate has raged on in some form or another in the sciences and humanities for centuries, and a full discussion is beyond the scope of this chapter. But the point I want to emphasize here is an obvious one whose full implications are rarely appreciated: science is fundamentally a human and cultural activity. For better or for worse, there is no getting rid of values in science (Douglas 2009).\n\nFurther Reading\nThere is plenty of work in science and technology studies as well as the sociology, history, and philosophy of science that is relevant to this discussion. I recommend reading Heather Douglas’ (2009) Science, policy, and the value-free ideal and Collins et al. (2020) Experts and the Will of the People. Both books articulate realistic normative models for science in social and political context. Finally, Green (2021) (discussed below) is worth reading for a more expliticly political take on the practice of data science.\n\nNot only is it impossible and pointless to try to get rid of values in science, neutrality itself is an illusion. Every decision that we make in the context of collecting data, applying models, interpreting outputs, and making decisions is part of imagining and structuring the world in particular ways, and to the extent that those decisions impact who gets what, these decisions are political. Neutrality is not an answer here. As Green (2021) points out, efforts to resist reform are just as political as any effort for reform, and the only people who get to claim “neutrality” are the ones whose perspective and interests are already widely entrenched. Everyone else is denied that stance. There really is no getting out of politics, whether we want out or not.\nGreen (2021) uses the case of predictive policing and systemic racism to make an argument we will return to when considering what and how we will and will not do computational social science.\n\n“… the very act of choosing to develop predictive policing algorithms is not at all neutral. Accepting common definitions of crime and how to address it does not allow data scientists to remove themselves from politics – it merely allows them to seem removed from politics, when in fact they are upholding the politics that have led to our current social conditions.” (Page 16)\n\nand\n\n“Whether or not the data scientists … recognize it, their decisions about what problems to work on, what data to use, and what solutions to propose involve normative stances that affect the distribution of power, status, and rights across society. They are, in other words, engaging in political activity.” (Page 20)\n\nThere are three core related insights here: (1) it is not possible to be “neutral;” (2) striving for neutrality is fundamentally conservative in that it maintains the status quo, whatever that may be; and (3) while you are entitled to conservatism if that’s what you want, you should be honest and call it what it is: conservativism, not neutrality. You don’t need to adopt a specific political stance to do good science, but doing good science, doing ethical and professionally responsible science, means articulating those values and making them explicit. You can see this as an extension of transparency if you like: you have values that shape your science, whether you know it or not. It is incumbent upon you to identify those values, understand their role, to make them explicit, and use that reflexive knowledge to do better science in service of your articulated and carefully considered values.\nGreen (2021) argues that abstract ethical principles are not enough, we also need explicit normative values. But doesn’t that run against the value-free ideal? Yes? Doesn’t that make for bad science? No. Quite the opposite, actually. Nothing good can come from pretending that science is not fundamentally a human and cultural endeavor (Collins et al. 2020; Douglas 2009). There is no being free from social standpoints or political and cultural contexts. And that does not devalue or diminish science in any way. The problem is not that we find values in places (i.e., sciences) where they don’t belong, it’s that those values are usually hidden, intentionally or unintentionally; they are not recognized as values, they are implicit, smuggled in. And they affect people’s lives.\n\n35.7.1 Critical Questions to Ask Yourself\nWe do not just make neutral tools that reveal some value-free Truth about the world. How will your tools be used? Are you developing or improving tools that could be used to violate people’s rights? That could infringe on their privacy or manipulate their informational environments and emotional/affective states? Could it undermine their autonomy, identify, or self-presentation? Could it out their secrets, or expose intimate details of their lives? Does it assign them membership in groups they don’t identify themselves with, such as methods that automatically estimate membership in some sort of social category?\nIf you consider these questions, you will quite possibly find yourself with the start of your very own “what I won’t build” list, articulated so clearly by Rachael Tatmam (2020) in her Widening NLP keynote. What will you not build? How will you not do computational social science or data science?\nI am framing this as a question of professional responsibility in part because much of the mess that data scientists and computational social scientists can find themselves in, wittingly or unwittingly, stems directly from defining our scientific work and roles in society as lacking agency, power, and responsibility for the way our work is used, and how it acts back on the world, and for avoiding politics as if it tainted our science rather than making it better. By framing it as a professional responsibility, I’m casting it as the cost of entry: ignoring these issues or defining them as not our/your responsibility is professionally irresponsible at best.\nIt is not enough to think about these things, they have to have an impact on our professional practice. Some of that, most in fact, is not a matter of technical skill. As we’ve already discussed, much is a matter of explicating your own values, whatever they might be, and making them more explicit. It’s about making decisions about what you will and won’t do for explicitly-articulated ethical and political reasons. Doing so does not mean injecting values into “science” that would otherwise be “value-free,” nor does it mean compromising the integrity of our research work. Doing so results in better science, but more importantly it contributes to a world that is better for everyone, including us.\nIn addition to decisions about what you will and won’t do in data science and computational social science, you will need to make specific decisions about how to do the things you’ve decided you will do. At a minimum, the cost of entry here should be to do your work in ways that are as transparent, accountable, and reproducible as possible.\n\nFurther Reading\nThere is a growing movement in the machine learning community, and more recently computational research in general, towards embedding fairness, transparency, and accountability (see, for example, the FAccT conference) into concrete research practices. It has also motivated discussions of prioritizing interpretable and causal models (e.g., Rudin 2019; Kusner and Loftus 2020) and better standards and documentation for data and models (e.g., Gebru et al. 2018; Mitchell et al. 2019; Nielse 2021; McLevey, Browne, and Crick 2021; Holland, Hosny, and Newman 2020), and research with secondary data (e.g., Weston et al. 2019).\n\nIn the kinds of cases that Cathy O’Neil (2016) and others discuss, the central idea is that to be accountable one has to be able to explain to those whose lives we affect how decisions where made not just in general, but in their case. If a bank uses a model that denies you a loan, you have a right to know why. Yet many widely-used cutting edge models used in the field, like most contemporary neural network models, can include thousands or millions of parameters that are learned from data and extraordinarily difficult to understand. Some of the really large-scale language models that make the news headlines have billions. And the variables these models use – generally known as features – are often low-level, like individual words or pixels. This has prompted two movements: (1) towards using less complex models that produce directly interpretable results, from humble logistic regressions to hierarchical Bayesian models instead of more complex models; and (2) developing new “explainability” models that attempt to inject a bit of interpretability into more complex models.\nPart of doing ethical, fair, and just computational and data science is about using models in ways that are appropriate for the problem at hand. Often this will mean putting down your neural network and picking up your logistic regression. But that doesn’t mean that the more complex models don’t have a place, they do! In fact, as Nelson (2021) and others have argued, they can even enable approaches to computational research that are informed by intersectionality theory.\nAs always, part of what makes this a challenge is that there is no checklist here. That said, here’s a non-exhaustive checklist to get you started thinking thorough some of these ethical and political considerations in computational social science and data science.\n\nHave the people represented by my data provided informed consent? If not, have I fully justified its use?\nHow important is privacy? Are any participants particularly at risk? Are any data particularly sensitive?\nHow important is transparency? How much of my data and process can I reveal to increase accountability and reproducibility?\nWhat kind of data might my data be linked with? Does this pose any risks?\nWhat could other people who have more resources do with my work?\nShould this work exist? Is it robust enough to be used in high-stakes contexts?\nWhat values have I used to guide this research? Have I made those explicitly clear?\nWhat kind of work will I do? What kind of work will I not do? How does this research fit into that?\n\nIf you can provide answers to these questions (and any more that apply) that would satisfy you coming from others, as well as yourself, you will be taking a much more proactive approach to conducting ethical and principled computational social science.",
    "crumbs": [
      "**PROFESSIONAL RESPONSIBILITIES**",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Ethical CSS</span>"
    ]
  },
  {
    "objectID": "ethical-css.html#conclusion",
    "href": "ethical-css.html#conclusion",
    "title": "35  Ethical CSS",
    "section": "35.8 CONCLUSION",
    "text": "35.8 CONCLUSION\n\n35.8.1 Key Points\n\nKnowledge of network structure can provide information that can be used to influence, for good or ill, that network.\nAnonymizing data is not a matter of removing names. The vast wealth of data in the digital age provides many ways to de-anonymize data, so more advanced techniques are needed to protect privacy.\nTransparency in research is important for producing better science that is reproducible, accountable, and more open to critique.\nPrivacy and transparency are in direct opposition to each other; we must balance the two principles according to the contextual importance of both.\nAlgorithms are not impartial. They reproduce human biases and goals, and they hide individual accountability.\nScience is a human and cultural endeavour. It has never been value-free. We can make science even better by making our values explicit, rather than hiding them.\nWhile ethical standards lag behind new technologies, doing ethical and principled computational social science requires holding ourselves to higher standards than are the current norm.\n\n\n\n\n\nAnanny, Mike, and Kate Crawford. 2018. “Seeing Without Knowing: Limitations of the Transparency Ideal and Its Application to Algorithmic Accountability.” New Media & Society 20 (3): 973–89.\n\n\nAngwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. “Machine Bias.” ProPublica.\n\n\nBeninger, Kelsey. 2017. “Social Media Users’ Views on the Ethics of Social Media Research.” The Sage Handbook of Social Media Research Methods. London: Sage, 57–73.\n\n\nBenjamin, Ruha. 2019. Race After Technology: Abolitionist Tools for the New Jim Code. Polity Press.\n\n\nBirhane, Abeba, and Fred Cummins. 2019. “Algorithmic Injustices: Towards a Relational Ethics.” arXiv Preprint arXiv:1912.07376.\n\n\nBolukbasi, Tolga, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. 2016. “Man Is to Computer Programmer as Woman Is to Homemaker? Debiasing Word Embeddings.” arXiv Preprint arXiv:1607.06520.\n\n\nBuolamwini, Joy, and Timnit Gebru. 2018. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” In Conference on Fairness, Accountability and Transparency, 77–91. PMLR.\n\n\nCollins, Harry, Robert Evans, Darrin Durant, and Martin Weinel. 2020. “Experts and the Will of the People.” Cham: Palgrave Macmillan.\n\n\nDenton, Emily, Alex Hanna, Razvan Amironesei, Andrew Smart, Hilary Nicole, and Morgan Klaus Scheuerman. 2020. “Bringing the People Back in: Contesting Benchmark Machine Learning Datasets.” arXiv Preprint arXiv:2007.07399.\n\n\nDiakopoulos, Nicholas. 2017. “Enabling Accountability of Algorithmic Media: Transparency as a Constructive and Critical Lens.” In Transparent Data Mining for Big and Small Data, 25–43. Springer.\n\n\n———. 2020. “Transparency.” In The Oxford Handbook of Ethics of AI.\n\n\nDouglas, Heather. 2009. Science, Policy, and the Value-Free Ideal. University of Pittsburgh Pre.\n\n\nEubanks, Virginia. 2018. Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor. St. Martin’s Press.\n\n\nGebru, Timnit. 2018. “How to Stop Artificial Intelligence from Marginalizing Communities?” https://www.youtube.com/watch?v=PWCtoVt1CJM.\n\n\n———. 2020. “Race and Gender.” The Oxford Handbook of Ethics of AI, 251–69.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2018. “Datasheets for Datasets.” arXiv Preprint arXiv:1803.09010.\n\n\nGoel, Vindu. 2014. “As Data Overflows Online, Researchers Grapple with Ethics.” The New York Times 12.\n\n\nGonen, Hila, and Yoav Goldberg. 2019. “Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings but Do Not Remove Them.” arXiv Preprint arXiv:1903.03862.\n\n\nGreen, Ben. 2021. “Data Science as Political Action.”\n\n\nGrimmelmann, James. 2015. “The Law and Ethics of Experiments on Social Media Users.” Colo. Tech. LJ 13: 219.\n\n\nHamidi, Foad, Morgan Klaus Scheuerman, and Stacy Branham. 2018. “Gender Recognition or Gender Reductionism? The Social Implications of Embedded Gender Recognition Systems.” In Proceedings of the 2018 Chi Conference on Human Factors in Computing Systems, 1–13.\n\n\nHandcock, Mark, David Hunter, Carter Butts, Steven Goodreau, and Martina Morris. 2003. “Statnet: Software Tools for the Statistical Modeling of Network Data.” Seattle, WA. Version 2.\n\n\nHanna, Alex, Emily Denton, Andrew Smart, and Jamila Smith-Loud. 2020. “Towards a Critical Race Methodology in Algorithmic Fairness.” In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 501–12.\n\n\nHealy, Kieran. 2013. “Using Metadata to Find Paul Revere.” https://kieranhealy.org/blog/archives/2013/06/09/using-metadata-to-find-paul-revere/.\n\n\nHogan, Bernie. 2021. “Networks Are a Lens for Power: A Commentary on the Recent Advances in the Ethics of Social Networks Special Issue.” Social Networks.\n\n\nHolland, Sarah, Ahmed Hosny, and Sarah Newman. 2020. “The Dataset Nutrition Label.” Data Protection and Privacy: Data Protection and Democracy, 1.\n\n\nKusner, Matt, and Joshua Loftus. 2020. “The Long Road to Fairer Algorithms.” Nature Publishing Group.\n\n\nLewis, Kevin, Jason Kaufman, Marco Gonzalez, Andreas Wimmer, and Nicholas Christakis. 2008. “Tastes, Ties, and Time: A New Social Network Dataset Using Facebook. Com.” Social Networks 30 (4): 330–42.\n\n\nLusher, Dean, Johan Koskinen, and Garry Robins. 2013. Exponential Random Graph Models for Social Networks: Theory, Methods, and Applications. Vol. 35. Cambridge University Press.\n\n\nMcLevey, John, Pierson Browne, and Tyler Crick. 2021. “Reproducibility, Transparency, and Principled Data Processing.” In Handbook of Computational Social Science. Routledge.\n\n\nMitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. “Model Cards for Model Reporting.” In Proceedings of the Conference on Fairness, Accountability, and Transparency, 220–29.\n\n\nNelson, Laura. 2021. “Leveraging the Alignment Between Machine Learning and Intersectionality: Using Word Embeddings to Measure Intersectional Experiences of the Nineteenth Century US South.” Poetics, 101539.\n\n\nNielse, Aileen. 2021. Practical Fairness: Achieving Fair and Secure Data Models. O’Reilly.\n\n\nNissim, Malvina, Rik van Noord, and Rob van der Goot. 2020. “Fair Is Better Than Sensational: Man Is to Doctor as Woman Is to Doctor.” Computational Linguistics 46 (2): 487–97.\n\n\nNoble, Safiya Umoja. 2018. Algorithms of Oppression: How Search Engines Reinforce Racism. NYU Press.\n\n\nO’Neil, Cathy. 2016. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. Crown.\n\n\nRudin, Cynthia. 2019. “Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead.” Nature Machine Intelligence 1 (5): 206–15.\n\n\nSalganik, Matthew. 2019. Bit by Bit: Social Research in the Digital Age. Princeton University Press.\n\n\nSloan, Luke, and Anabel Quan-Haase. 2017. “A Retrospective on State of the Art Social Media Research Methods: Ethical Decisions, Big-Small Data Rivalries and the Spectre of the 6Vs.” The SAGE Handbook of Social Media Research Methods. Sage: London.\n\n\nSweeney, Latanya. 2002. “K-Anonymity: A Model for Protecting Privacy.” International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems 10 (05): 557–70.\n\n\nTatmam, Rachel. 2020. “What i Won’t Build.”\n\n\nTubaro, Paola, Louise Ryan, Antonio Casilli, and Alessio D’angelo. 2020. “Social Network Analysis: New Ethical Approaches Through Collective Reflexivity. Introduction to the Special Issue of Social Networks.” Social Networks.\n\n\nUpsahl, Kurt. 2013. “Why Metadata Matters.” Electronic Frontier Foundation https://www.eff.org/deeplinks/2013/06/why-metadata-matters.\n\n\nVries, Terrance de, Ishan Misra, Changhan Wang, and Laurens van der Maaten. 2019. “Does Object Recognition Work for Everyone?” In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 52–59.\n\n\nWest, Sarah Myers, Meredith Whittaker, and Kate Crawford. 2019. “Discriminating Systems.” AI Now.\n\n\nWeston, Sara J, Stuart J Ritchie, Julia M Rohrer, and Andrew K Przybylski. 2019. “Recommendations for Increasing the Transparency of Analysis of Preexisting Data Sets.” Advances in Methods and Practices in Psychological Science 2 (3): 214–27.\n\n\nZimmer, Michael. 2010. “‘But the Data Is Already Public’: On the Ethics of Research in Facebook.” Ethics and Information Technology 12 (4): 313–25.",
    "crumbs": [
      "**PROFESSIONAL RESPONSIBILITIES**",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Ethical CSS</span>"
    ]
  },
  {
    "objectID": "open-css.html",
    "href": "open-css.html",
    "title": "36  Open CSS",
    "section": "",
    "text": "New chapter coming in fall 2024.",
    "crumbs": [
      "**PROFESSIONAL RESPONSIBILITIES**",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Open CSS</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "adams, jimi. 2020. Gathering Social Network Data. SAGE\nPublications Incorporated.\n\n\nAnanny, Mike, and Kate Crawford. 2018. “Seeing Without Knowing:\nLimitations of the Transparency Ideal and Its Application to Algorithmic\nAccountability.” New Media & Society 20 (3): 973–89.\n\n\nAngwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016.\n“Machine Bias.” ProPublica.\n\n\nArmstrong, Elizabeth, and Mary Bernstein. 2008. “Culture, Power,\nand Institutions: A Multi-Institutional Politics Approach to Social\nMovements.” Sociological Theory 26 (1): 74–99.\n\n\nArtetxe, Mikel, Gorka Labaka, and Eneko Agirre. 2016. “Learning\nPrincipled Bilingual Mappings of Word Embeddings While Preserving\nMonolingual Invariance.” In Proceedings of the 2016\nConference on Empirical Methods in Natural Language Processing,\n2289–94.\n\n\nBall, Patrick. 2016. “Principled Data Processing.” Data\n& Society.\n\n\nBarabási, Albert-László, and Réka Albert. 1999. “Emergence of\nScaling in Random Networks.” Science 286 (5439): 509–12.\n\n\nBarton, Allen. 1968. “Bringing Society Back in Survey Research and\nMacro-Methodology.” The American Behavioral Scientist 12\n(2): 1.\n\n\nBearman, Peter, James Moody, and Katherine Stovel. 2004. “Chains\nof Affection: The Structure of Adolescent Romantic and Sexual\nNetworks.” American Journal of Sociology 110 (1): 44–91.\n\n\nBenford, Robert. 1993. “Frame Disputes Within the Nuclear\nDisarmament Movement.” Social Forces 71 (3): 677–701.\n\n\nBenford, Robert, and David Snow. 2000. “Framing Processes and\nSocial Movements: An Overview and Assessment.” Annual Review\nof Sociology 26 (1): 611–39.\n\n\nBeninger, Kelsey. 2017. “Social Media Users’ Views on the Ethics\nof Social Media Research.” The Sage Handbook of Social Media\nResearch Methods. London: Sage, 57–73.\n\n\nBenjamin, Ruha. 2019. Race After Technology: Abolitionist Tools for\nthe New Jim Code. Polity Press.\n\n\nBerelson, Bernard. 1952. “Content Analysis in Communication\nResearch.”\n\n\nBiernacki, Richard. 2009. “After Quantitative Cultural Sociology:\nInterpretive Science as a Calling.” In Meaning and\nMethod, 125–213. Routledge.\n\n\n———. 2012. Reinventing Evidence in Social Inquiry: Decoding Facts\nand Variables. Springer.\n\n\n———. 2015. “How to Do Things with Historical Texts.”\nAmerican Journal of Cultural Sociology 3 (3): 311–52.\n\n\nBirhane, Abeba, and Fred Cummins. 2019. “Algorithmic Injustices:\nTowards a Relational Ethics.” arXiv Preprint\narXiv:1912.07376.\n\n\nBlei, David. 2012. “Probabilistic Topic Models.”\nCommunications of the ACM 55 (4): 77–84.\n\n\n———. 2017. “Variational Inference: Foundations and\nInnovations.” Simons Institute: Computational Challenges in\nMachine Learning.\n\n\nBlei, David M, and John D Lafferty. 2006. “Dynamic Topic\nModels.” In Proceedings of the 23rd International Conference\non Machine Learning, 113–20.\n\n\nBlei, David, and John Lafferty. 2009. “Topic Models.”\nText Mining: Classification, Clustering, and Applications 10\n(71): 34.\n\n\nBlei, David, Andrew Ng, and Michael I Jordan. 2003. “Latent\nDirichlet Allocation.” The Journal of Machine Learning\nResearch 3: 993–1022.\n\n\nBolukbasi, Tolga, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and\nAdam Kalai. 2016. “Man Is to Computer Programmer as Woman Is to\nHomemaker? Debiasing Word Embeddings.” arXiv Preprint\narXiv:1607.06520.\n\n\nBonacich, Phillip. 1987. “Power and Centrality: A Family of\nMeasures.” American Journal of Sociology 92 (5):\n1170–82.\n\n\nBonikowski, Bart. 2017. “Ethno-Nationalist Populism and the\nMobilization of Collective Resentment.” The British Journal\nof Sociology 68: S181–213.\n\n\nBonikowski, Bart, and Noam Gidron. 2016. “The Populist Style in\nAmerican Politics: Presidential Campaign Discourse, 1952–1996.”\nSocial Forces 94 (4): 1593–1621.\n\n\nBorgatti, Stephen, and Martin Everett. 2020. Three Perspectives on\nCentrality. Edited by Ryan Light and James Moody. Oxford University\nPress.\n\n\nBorgatti, Stephen, Martin Everett, and Jeffrey Johnson. 2018.\nAnalyzing Social Networks. Sage.\n\n\nBrailly, Julien, Guillaume Favre, Josiane Chatellet, and Emmanuel\nLazega. 2016. “Embeddedness as a Multilevel Problem: A Case Study\nin Economic Sociology.” Social Networks 44: 319–33.\n\n\nBreiger, Ronald. 1974. “The Duality of Persons and Groups.”\nSocial Forces 53 (2): 181–90.\n\n\nBrekhus, Wayne, and Gabe Ignatow. 2019. The Oxford Handbook of\nCognitive Sociology. Oxford University Press.\n\n\nBrienza, Justin, Franki Kung, Henri Santos, Ramona Bobocel, and Igor\nGrossmann. 2018. “Wisdom, Bias, and Balance: Toward a\nProcess-Sensitive Measurement of Wisdom-Related Cognition.”\nJournal of Personality and Social Psychology 115 (6): 1093.\n\n\nBuhari-Gulmez, Didem. 2010. “Stanford School on Sociological\nInstitutionalism: A Global Cultural Approach.” International\nPolitical Sociology 4 (3): 253–70.\n\n\nBuolamwini, Joy, and Timnit Gebru. 2018. “Gender Shades:\nIntersectional Accuracy Disparities in Commercial Gender\nClassification.” In Conference on Fairness, Accountability\nand Transparency, 77–91. PMLR.\n\n\nCaren, Neal. 2007. “Political Process Theory.” The\nBlackwell Encyclopedia of Sociology.\n\n\nCentola, Damon. 2018. How Behavior Spreads: The Science of Complex\nContagions. Princeton University Press Princeton, NJ.\n\n\nCerulo, Karen. 2002. Culture in Mind: Toward a Sociology of Culture\nand Cognition. Psychology Press.\n\n\nCerulo, Karen A. 2010. “Mining the Intersections of Cognitive\nSociology and Neuroscience.” Poetics 38 (2): 115–32.\n\n\nCharmaz, Kathy. 2006. Constructing Grounded Theory: A Practical\nGuide Through Qualitative Analysis. Sage.\n\n\nChollet, Francois. 2018. Deep Learning with Python. Vol. 361.\nManning New York.\n\n\nCollins, Harry, Robert Evans, Darrin Durant, and Martin Weinel. 2020.\n“Experts and the Will of the People.” Cham: Palgrave\nMacmillan.\n\n\nCollins, Patricia. 2015. “Intersectionality’s Definitional\nDilemmas.” Annual Review of Sociology 41: 1–20.\n\n\nCollins, Patricia, and Sirma Bilge. 2020. Intersectionality.\nJohn Wiley & Sons.\n\n\nCoppedge, Michael, John Gerring, Adam Glynn, Carl Henrik Knutsen,\nStaffan Lindberg, Daniel Pemstein, Brigitte Seim, Svend-Erik Skaaning,\nand Jan Teorell. 2020. Varieties of Democracy: Measuring Two\nCenturies of Political Change. Cambridge University Press.\n\n\nCrawford, Kate, and Trevor Paglen. 2019. “Excavating AI: The\nPolitics of Images in Machine Learning Training Sets.”\nExcavating AI.\n\n\nCrenshaw, Kimberlé. 1989. “Demarginalizing the Intersection of\nRace and Sex: A Black Feminist Critique of Antidiscrimination Doctrine,\nFeminist Theory and Antiracist Politics.” U. Chi. Legal\nf., 139.\n\n\nCrossley, Nick. 2010. Towards Relational Sociology. Routledge.\n\n\nCrossley, Nick, Elisa Bellotti, Gemma Edwards, Martin G Everett, Johan\nKoskinen, and Mark Tranmer. 2015. Social Network Analysis for\nEgo-Nets: Social Network Analysis for Actor-Centred Networks. Sage.\n\n\nDeerwester, Scott, Susan Dumais, George Furnas, Thomas Landauer, and\nRichard Harshman. 1990. “Indexing by Latent Semantic\nAnalysis.” Journal of the American Society for Information\nScience 41 (6): 391–407.\n\n\nDenton, Emily, Alex Hanna, Razvan Amironesei, Andrew Smart, Hilary\nNicole, and Morgan Klaus Scheuerman. 2020. “Bringing the People\nBack in: Contesting Benchmark Machine Learning Datasets.”\narXiv Preprint arXiv:2007.07399.\n\n\nDi Carlo, Valerio, Federico Bianchi, and Matteo Palmonari. 2019.\n“Training Temporal Word Embeddings with a Compass.” In\nProceedings of the AAAI Conference on Artificial Intelligence,\n33:6326–34. 01.\n\n\nDiakopoulos, Nicholas. 2017. “Enabling Accountability of\nAlgorithmic Media: Transparency as a Constructive and Critical\nLens.” In Transparent Data Mining for Big and Small\nData, 25–43. Springer.\n\n\n———. 2020. “Transparency.” In The Oxford Handbook of\nEthics of AI.\n\n\nDieng, Adji B, Francisco JR Ruiz, and David M Blei. 2020. “Topic\nModeling in Embedding Spaces.” Transactions of the\nAssociation for Computational Linguistics 8: 439–53.\n\n\nDieng, Adji, Francisco Ruiz, and David Blei. 2019. “The Dynamic\nEmbedded Topic Model.” arXiv Preprint arXiv:1907.05545.\n\n\nDiMaggio, Paul. 1997. “Culture and Cognition.” Annual\nReview of Sociology 23 (1): 263–87.\n\n\nDiMaggio, Paul, Manish Nag, and David Blei. 2013. “Exploiting\nAffinities Between Topic Modeling and the Sociological Perspective on\nCulture: Application to Newspaper Coverage of US Government Arts\nFunding.” Poetics 41 (6): 570–606.\n\n\nDiPrete, Thomas, and Gregory Eirich. 2006. “Cumulative Advantage\nas a Mechanism for Inequality: A Review of Theoretical and Empirical\nDevelopments.” Annu. Rev. Sociol. 32: 271–97.\n\n\nDomingos, Pedro. 2015. The Master Algorithm: How the Quest for the\nUltimate Learning Machine Will Remake Our World. Basic Books.\n\n\nDouglas, Heather. 2009. Science, Policy, and the Value-Free\nIdeal. University of Pittsburgh Pre.\n\n\nDuckett, Jon. 2011. HTML & CSS: Design and Build Websites.\nVol. 15. Wiley Indianapolis, IN.\n\n\nDumais, Susan. 2004. “Latent Semantic Analysis.” Annual\nReview of Information Science and Technology 38 (1): 188–230.\n\n\nEdelmann, Achim, and John Mohr. 2018. “Formal Studies of Culture:\nIssues, Challenges, and Current Trends.” Poetics 68:\n1–9.\n\n\nElbourne, Paul. 2011. Meaning: A Slim Guide to Semantics.\nOxford University Press.\n\n\nEmirbayer, Mustafa. 1997. “Manifesto for a Relational\nSociology.” American Journal of Sociology 103 (2):\n281–317.\n\n\nEubanks, Virginia. 2018. Automating Inequality: How High-Tech Tools\nProfile, Police, and Punish the Poor. St. Martin’s Press.\n\n\nEvans, James A, and Pedro Aceves. 2016. “Machine Translation:\nMining Text for Social Theory.” Annual Review of\nSociology 42: 21–50.\n\n\nFeld, Scott. 1991. “Why Your Friends Have More Friends Than You\nDo.” American Journal of Sociology 96 (6): 1464–77.\n\n\nField, Andy, Jeremy Miles, and Zoë Field. 2012. Discovering\nStatistics Using r. Sage publications.\n\n\nFirth, John. 1957. “A Synopsis of Linguistic Theory,\n1930-1955.” Studies in Linguistic Analysis.\n\n\nFranzosi, Roberto. 2004. From Words to Numbers: Narrative, Data, and\nSocial Science. Cambridge University Press.\n\n\nFreeman, Linton. 2004. “The Development of Social Network\nAnalysis.” A Study in the Sociology of Science 1 (687):\n159–67.\n\n\nGarg, Nikhil, Londa Schiebinger, Dan Jurafsky, and James Zou. 2018.\n“Word Embeddings Quantify 100 Years of Gender and Ethnic\nStereotypes.” Proceedings of the National Academy of\nSciences 115 (16): E3635–44.\n\n\nGebru, Timnit. 2018. “How to Stop Artificial Intelligence from\nMarginalizing Communities?” https://www.youtube.com/watch?v=PWCtoVt1CJM.\n\n\n———. 2020. “Race and Gender.” The Oxford Handbook of\nEthics of AI, 251–69.\n\n\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman\nVaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2018.\n“Datasheets for Datasets.” arXiv Preprint\narXiv:1803.09010.\n\n\nGelman, Andrew. 2004. “Exploratory Data Analysis for Complex\nModels.” Journal of Computational and Graphical\nStatistics 13 (4): 755–79.\n\n\nGelman, Andrew, and Cosma Rohilla Shalizi. 2013. “Philosophy and\nthe Practice of Bayesian Statistics.” British Journal of\nMathematical and Statistical Psychology 66 (1): 8–38.\n\n\nGéron, Aurélien. 2019. Hands-on Machine Learning with Scikit-Learn,\nKeras, and TensorFlow: Concepts, Tools, and Techniques to Build\nIntelligent Systems. O’Reilly Media.\n\n\nGlaser, Barney, and Anselm Strauss. 1999. Discovery of Grounded\nTheory: Strategies for Qualitative Research. Aldine Transaction.\n\n\nGoel, Vindu. 2014. “As Data Overflows Online, Researchers Grapple\nwith Ethics.” The New York Times 12.\n\n\nGonen, Hila, and Yoav Goldberg. 2019. “Lipstick on a Pig:\nDebiasing Methods Cover up Systematic Gender Biases in Word Embeddings\nbut Do Not Remove Them.” arXiv Preprint\narXiv:1903.03862.\n\n\nGreen, Ben. 2021. “Data Science as Political Action.”\n\n\nGrimmelmann, James. 2015. “The Law and Ethics of Experiments on\nSocial Media Users.” Colo. Tech. LJ 13: 219.\n\n\nHamidi, Foad, Morgan Klaus Scheuerman, and Stacy Branham. 2018.\n“Gender Recognition or Gender Reductionism? The Social\nImplications of Embedded Gender Recognition Systems.” In\nProceedings of the 2018 Chi Conference on Human Factors in Computing\nSystems, 1–13.\n\n\nHandcock, Mark, David Hunter, Carter Butts, Steven Goodreau, and Martina\nMorris. 2003. “Statnet: Software Tools for the Statistical\nModeling of Network Data.” Seattle, WA. Version 2.\n\n\nHanna, Alex, Emily Denton, Andrew Smart, and Jamila Smith-Loud. 2020.\n“Towards a Critical Race Methodology in Algorithmic\nFairness.” In Proceedings of the 2020 Conference on Fairness,\nAccountability, and Transparency, 501–12.\n\n\nHanneman, Robert, and Mark Riddle. 2005. “Introduction to Social\nNetwork Methods.” University of California Riverside.\n\n\nHarrigan, Nicholas, Giuseppe (Joe) Labianca, and Filip Agneessens. 2020.\n“Negative Ties and Signed Graphs Research: Stimulating Research on\nDissociative Forces in Social Networks.” Social Networks\n60: 1–10.\n\n\nHarris, Zellig S. 1954. “Distributional Structure.”\nWord 10 (2-3): 146–62.\n\n\nHealy, Kieran. 2013. “Using Metadata to Find Paul Revere.”\nhttps://kieranhealy.org/blog/archives/2013/06/09/using-metadata-to-find-paul-revere/.\n\n\n———. 2018. Data Visualization: A Practical Introduction.\nPrinceton University Press.\n\n\nHealy, Kieran, and James Moody. 2014. “Data Visualization in\nSociology.” Annual Review of Sociology 40: 105–28.\n\n\nHoffman, Matthew, Francis Bach, and David Blei. 2010. “Online\nLearning for Latent Dirichlet Allocation.” In Advances in\nNeural Information Processing Systems, 856–64. Citeseer.\n\n\nHogan, Bernie. 2021. “Networks Are a Lens for Power: A Commentary\non the Recent Advances in the Ethics of Social Networks Special\nIssue.” Social Networks.\n\n\nHolland, Sarah, Ahmed Hosny, and Sarah Newman. 2020. “The Dataset\nNutrition Label.” Data Protection and Privacy: Data\nProtection and Democracy, 1.\n\n\nIgnatow, Gabe, and Rada Mihalcea. 2016. Text Mining: A Guidebook for\nthe Social Sciences. Sage Publications.\n\n\nIgnatow, Gabriel. 2009. “Culture and Embodied Cognition: Moral\nDiscourses in Internet Support Groups for Overeaters.” Social\nForces 88 (2): 643–69.\n\n\nJones, Jason, Mohammad Ruhul Amin, Jessica Kim, and Steven Skiena. 2020.\n“Stereotypical Gender Associations in Language Have Decreased over\nTime.” Sociological Science 7: 1–35.\n\n\nJoos, Martin. 1950. “Description of Language Design.”\nThe Journal of the Acoustical Society of America 22 (6): 701–7.\n\n\nJordan, Michael. 2003. “An Introduction to Probabilistic Graphical\nModels.” preparation.\n\n\n———. 2004. “Graphical Models.” Statistical Science\n19 (1): 140–55.\n\n\nJurafsky, Dan, and Martin Hand. 2009. Speech & Language\nProcessing. 2nd ed. Pearson Prentice Hall.\n\n\nKarrer, Brian, and Mark Newman. 2011. “Stochastic Blockmodels and\nCommunity Structure in Networks.” Physical Review E 83\n(1): 016107.\n\n\nKitts, James. 2014. “Beyond Networks in Structural Theories of\nExchange: Promises from Computational Social Science.” In\nAdvances in Group Processes. Emerald Group Publishing Limited.\n\n\nKitts, James, and Eric Quintane. 2020. “Rethinking Social Networks\nin the Era of Computational Social Science.” The Oxford\nHandbook of Social Networks, 71.\n\n\nKoller, Daphne, and Nir Friedman. 2009. Probabilistic Graphical\nModels: Principles and Techniques. MIT press.\n\n\nKozlowski, Austin, Matt Taddy, and James Evans. 2019. “The\nGeometry of Culture: Analyzing the Meanings of Class Through Word\nEmbeddings.” American Sociological Review 84 (5):\n905–49.\n\n\nKrippendorff, Klaus. 2019. Content Analysis: An Introduction to Its\nMethodology. Sage.\n\n\nKruschke, John. 2014. “Doing Bayesian Data Analysis: A Tutorial\nwith r, JAGS, and Stan.”\n\n\nKusner, Matt, and Joshua Loftus. 2020. “The Long Road to Fairer\nAlgorithms.” Nature Publishing Group.\n\n\nLambert, Ben. 2018. A Student’s Guide to Bayesian Statistics.\nSage.\n\n\nLasswell, Harold. 1927. Propaganda Technique in the World War.\nRavenio Books.\n\n\nLazega, Emmanuel, and Tom Snijders. 2015. Multilevel Network\nAnalysis for the Social Sciences: Theory, Methods and Applications.\nVol. 12. Springer.\n\n\nLee, Monica, and John Levi Martin. 2015a. “Coding, Counting and\nCultural Cartography.” American Journal of Cultural\nSociology 3 (1): 1–33.\n\n\n———. 2015b. “Response to Biernacki, Reed, and Spillman.”\nAmerican Journal of Cultural Sociology 3 (3): 380–415.\n\n\nLewis, Kevin, Jason Kaufman, Marco Gonzalez, Andreas Wimmer, and\nNicholas Christakis. 2008. “Tastes, Ties, and Time: A New Social\nNetwork Dataset Using Facebook. Com.” Social Networks 30\n(4): 330–42.\n\n\nLin, Jianhua. 1991. “Divergence Measures Based on the Shannon\nEntropy.” IEEE Transactions on Information Theory 37\n(1): 145–51.\n\n\nLinzhuo, Li, Wu Lingfei, and Evans James. 2020. “Social\nCentralization and Semantic Collapse: Hyperbolic Embeddings of Networks\nand Text.” Poetics 78: 101428.\n\n\nLizardo, Omar, Brandon Sepulvado, Dustin S Stoltz, and Marshall A\nTaylor. 2019. “What Can Cognitive Neuroscience Do for Cultural\nSociology?” American Journal of Cultural Sociology,\n1–26.\n\n\nLusher, Dean, Johan Koskinen, and Garry Robins. 2013. Exponential\nRandom Graph Models for Social Networks: Theory, Methods, and\nApplications. Vol. 35. Cambridge University Press.\n\n\nLynch, Scott, and Bryce Bartlett. 2019. “Bayesian Statistics in\nSociology: Past, Present, and Future.” Annual Review of\nSociology 45: 47–68.\n\n\nMa, Eric. 2021. Data Science Bootstrap: A Practical Guide to Getting\nOrganized for Your Best Data Science Work. LeanPub.\n\n\nMartin, Osvaldo. 2018. Bayesian Analysis with Python: Introduction\nto Statistical Modeling and Probabilistic Programming Using Pymc and\nArviZ. Packt Publishing Ltd.\n\n\nMcAdam, Doug. 2010. Political Process and the Development of Black\nInsurgency, 1930-1970. University of Chicago Press.\n\n\nMcCammon, Holly. 2009. “Beyond Frame Resonance: The Argumentative\nStructure and Persuasive Capacity of Twentieth-Century US Women’s\nJury-Rights Frames.” Mobilization: An International\nQuarterly 14 (1): 45–64.\n\n\n———. 2012. The US Women’s Jury Movements and Strategic Adaptation: A\nMore Just Verdict. Cambridge University Press.\n\n\nMcCammon, Holly J, Courtney Sanders Muse, Harmony D Newman, and Teresa M\nTerrell. 2007. “Movement Framing and Discursive Opportunity\nStructures: The Political Successes of the US Women’s Jury\nMovements.” American Sociological Review 72 (5): 725–49.\n\n\nMcCarthy, John D, and Mayer N Zald. 1977. “Resource Mobilization\nand Social Movements: A Partial Theory.” American Journal of\nSociology 82 (6): 1212–41.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\nwith Examples in r and Stan. CRC press.\n\n\nMcLevey, John, Pierson Browne, and Tyler Crick. 2021.\n“Reproducibility, Transparency, and Principled Data\nProcessing.” In Handbook of Computational Social\nScience. Routledge.\n\n\nMcLevey, John, Tyler Crick, Browne Pierson, and Darrin Durant. 2021.\n“Word Embeddings and the Structural and Cultural Dimensions of\nDemocracy and Autocracy, 1900-2020.” Canadian Review of\nSociology 7: 544–69.\n\n\nMerton, Robert K. 1968. “The Matthew Effect in Science: The Reward\nand Communication Systems of Science Are Considered.”\nScience 159 (3810): 56–63.\n\n\nMeyer, John W, Georg Krücken, and Gili Drori. 2009. World Society:\nThe Writings of John w. Meyer. Oxford University Press.\n\n\nMikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.\n“Efficient Estimation of Word Representations in Vector\nSpace.” arXiv Preprint arXiv:1301.3781.\n\n\nMikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean.\n2013. “Distributed Representations of Words and Phrases and Their\nCompositionality.” In Advances in Neural Information\nProcessing Systems, 3111–19.\n\n\nMikolov, Tomáš, Wen-tau Yih, and Geoffrey Zweig. 2013. “Linguistic\nRegularities in Continuous Space Word Representations.” In\nProceedings of the 2013 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language\nTechnologies, 746–51.\n\n\nMimno, David, Hanna Wallach, Edmund Talley, Miriam Leenders, and Andrew\nMcCallum. 2011. “Optimizing Semantic Coherence in Topic\nModels.” In Proceedings of the 2011 Conference on Empirical\nMethods in Natural Language Processing, 262–72.\n\n\nMische, Ann. 2011. “Relational Sociology, Culture, and\nAgency.” The SAGE Handbook of Social Network Analysis,\n80–97.\n\n\nMitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy\nVasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and\nTimnit Gebru. 2019. “Model Cards for Model Reporting.” In\nProceedings of the Conference on Fairness, Accountability, and\nTransparency, 220–29.\n\n\nMitchell, Ryan. 2018. Web Scraping with Python: Collecting More Data\nfrom the Modern Web. \"O’Reilly\".\n\n\nMogadala, Aditya, and Achim Rettinger. 2016. “Bilingual Word\nEmbeddings from Parallel and Non-Parallel Corpora for Cross-Language\nText Classification.” In Proceedings of the 2016 Conference\nof the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, 692–702.\n\n\nMohr, John. 1998. “Measuring Meaning Structures.”\nAnnual Review of Sociology 24 (1): 345–70.\n\n\nMohr, John, Christopher Bail, Margaret Frye, Jennifer Lena, Omar\nLizardo, Terence McDonnell, Ann Mische, Iddo Tavory, and Frederick\nWherry. 2020. Measuring Culture. Columbia University Press.\n\n\nMohr, John, and Petko Bogdanov. 2013. “Introduction—Topic Models:\nWhat They Are and Why They Matter.” Elsevier.\n\n\nMohr, John, Robin Wagner-Pacifici, and Ronald Breiger. 2015.\n“Toward a Computational Hermeneutics.” Big Data &\nSociety 2 (2): 2053951715613809.\n\n\nMüller, Andreas, and Sarah Guido. 2016. Introduction to Machine\nLearning with Python: A Guide for Data Scientists. \" O’Reilly\nMedia, Inc.\".\n\n\nMurphy, Kevin. 2012. Machine Learning: A Probabilistic\nPerspective. MIT press.\n\n\nMützel, Sophie, and Ronald Breiger. 2020. “Duality Beyond Persons\nand Groups.” The Oxford Handbook of Social Networks,\n392.\n\n\nNeblo, Michael, Kevin Esterling, and David Lazer. 2018. Politics\nwith the People: Building a Directly Representative Democracy. Vol.\n555. Cambridge University Press.\n\n\nNelson, Laura. 2015. “Political Logics as Cultural Memory:\nCognitive Structures, Local Continuities, and Women’s Organizations in\nChicago and New York City.” Working Paper.\n\n\n———. 2017. “Computational Grounded Theory: A Methodological\nFramework.” Sociological Methods & Research, 1–40.\n\n\n———. 2021. “Leveraging the Alignment Between Machine Learning and\nIntersectionality: Using Word Embeddings to Measure Intersectional\nExperiences of the Nineteenth Century US South.”\nPoetics, 101539.\n\n\nNelson, Laura K, Derek Burk, Marcel Knudsen, and Leslie McCall. 2021.\n“The Future of Coding: A Comparison of Hand-Coding and Three Types\nof Computer-Assisted Text Analysis Methods.” Sociological\nMethods & Research 50 (1): 202–37.\n\n\nNelson, Laura, Derek Burk, Marcel Knudsen, and Leslie McCall. 2018.\n“The Future of Coding: A Comparison of Hand-Coding and Three Types\nof Computer-Assisted Text Analysis Methods.” Sociological\nMethods & Research, 0049124118769114.\n\n\nNeuendorf, Kimberly A. 2016. The Content Analysis Guidebook.\nSage.\n\n\nNielse, Aileen. 2021. Practical Fairness: Achieving Fair and Secure\nData Models. O’Reilly.\n\n\nNissim, Malvina, Rik van Noord, and Rob van der Goot. 2020. “Fair\nIs Better Than Sensational: Man Is to Doctor as Woman Is to\nDoctor.” Computational Linguistics 46 (2): 487–97.\n\n\nNivre, Joakim, and Chiao-Ting Fang. 2017. “Universal Dependency\nEvaluation.” In Proceedings of the NoDaLiDa 2017 Workshop on\nUniversal Dependencies (UDW 2017), 86–95.\n\n\nNoble, Safiya Umoja. 2018. Algorithms of Oppression: How Search\nEngines Reinforce Racism. NYU Press.\n\n\nO’Neil, Cathy. 2016. Weapons of Math Destruction: How Big Data\nIncreases Inequality and Threatens Democracy. Crown.\n\n\nPapakyriakopoulos, Orestis, Simon Hegelich, Juan Carlos Medina Serrano,\nand Fabienne Marco. 2020. “Bias in Word Embeddings.” In\nProceedings of the 2020 Conference on Fairness, Accountability, and\nTransparency, 446–57.\n\n\nPearl, Judea, and Dana Mackenzie. 2018. The Book of Why: The New\nScience of Cause and Effect. Basic books.\n\n\nPeixoto, Tiago. 2014. “Hierarchical Block Structures and\nHigh-Resolution Model Selection in Large Networks.” Physical\nReview X 4 (1): 011047.\n\n\nPeixoto, Tiago P. 2019. “Bayesian Stochastic\nBlockmodeling.” Advances in Network Clustering and\nBlockmodeling, 289–332.\n\n\nPerrin, Andrew J, and Stephen Vaisey. 2008. “Parallel Public\nSpheres: Distance and Discourse in Letters to the Editor.”\nAmerican Journal of Sociology 114 (3): 781–810.\n\n\nPerry, Brea, Bernice Pescosolido, Mario Small, and Ann McCranie. 2020.\n“Introduction to the Special Issue on Ego Networks.”\nNetwork Science 8 (2): 137–41.\n\n\nPrabhu, Vinay Uday, and Abeba Birhane. 2020. “Large Image\nDatasets: A Pyrrhic Win for Computer Vision?” arXiv Preprint\narXiv:2006.16923.\n\n\nPrell, Christina. 2012. Social Network Analysis: History, Theory and\nMethodology. Sage.\n\n\nPrice, Derek De Solla. 1965. “Networks of Scientific\nPapers.” Science, 510–15.\n\n\n———. 1986. Little Science, Big Science... And Beyond. Vol. 480.\nColumbia University Press New York.\n\n\nReed, Isaac Ariail. 2015. “Counting, Interpreting and Their\nPotential Interrelation in the Human Sciences.” American\nJournal of Cultural Sociology 3 (3): 353–64.\n\n\nRheault, Ludovic, and Christopher Cochrane. 2020. “Word Embeddings\nfor the Analysis of Ideological Placement in Parliamentary\nCorpora.” Political Analysis 28 (1): 112–33.\n\n\nRoberts, Margaret E, Brandon M Stewart, Dustin Tingley, Christopher\nLucas, Jetson Leder-Luis, Shana Kushner Gadarian, Bethany Albertson, and\nDavid G Rand. 2014. “Structural Topic Models for Open-Ended Survey\nResponses.” American Journal of Political Science 58\n(4): 1064–82.\n\n\nRoberts, Margaret, Brandon Stewart, Dustin Tingley, and Edoardo Airoldi.\n2013. “The Structural Topic Model and Applied Social\nScience.” In Advances in Neural Information Processing\nSystems Workshop on Topic Models: Computation, Application, and\nEvaluation, 4:1–20. Harrahs; Harveys, Lake Tahoe.\n\n\nRobins, Garry. 2015. Doing Social Network Research: Network-Based\nResearch Design for Social Scientists. Sage.\n\n\nRöder, Michael, Andreas Both, and Alexander Hinneburg. 2015.\n“Exploring the Space of Topic Coherence Measures.” In\nProceedings of the Eighth ACM International Conference on Web Search\nand Data Mining, 399–408.\n\n\nRosenblatt, Frank. 1958. “The Perceptron: A Probabilistic Model\nfor Information Storage and Organization in the Brain.”\nPsychological Review 65 (6): 386.\n\n\nRosen-Zvi, Michal, Thomas Griffiths, Mark Steyvers, and Padhraic Smyth.\n2012. “The Author-Topic Model for Authors and Documents.”\narXiv Preprint arXiv:1207.4169.\n\n\nRuder, Sebastian, Ivan Vulić, and Anders Søgaard. 2019. “A Survey\nof Cross-Lingual Word Embedding Models.” Journal of\nArtificial Intelligence Research 65: 569–631.\n\n\nRudin, Cynthia. 2019. “Stop Explaining Black Box Machine Learning\nModels for High Stakes Decisions and Use Interpretable Models\nInstead.” Nature Machine Intelligence 1 (5): 206–15.\n\n\nRumelhart, David, Geoffrey Hinton, and Ronald Williams. 1986.\n“Learning Representations by Back-Propagating Errors.”\nNature 323 (6088): 533–36.\n\n\nSalganik, Matthew. 2019. Bit by Bit: Social Research in the Digital\nAge. Princeton University Press.\n\n\nSalvatier, John, Thomas Wiecki, and Christopher Fonnesbeck. 2016.\n“Probabilistic Programming in Python Using Pymc.” PeerJ\nComputer Science 2: e55.\n\n\nScott, John. 2017. Social Network Analysis. Sage.\n\n\nSloan, Luke, and Anabel Quan-Haase. 2017. “A Retrospective on\nState of the Art Social Media Research Methods: Ethical Decisions,\nBig-Small Data Rivalries and the Spectre of the 6Vs.” The\nSAGE Handbook of Social Media Research Methods. Sage: London.\n\n\nSmall, Mario Luis. 2011. “How to Conduct a Mixed Methods Study:\nRecent Trends in a Rapidly Growing Literature.” Annual Review\nof Sociology 37: 57–86.\n\n\nSmall, Mario, Brea Perry, Bernice Pescosolido, and Ned Smith, eds. 2021.\nPersonal Networks: Classic Readings and New Directions in\nEgo-Centric Analysis. Cambridge University Press.\n\n\nSmith, Jackie, and Tina Fetner. 2009. “Structural Approaches in\nthe Sociology of Social Movements.” In Handbook of Social\nMovements Across Disciplines, 13–57. Springer.\n\n\nSnow, David, Robert Benford, Holly McCammon, Lyndi Hewitt, and Scott\nFitzgerald. 2014. “The Emergence, Development, and Future of the\nFraming Perspective: 25+ Years Since\" Frame Alignment\".”\nMobilization: An International Quarterly 19 (1): 23–46.\n\n\nSpillman, Lyn. 2015. “Ghosts of Straw Men: A Reply to Lee and\nMartin.” American Journal of Cultural Sociology 3 (3):\n365–79.\n\n\nSteed, Ryan, and Aylin Caliskan. 2021. “Image Representations\nLearned with Unsupervised Pre-Training Contain Human-Like\nBiases.” In Proceedings of the 2021 ACM Conference on\nFairness, Accountability, and Transparency, 701–13.\n\n\nStoltz, Dustin, and Marshall Taylor. 2019. “Concept Mover’s\nDistance: Measuring Concept Engagement via Word Embeddings in\nTexts.” Journal of Computational Social Science 2 (2):\n293–313.\n\n\nStovel, Katherine, and Lynette Shaw. 2012. “Brokerage.”\nAnnual Review of Sociology 38: 139–58.\n\n\nSweeney, Latanya. 2002. “K-Anonymity: A Model for Protecting\nPrivacy.” International Journal of Uncertainty, Fuzziness and\nKnowledge-Based Systems 10 (05): 557–70.\n\n\nSyed, Shaheen, and Marco Spruit. 2018. “Selecting Priors for\nLatent Dirichlet Allocation.” In 2018 IEEE 12th International\nConference on Semantic Computing (ICSC), 194–202. IEEE.\n\n\nTabachnick, Barbara G, Linda S Fidell, and Jodie B Ullman. 2007.\nUsing Multivariate Statistics. Vol. 5. Pearson Boston, MA.\n\n\nTatmam, Rachel. 2020. “What i Won’t Build.”\n\n\nTaylor, Marshall, and Dustin Stoltz. 2020. “Concept Class\nAnalysis: A Method for Identifying Cultural Schemas in Texts.”\nSociological Science 7: 544–69.\n\n\nTubaro, Paola, Louise Ryan, Antonio Casilli, and Alessio D’angelo. 2020.\n“Social Network Analysis: New Ethical Approaches Through\nCollective Reflexivity. Introduction to the Special Issue of Social\nNetworks.” Social Networks.\n\n\nUpsahl, Kurt. 2013. “Why Metadata Matters.” Electronic\nFrontier Foundation https://www.eff.org/deeplinks/2013/06/why-metadata-matters.\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential\nTools for Working with Data. \" O’Reilly Media, Inc.\".\n\n\nVasiliev, Yuli. 2020. Natural Language Processing with Python and\nSpaCy: A Practical Introduction. No Starch Press.\n\n\nVries, Terrance de, Ishan Misra, Changhan Wang, and Laurens van der\nMaaten. 2019. “Does Object Recognition Work for Everyone?”\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition Workshops, 52–59.\n\n\nWallach, Hanna M, David M Mimno, and Andrew McCallum. 2009.\n“Rethinking LDA: Why Priors Matter.” In Advances in\nNeural Information Processing Systems, 1973–81.\n\n\nWallach, Hanna, Iain Murray, Ruslan Salakhutdinov, and David Mimno.\n2009. “Evaluation Methods for Topic Models.” In\nProceedings of the 26th Annual International Conference on Machine\nLearning, 1105–12.\n\n\nWang, Chong, David Blei, and David Heckerman. 2012. “Continuous\nTime Dynamic Topic Models.” arXiv Preprint\narXiv:1206.3298.\n\n\nWard, Brian. 2021. How Linux Works: What Every Superuser Should\nKnow. no starch press.\n\n\nWest, Sarah Myers, Meredith Whittaker, and Kate Crawford. 2019.\n“Discriminating Systems.” AI Now.\n\n\nWeston, Sara J, Stuart J Ritchie, Julia M Rohrer, and Andrew K\nPrzybylski. 2019. “Recommendations for Increasing the Transparency\nof Analysis of Preexisting Data Sets.” Advances in Methods\nand Practices in Psychological Science 2 (3): 214–27.\n\n\nWhite, Harrison. 1992. Identity and Control: How Social Formations\nEmerge. Princeton university press.\n\n\nWittgenstein, Ludwig. 1953. “Philosophical Investigations: The\nEnglish Text of the Third Edition.”\n\n\nYpma, Tjalling J. 1995. “Historical Development of the\nNewton–Raphson Method.” SIAM Review 37 (4): 531–51.\n\n\nZimmer, Michael. 2010. “‘But the Data Is Already\nPublic’: On the Ethics of Research in Facebook.”\nEthics and Information Technology 12 (4): 313–25.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "acknowledgements.html",
    "href": "acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "Acknowledgements for the Continuous Development Edition\nTo come.",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "acknowledgements.html#acknowledgements-from-the-print-edition",
    "href": "acknowledgements.html#acknowledgements-from-the-print-edition",
    "title": "Acknowledgements",
    "section": "Acknowledgements from the Print Edition",
    "text": "Acknowledgements from the Print Edition\nThis book would not exist in its current form, or perhaps at all, without the support, advice, and contributions of a great many other people. I am extremely grateful to all of them.\nFirst and foremost, thanks to my PhD students Pierson Browne, Tyler Crick, and Alexander (Sasha) Graham for their many contributions to this book, especially in the final 6 months of work, when all three made a series of invaluable contributions to the project. Pierson Browne constructed the datasets used in the Bayesian regression modelling chapters and was instrumental in the development of all Bayesian data analysis chapters. Similarly, Tyler Crick was instrumental in the development of the chapters on stochastic block models, generative topic models, word embeddings, and transformers. He also put considerable effort into cleaning the Enron communication network. Sasha Graham was instrumental in the development of the chapters on network analysis and research ethics, and helped track the many complex knowledge and skill dependencies that stretch across all 33 chapters. He also helped identify something on the order of 50,000 cuttable words as production deadlines loomed. Pierson, Tyler, and Sasha have improved this book immeasurably, and I am very grateful for it. Thanks to Laine Bourassa and Alec for tolerating the same sort of working-all-hours nonsense that my partner has tolerated for far too long, even though they should not have had to.\nSecondly, thanks to Jillian Anderson for providing an extremely thorough technical review of the manuscript, especially given that the word count of the version she reviewed was closer to 300,000 than 200,000. She has improved the manuscript in many ways, including by sharing her thoughts on data science, machine learning, ethics, computation, and professional development in our many conversations over the years. I’m especially grateful for all the practical wisdom she shared while we were co-teaching at GESIS in Cologne in early 2020. Her experience and expertise has had a strong influence on how I’ve come to see the field.\nI would like to thank the students who have worked in my research lab (Netlab) at the University of Waterloo over the years, and who have all been centrally involved in my own learning process in computational social science. In addition to Pierson, Tyler, Sasha, and Jillian (who is now a big data developer at Simon Fraser University), I’d like to thank Reid McIlroy-Young, Joel Becker, Mumtahin Monzoor, Tiffany Lin, Rachel Wood, Alex de Witt, Alexis Foss Hill, Steve McColl, Brittany Etmanski, and Evaleen Hellinga. Thanks also to ‘honorary’ members of the lab, Yasmin Koop-Monteiro, Adam Howe, and Francois Lapachelle from the University of British Columbia, and Yixi Yang from Memorial University.\nThank you to all the undergraduate and graduate students who have taken courses in computational social science with me at the University of Waterloo. There are far too many to name, but I am especially grateful to those students whose feedback and level of engagement has resulted in substantial improvements to the material in this book. Thanks especially to Michelle Ashburner, Silas Tsui, Sunny Luo, Zane Elias, Ben Ang, Cassidy Raynham, Stephanie Reimer Rempel, Jordan Klassen, David Borkenhagen, Emilie Caron, Khoa Tran, Tyler Kruger, Nate Flatch, and Terry Zhang. Similarly, I would like to thank participants in workshops I’ve taught with preliminary chapter drafts at the University of Waterloo, the University of British Columbia, and at GESIS Leibniz Institute for the Social Sciences in Cologne. In particular, I would like to thank Stijn Daenekindt, Jessica Herzing, Tina Kretshel, Janice Aurini, Ali Marin, Eugena Kwon, Xiaowei Li, and Soli Dubash.\nMany colleagues and friends have improved the content in this book (sometimes even unwittingly) via conversation and in some cases directly commenting on chapter drafts. I am especially grateful to Allyson Stokes, Peter Carrington, Johan Koskinen, David Tindall, Anabel Quan-Haase, Lorien Jasny, Mario Diani, Tuomas Yla-Anttila, Antti Gronow, Lasse Folke-Henriksen, Andrew Osmond, Ian Milligan, Igor Grossman, Howard Ramos, Marlene Mauk, Rochelle Terman, Jennifer Earl, Tina Fetner, Heather Douglas, Bernie Hogan, Raphael Heiberger, Jan Riebling, Alix Rule, Bonnie Erickson, Qiang Fu, Luka Kronegger, Deena Abul Fottouh, Harry Collins, Rob Evans, Martin Innes, Kate Duart, Alun Preece, Darrin Durant, Martin Weinel, Nicky Priaulx, Nina Kolleck, Marty Cooke, Owen Gallupe, Vanessa Schweizer, Rob Gorbet, Katie Plaisance, Ed Jernigan, Rob Nolan, Rashmee Singh, and John Scott. Thanks also to the anonymous reviewers who provided extremely helpful comments on draft chapters.\nThanks to my colleagues in the Faculty of Environment (ENV) at the University of Waterloo for providing an intellectual home where interdisciplinary research and teaching is encouraged and valued. I’m especially grateful to the faculty, staff, and students in Knowledge Integration, who have led by interdisciplinary example, curious open-mindedness, and a willingness to slow down and learn from people who do things differently. Thanks also to my ENV friends and colleagues for providing inspiration and support, and for keeping things in perspective.\nAt Sage, I would like to thank Jai Seaman for believing in this project from the start and doing everything she could to support it, including during tough times when the work felt insurmountable. Jai’s editorial experience, expertise, patience, and goodwill helped make a book out of a sprawling collection of chapters. Charlotte Bush similarly played a central role in making this project a reality, and I am thankful to her for her expertise and patience. Thanks to Ian Antcliff for expertly guiding the manuscript through the production process despite disruptions related to COVID-19, and to everyone who contributed to the production of the book despite the challenges of the pandemic.\nThanks to my endlessly patient and understanding partner Allyson Stokes. She has supported and improved this project in many ways over the years, both as a partner and intellectually as a fellow sociologist. Finally, thank you to my friends and family for all the support, love, and much-needed fun.\nThis book was written using a combination of markdon, LaTeX, and Python written using open source text editors and Jupyter Notebooks, with pandoc for document conversion, git for version control, and conda and Docker for virtualization. Thanks to the many people who develop and maintain these and other open source tools.",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "38  Courses and Workshops",
    "section": "",
    "text": "38.1 Quantitative Research Methods\nTable 38.1 recommends a sequence of chapters suitable for a generic course in quantitative research methods at the undergraduate or graduate level. For an undergraduate course, I would rarely assign additional readings and would not emphasize coding skills or advanced computational methods and models. The content would be more or less the same as other quantitative methods courses, only taught in Python instead of R, Stata, or GUI applications like SPSS (which should be avoided, of course).\nThis schedule could easily be adapted. For example, more or less time could be spent introducing Python at the start of the course, or on collecting data from the web. The chapters on text analysis could be swapped out for network analysis instead, but my recommendation would be to stick with text as data in a course like this. Alternatively, those weeks could be cut entirely to create more time for teaching linear regression and multilevel models (considering that these chapters cover the full workflow for developing, assessing, and comparing models), or to create room for generalized linear models or causal analysis with observational data.\nI prefer to teach research ethics throughout the course and reserve dedicated space for deep discussions about the ethics and politics of research at the end of the course, when students have a bit of experience. However, this is just a personal preference—you could teach that content anywhere in the course sequence.",
    "crumbs": [
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Courses and Workshops</span>"
    ]
  },
  {
    "objectID": "courses.html#quantitative-research-methods",
    "href": "courses.html#quantitative-research-methods",
    "title": "38  Courses and Workshops",
    "section": "",
    "text": "12 Weeks\nAdaptable for Undergraduate or Graduate\n\n\n\n\n\n\n\nTable 38.1: Recommended Chapter Sequence for a Quantitative Research Methods course.\n\n\n\n\n\n\n\n\n\n\nWeek\nTopic\nChapters\n\n\n\n\n01\nIntroduction\n“Introduction” and “Getting started with research computing”\n\n\n02\nPython for researchers\n“Python 101” and “Python 102”\n\n\n03\nSampling and survey data\n“Sampling and survey data”\n\n\n04\nData from the web\n“Web data (APIs)” and “Web data (Scraping)”\n\n\n05\nExploratory data analysis (EDA)\n“Processing structured data” and “Exploratory data analysis and visualization”\n\n\n06\nExploratory data analysis (EDA)\n“Association and latent factors”\n\n\n07\nText as data\n“Text as data” and “Text similarity and latent semantic space”\n\n\n08\nInference\n“Prediction”, “Probability”, and “Credibility”\n\n\n09\nInference\n“Causality”\n\n\n10\nLinear models\n“Linear regression”\n\n\n11\nMultilevel models\n“Multilevel regression” and/or “Generalized linear models”\n\n\n12\nResearch ethics\n“Research ethics, politics, and practices”",
    "crumbs": [
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Courses and Workshops</span>"
    ]
  },
  {
    "objectID": "courses.html#introduction-to-computational-social-science-undergraduate-12-weeks",
    "href": "courses.html#introduction-to-computational-social-science-undergraduate-12-weeks",
    "title": "38  Courses and Workshops",
    "section": "38.2 Introduction to Computational Social Science (Undergraduate, 12 Weeks)",
    "text": "38.2 Introduction to Computational Social Science (Undergraduate, 12 Weeks)\n\n12 Weeks\nUndergraduate\n\nThis sequence introduces undergraduates to computational social science by blending foundational coding skills, data collection techniques, and an introduction to core computational methods. The course focuses on simple but powerful methods that build the foundation for more advanced work later. Depending on the focus of the course, the instructor could switch out the chapters on text or networks, and more time could be allocated to specific areas like machine learning or causal inference.\n\n\n\nTable 38.2: Recommended Chapter Sequence for an Undergraduate Introduction to Computational Social Science course.\n\n\n\n\n\n\n\n\n\n\nWeek\nTopic\nChapters\n\n\n\n\n01\nIntroduction\n“Introduction” and “Getting started with research computing”\n\n\n02\nPython basics\n“Python 101” and “Python 102”\n\n\n03\nData collection\n“Sampling and survey data”\n\n\n04\nWeb data\n“Web data (APIs)” and “Web data (Scraping)”\n\n\n05\nExploratory data analysis\n“Exploratory data analysis and visualization”\n\n\n06\nWorking with structured data\n“Processing structured data”\n\n\n07\nText analysis introduction\n“Text as data”\n\n\n08\nText analysis advanced\n“Text similarity and latent semantic space”\n\n\n09\nIntroduction to networks\n“Social networks and relational thinking”\n\n\n10\nMachine learning basics\n“Machine and statistical learning”\n\n\n11\nPrediction and inference\n“Prediction” and “Probability”\n\n\n12\nResearch ethics\n“Research ethics, politics, and practices”",
    "crumbs": [
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Courses and Workshops</span>"
    ]
  },
  {
    "objectID": "courses.html#introduction-to-computational-social-science-graduate-12-weeks",
    "href": "courses.html#introduction-to-computational-social-science-graduate-12-weeks",
    "title": "38  Courses and Workshops",
    "section": "38.3 Introduction to Computational Social Science (Graduate, 12 Weeks)",
    "text": "38.3 Introduction to Computational Social Science (Graduate, 12 Weeks)\n\n12 Weeks\nGraduate\n\nThis sequence is designed for a graduate-level introduction to computational social science. While many of the topics overlap with the undergraduate version, this course delves deeper into advanced methods, placing a stronger emphasis on model-based inference, causality, and latent structure analysis. Instructors may want to adjust the number of weeks spent on programming based on the cohort’s experience level.\n\n\n\nTable 38.3: Recommended Chapter Sequence for a Graduate Introduction to Computational Social Science course.\n\n\n\n\n\n\n\n\n\n\nWeek\nTopic\nChapters\n\n\n\n\n01\nIntroduction\n“Introduction” and “Getting started with research computing”\n\n\n02\nPython for researchers\n“Python 101” and “Python 102”\n\n\n03\nData collection\n“Sampling and survey data”\n\n\n04\nWeb data\n“Web data (APIs)” and “Web data (Scraping)”\n\n\n05\nExploratory data analysis\n“Exploratory data analysis and visualization”\n\n\n06\nLatent factors\n“Association and latent factors”\n\n\n07\nText analysis\n“Text as data” and “Text similarity and latent semantic space”\n\n\n08\nNetwork analysis\n“Social networks and relational thinking” and “Structural similarity and latent social space”\n\n\n09\nMachine learning\n“Machine and statistical learning”\n\n\n10\nPrediction and inference\n“Prediction”, “Probability”, and “Credibility”\n\n\n11\nCausal inference\n“Causality” and “Causal analysis”\n\n\n12\nResearch ethics\n“Research ethics, politics, and practices”",
    "crumbs": [
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Courses and Workshops</span>"
    ]
  },
  {
    "objectID": "courses.html#text-as-data-computational-text-analysis-12-weeks",
    "href": "courses.html#text-as-data-computational-text-analysis-12-weeks",
    "title": "38  Courses and Workshops",
    "section": "38.4 Text as Data / Computational Text Analysis (12 Weeks)",
    "text": "38.4 Text as Data / Computational Text Analysis (12 Weeks)\n\n12 Weeks\nAdaptable for Undergraduate or Graduate\n\nThis course focuses on computational text analysis. It is suitable for both undergraduate and graduate students, though graduate students might dive more deeply into advanced modeling techniques such as Structural Topic Models (STM) and transformer-based models. Instructors may want to adjust the depth of machine learning content based on the course’s focus.\n\n\n\nTable 38.4: Recommended Chapter Sequence for a Text as Data course.\n\n\n\n\n\n\n\n\n\n\nWeek\nTopic\nChapters\n\n\n\n\n01\nIntroduction to text analysis\n“Introduction” and “Text as data”\n\n\n02\nPython basics for text\n“Python 101” and “Python 102”\n\n\n03\nText pre-processing\n“Text as data”\n\n\n04\nExploratory text analysis\n“Text similarity and latent semantic space”\n\n\n05\nTopic modeling with LDA\n“Latent topics in text (LDA)”\n\n\n06\nStructural topic models (STM)\n“Latent topics in text (STM)”\n\n\n07\nMachine learning for text\n“Machine and statistical learning”\n\n\n08\nNeural networks for NLP\n“Artificial neural networks (FNN, RNN, CNN)”\n\n\n09\nTransformers and self-attention\n“Transformers and self-attention”\n\n\n10\nAdvanced applications\n“Latent topics in text and images using transformers”\n\n\n11\nInference and causality\n“Prediction”, “Probability”, “Credibility”, and “Causality”\n\n\n12\nResearch ethics\n“Research ethics, politics, and practices”",
    "crumbs": [
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Courses and Workshops</span>"
    ]
  },
  {
    "objectID": "survey-data.html",
    "href": "survey-data.html",
    "title": "7  Survey Data",
    "section": "",
    "text": "7.1 Foundations of survey methodology: The total survey error framework\nThe Total Survey Error (TSE) framework is a comprehensive approach used in survey methodology to identify, evaluate, and minimize the errors that can occur throughout the survey process, compromising validity and reliability. These series of figures below – reproduced / adapted from Groves et al. (2011) Survey Methodology – illustrate different aspects of the survey lifecycle and identifies critical points where errors may arise, both in the design and execution of a survey.\nFigure 7.1 illustrates the survey lifecycle from a design perspective, broken down into two main components: measurement** and representation. On one hand, measurement starts from the construct—the concept researchers aim to measure—and moves through the stages of actual response collection to the final survey statistic. Errors in this part of the process include measurement error, which occurs when there is a discrepancy between the concept researchers want to measure and what is actually measured (due to poorly designed questions, interviewer bias, etc.). On the other hand, Representation starts with the target population and moves through sampling to the final respondents who actually complete the survey. At each stage, sampling error, coverage error, and nonresponse error can occur, affecting how well the sample represents the broader population. Finally, post-survey adjustments help mitigate errors from nonresponse or misrepresentation but can also introduce adjustment error if done incorrectly.\nFigure 7.2 outlines the stages involved in conducting a survey, starting with defining research objectives, and moving through sampling, questionnaire construction, data collection, data processing, and analysis. At the beginning of this process, researchers define research objectives that guide decisions about the sampling frame and mode of collection (in-person, telephone, online, etc.). These decisions influence both coverage and nonresponse errors. The process also includes pretesting the questionnaire to ensure clarity and reliability, which is an important step to minimize measurement error—errors that arise due to poorly worded questions or ambiguous response options. Finally, data adjustments and post-survey edits address additional errors that might occur, such as processing errors or misreporting during data entry.\nFigure 7.3 offers a closer look at the survey lifecycle, specifically through the lens of errors that can affect measurement and representation. On the measurement side, errors are introduced from the moment the construct is conceptualized through measurement validity (whether the survey measures what it is supposed to) and measurement error (differences between the true value and the reported value). These can arise due to issues like unclear questions, recall bias, or misunderstanding by respondents. On the representation side, errors can occur at several stages:\nFinally, Figure 7.4 highlights coverage error, more specifically the relationship between a target population (the group you want to generalize to) and the frame population (the group you can actually sample from). The covered population is the overlap between the target and frame populations, representing the portion of the target population that is actually included in the sampling frame. Undercoverage occurs when parts of the target population are not included in the sampling frame, meaning some eligible respondents are left out of the survey. A related problem is overcoverage, which can result when ineligible units are mistakenly included in the sampling frame even though they do not belong to the target population. Minimizing coverage error is critical for ensuring that survey results can be generalized accurately to the broader population.\nOverall, the Total Survey Error framework emphasizes that errors can arise at all stages of the survey process, from design to data collection, and from data processing to analysis. It classifies errors broadly into representation errors (which occur due to issues with sampling and coverage) and measurement errors (which arise due to problems in how responses are collected and interpreted). Understanding and mitigating each type of error is crucial for improving survey accuracy. For example, careful question design (addressed in the article content you referenced on question characteristics and methods for assessing them) can reduce measurement error, while using appropriate sampling methods and post-survey adjustments can mitigate problems with coverage error and nonresponse error.",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Survey Data</span>"
    ]
  },
  {
    "objectID": "survey-data.html#imports",
    "href": "survey-data.html#imports",
    "title": "6  Processing Structured Data Survey Data",
    "section": "7.4 Imports",
    "text": "7.4 Imports\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom dcss import download_dataset\nfrom dcss import set_style\nset_style()",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>~~Processing Structured Data~~ Survey Data</span>"
    ]
  },
  {
    "objectID": "survey-data.html#practical-pandas-first-steps",
    "href": "survey-data.html#practical-pandas-first-steps",
    "title": "6  Processing Structured Data Survey Data",
    "section": "7.5 PRACTICAL PANDAS: FIRST STEPS",
    "text": "7.5 PRACTICAL PANDAS: FIRST STEPS\n\n7.5.1 Getting Data into Pandas\nThe Pandas package makes it easy to load data from an external file directly into a dataframe object. It uses one of many reader functions that are part of a suite of I/O (input / output, read / write) tools. I’ve listed some common examples in the table below. Information on these and other reader functions can be found in the Pandas documentation, which also provides useful information about the parameters for each method (e.g. how to specify what sheet you want from an Excel spreadsheet, or whether to write the index to a new CSV file).\n\nI/0 Methods for Pandas\n\n\nData Description\nReader\nWriter\n\n\n\n\nCSV\nread_csv()\nto_csv()\n\n\nJSON\nread_json()\nto_json()\n\n\nMS Excel and OpenDocument (ODF)\nread_excel()\nto_excel()\n\n\nStata\nread_stata()\nto_stata()\n\n\nSAS\nread_sas()\nNA\n\n\nSPSS\nread_spss()\nNA\n\n\n\nI will focus on the read_csv() function to demonstrate the general process. The only required argument is that we provide the path to the file location, but there are many useful arguments that you can pass, such as the file encoding. By default, Pandas assumes your data is encoded with UTF-8. If you see an encoding error or some strange characters in your data, you can try a different encoding, such as latin1.\nThis chapter will use data from the Varities of Democracy (VDEM) dataset. VDEM is an ongoing research project to measure the level of democracy in governments around the world and updated versions of the dataset are released on an ongoing basis. The research is led by a team of over 50 social scientists who coordinate the collection and analysis of expert assessments from over 3,200 historians and Country Experts (CEs). From these assessments, the VDEM project has created a remarkably complex array of indicators designed to align with five high-level facets of democracy: electoral, liberal, participatory, deliberative, and egalitarian. The dataset extends back to 1789 and is considered the gold standard of quantitative data about global democratic developments. You can find the full codebook online, and I strongly recommend that you download it and consult it as you work with this data. You can find the full dataset at (https://www.v-dem.net/en/data/data/v-dem-dataset-v11/) and the codebook here (https://www.v-dem.net/figures/filer_public/e0/7f/e07f672b-b91e-4e98-b9a3-78f8cd4de696/v-dem_codebook_v8.pdf). The filtered and subsetted version we will use in this book can be downloaded using the download_dataset() function. Note that this will also download additional VDEM materials, including the codebook.\nLet’s load the CSV file into a Pandas dataframe.\nvdem_data_url = \"https://www.dropbox.com/scl/fo/6ay4x2qo4svyo92wbvlxt/ACtUxCDoLYxLujkekHdXiJ4?rlkey=lhmhiasjkv3ndvyxjxapi24sk&st=2p76a0dw&dl=0\"\n\ndownload_dataset(\n    vdem_data_url,\n    save_path='data/vdem'\n)\ndf = pd.read_csv(\n    'data/vdem/V-Dem-CY-Full+Others-v10.csv',\n    low_memory=False\n)\nOnce you have your data loaded, one of the first things you will want to know is how many rows and columns there are. You can do this using the .shape attribute of the dataframe.\ndf.shape\nThis is a fairly large dataset. It has 27,013 observations and 4,108 variables! First, I will construct a new dataframe from this one that contains only the columns I want.\n\n\n7.5.2 What Do You Need? Selecting Columns\nI will create a list of the variable names I want to retain, and call the original dataframe followed by the name of the list in square brackets. In this case, I will retain the following variables:\n\nthe country name,\nthe country ID,\nthe geographic region,\nthe year,\nthe polyarchy index,\nthe liberal democracy index,\nthe participatory democracy index,\nthe deliberative democracy index, and\nthe egalitarian democracy index,\nwhether Internet users’ privacy and their data is legally protected,\nhow polarized the country is on political issues, and\nlevels of political violence.\nwhether or not the country is a democracy\n\nI will call the new dataframe sdf, for ‘subsetted dataframe.’ Of course, you can call it anything you like. If you are going to be working with multiple dataframes in the same script or notebook, then it’s a good idea to give them much more descriptive names. For now, I am only working with two, so I will use df for the full dataframe and sdf for the dataframe with a subset of the original variables. I will make careful note of any dataframes I add.\nsubset_vars = ['country_name', 'country_text_id', 'e_regiongeo', 'year', 'v2x_polyarchy', 'v2x_libdem', 'v2x_partipdem', 'v2x_delibdem', 'v2x_egaldem', 'v2smprivex', 'v2smpolsoc', 'v2caviol', 'e_boix_regime']\nsdf = df[subset_vars]\nsdf.shape\nWe’ve created a new dataframe called sdf. It still has 27,013 rows, but only 13 variables. We can print their names using the .columns attribute for the dataframe.\nlist(sdf.columns)\n\n7.5.2.1 What’s in Your dataframe?\nWe can use the .info() method to see: the total number of observations, the total number of columns, the names of the columns, the number of non-missing observations for each, the datatype for each variable, the number of variables that contain data of each type (e.g. integers and floats), and the total amount of memory used by the dataframe.\nsdf.info()\nThe datatypes in this dataframe are float64 (numbers with decimals), int64 (integers), and object. In Pandas, object refers to columns that contain strings, or mixed types, such as strings and integers (object encompasses many more things, too: it’s a catchall category). Pandas can also work with booleans (True or False), categorical variables, and some specialized datetime objects. Recall how we selected columns to make our dataset. In the code below, I use the same idea to show only a few variables, rather than all 35, to save space. We will explain this a little more later in the chapter.\nWe can also use the .describe() method to get summary information about the quantitative variables in our dataset, including the number of non-missing information, the mean and standard deviation, and a five number summary:\nsdf[['e_regiongeo', 'year', 'v2x_polyarchy']].describe()\n\n\n\n7.5.3 Heads, Tails, and Samples\nWe can also inspect the “head” or the “tail” of our dataframe using the .head() and .tail() methods, which default to the first or last 5 rows in a dataframe unless you provide a different number as an argument, such as .head(10).\nsdf[['country_name', 'year', 'v2x_libdem']].head()\nsdf[['country_name', 'year', 'v2x_libdem']].tail(3)\nIf you would prefer a random sample of rows, you can use the .sample() method, which requires you to specify the number of rows you want to sample.\nsdf[['country_name', 'year', 'v2x_libdem']].sample(15)\n\n\n7.5.4 What Do You Need? Filtering Rows\nWhen we executed the .describe() method earlier, you may have noticed that the range for the year variable is 1789-2019. Let’s say we have a good reason to focus on the years from 1900-2019. We will have to filter the data to have only the rows that meet my needs.\nThere are several ways to filter rows, including slices (e.g. all observations between index \\(i\\) and index \\(j\\)), or according to some sort of explicit condition, such as “rows where the year &gt;= 1900.” Note that when we filter or slice a dataframe, the new object is just a view of the original and still refers to the same data. Pandas will warn us if we try to modify the filtered object, so a lot of the time, things are smoother if we make a new copy.\nrowfilter = sdf['year'] &gt;= 1900\nfsdf = sdf[rowfilter].copy()\nfsdf.info()\nWe could also do this using the .query() method, which accepts a boolean expression as a string.\nalternate_fsdf = sdf.query('year &gt;= 1900').copy()\nalternate_fsdf.info()\nOur final dataframe – which I have called fsdf for filtered and subsetted dataframe – now has 13 columns (from 4,108) and 18,787 observations (from 27,013).\n\n\n7.5.5 Writing Data to Disk\nJust as I read our initial CSV file into Pandas using the read_csv() function, I can write this new dataframe to disk using the write_csv() function.\nfsdf.to_csv('data/vdem_filtered_subset.csv', index=False)",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>~~Processing Structured Data~~ Survey Data</span>"
    ]
  },
  {
    "objectID": "survey-data.html#understanding-pandas-data-structures",
    "href": "survey-data.html#understanding-pandas-data-structures",
    "title": "6  Processing Structured Data Survey Data",
    "section": "7.6 UNDERSTANDING PANDAS DATA STRUCTURES",
    "text": "7.6 UNDERSTANDING PANDAS DATA STRUCTURES\nNow let’s discuss Pandas’ main data structures, Series and DataFrames, and how they relate to one another.\n\n7.6.1 The Series\nEach column in a dataframe is an object called a Series. A Series is a one-dimensional object (e.g. a vector of numbers) with an index, which is itself a vector, or array, of labels.\nFor example, the column v2x_delibdem in fsdf is a Series containing floats and the index label for each observation. Printing a sample of 15 observations gives me a numerical index for each observation on the left and the actual value on the right. The index values are ordered in the Series itself, but they are out of sequence here because we pulled a random sample. As this is for demonstration purposes, I’ve included a random_state value to ensure you get the same sample that I do if you re-run this block.\nfsdf['v2x_delibdem'].sample(15, random_state = 42)\nIn most cases, the default index for a Series or dataframe is an immutable vector of integers:\nfsdf.index\nWe can easily modify an index so that it is made of up some other type of vector instead, including a string. Surprisingly, index values do not need to be unique. This enables some powerful techniques, but most of the time, you should avoid manually changing indexes.\n\n7.6.1.1 Accessing a Specific Row by its Index\nWe can use the index to retrieve specific rows from a dataframe or specific values from a Series, much as we would if we were selecting an element from a list, tuple, or array. The easiest way to do this is to pass the index value (e.g. 202) to .loc[]. As you can see below, the result is the observation-specific value for each variable in the dataframe.\nfsdf.loc[202]\nfsdf['v2x_delibdem'].loc[202]\nfsdf['v2x_delibdem'].loc[20000]\nNote that .loc does not refer to the 202nd row of the dataframe. If you were looking closely at the .index command above, you might have noticed the dataframe only contains 18,787 rows but .loc can still return row 20,000 - the index didn’t change when you removed a bunch of rows from the dataframe. Think of .loc as accessing a dictionary of the index values - it will even give a KeyError if you ask for an element that doesn’t exist.\nInstead, if we want the access the n-th row of a dataframe, we can use .iloc[n]. Think of the index as a list and you’re referring to an element of that list by its list index. Let’s use .iloc to select the last element in the dataframe. Note that the index position for the last element will be 18,786 even though the dataframe length is 18,787, because Python data structures are almost always 0-indexed. Here you see the index of the row, which was formerly the row number, as the Name at the bottom.\nfsdf.iloc[18786]\nIf there isn’t a reason to retain the original indexing of the unfiltered dataframe, it’s usually a good idea to reset the index.\nfsdf.reset_index(inplace = True, drop = True)\nfsdf.loc[18786]\nAfterwards, .loc and .iloc become fairly interchangeable, with a few exceptions: .loc has dictionary-like capabilities whereas .iloc is more list-like. Now, let’s take a closer look at the dataframe.\n\n\n\n7.6.2 Dataframes\nDataframes in Pandas are really just collections of Series that are aligned on the same index values. In other words, the Series we worked with previously have their own indices when we work with them as standalone Series, but in the fsdf dataframe, they share an index.\nAs you’ve already seen, dataframes are organized with variables in the columns and observations in the rows, and you can grab a single Series from a dataframe using square brakets – let’s do that now, using the fsdf dataframe:\ndeliberative = fsdf['v2x_delibdem']\nNote that we can also use dot notation to select columns. fsdf.v2x_delibdem is functionally equivalent to fsdf['v2x_delibdem'], and may be used interchangeably.\nWe are not limited to selecting columns that already exist in our dataset. You can also create and add new ones. For example, you can create a new column called “21 Century” and assign Boolean value based on whether the observation is in the 2000s.\nfsdf['21 Century'] = fsdf['year'] &gt;= 2000\nfsdf[['21 Century']].value_counts().reset_index()\nSometimes, the new columns created are transformations of a Series that already exists in the dataframe. For example, you can create a new missing_political_violence_data column which will be True when the v2caviol Series (levels of political violence) is empty and False otherwise.\nfsdf['missing_political_violence_data'] = fsdf['v2caviol'].isna()\nfsdf['missing_political_violence_data'].value_counts().reset_index()\nAs you can see from executing value_counts(), there is missing data on levels of political violence for 6042 observations.\n\n\n7.6.3 Missing Data\nIt’s important to understand how missing data is handled. Missing data is common in real-world datasets, and it can be missing for multiple reasons! Generally, Pandas uses the np.nan value to represent missing data. NumPy’s np.nan value is a special case of a floating point number representing an unrepresentable value. These kinds of values are called NaNs (Not a Number).\nimport numpy as np\n\ntype(np.nan)\nnp.nan cannot be used in equality tests, since any comparison to a np.nan value will evaluate as False. This includes comparing np.nan to itself.\nn = np.nan\nn == n\nnp.nan values do not evaluate to False or None. This can make it difficult to distinguish missing values. You can use the np.isnan() function for this purpose, and it is especially useful in control flow.\nif np.nan is None:\n    print('NaN is None')\nif np.nan:\n    print('NaN evaluates to True in control flow')\nif np.isnan(np.nan):\n    print('NaN is considered a NaN value in NumPy')\nAdditionally, np.nan values are generally excluded from Pandas functions that perform calculations over dataframes, rows, or columns. For example, documentation often stipulates that a calculation is done over all values, excluding NaN or NULL values.\ntotal = len(fsdf['v2caviol'])\ncount = fsdf['v2caviol'].count()\nprint(f'Total: {total}')\nprint(f'Count: {count}')\nprint(f'Diff: {total-count}')\nThe total number of items in the v2caviol column (political violence) is much higher than the counts received from the count() function. If what we learned above is correct, this difference should be accounted for when we discover how many items in this column are NaNs.\nnans = fsdf['v2caviol'].isna().sum()\nprint(' NaNs: {}'.format(nans))\nAs you can probably tell, the .isna() method, which is similar to np.isnan() but covers additional cases, can be very useful in transforming and filtering data.",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>~~Processing Structured Data~~ Survey Data</span>"
    ]
  },
  {
    "objectID": "survey-data.html#aggregation-grouped-operations",
    "href": "survey-data.html#aggregation-grouped-operations",
    "title": "6  Processing Structured Data Survey Data",
    "section": "7.7 AGGREGATION & GROUPED OPERATIONS",
    "text": "7.7 AGGREGATION & GROUPED OPERATIONS\nData analysis projects often involve aggregation or grouped operations. For example, we might want to compute and compare summary statistics for observations that take different values on a categorical variable. It can be helpful to be able to carve up the dataset itself, performing operations on different subsets of data. We’re going to do that using the .groupby() method, which partitions the dataframe into groups based on the values of a given variable. We can then perform operations on the resulting groups. Let’s group our countries into geographic regions using the e_regiongeo variable.\ngrouped = fsdf.groupby('e_regiongeo')\nThe above code returns a grouped object that we can work with. Let’s say we want to pull out a specific group, like South East Asia, which is represented in the data using the numerical ID 13. I know this because the relevant information is provided in the VDEM codebook, which I suggest you keep open whenever you are working with the VDEM data.\nWe can use the get_group() method to pull a group from the grouped object. (Note that the .get_group() code below is equivalent to fsdf[fsdf['e_regiongeo'] == 13].)\nsouth_east_asia = grouped.get_group(13)\nsouth_east_asia[['country_name', 'year', 'e_boix_regime']].head()\nThe data stored in south_east_asia are all of the observations of South East Asian countries in the VDEM data, stored now in their own dataframe. .get_group() is yet another way to extract a subset of a dataframe (by way of a groupby object), and is especially useful when the subset of data you want to work with is only observations with a particular value for a categorical variable in your data.\nGenerally speaking, when we group a dataset like this it’s because we want to compute something for a group within the dataset, or for multiple groups that we want to compare. We can do this by specifying the grouped object, the Series we want to perform an operation on, and finally the operation we want to perform. For example, let’s compute the median polyarchy score for countries in each of the regions in the dataset.\npoly = grouped['v2x_polyarchy'].median()\npoly.head()\nIt would be more useful to see the name of the region rather than its numeric label. We can do this by creating a dictionary that maps the numeric IDs to the region name, and then use the .map() method to tell Pandas were to lookup the values it needs to create a new column with the country names. First, the dictionary:\nregions = {\n    1:'Western Europe',\n    2:'Northern Europe',\n    3:'Southern Europe',\n    4:'Eastern Europe',\n    5:'Northern Africa',\n    6:'Western Africa',\n    7:'Middle Africa',\n    8:'Eastern Africa',\n    9:'Southern Africa',\n    10:'Western Asia',\n    11:'Central Asia',\n    12:'East Asia',\n    13:'South-East Asia',\n    14:'South Asia',\n    15:'Oceania', # (including Australia and the Pacific)\n    16:'North America',\n    17:'Central America',\n    18:'South America',\n    19:'Caribbean' # (including Belize Cuba Haiti Dominican Republic)\n}\nAnd now we can pass this dictionary into the .map() method applied to the fsdf['e_regiongeo'] Series, creating a new Series called fsdf['Region']\nfsdf['Region'] = fsdf['e_regiongeo'].map(regions)\nIt is also possible to group by multiple variables, such as geographic region and year, and then perform an operation on those slightly more fine-grained groups. This will result in 2,211 groups, so we will preview a random sample of 10.\ngrouped = fsdf.groupby(['Region', 'year'])\npoly = grouped['v2x_polyarchy'].median()\npoly.reset_index()\npd.DataFrame(poly).reset_index().sample(10)\nWe can perform other types of operations on the grouped object itself, such as computing the number of observations in each group (equivalent to value_counts()).\ngrouped.size().sort_values(ascending=False)\nFinally, we can perform multiple operations on a grouped object by using the agg() method. The agg() method will apply one or more aggregate functions to a grouped object, returning the results of each.\n#| warning: false\nwith_agg = grouped['v2x_polyarchy'].agg([min, np.median, 'max', 'count'])\nwith_agg.reset_index().sample(10)\nWe can even define our own function for agg() to use! If we’re willing to pass a dictionary, .agg() also lets us apply different functions to multiple variables at the same time! Instead of passing one list per function, you can use a dictionary where the column names are the keys and the functions are the values (you can also pass a list of functions) to perform some truly involved aggregration all in one line of code.",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>~~Processing Structured Data~~ Survey Data</span>"
    ]
  },
  {
    "objectID": "survey-data.html#working-with-time-series-data",
    "href": "survey-data.html#working-with-time-series-data",
    "title": "6  Processing Structured Data Survey Data",
    "section": "7.8 WORKING WITH TIME SERIES DATA",
    "text": "7.8 WORKING WITH TIME SERIES DATA\nMany real world datasets include a temporal component. This is especially true if you are working with data that comes from the web, which may have precise timestamps for things like the time an email was sent, or a news story was published. Strings are often used to store dates and times, but this is not ideal because strings don’t take advantage of the unique properties of time. It is difficult to sort dates if they are stored in strings with strange formats, for example.\n\"Monday Mar 2, 1999\" &gt; \"Friday Feb 21, 2020\"\nExtracting features like day, month, or timezone from strings can be time-consuming an error-prone. This is why Pandas and Python have implemented special types for date/time objects, called [Timestamp](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html) and [Datetime](https://docs.python.org/2/library/datetime.html), respectively. These are essentially equivalent to one another.\nThe VDEM data contains an enormous amount of temporal data, but all at the level of the year. Let’s switch over to a different dataset that has more fine-grained temporal data, and more closely resembles data that you would obtain from the web. In this case, we are going to use some data on Russian information operations targeting the 2016 American Presidential Election. You can read a bit about this data on the FiveThirtyEight blogpost Why We’re Sharing 3 Million Russian Troll Tweets.\nUnlike the VDEM data, the Russian Troll Tweets come as a collection of csv files. We will use a clever little trick to load up all the data in a single dataframe. The code block below iterates over each file in the russian-troll-tweets/ subdirectory in the data directory. If the file extension is csv, is reads the csv into memory as a dataframe. All of the dataframes are then concatenated into a single dataframe containing data on ~ 3M tweets.\nrussian_troll_data_url = \"https://www.dropbox.com/scl/fo/a3uxioa2wd7k8x8nas0iy/AH5qjXAZvtFpZeIID0sZ1xA?rlkey=p1471igxmzgyu3lg2x93b3r1y&st=xvhtn8gi&dl=0\"\n\ndownload_dataset(\n    russian_troll_data_url,\n    save_path='data/russian_troll_tweets'\n)\ndata_dir = os.listdir(\"data/russian_troll_tweets/\")\n\nfiles = [f for f in data_dir if 'csv' in f]\n\ntweets_df = pd.concat((pd.read_csv(\n    f'{\"data/russian_troll_tweets/\"}/{f}',\n    encoding='utf-8', low_memory=False) for f in files), ignore_index=True)\n\ntweets_df.info()\nAs you can see, we have two datatypes in our dataframe: object and int64. Remember that Pandas uses object to refer to columns that contain strings, or which contain mixed types, such as strings and integers. In this case, they refer to strings.\nOne further thing to note about this dataset: each row is a tweet from a specific account, but some of the variables describe attributes of the tweeting accounts, not of the tweet itself. For example, followers describes the number of followers that the account had at the time it sent the tweet. This makes sense, because tweets don’t have followers, but accounts do. We need to keep this in mind when working with this dataset.\nWe can convert date strings from a column or Series into Timestamps using the to_datetime function. We will do that here, assigning the new datetime objects to new variables. Note that this code will take a bit of time to run when executed on all 3 million tweets (if your computer isn’t the strongest, you might want to consider first using tweets_df.sample() to reduce the size of the dataframe).\ntweets_df['dt_publish_date'] = pd.to_datetime(tweets_df['publish_date'])\ntweets_df['dt_harvested_date'] = pd.to_datetime(tweets_df['harvested_date'])\ntweets_df[['author', 'content', 'publish_date']].sample(5)\nIn order, the datetime object fields are as follows: year-month-day hour:minute:second:microsecond. To retrieve an integer corresponding to the month when the tweet was published:\ntweets_df['dt_publish_date'].dt.month\nWhen our date and time variables are stored as datetime objects, we can access many time-specific attributes using dot notation. The Pandas documentation includes many examples of the kinds of temporal units and other functionality.\nWe can also sort our dataframe based on publish_date because Pandas knows that it is working with datetime objects.\nsorted_df = tweets_df.sort_values(['dt_publish_date'])\nWe can also add and subtract datetime columns to create new columns.\ntweets_df['days_until_harvest'] = tweets_df['dt_harvested_date'] - tweets_df['dt_publish_date']\ntweets_df['days_until_harvest'].sample(10)\nLet’s create new variables for the Year, Month, and Day each tweet was created on. We can do this by using the year, month, and day attributes on the datetime object.\ntweets_df['Year'] = tweets_df['dt_publish_date'].dt.year\ntweets_df['Month'] = tweets_df['dt_publish_date'].dt.month\ntweets_df['Day'] = tweets_df['dt_publish_date'].dt.day\nPandas offers specialized tools for grouping data into various segments of time. This involves converting a time series at one level into another (e.g. from days to weeks), and is known as resampling. Within resampling broadly, upsampling aggregates dates / times and downsampling disaggregates dates / times. Let’s upsample our data to plot the number of Tweets per day.\nThe first thing we will do is use the datetime object dt_publish_date as an index. This will let us easily group observations by resampling dates.\ntweets_df = tweets_df.set_index('dt_publish_date')\nWe can now use the .resample() method with the argument D to specify that we want to group by day. The table below provides some other options you can use when resampling dates.\n\nUnits of time in Pandas. You can use any of these units to upsample or downsample temporal data.\n\n\nValue\nDescription\n\n\n\n\nB\nbusiness day frequency\n\n\nC\ncustom business day frequency (experimental)\n\n\nD\ncalendar day frequency\n\n\nW\nweekly frequency\n\n\nM\nmonth end frequency\n\n\nBM\nbusiness month end frequency\n\n\nCBM\ncustom business month end frequency\n\n\nMS\nmonth start frequency\n\n\nBMS\nbusiness month start frequency\n\n\nCBMS\ncustom business month start frequency\n\n\nQ\nquarter end frequency\n\n\nBQ\nbusiness quarter endfrequency\n\n\nQS\nquarter start frequency\n\n\nBQS\nbusiness quarter start frequency\n\n\nA\nyear end frequency\n\n\nBA\nbusiness year end frequency\n\n\nAS\nyear start frequency\n\n\nBAS\nbusiness year start frequency\n\n\nBH\nbusiness hour frequency\n\n\nH\nhourly frequency\n\n\nT\nminutely frequency\n\n\nS\nsecondly frequency\n\n\nL\nmilliseonds\n\n\nU\nmicroseconds\n\n\nN\nnanosecondsa\n\n\n\nWe will also use the .size() method to determine the number of tweets that were produced each day.\ngrouped_cal_day = tweets_df.resample('D').size()\ngrouped_cal_day\nAt this point, we are going to visualize the results of our work with a line plot. We are going to do this with the Seaborn and matplotlib packages, which we will discuss in the next chapter. For now, focus on the visualization and ignore the code. The code blocks below produces Figures Figure 8.1 and Figure 8.2.\n#| output: false\nsns.lineplot(data=grouped_cal_day, color='#32363A')\nsns.despine()\nplt.savefig('figures/06_01.png', dpi=300)\n\n\n\n\n\n\nFigure 7.1: caption…\n\n\n\nDays may not be the best unit of time to work with in this case. We can, of course, upsample from days to weeks instead, and produce the same plot.\nweekly = tweets_df.resample('W').size()\nweekly.head()\n#| output: false\nax = sns.lineplot(data=weekly, color='#32363A')\nax.set_xlabel('\\nWeekly observations')\nax.set_ylabel('Number of Tweets\\n')\nsns.despine()\nplt.savefig('figures/06_02.png', dpi=300)\n\n\n\n\n\n\nFigure 7.2: caption…\n\n\n\nThe plot is much cleaner when we count at the level of weeks rather than days.",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>~~Processing Structured Data~~ Survey Data</span>"
    ]
  },
  {
    "objectID": "survey-data.html#combining-dataframes",
    "href": "survey-data.html#combining-dataframes",
    "title": "6  Processing Structured Data Survey Data",
    "section": "7.9 COMBINING DATAFRAMES",
    "text": "7.9 COMBINING DATAFRAMES\nCombining dataframes is a very common task. In fact, though it might not seem obvious, combining datasets is one of the most valuable skills you can have when doing computational social science. Here, we will consider some of the most common approaches: concatenating and merging, and we will briefly describe a more advanced set of methods commonly referred to as record linkage.\nConcatenating a dataframe is conceptually pretty simple - think of it like attaching the rows or columns of one dataframe below/to the right of the last row/column of another dataframe. For this to be useful, the two dataframes should have at least one row or column in common, but usually you would only concatenate if there were many such overlapping entries.\nfull_df =  pd.read_csv(\"data/vdem/filtered_subset.csv\")\ndf_australia = full_df.query(\"country_name == 'Australia'\")\nlen(df_australia)\ndf_sa = full_df.query(\"country_name == 'South Africa'\")\nlen(df_sa)\nThe default behaviour for pd.concat() is to perform a row-wise join, which it refers to as axis=0. We can override this default by specifying axis=1, which will produce a column-wise join:\nconcatenated = pd.concat([df_australia, df_sa], axis=1)\nlen(concatenated)\nWhen we concatenate the two dataframes the number of columns stays the same but the number of rows increases, accounting for the rows in both the original dataframes. Normally, this kind of concatenation would result in a different number of columns, but in this case, the two dataframes we joined had the exact same columns (which makes sense, given that they were both extracted from the same parent dataframe).\n\n7.9.1 Merging\nAn alternative way to combine datasets is to merge them. If you want to create a dataframe that contains columns from multiple datasets but is aligned on rows according to some column (or set of columns), you probably want to use the merge() function. To illustrate this, we will work with data from two different sources. The first is the VDEM data we used in first part of this chapter (fsdf). The second is a dataset from Freedom House on levels of internet freedom in 65 countries. More information is available at https://freedomhouse.org/countries/freedom-net/scores.\ndownload_dataset(\n    'https://www.dropbox.com/scl/fo/fnw5yrslxza9plhqnqhxr/AA7997oGIdd3k3EjluHyLBc?rlkey=hr93qtcdp6uh7d3lsfzbc6nr6&st=bz0xzw41&dl=0',\n    'data/freedom_house/'\n)\n\nfreedom_df = pd.read_csv( \"data/freedom_house/internet_freedoms_2020.csv\")\nTo merge these dataframes we need to find a column which can be used to match rows from one dataframe to the rows in the other. The columns don’t need to have the same name, just values that can be matched with one another. Whatever columns we choose will be called “keys” in our merge. In our case this will be the country name columns from each dataset.\nfsdf.columns\nfreedom_df.columns\nWe will use the merge function to combine these two dataframes using ‘country_name’ and ‘Country’. We’re going to do an inner merge, which is the default if the option isn’t set, and will keep only the keys (ie. countries) that appear in both dataframes.\nmerged = pd.merge(fsdf, freedom_df, how='inner', left_on='country_name', right_on='Country')\nprint('merged has {} rows and {} columns'.format(len(merged), len(merged.columns)))\nlen(fsdf) + len(freedom_df)\nYou should see 5 new columns in the merged dataframe compared to the fsdf one. Notice how many rows each of the dataframes have: many fewer rows than the original VDEM dataframe but many more than the Freedom House dataframe. So in our case, if a row’s country doesn’t appear in the other dataset, that row will not be included in the merged dataframe.\nThis can be adjusted using the how parameter. There are five ways of merging dataframes in Pandas: left, right, outer, inner, and cross. Check out the documentation to see how the other four methods work.\nThere are ways to improve the matching, either manual methods or semi-automated methods such as record linkage, described below. Let’s see which countries aren’t common between the dataframes, using a set operation ^ (XOR), which returns a set of elements from the combination of set1 and set2 that are either not in set1 or not in set2.\nfsdf_set = set(fsdf['country_name'].tolist())\nfreedom_set = set(freedom_df['Country'].tolist())\n\nunmatched = fsdf_set ^ freedom_set\n\nprint('Total countries: ' + str(len(fsdf_set) + len(freedom_set)))\nprint('Unmatched countries: ' + str(len(unmatched)))\nWe can then use the & set operator to see which of the missing countries are present in each of the country sets. If the data is small enough, we can print the two sets as sorted lists in a dataframe. The most obvious manual change we could do here is make “United States” and “United States of America” consistent but we would also expect Myanmar to be in the VDEM data. We could also make this change manually by knowing that Myanmar was referred to as Burma until 1989. However, it just so happens that at the top of the south_east_asia aggregated group dataframe from earlier, “Burma/Myanmar” was the name used, rather than Burma alone. For a more complex but automated solution to disambiguating different versions of country names, we would have to use some form of record linkage, discussed briefly below.\nfsdf_missing = list(fsdf_set & unmatched)\nfsdf_missing.sort()\nfreedom_missing = list(freedom_set & unmatched)\nfreedom_missing.sort()\npd.DataFrame({'VDEM': pd.Series(fsdf_missing), 'Freedom': pd.Series(freedom_missing)})\n\n\n7.9.2 Record Linkage\nThe merge function works great when you can make exact matches between columns. It also works really well because checking for exact matches has been optimized in Pandas. However, it’s often the case that we need to combine datasets which cannot be merged based on exact matches.\nInstead, we often have to use inexact matching (aka “fuzzy matching” or “approximate matching”) to combine datasets. Typically, this involves using some similarity metric to measure how close two keys are to one another. Then a match is made based on thresholds, rules, or a nearest-neighbour approach. However, naively calculating similarity between all possible key combinations results in incredibly lengthy compute times. Instead, there are ways to exclude some key pairs from the beginning. This allows you to drastically reduce the number of comparisons you need to make. Additionally, inexact matching can leverage machine learning techniques which uses human curated examples to learn to predict whether two rows should be matched with one another.\nIf this “more advanced” approach to combining datasets is of interest, I highly suggest looking into the recordlinkage Python package.\n\nFurther Reading\nMuch of what I introduce in this chapter is foundational; you’ll build on that foundation in later chapters. But if you are looking for a slower and more comprehensive introduction to Pandas and Numpy, then I would recommend VanderPlas’ (2016) Python Data Science Handbook.",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>~~Processing Structured Data~~ Survey Data</span>"
    ]
  },
  {
    "objectID": "survey-data.html#conclusion",
    "href": "survey-data.html#conclusion",
    "title": "7  Survey Data",
    "section": "7.8 Conclusion",
    "text": "7.8 Conclusion\nIn this chapter, we’ve covered the essential steps in processing and cleaning survey data, from understanding the sample design and applying weights to handling missing data and recoding variables. These are the foundational tasks that will set you up for more advanced analysis in future chapters. In the next chapter, we’ll explore web and document data, where we’ll encounter many of the same issues—like missing data and recoding—but in a different context. Stay tuned!\n\n\n\n\nGroves, Robert, Floyd Fowler Jr, Mick Couper, James Lepkowski, Eleanor Singer, and Roger Tourangeau. 2011. Survey Methodology. John Wiley & Sons.\n\n\nSchaeffer, Nora Cate, and Jennifer Dykema. 2020. “Advances in the Science of Asking Questions.” Annual Review of Sociology 46 (1): 37–60.",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Survey Data</span>"
    ]
  },
  {
    "objectID": "survey-data.html#key-points",
    "href": "survey-data.html#key-points",
    "title": "6  Processing Structured Data Survey Data",
    "section": "7.11 Key Points",
    "text": "7.11 Key Points\n\nIn this chapter, we expanded into the world of processing structured data using Pandas; these are critical skills for computational social scientists\nWe covered the basic Pandas data structures, Series and dataframes, and the index and datetime objects\nWe discussed how to subset dataframes by selecting columns and filtering rows, followed by a discussion of how to do systematic comparisons by performing operations on grouped dataframes\nWe then discussed how to combine multiple dataframes using merge and concatenate and introduced the general idea of record linkage.\n\n\n\n\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential Tools for Working with Data. \" O’Reilly Media, Inc.\".",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>~~Processing Structured Data~~ Survey Data</span>"
    ]
  },
  {
    "objectID": "python-101.html",
    "href": "python-101.html",
    "title": "2  Python 101",
    "section": "",
    "text": "2.1 Learning Python\nPython is designed to maximize human readability, and as a consequence, it’s common to feel that you have a good understanding of something because the code on the page makes sense and seems obvious, only to find yourself at a loss when you try to write the same code yourself. That’s totally normal. You must resist the temptation to copy and paste. You will understand more, identify what you don’t understand, and gain mastery faster if you actually type the code out yourself. Your future self will thank you.\nIf you are new to Python but have some experience doing scripted data analysis in another language, I suggest that you approach Python as if you were a beginner. It’s common to start working in Python by “translating” your R or Stata scripts into Python. While there are many generic programming concepts that are used in those languages and many others, efforts to “translate” your scripts will lead to writing poor Python code and will slow down your learning, causing problems later. While a certain amount of this is unavoidable, you will benefit from minimizing it. When you are working in Python, it is generally better to embrace doing things the “Pythonic” way.",
    "crumbs": [
      "**RESEARCH COMPUTING**",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Python 101</span>"
    ]
  },
  {
    "objectID": "web-data-apis.html",
    "href": "web-data-apis.html",
    "title": "7  Web data (APIs)",
    "section": "",
    "text": "7.1 Application Programming Interfaces (APIs)\nUnderstanding APIs starts with the notion of an interface—the “I” in API. An interface allows different systems or components to communicate and interact with each other. In the context of software, an Application Programming Interface (API) is a set of rules and protocols that allows one piece of software to interact with another.\nYou’re already familiar with user interfaces (UIs)—the buttons, menus, and controls you use to interact with software applications. UIs abstract the complexity of the underlying system, allowing you to perform tasks without needing to understand how they are implemented. Similarly, APIs allow software applications to interact with each other without exposing the internal complexities. They provide a set of predefined functions and protocols that developers can use to access the features or data of another application or service. In programming languages like Python, the functions and libraries you use are themselves APIs. They abstract low-level operations into high-level functions that are easier to use and understand. For example, when you use the print() function, you’re leveraging an API that handles the complexities of displaying output to the console.\nWhen using web APIs for data collection, a bit of knowledge about the standardized protocols that devices use to communicate on the internet goes a long way.\nWhen you use an APIs for data collection, your program sends HTTP requests to specific URLs (endpoints) provided by the API. These requests include parameters that specify the data you want. The server then processes your request and sends back data, typically in formats like JSON or XML. You store that data locally (to avoid making redundant API calls), and then process it for analysis.",
    "crumbs": [
      "**OBTAINING DATA**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Web data (APIs)</span>"
    ]
  },
  {
    "objectID": "mindful-modelling.html",
    "href": "mindful-modelling.html",
    "title": "5  Mindful modelling",
    "section": "",
    "text": "In Progress\n\n\n\nThis is a “shitty first draft,” with some sections being shittier than others. Normally I wouldn’t share work so early in it’s development, but I’m interested in your feedback. Feel free to skim, but know that I am actively developing this chapter and it will be going through some very extensive changes in the coming weeks.",
    "crumbs": [
      "**WORKFLOW**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mindful modelling</span>"
    ]
  },
  {
    "objectID": "latent-factors.html",
    "href": "latent-factors.html",
    "title": "11  Reduction and latent dimensions",
    "section": "",
    "text": "11.1 Imports and Data Preparation\nWe will make extensive use of the Sklearn package in this chapter. Sklearn is very important and widely-used in machine learning, and it features heavily in the rest of this book. Some of the methods we introduce here are also implemented in other Python packages, including statsmodels, which implements a wide-variety of statistical models. If it suits your needs better, you should feel free to use statsmodels instead.\nWe will continue working with the VDEM data in this chapter, filtered to contain observations from 2019 only.\nNow that we have the VDEM data from 2019 loaded up, we can select the columns we will use in our analyses. In this case, we want the country name as well as a series of variables related to political deliberation, civil society, media and internet, private and political liberties, and the executive. The specific variables we will use in each of these categories are given in Table 11.1. Given that I don’t have space to discuss each variable (there are 35 of them in total), I recommend that you consult the VDEM codebook to ensure you know what each represents.\nWe will create a list of these indicator variables names that we can use to subset the larger dataframe.\nWe can now subset the original dataframe so that it includes only these variables, and then use the country names as the dataframe index.\nThe resulting dataframe has 179 observations (each one a country in 2019) and our 35 variables. Before moving on, we can do a quick check to see whether we have any problems with missing data. The code below counts the number of variables with missing (1) and non-missing (0) data. None of our variables have missing data.\nAll we need to finish preparing our data is to get our indicator variables into a Numpy array. We will do some additional cleaning a bit later in the chapter.",
    "crumbs": [
      "**DISCOVERY**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Reduction and latent dimensions</span>"
    ]
  },
  {
    "objectID": "latent-factors.html#imports-and-data-preparation",
    "href": "latent-factors.html#imports-and-data-preparation",
    "title": "11  Reduction and latent dimensions",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nimport random\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, silhouette_samples\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom dcss import set_style\nset_style()\n\n# Data downloaded in Chapter 6\ndf = pd.read_csv(\n    'data/vdem/V-Dem-CY-Full+Others-v10.csv', low_memory=False\n) \n\ndf = df.query('year == 2019').reset_index()\ndf.shape\n\n\n\n\nTable 11.1: Table: VDEM variables used in this chapter.\n\n\n\n\n\n\n\n\n\n\n\n\nDeliberation\nCivil Society\nMedia & Internet\nPrivate & Political Liberties\nThe Executive\n\n\n\n\nv2dlreason\nv2cseeorgs\nv2mecenefm\nv2cldiscm\nv2exrescon\n\n\nv2dlcommon\nv2csreprss\nv2mecenefi\nv2cldiscw\nv2exbribe\n\n\nv2dlcountr\nv2cscnsult\nv2mecenefibin\nv2clacfree\nv2exembez\n\n\nv2dlconslt\nv2csprtcpt\nv2mecrit\nv2clrelig\nv2excrptps\n\n\nv2dlengage\nv2csgender\nv2merange\nv2clfmove\nv2exthftps\n\n\nv2dlencmps\nv2csantimv\nv2mefemjrn\n\n\n\n\nv2dlunivl\nv2csrlgrep\nv2meharjrn\n\n\n\n\n\nv2csrlgcon\nv2meslfcen\n\n\n\n\n\n\nv2mebias\n\n\n\n\n\n\nv2mecorrpt\n\n\n\n\n\n\n\n\n\nindicators = [\n    'v2dlreason', \n    'v2dlcommon', \n    'v2dlcountr', \n    'v2dlconslt', \n    'v2dlengage',\n    'v2dlencmps', \n    'v2dlunivl', \n    'v2cseeorgs', \n    'v2csreprss', \n    'v2cscnsult',\n    'v2csprtcpt', \n    'v2csgender', \n    'v2csantimv', \n    'v2csrlgrep', \n    'v2csrlgcon',\n    'v2mecenefm', \n    'v2mecenefi', \n    'v2mecenefibin', \n    'v2mecrit', \n    'v2merange',\n    'v2mefemjrn', \n    'v2meharjrn', \n    'v2meslfcen', \n    'v2mebias', \n    'v2mecorrpt',\n    'v2exrescon', \n    'v2exbribe', \n    'v2exembez', \n    'v2excrptps', \n    'v2exthftps',\n    'v2cldiscm', \n    'v2cldiscw', \n    'v2clacfree', \n    'v2clrelig', \n    'v2clfmove'\n]\n\ncountries = df['country_name'].tolist()\ndf = df.set_index('country_name')[indicators]\ndf.shape\n\ndf.isna().sum().value_counts()\n\nX = df.to_numpy()",
    "crumbs": [
      "**DISCOVERY**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Reduction and latent dimensions</span>"
    ]
  },
  {
    "objectID": "latent-factors.html#latent-variables-and-the-curse-of-dimensionality",
    "href": "latent-factors.html#latent-variables-and-the-curse-of-dimensionality",
    "title": "11  Reduction and latent dimensions",
    "section": "11.2 LATENT VARIABLES AND THE CURSE OF DIMENSIONALITY",
    "text": "11.2 LATENT VARIABLES AND THE CURSE OF DIMENSIONALITY\nBefore getting into “latent variables,” let’s clear up a bit of terminology. In this chapter, and many that follow, we’ll talk about the “dimensions” and “dimensionality” of dataframes and matrices. All of this talk of “dimensions” is really just about the number of variables we are using. If we have 10 variables, we have 10 dimensions; 147,002 variables, 147,002 dimensions. When we have a lot of variables, we often describe our dataset as “high-dimensional.”\nHigh-dimensional datasets pose all sorts of problems for statistical and machine learning models that low-dimensional datasets do not. That’s why we refer to this situation as the curse of dimensionality even if it might seem like an embarrassment of riches. Typically, we reduce the number of variables we are working with by manually selecting the variables of interest, or by performing some sort of dimensionality reduction on the dataset that mitigates problems associated with the curse of dimensionality. Below, you will learn about two different but related approaches to dimensionality reduction, one driven by theory and measurement, the other by data-driven induction.\n\n11.2.1 Theory First: Measuring Latent Variables with Exploratory Factor Analysis\nWe social scientists spend a huge amount of time trying to measure things that we can’t directly observe. If you take a moment, you can probably list dozens of such things: political ideology, religiosity, well-being, job satisfaction, social capital, opportunity costs, social anxiety, confirmation bias, populism, introversion, personality types, emotional labour, resource mobilization, and so on. In earlier chapters of this book we spent a fair amount of time working with five variables that measure things we can’t directly observe:\n\na country-level measure of the principle of electoral democracy (polyarchy),\na country-level measure of the principle of liberal democracy,\na country-level measure of the principle of participatory democracy,\na country-level measure of the principle of egalitarian democracy, and\na country-level measure of the principle of deliberative democracy.\n\nWhen I say that none of these five “principles” of democracy can be directly observed, I mean that quite literally: you can’t look at a country and eyeball the amount of deliberative democracy you see, as if it had material form. That’s because deliberative democracy is not a material thing in the world, it’s an abstract concept developed by social scientists and political philosophers in the context of theoretical debate and applied research. Because these are unobservable abstract concepts, we call the variables that represent them latent or hidden.\nWhen we develop theoretical concepts, we can’t assume other researchers share our meanings, or that our meaning is somehow self-evident. Instead, we put in quite a lot of work to ensure conceptual clarity, as that is the only way to advance the state of our collective knowledge. These abstract concepts are essential in the quest to advance collective knowledge, but to treat them as if they were “real” material things, and not theoretical constructs, is a mistake called reification.\nAbstract concepts, like the principles of democracy above, are meaningful both in our everyday lives and in relation to broader theories and paradigms, but because they can’t be directly observed, they are difficult to measure empirically. This requires us to adopt measurement strategies that combine careful reasoning and logic with measurement models that we carefully and transparently validate. The first step in this process is specification, which involves developing conceptual and operational definitions of concepts. Specification is essential because it helps ensure we are talking about the same thing; in social science, as in everyday life, you can’t get very far just making up your own definitions of things, however good those definitions might be.\nA conceptual definition involves defining an abstract concept in relation to other concepts whose meaning is more widely shared, usually by breaking it down into more concrete aspects or dimensions. For example, my friend and colleague Igor Grossmann conducts fascinating psychological research on wisdom. What exactly, is “wisdom,” and how does one study it scientifically? Even something as grand and abstract as “wisdom” can be specified in terms of concrete dimensions that, together, speak to the more general concept. For example, as Brienza et al. (2018) propose, wisdom can be represented by the extent to which people demonstrate intellectual humility, recognition of uncertainty and change, consideration of the broader context at hand and perspectives of others, and the integration of these perspectives/compromise in specific situations.\nThe various dimensions that are specified in the conceptual definition of wisdom are, of course, other concepts. The difference between an operational definition and a conceptual one is that an operational definition describes the specific operations that one would have to perform to generate empirical observations (i.e., data) for each of the various dimensions. The variables that contain data on these specific dimensions are typically called indicator variables. Operational definitions involve specifying things like the level at which to measure something, the type of variables to use (e.g. ordinal, interval, ratio, categorical), the range of variation those variables should have, and so on. A good operational definition of a concept enables one to measure the concept by measuring the concept’s dimensions with a high degree of reliability and validity, and then aggregating the measures of specific dimensions into a measure of the abstract concept that also has high reliability and validity. In the case of measuring wisdom, for example, Brienza et al. (2018) outline an explicit measurement strategy that attempts to mitigate social desirability biases (which inevitably come into play when you ask people about the wisdom of their reasoning) by assessing how people respond to specific scenarios. They provide an online supplement that includes the specific survey instruments used to collect the data according to the operational definitions laid out in their paper.\nLet’s return to our abstract “principles of democracy.” The principle of electoral democracy, for example, is represented by the five dimensions listed below. The set of accompanying questions come straight from the VDEM codebook (Coppedge et al. 2020).\n\nFreedom of expression: “To what extent does government respect press and media freedom, the freedom of ordinary people to discuss political matters at home and in the public sphere, as well as the freedom of academic and cultural expression?” (variable: v2x_freexp_altinf)\nFreedom of association: “To what extent are parties, including opposition parties, allowed to form and to participate in elections, and to what extent are civil society organizations able to form and to operate freely?” (variable: v2x_frassoc_thick)\nShare of adult citizens with suffrage: “What share of adult citizens (as defined by statute) has the legal right to vote in national elections?” (variable: v2x_suffr)\nFree and fair elections: “To what extent are elections free and fair?” (variable: v2xel_frefair)\nOfficials are elected: “Is the chief executive and legislature appointed through popular elections?” (variable: v2x_elecoff)\n\nEach dimension is a bit more concrete than “electoral democracy,” but for the most part, we still can’t directly observe these dimensions. Perhaps you noticed that some contain multiple questions! The first dimension, for example, contains several questions about freedom of the press and media, freedom of ordinary people, and freedom of academic and cultural expression. In this case, each of the five dimensions that make up the higher-level measure of electoral democracy are called indices, which is a type of measure that is constructed by combining the values of lower-level indicator variables.\nFor example, the freedom of expression dimension represented by the index variable v2x_freexp_altinf is constructed from the values of the variables government censorship effort (v2mecenefm), harassment of journalists (v2meharjrn), media self-censorship (v2meslfcen), freedom of discussion (v2xcl_disc), freedom of academic and cultural expression (v2clacfree), levels of media bias (v2mebias), how critical the media is (v2mecrit), and the diversity of perspectives promoted by the media (v2merange). These lower-level indicators are easier to observe than the higher level index variables above them, or the even higher still indices representing types of democracies. If you want to learn more about the conceptual and operational definitions of these principles of democracy, as well as the specific measurement models used, you can consult Coppedge et al. (2020 CITE).\nThe difference between these indices and indicator variables maps directly back to the process of specification; the variables we use to record observations about the specific dimensions of concepts are indicator variables because they indicate part of the concept, and the overall concept is measures by combining the values for those indicators into an index. Indices are composite measures because they are created by systematically and transparently combining multiple indicators.\nWhen we want (or need) to measure something really big and abstract like a concept that is part of a larger theory (e.g. the amount of deliberative democracy that we see in a given country at some point in time), we break the big abstract concept down into various different dimensions, and sometimes we break those dimensions down into even smaller ones. The measures for the higher-level concepts are indices constructed by combining the values of lower-level indicator variables.\nThis general idea is sketched out in Figure 11.1 below, with example indicator variables on the top feeding into mid-level index measures for latent concepts (in gray), which in turn feed in to the high-level index measures of the latent concept of the principle of electoral democracy, or “polyarchy” (also in gray). The ...s are meant to emphasize that there are other indicators that feed into the mid-level indices in addition to those shown here.\n\n\n\n\n\n\nFigure 11.1: cap\n\n\n\nWhen trying to measure latent variables, degree of electoral democracy or freedom of association, we typically perform some sort of factor analysis that tells us whether the indicators we observed and measured (e.g. the share of adult citizens with voting rights and the power of the Head of State relative to the Head of Government) are indeed likely to reflect some underlying “factor.”\nA “factor” is simply a subset of highly correlated variables that have been combined into a single composite variable. If v2x_freexp_altinf, v2x_frassoc_thick, v2x_suffr, v2xel_frefair, and v2x_elecoff are all highly correlated with one another (and not strongly correlated with other variables), it might be because they are all indicating different dimensions of the same underlying latent construct: electoral democracy. The factor analysis lets us take a larger set of variables, of which some are highly correlated with one another, and reduce them to a smaller subset of explanatory factors. Depending on the type of factor analysis you conduct, those factors may or may not be correlated with one another, but usually they are not.\nWhen we conduct a factor analysis, we also compute factor loadings that clarify the relationship between each of the original variables in our analysis and the underlying factors extracted in the analysis. Variables that are strongly associated with the latent factor contribute more to that factor, and hence have higher loading. The specific factor loadings we can compute vary a bit depending on how we are approaching things. If we assume that the latent variables might be at least somewhat correlated with one another (which is a very reasonable assumption!), then we compute two sets of factor loadings, one being the Pearson correlation coefficients between the variables and the latent factors (a “structure matrix”) and one being coefficients from a linear regression (a “pattern matrix”).\nIf we assume that the latent variables are not correlated with one another (rarely a reasonable assumption, but it has its place), then there is only one set of factor loadings (either the correlation coefficients or the regression coefficients, which in this scenario would be the same). These loading scores are often “rotated” to help make them more substantively interpretable. Though we won’t discuss them here, the type of rotation you perform depends on whether you think the factors are correlated with one another. If you suspect they are at least somewhat correlated with one another, then you would use an oblique rotation, and if you suspect they aren’t, you whould choose an orthogonal rotation.\nFactor loadings describe how specific variables (e.g., government intimidation of the opposition) contribute to a latent factor. Factor scores, on the other hand, tell us how specific observations (e.g., the United States of America in 2020) score on a given latent factor (e.g., electoral democracy). You can probably see where I’m going with this: if the latent factors represent meaningful variables that we want to measure but can’t observe, then the factor scores that describe how an observation is related to that latent factor is the measurement of that observation for that latent factor. For example, on the egalitarian democracy measurement variable in 2019, Namibia scored 0.453, Egypt scored 0.118, France scored 0.773, North Korea scored 0.096, Vanuatu scored 0.566, Senegal scored 0.517, Canada scored 0.776, and Ukraine scored 0.316. Where did these numbers come from? The egalitarian democracy variable is a latent index variable constructed from several other indices, which are in turn constructed from more concrete low-level indicators. The latent variables and the individual country scores are mathematically constructed using factor analysis.\nIn the interest of space, we will not actually conduct a theory-oriented factor analysis in this chapter. Instead, we will focus on a different approach that is more inductive and data-driven: Principal Components Analysis (PCA).\n\nFurther Reading\nChapter 13 from Barbara Tabachinick and Linda Fidell’s (2007) Using Multivariate Statistics and Chapter 17 of Field, Miles, and Field (2012) Discovering Statistics Using R both provide a good introduction to exploratory factor analysis and PCA as widely-practices in the social and cognitive sciences. Chapter 8 of G{'e}ron’s (2019) Hands-on Machine Learning and Chapter 3 of M{\"u}ller and Guido’s (2016) Introduction to Machine Learning with Python provide a good introduction to “dimensionality reduction” in machine learning.",
    "crumbs": [
      "**DISCOVERY**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Reduction and latent dimensions</span>"
    ]
  },
  {
    "objectID": "latent-factors.html#conducting-a-principal-component-analysis-in-sklearn",
    "href": "latent-factors.html#conducting-a-principal-component-analysis-in-sklearn",
    "title": "11  Reduction and latent dimensions",
    "section": "11.3 CONDUCTING A PRINCIPAL COMPONENT ANALYSIS IN SKLEARN",
    "text": "11.3 CONDUCTING A PRINCIPAL COMPONENT ANALYSIS IN SKLEARN\n\n11.3.1 Standardization\nWe did most of the necessary pre-processing at the start of the chapter when we imported our data, filtered the rows, selected the relevant columns, and then converted the data to a Numpy ndarray, which is a nice way of storing matrix data. There is, however, one very important piece of pre-processing that we need to do before we conduct a PCA: scaling our variables via z-score normalization, or “standardization.”\nRemember, PCA reduces the dimensionality of a dataset by constructing “principle components” from highly correlated features. If the variance contained in any one component differs from the variance contained in another because of the scales for the features that contribute to it, then PCA will make consequential mistakes. In short, PCA is heavily impacted by feature scaling. To prevent any such issues, we can use Sklearn’s StandardScaler(), which performs z-score normalization on each feature. The z-score normalization ensures we are comparing things on the same scales.\nX = StandardScaler().fit_transform(X) \nMany statistical and machine learning models require standardization. If you need a refresher, you can consult the sub-section below. Otherwise you are free to skip over it. I recommend consulting the documentation whenever you use a model with which you are unfamiliar.\n\n11.3.1.1 A Brief Refresher on Variance, Standard Deviation, and Z-score Normalization\nFirst, let’s very briefly revisit the concepts of variance and standard deviation. Variance is a statistical measure of how spread out or clustered the values in a data set are. More specifically, it’s a measure of how far each value is from the mean. Variance is usually represented with the symbol \\(\\sigma^2\\). A larger variance means that the values are more spread out, while a smaller variance means that they are more clustered around the mean. Let’s use some very simple examples to see how this works.\nABCD = {\n    'A': [1, 1, 1, 1, 1], # no variance...\n    'B': [1, 2, 3, 4, 5], # some variance...\n    'C': [-1, 1, 3, 5, 7], # a bit more variance...\n    'D': [-10, -9, 3, 4, 4] # still more variance...\n}\n\nfor k, v in ABCD.items():\n    print(f'{k} has a variance of {np.round(np.var(v), 3)}.')\nThe standard deviation of a data set is the square root of the variance (\\(\\sigma^2\\)), and is therefore represented with the symbol \\(\\sigma\\).\nfor k, v in ABCD.items():\n    print(f'{k} has a standard deviation of {np.round(np.std(v), 3)}.')\nA z-score is a measure of how far an observation’s value (\\(x\\)) is from the mean (\\(\\mu\\)), standardized by dividing by the standard deviation (\\(\\sigma\\)). Thus, an observation \\(x\\) has a z-score:\n\\[\nz = \\frac{x - \\mu}{\\sigma}\n\\]\nWhile there are other ways of standardizing data, usually when we are standardizing our data, we are converting each observed value into a z-score. Below, we use the zscore() function from the stats module of a package called scipy. Note that the values in A all return nan because they have a standard deviation of 0, which means there is no variance.\nfor k, v in ABCD.items():\n    print(f'The values in {k} have the following Z-scores: {np.round(zscore(v), 3)}.')\n\n\n\n11.3.2 Back to PCA!\nNow that our data has been standardized, we can conduct the PCA. When we initialize the model object with PCA(), we have the option of telling sklean to compute a specific number of components (e.g. pass the number 15 to get back the 15 principal components that account for the most variance) or a float specifying the amount of variance we want to be accounted for by the PCA (e.g. pass the number .9 to produce a solution that accounts for 90% of the variance). In this example, we will not specify either.\nOnce we initialize the model object, we can use the .fit_transform() method on our standardized array \\(X\\).\npca = PCA()\npca_results = pca.fit_transform(X)\nLet’s take a look at the results!\nBelow, we create a dataframe representation of the Numpy matrix returned by pca.fit_transform(X) because it’s a bit easier to read. As you can see, each country is represented as a row and each principal component as a column. The cells indicate the association between each country and component pairing. These scores don’t have any meaning to us just yet, but they will become more clear shortly.\nres = pd.DataFrame(pca_results, index=countries)\nres.columns=[f'PC {i}' for i in res.columns]\n\nres['PC 0'].head()\nEach of the 35 principal components we have constructed accounts for some amount of variance in the dataset. The components are ordered such that the first component accounts for the most variance, followed by the second, third, fourth, and so on. The amount of variance that each individual component accounts for is stored in the pca model object as an attribute (explained_variance_ratio_), which means we can access it using dot notation. Because we used the default parameters, rather than specifying the n_components parameter, the explained variance ratio scores will sum to 1, which means that together the principal components account for 100% of the variance in the data.\nevr = pca.explained_variance_ratio_\nevr\nThe first value in the evr array above is roughly .6, which means that the first principle component contains roughly 60% of the variance in the dataset. You can interpret the rest of the numbers the same way: the second component contains roughly 9% of the variance, the third roughly 4% of the variance, and so on. In this particular example, a quick glance at this array alone suggests that the first component accounts for substantially more variance than any of the others.\nprint(f'The sum of the array is: {np.round(np.sum(evr), 2)}')\nUsually, we want to see how much cumulative variance is accounted for by some subset of principal components, starting with the first component. In other words, how much variance is accounted for by each component and those before it. The cumulative variance of the first three components, for example, is:\nnp.sum(evr[:3]) \nKnowing how the explained cumulative variance changes with each additional principal component is useful because we typically want to work with some subset of the components rather than the entire set of variables. That is, after all, generally the point of using a data-driven dimensionality reduction method like PCA. If you are going to work with a subset, you should know how much information you kept and how much you threw away.\nLet’s create a Series containing information on the cumulative explained variance for the components in our PCA. We can do this by passing the array of explained variance ratios (evr) to Numpy’s cumsum() function, which is short for cumulative sum. The Series tells us how much variance is accounted for by each component and those preceding it (remember, the index starts with 0, so the 0th element of the series represents the first component).\ncve = pd.Series(np.cumsum(evr))\ncve[:12]\nIn this case, a simple preview of the cumulative explained variance tells us that the first two components alone account for 68% of the variance in the dataset, which is a very substantial amount. Similarly, we can see that the first 12 components still account for 90% of the variance – pretty good considering we started with 35 indicator variables!\nThis is only part of the picture, though. Let’s plot the proportion of cumulative explained variance for each successive principal component. Notice that, by default, PCA will construct a number of components equal to the number of original variables in the dataset. You should be able to see the diminishing returns, here, even if they set in rather smoothly. The code below produces Figure 11.2.\nfig, ax = plt.subplots()\nsns.lineplot(x=cve.index, y=cve)\nplt.scatter(x=cve.index, y=cve)\nax.set(xlabel='Principal component ID',\n       ylabel='Proportion of explained variance (cumulative)')\nax.set(ylim=(0, 1.1))\nsns.despine()\nplt.savefig('figures/08-01.png', dpi=300)\n\n\n\n\n\n\nFigure 11.2: png\n\n\n\n\n\n11.3.3 Matrix Decomposition: Eigenvalues, Eigenvectors, and Extracting Components\nWhen you conduct a principal component analysis, you only want to keep some components. You’re trying to reduce dimensionality while preserving as much information (in the form of variance) possible. So, which components do you extract?\nThe above plot of cumulative variance accounted for can be helpful if you want to preserve a certain amount of variance in the data. An alternative approach is to construct a scree plot to determine which components are substantively important enough to use. To understand how to construct and interpret a scree plot, you need to know a little bit more about how PCA works, and more specifically what role eigenvalues and eigenvectors play in a PCA, and how those roles differ from loadings in factor analysis.\nAs discussed earlier, PCA creates a covariance matrix of standardized variables (or sometimes a correlation matrix). We can understand the structure and other properties of these matrices mathematically using eigen-decomposition. We won’t get into the linear algebra here, but here’s the basic idea: We can decompose the matrix into two parts. The first are the principal components themselves, which are directions of axes where there is the most variance (eigenvectors). The second part is the amount of variance that is accounted for by the principal components (eigenvalues). Every eigenvector has an eigenvalue and there is an eigenvector (principal component) for every variable in the original data. A very important feature of PCA is that the first principal component accounts for the greatest possible variance in the data set. The second principal component is calculated in the same way as the first, except that it cannot be correlated with the first. This continues for each principal component until you have as many as you do variables. It is important to remember that all of the information about scale (the amount of variance explained) is contained in the eigenvalues. The eigenvectors in a PCA do not tell us anything about the magnitude of explained variance.\nScree plots graph the eigenvalues for each component in the PCA, which you now know represents the amount of variance that each component accounts for. The higher the eigenvalue, the more important the component. Because eigenvalues represent the amount of explained variance, Sklearn helpfully stores them in the explained_variance_ attribute of the PCA model object.1 This also makes it very straightforward to create a scree plot. The code below produces Figure 11.3.\neigenvalues = pd.Series(pca.explained_variance_)\n\nfig, ax = plt.subplots()\nsns.lineplot(x=eigenvalues.index, y=eigenvalues)\nplt.scatter(x=eigenvalues.index, y=eigenvalues)\nax.set(xlabel='Principal component ID', ylabel='Eigenvalue')\nsns.despine()\nplt.savefig('figures/08-03.png', dpi=300)\n\n\n\n\n\n\nFigure 11.3: png\n\n\n\nThis figure should be straightforward to understand. The first few components are more important than the others. In a scree plot, you are usually looking for an inflection point, i.e., a point where the slope of the line changes rather abruptly. Usually, that point is clear, but we can also inspect the eigenvalues themselves if we want a bit more precision, just remember that the eigenvalues are 0-indexed, so 0 represents the first component, 1 represents the second component, and so on.\neigenvalues.head(10)\nWe might use the fact that dropping from 3.14 to 1.48 (a decrease of more than 50%) is significantly greater than the drop from 1.48 to 1.12, and from 1.12 to 1.01, to identify an inflection point at 1.48. The general rule is that you extract the components to the left of the inflection point, excluding the component at the inflection point itself. However, there are debates about whether it is best to keep all components with eigenvalues higher than some threshold, such as 1, the idea being that this is still quite a lot of variation even if less than the other components. In this example, cutting at the inflection point would be the third component, which means we would extract the first two. On the other hand, if we go with a threshold of 1, then we would take the first 5.\nWhen different rules suggest different courses of action, the best solution is the one most aligned with your goals. One reason why researchers perform PCA is because they want to do some sort of regression analysis but have a bad multi-collinearity problem. In that case, keep lots of components! It is better to keep information than throw it away unless you really need to throw some away. If, conversely, you are trying to visualize a high-dimensional dataset by collapsing it down to two significant dimensions, then you should only extract those two components provided they contain a lot of variance.\nHere, we will extract the first two because they preserve a lot of variance, and because the next thing I want to do is create a simple visualization of where the countries in our analysis are positioned in terms of these latent dimensions, and creating informative visualizations in three or more dimensions is a fool’s errand.\ncomponent_1 = pca_results[:, 0]\ncomponent_2 = pca_results[:, 1]\n\nPC12 = pd.DataFrame(zip(component_1, component_2), columns=['PC1', 'PC2'])\nWe can now easily visualize how the countries in our dataset are positioned in relation to these two principal components. Let’s grab the country names from our metadata variables to use in the visualization, which will be a simple density plot with country names indicating where each country is given these two components.\nPC12['Country'] = countries\nax = sns.kdeplot(data=PC12, x='PC1', y='PC2', alpha=.8, fill=True)\nfor i, country in enumerate(PC12['Country']):\n    ax.text(PC12['PC1'][i],\n            PC12['PC2'][i],\n            country,\n            horizontalalignment='left',\n            size=3,\n            color='black',\n            weight='normal')\nax.set(xticklabels=[], yticklabels=[])\nax.set(\n    xlabel=\n    f'$\\longleftarrow$ PC1 (eigenvalue: {np.round(eigenvalues.loc[0], 2)}) $\\longrightarrow$',\n    ylabel=\n    f'$\\longleftarrow$ PC2 (eigenvalue: {np.round(eigenvalues.loc[1], 2)}) $\\longrightarrow$'\n)\n\nplt.savefig('figures/08-04.png', dpi=300)\n\n\n\n\n\n\nFigure 11.4: png\n\n\n\nWhile the text is dense in Figure 11.4 (a high resolution version is available in the online supplement), careful inspection should lead to noticing several patterns. The first principal component is defined by the opposition between countries like Norway, Denmark, Switzerland, Luxembourg, and Germany on the one hand, and by Burundi, Tukmenistan, Syria, Eritrea, and North Korea on the other. The second principal component is defined by the opposition of countries like Haiti, Dominican Republic, Nigeria, Gabon, and Honduras on the one hand, and Laos, Eritrea, United Arab Emirates, China, and Singapore on the other. The eigenvalue is much higher for the first principal component, suggesting that the interpretation of the differences between countries on the left and the right of the graph is most important.\nThis is not a factor analysis. We have not guided the PCA towards this solution. Instead, we have obtained these two latent dimensions mathematically, through matrix decomposition, and projected the countries onto that latent space. These two dimensions only represent 68% of the variance in the dataset, but when you think about it, that’s a lot of information for just two variables. The challenge, given that this is computationally inductive, is to do the qualitative and historical work necessary to interpret this representation of the latent structure in the data. However, don’t forget that the only information the PCA has to work with comes from our original variables, so those variables are a great place to start.",
    "crumbs": [
      "**DISCOVERY**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Reduction and latent dimensions</span>"
    ]
  },
  {
    "objectID": "latent-factors.html#conclusion",
    "href": "latent-factors.html#conclusion",
    "title": "11  Reduction and latent dimensions",
    "section": "11.4 CONCLUSION",
    "text": "11.4 CONCLUSION",
    "crumbs": [
      "**DISCOVERY**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Reduction and latent dimensions</span>"
    ]
  },
  {
    "objectID": "latent-factors.html#key-points",
    "href": "latent-factors.html#key-points",
    "title": "11  Reduction and latent dimensions",
    "section": "11.5 Key Points",
    "text": "11.5 Key Points\n\nWe learned about latent variables and the differences between theory-driven and data-driven dimensionality reduction\nDiscussed distinctions between Factor Analysis and Principal Component Analysis\nConducted a PCA\n\n\n\n\n\n\nBrienza, Justin, Franki Kung, Henri Santos, Ramona Bobocel, and Igor Grossmann. 2018. “Wisdom, Bias, and Balance: Toward a Process-Sensitive Measurement of Wisdom-Related Cognition.” Journal of Personality and Social Psychology 115 (6): 1093.\n\n\nCoppedge, Michael, John Gerring, Adam Glynn, Carl Henrik Knutsen, Staffan Lindberg, Daniel Pemstein, Brigitte Seim, Svend-Erik Skaaning, and Jan Teorell. 2020. Varieties of Democracy: Measuring Two Centuries of Political Change. Cambridge University Press.\n\n\nField, Andy, Jeremy Miles, and Zoë Field. 2012. Discovering Statistics Using r. Sage publications.\n\n\nGéron, Aurélien. 2019. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O’Reilly Media.\n\n\nMüller, Andreas, and Sarah Guido. 2016. Introduction to Machine Learning with Python: A Guide for Data Scientists. \" O’Reilly Media, Inc.\".\n\n\nTabachnick, Barbara G, Linda S Fidell, and Jodie B Ullman. 2007. Using Multivariate Statistics. Vol. 5. Pearson Boston, MA.",
    "crumbs": [
      "**DISCOVERY**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Reduction and latent dimensions</span>"
    ]
  },
  {
    "objectID": "latent-factors.html#footnotes",
    "href": "latent-factors.html#footnotes",
    "title": "11  Reduction and latent dimensions",
    "section": "",
    "text": "If you are looking for the eigenvectors, Sklearn stores them in the .components_ attribute.↩︎",
    "crumbs": [
      "**DISCOVERY**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Reduction and latent dimensions</span>"
    ]
  },
  {
    "objectID": "workflow.html",
    "href": "workflow.html",
    "title": "5  Workflow",
    "section": "",
    "text": "In Progress\n\n\n\nThis is a new chapter! I’m currently revising an initial draft, which I will publish here in the coming weeks.",
    "crumbs": [
      "**WORKING ITERATIVELY**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Workflow</span>"
    ]
  },
  {
    "objectID": "audio-and-document-data.html",
    "href": "audio-and-document-data.html",
    "title": "9  Audio and document data",
    "section": "",
    "text": "Coming Soon!\n\n\n\nThis is a new chapter. I am actively working on it in fall 2024 and intend to publish a working draft in early winter 2025. It will include using some deep learning models for the audio and document data, so I need to think about how best to integrate the chapter this early in the book, long before deep learning is introduced. Stay tuned! 😎",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Audio and document data</span>"
    ]
  },
  {
    "objectID": "reduction-and-latent-dimensions.html",
    "href": "reduction-and-latent-dimensions.html",
    "title": "13  Reduction and latent dimensions",
    "section": "",
    "text": "13.1 Imports and Data Preparation\nWe will make extensive use of the Sklearn package in this chapter. Sklearn is very important and widely-used in machine learning, and it features heavily in the rest of this book. Some of the methods we introduce here are also implemented in other Python packages, including statsmodels, which implements a wide-variety of statistical models. If it suits your needs better, you should feel free to use statsmodels instead.\nWe will continue working with the VDEM data in this chapter, filtered to contain observations from 2019 only.\nNow that we have the VDEM data from 2019 loaded up, we can select the columns we will use in our analyses. In this case, we want the country name as well as a series of variables related to political deliberation, civil society, media and internet, private and political liberties, and the executive. The specific variables we will use in each of these categories are given in Table 13.1. Given that I don’t have space to discuss each variable (there are 35 of them in total), I recommend that you consult the VDEM codebook to ensure you know what each represents.\nWe will create a list of these indicator variables names that we can use to subset the larger dataframe.\nWe can now subset the original dataframe so that it includes only these variables, and then use the country names as the dataframe index.\nThe resulting dataframe has 179 observations (each one a country in 2019) and our 35 variables. Before moving on, we can do a quick check to see whether we have any problems with missing data. The code below counts the number of variables with missing (1) and non-missing (0) data. None of our variables have missing data.\nAll we need to finish preparing our data is to get our indicator variables into a Numpy array. We will do some additional cleaning a bit later in the chapter.",
    "crumbs": [
      "**DISCOVERY**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Reduction and latent dimensions</span>"
    ]
  },
  {
    "objectID": "reduction-and-latent-dimensions.html#imports-and-data-preparation",
    "href": "reduction-and-latent-dimensions.html#imports-and-data-preparation",
    "title": "13  Reduction and latent dimensions",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom scipy.stats import zscore\nimport random\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, silhouette_samples\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom dcss import set_style\nset_style()\n\n# Data downloaded in Chapter 6\ndf = pd.read_csv(\n    'data/vdem/V-Dem-CY-Full+Others-v10.csv', low_memory=False\n) \n\ndf = df.query('year == 2019').reset_index()\ndf.shape\n\n\n\n\nTable 13.1: Table: VDEM variables used in this chapter.\n\n\n\n\n\n\n\n\n\n\n\n\nDeliberation\nCivil Society\nMedia & Internet\nPrivate & Political Liberties\nThe Executive\n\n\n\n\nv2dlreason\nv2cseeorgs\nv2mecenefm\nv2cldiscm\nv2exrescon\n\n\nv2dlcommon\nv2csreprss\nv2mecenefi\nv2cldiscw\nv2exbribe\n\n\nv2dlcountr\nv2cscnsult\nv2mecenefibin\nv2clacfree\nv2exembez\n\n\nv2dlconslt\nv2csprtcpt\nv2mecrit\nv2clrelig\nv2excrptps\n\n\nv2dlengage\nv2csgender\nv2merange\nv2clfmove\nv2exthftps\n\n\nv2dlencmps\nv2csantimv\nv2mefemjrn\n\n\n\n\nv2dlunivl\nv2csrlgrep\nv2meharjrn\n\n\n\n\n\nv2csrlgcon\nv2meslfcen\n\n\n\n\n\n\nv2mebias\n\n\n\n\n\n\nv2mecorrpt\n\n\n\n\n\n\n\n\n\nindicators = [\n    'v2dlreason', \n    'v2dlcommon', \n    'v2dlcountr', \n    'v2dlconslt', \n    'v2dlengage',\n    'v2dlencmps', \n    'v2dlunivl', \n    'v2cseeorgs', \n    'v2csreprss', \n    'v2cscnsult',\n    'v2csprtcpt', \n    'v2csgender', \n    'v2csantimv', \n    'v2csrlgrep', \n    'v2csrlgcon',\n    'v2mecenefm', \n    'v2mecenefi', \n    'v2mecenefibin', \n    'v2mecrit', \n    'v2merange',\n    'v2mefemjrn', \n    'v2meharjrn', \n    'v2meslfcen', \n    'v2mebias', \n    'v2mecorrpt',\n    'v2exrescon', \n    'v2exbribe', \n    'v2exembez', \n    'v2excrptps', \n    'v2exthftps',\n    'v2cldiscm', \n    'v2cldiscw', \n    'v2clacfree', \n    'v2clrelig', \n    'v2clfmove'\n]\n\ncountries = df['country_name'].tolist()\ndf = df.set_index('country_name')[indicators]\ndf.shape\n\ndf.isna().sum().value_counts()\n\nX = df.to_numpy()",
    "crumbs": [
      "**DISCOVERY**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Reduction and latent dimensions</span>"
    ]
  },
  {
    "objectID": "reduction-and-latent-dimensions.html#latent-variables-and-the-curse-of-dimensionality",
    "href": "reduction-and-latent-dimensions.html#latent-variables-and-the-curse-of-dimensionality",
    "title": "13  Reduction and latent dimensions",
    "section": "13.2 LATENT VARIABLES AND THE CURSE OF DIMENSIONALITY",
    "text": "13.2 LATENT VARIABLES AND THE CURSE OF DIMENSIONALITY\nBefore getting into “latent variables,” let’s clear up a bit of terminology. In this chapter, and many that follow, we’ll talk about the “dimensions” and “dimensionality” of dataframes and matrices. All of this talk of “dimensions” is really just about the number of variables we are using. If we have 10 variables, we have 10 dimensions; 147,002 variables, 147,002 dimensions. When we have a lot of variables, we often describe our dataset as “high-dimensional.”\nHigh-dimensional datasets pose all sorts of problems for statistical and machine learning models that low-dimensional datasets do not. That’s why we refer to this situation as the curse of dimensionality even if it might seem like an embarrassment of riches. Typically, we reduce the number of variables we are working with by manually selecting the variables of interest, or by performing some sort of dimensionality reduction on the dataset that mitigates problems associated with the curse of dimensionality. Below, you will learn about two different but related approaches to dimensionality reduction, one driven by theory and measurement, the other by data-driven induction.\n\n13.2.1 Theory First: Measuring Latent Variables with Exploratory Factor Analysis\nWe social scientists spend a huge amount of time trying to measure things that we can’t directly observe. If you take a moment, you can probably list dozens of such things: political ideology, religiosity, well-being, job satisfaction, social capital, opportunity costs, social anxiety, confirmation bias, populism, introversion, personality types, emotional labour, resource mobilization, and so on. In earlier chapters of this book we spent a fair amount of time working with five variables that measure things we can’t directly observe:\n\na country-level measure of the principle of electoral democracy (polyarchy),\na country-level measure of the principle of liberal democracy,\na country-level measure of the principle of participatory democracy,\na country-level measure of the principle of egalitarian democracy, and\na country-level measure of the principle of deliberative democracy.\n\nWhen I say that none of these five “principles” of democracy can be directly observed, I mean that quite literally: you can’t look at a country and eyeball the amount of deliberative democracy you see, as if it had material form. That’s because deliberative democracy is not a material thing in the world, it’s an abstract concept developed by social scientists and political philosophers in the context of theoretical debate and applied research. Because these are unobservable abstract concepts, we call the variables that represent them latent or hidden.\nWhen we develop theoretical concepts, we can’t assume other researchers share our meanings, or that our meaning is somehow self-evident. Instead, we put in quite a lot of work to ensure conceptual clarity, as that is the only way to advance the state of our collective knowledge. These abstract concepts are essential in the quest to advance collective knowledge, but to treat them as if they were “real” material things, and not theoretical constructs, is a mistake called reification.\nAbstract concepts, like the principles of democracy above, are meaningful both in our everyday lives and in relation to broader theories and paradigms, but because they can’t be directly observed, they are difficult to measure empirically. This requires us to adopt measurement strategies that combine careful reasoning and logic with measurement models that we carefully and transparently validate. The first step in this process is specification, which involves developing conceptual and operational definitions of concepts. Specification is essential because it helps ensure we are talking about the same thing; in social science, as in everyday life, you can’t get very far just making up your own definitions of things, however good those definitions might be.\nA conceptual definition involves defining an abstract concept in relation to other concepts whose meaning is more widely shared, usually by breaking it down into more concrete aspects or dimensions. For example, my friend and colleague Igor Grossmann conducts fascinating psychological research on wisdom. What exactly, is “wisdom,” and how does one study it scientifically? Even something as grand and abstract as “wisdom” can be specified in terms of concrete dimensions that, together, speak to the more general concept. For example, as Brienza et al. (2018) propose, wisdom can be represented by the extent to which people demonstrate intellectual humility, recognition of uncertainty and change, consideration of the broader context at hand and perspectives of others, and the integration of these perspectives/compromise in specific situations.\nThe various dimensions that are specified in the conceptual definition of wisdom are, of course, other concepts. The difference between an operational definition and a conceptual one is that an operational definition describes the specific operations that one would have to perform to generate empirical observations (i.e., data) for each of the various dimensions. The variables that contain data on these specific dimensions are typically called indicator variables. Operational definitions involve specifying things like the level at which to measure something, the type of variables to use (e.g. ordinal, interval, ratio, categorical), the range of variation those variables should have, and so on. A good operational definition of a concept enables one to measure the concept by measuring the concept’s dimensions with a high degree of reliability and validity, and then aggregating the measures of specific dimensions into a measure of the abstract concept that also has high reliability and validity. In the case of measuring wisdom, for example, Brienza et al. (2018) outline an explicit measurement strategy that attempts to mitigate social desirability biases (which inevitably come into play when you ask people about the wisdom of their reasoning) by assessing how people respond to specific scenarios. They provide an online supplement that includes the specific survey instruments used to collect the data according to the operational definitions laid out in their paper.\nLet’s return to our abstract “principles of democracy.” The principle of electoral democracy, for example, is represented by the five dimensions listed below. The set of accompanying questions come straight from the VDEM codebook (Coppedge et al. 2020).\n\nFreedom of expression: “To what extent does government respect press and media freedom, the freedom of ordinary people to discuss political matters at home and in the public sphere, as well as the freedom of academic and cultural expression?” (variable: v2x_freexp_altinf)\nFreedom of association: “To what extent are parties, including opposition parties, allowed to form and to participate in elections, and to what extent are civil society organizations able to form and to operate freely?” (variable: v2x_frassoc_thick)\nShare of adult citizens with suffrage: “What share of adult citizens (as defined by statute) has the legal right to vote in national elections?” (variable: v2x_suffr)\nFree and fair elections: “To what extent are elections free and fair?” (variable: v2xel_frefair)\nOfficials are elected: “Is the chief executive and legislature appointed through popular elections?” (variable: v2x_elecoff)\n\nEach dimension is a bit more concrete than “electoral democracy,” but for the most part, we still can’t directly observe these dimensions. Perhaps you noticed that some contain multiple questions! The first dimension, for example, contains several questions about freedom of the press and media, freedom of ordinary people, and freedom of academic and cultural expression. In this case, each of the five dimensions that make up the higher-level measure of electoral democracy are called indices, which is a type of measure that is constructed by combining the values of lower-level indicator variables.\nFor example, the freedom of expression dimension represented by the index variable v2x_freexp_altinf is constructed from the values of the variables government censorship effort (v2mecenefm), harassment of journalists (v2meharjrn), media self-censorship (v2meslfcen), freedom of discussion (v2xcl_disc), freedom of academic and cultural expression (v2clacfree), levels of media bias (v2mebias), how critical the media is (v2mecrit), and the diversity of perspectives promoted by the media (v2merange). These lower-level indicators are easier to observe than the higher level index variables above them, or the even higher still indices representing types of democracies. If you want to learn more about the conceptual and operational definitions of these principles of democracy, as well as the specific measurement models used, you can consult Coppedge et al. (2020 CITE).\nThe difference between these indices and indicator variables maps directly back to the process of specification; the variables we use to record observations about the specific dimensions of concepts are indicator variables because they indicate part of the concept, and the overall concept is measures by combining the values for those indicators into an index. Indices are composite measures because they are created by systematically and transparently combining multiple indicators.\nWhen we want (or need) to measure something really big and abstract like a concept that is part of a larger theory (e.g. the amount of deliberative democracy that we see in a given country at some point in time), we break the big abstract concept down into various different dimensions, and sometimes we break those dimensions down into even smaller ones. The measures for the higher-level concepts are indices constructed by combining the values of lower-level indicator variables.\nThis general idea is sketched out in Figure 13.1 below, with example indicator variables on the top feeding into mid-level index measures for latent concepts (in gray), which in turn feed in to the high-level index measures of the latent concept of the principle of electoral democracy, or “polyarchy” (also in gray). The ...s are meant to emphasize that there are other indicators that feed into the mid-level indices in addition to those shown here.\n\n\n\n\n\n\nFigure 13.1: cap\n\n\n\nWhen trying to measure latent variables, degree of electoral democracy or freedom of association, we typically perform some sort of factor analysis that tells us whether the indicators we observed and measured (e.g. the share of adult citizens with voting rights and the power of the Head of State relative to the Head of Government) are indeed likely to reflect some underlying “factor.”\nA “factor” is simply a subset of highly correlated variables that have been combined into a single composite variable. If v2x_freexp_altinf, v2x_frassoc_thick, v2x_suffr, v2xel_frefair, and v2x_elecoff are all highly correlated with one another (and not strongly correlated with other variables), it might be because they are all indicating different dimensions of the same underlying latent construct: electoral democracy. The factor analysis lets us take a larger set of variables, of which some are highly correlated with one another, and reduce them to a smaller subset of explanatory factors. Depending on the type of factor analysis you conduct, those factors may or may not be correlated with one another, but usually they are not.\nWhen we conduct a factor analysis, we also compute factor loadings that clarify the relationship between each of the original variables in our analysis and the underlying factors extracted in the analysis. Variables that are strongly associated with the latent factor contribute more to that factor, and hence have higher loading. The specific factor loadings we can compute vary a bit depending on how we are approaching things. If we assume that the latent variables might be at least somewhat correlated with one another (which is a very reasonable assumption!), then we compute two sets of factor loadings, one being the Pearson correlation coefficients between the variables and the latent factors (a “structure matrix”) and one being coefficients from a linear regression (a “pattern matrix”).\nIf we assume that the latent variables are not correlated with one another (rarely a reasonable assumption, but it has its place), then there is only one set of factor loadings (either the correlation coefficients or the regression coefficients, which in this scenario would be the same). These loading scores are often “rotated” to help make them more substantively interpretable. Though we won’t discuss them here, the type of rotation you perform depends on whether you think the factors are correlated with one another. If you suspect they are at least somewhat correlated with one another, then you would use an oblique rotation, and if you suspect they aren’t, you whould choose an orthogonal rotation.\nFactor loadings describe how specific variables (e.g., government intimidation of the opposition) contribute to a latent factor. Factor scores, on the other hand, tell us how specific observations (e.g., the United States of America in 2020) score on a given latent factor (e.g., electoral democracy). You can probably see where I’m going with this: if the latent factors represent meaningful variables that we want to measure but can’t observe, then the factor scores that describe how an observation is related to that latent factor is the measurement of that observation for that latent factor. For example, on the egalitarian democracy measurement variable in 2019, Namibia scored 0.453, Egypt scored 0.118, France scored 0.773, North Korea scored 0.096, Vanuatu scored 0.566, Senegal scored 0.517, Canada scored 0.776, and Ukraine scored 0.316. Where did these numbers come from? The egalitarian democracy variable is a latent index variable constructed from several other indices, which are in turn constructed from more concrete low-level indicators. The latent variables and the individual country scores are mathematically constructed using factor analysis.\nIn the interest of space, we will not actually conduct a theory-oriented factor analysis in this chapter. Instead, we will focus on a different approach that is more inductive and data-driven: Principal Components Analysis (PCA).\n\nFurther Reading\nChapter 13 from Barbara Tabachinick and Linda Fidell’s (2007) Using Multivariate Statistics and Chapter 17 of Field, Miles, and Field (2012) Discovering Statistics Using R both provide a good introduction to exploratory factor analysis and PCA as widely-practices in the social and cognitive sciences. Chapter 8 of G{'e}ron’s (2019) Hands-on Machine Learning and Chapter 3 of M{\"u}ller and Guido’s (2016) Introduction to Machine Learning with Python provide a good introduction to “dimensionality reduction” in machine learning.",
    "crumbs": [
      "**DISCOVERY**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Reduction and latent dimensions</span>"
    ]
  },
  {
    "objectID": "reduction-and-latent-dimensions.html#conducting-a-principal-component-analysis-in-sklearn",
    "href": "reduction-and-latent-dimensions.html#conducting-a-principal-component-analysis-in-sklearn",
    "title": "13  Reduction and latent dimensions",
    "section": "13.3 CONDUCTING A PRINCIPAL COMPONENT ANALYSIS IN SKLEARN",
    "text": "13.3 CONDUCTING A PRINCIPAL COMPONENT ANALYSIS IN SKLEARN\n\n13.3.1 Standardization\nWe did most of the necessary pre-processing at the start of the chapter when we imported our data, filtered the rows, selected the relevant columns, and then converted the data to a Numpy ndarray, which is a nice way of storing matrix data. There is, however, one very important piece of pre-processing that we need to do before we conduct a PCA: scaling our variables via z-score normalization, or “standardization.”\nRemember, PCA reduces the dimensionality of a dataset by constructing “principle components” from highly correlated features. If the variance contained in any one component differs from the variance contained in another because of the scales for the features that contribute to it, then PCA will make consequential mistakes. In short, PCA is heavily impacted by feature scaling. To prevent any such issues, we can use Sklearn’s StandardScaler(), which performs z-score normalization on each feature. The z-score normalization ensures we are comparing things on the same scales.\nX = StandardScaler().fit_transform(X) \nMany statistical and machine learning models require standardization. If you need a refresher, you can consult the sub-section below. Otherwise you are free to skip over it. I recommend consulting the documentation whenever you use a model with which you are unfamiliar.\n\n13.3.1.1 A Brief Refresher on Variance, Standard Deviation, and Z-score Normalization\nFirst, let’s very briefly revisit the concepts of variance and standard deviation. Variance is a statistical measure of how spread out or clustered the values in a data set are. More specifically, it’s a measure of how far each value is from the mean. Variance is usually represented with the symbol \\(\\sigma^2\\). A larger variance means that the values are more spread out, while a smaller variance means that they are more clustered around the mean. Let’s use some very simple examples to see how this works.\nABCD = {\n    'A': [1, 1, 1, 1, 1], # no variance...\n    'B': [1, 2, 3, 4, 5], # some variance...\n    'C': [-1, 1, 3, 5, 7], # a bit more variance...\n    'D': [-10, -9, 3, 4, 4] # still more variance...\n}\n\nfor k, v in ABCD.items():\n    print(f'{k} has a variance of {np.round(np.var(v), 3)}.')\nThe standard deviation of a data set is the square root of the variance (\\(\\sigma^2\\)), and is therefore represented with the symbol \\(\\sigma\\).\nfor k, v in ABCD.items():\n    print(f'{k} has a standard deviation of {np.round(np.std(v), 3)}.')\nA z-score is a measure of how far an observation’s value (\\(x\\)) is from the mean (\\(\\mu\\)), standardized by dividing by the standard deviation (\\(\\sigma\\)). Thus, an observation \\(x\\) has a z-score:\n\\[\nz = \\frac{x - \\mu}{\\sigma}\n\\]\nWhile there are other ways of standardizing data, usually when we are standardizing our data, we are converting each observed value into a z-score. Below, we use the zscore() function from the stats module of a package called scipy. Note that the values in A all return nan because they have a standard deviation of 0, which means there is no variance.\nfor k, v in ABCD.items():\n    print(f'The values in {k} have the following Z-scores: {np.round(zscore(v), 3)}.')\n\n\n\n13.3.2 Back to PCA!\nNow that our data has been standardized, we can conduct the PCA. When we initialize the model object with PCA(), we have the option of telling sklean to compute a specific number of components (e.g. pass the number 15 to get back the 15 principal components that account for the most variance) or a float specifying the amount of variance we want to be accounted for by the PCA (e.g. pass the number .9 to produce a solution that accounts for 90% of the variance). In this example, we will not specify either.\nOnce we initialize the model object, we can use the .fit_transform() method on our standardized array \\(X\\).\npca = PCA()\npca_results = pca.fit_transform(X)\nLet’s take a look at the results!\nBelow, we create a dataframe representation of the Numpy matrix returned by pca.fit_transform(X) because it’s a bit easier to read. As you can see, each country is represented as a row and each principal component as a column. The cells indicate the association between each country and component pairing. These scores don’t have any meaning to us just yet, but they will become more clear shortly.\nres = pd.DataFrame(pca_results, index=countries)\nres.columns=[f'PC {i}' for i in res.columns]\n\nres['PC 0'].head()\nEach of the 35 principal components we have constructed accounts for some amount of variance in the dataset. The components are ordered such that the first component accounts for the most variance, followed by the second, third, fourth, and so on. The amount of variance that each individual component accounts for is stored in the pca model object as an attribute (explained_variance_ratio_), which means we can access it using dot notation. Because we used the default parameters, rather than specifying the n_components parameter, the explained variance ratio scores will sum to 1, which means that together the principal components account for 100% of the variance in the data.\nevr = pca.explained_variance_ratio_\nevr\nThe first value in the evr array above is roughly .6, which means that the first principle component contains roughly 60% of the variance in the dataset. You can interpret the rest of the numbers the same way: the second component contains roughly 9% of the variance, the third roughly 4% of the variance, and so on. In this particular example, a quick glance at this array alone suggests that the first component accounts for substantially more variance than any of the others.\nprint(f'The sum of the array is: {np.round(np.sum(evr), 2)}')\nUsually, we want to see how much cumulative variance is accounted for by some subset of principal components, starting with the first component. In other words, how much variance is accounted for by each component and those before it. The cumulative variance of the first three components, for example, is:\nnp.sum(evr[:3]) \nKnowing how the explained cumulative variance changes with each additional principal component is useful because we typically want to work with some subset of the components rather than the entire set of variables. That is, after all, generally the point of using a data-driven dimensionality reduction method like PCA. If you are going to work with a subset, you should know how much information you kept and how much you threw away.\nLet’s create a Series containing information on the cumulative explained variance for the components in our PCA. We can do this by passing the array of explained variance ratios (evr) to Numpy’s cumsum() function, which is short for cumulative sum. The Series tells us how much variance is accounted for by each component and those preceding it (remember, the index starts with 0, so the 0th element of the series represents the first component).\ncve = pd.Series(np.cumsum(evr))\ncve[:12]\nIn this case, a simple preview of the cumulative explained variance tells us that the first two components alone account for 68% of the variance in the dataset, which is a very substantial amount. Similarly, we can see that the first 12 components still account for 90% of the variance – pretty good considering we started with 35 indicator variables!\nThis is only part of the picture, though. Let’s plot the proportion of cumulative explained variance for each successive principal component. Notice that, by default, PCA will construct a number of components equal to the number of original variables in the dataset. You should be able to see the diminishing returns, here, even if they set in rather smoothly. The code below produces Figure 13.2.\nfig, ax = plt.subplots()\nsns.lineplot(x=cve.index, y=cve)\nplt.scatter(x=cve.index, y=cve)\nax.set(xlabel='Principal component ID',\n       ylabel='Proportion of explained variance (cumulative)')\nax.set(ylim=(0, 1.1))\nsns.despine()\nplt.savefig('figures/08-01.png', dpi=300)\n\n\n\n\n\n\nFigure 13.2: png\n\n\n\n\n\n13.3.3 Matrix Decomposition: Eigenvalues, Eigenvectors, and Extracting Components\nWhen you conduct a principal component analysis, you only want to keep some components. You’re trying to reduce dimensionality while preserving as much information (in the form of variance) possible. So, which components do you extract?\nThe above plot of cumulative variance accounted for can be helpful if you want to preserve a certain amount of variance in the data. An alternative approach is to construct a scree plot to determine which components are substantively important enough to use. To understand how to construct and interpret a scree plot, you need to know a little bit more about how PCA works, and more specifically what role eigenvalues and eigenvectors play in a PCA, and how those roles differ from loadings in factor analysis.\nAs discussed earlier, PCA creates a covariance matrix of standardized variables (or sometimes a correlation matrix). We can understand the structure and other properties of these matrices mathematically using eigen-decomposition. We won’t get into the linear algebra here, but here’s the basic idea: We can decompose the matrix into two parts. The first are the principal components themselves, which are directions of axes where there is the most variance (eigenvectors). The second part is the amount of variance that is accounted for by the principal components (eigenvalues). Every eigenvector has an eigenvalue and there is an eigenvector (principal component) for every variable in the original data. A very important feature of PCA is that the first principal component accounts for the greatest possible variance in the data set. The second principal component is calculated in the same way as the first, except that it cannot be correlated with the first. This continues for each principal component until you have as many as you do variables. It is important to remember that all of the information about scale (the amount of variance explained) is contained in the eigenvalues. The eigenvectors in a PCA do not tell us anything about the magnitude of explained variance.\nScree plots graph the eigenvalues for each component in the PCA, which you now know represents the amount of variance that each component accounts for. The higher the eigenvalue, the more important the component. Because eigenvalues represent the amount of explained variance, Sklearn helpfully stores them in the explained_variance_ attribute of the PCA model object.1 This also makes it very straightforward to create a scree plot. The code below produces Figure 13.3.\neigenvalues = pd.Series(pca.explained_variance_)\n\nfig, ax = plt.subplots()\nsns.lineplot(x=eigenvalues.index, y=eigenvalues)\nplt.scatter(x=eigenvalues.index, y=eigenvalues)\nax.set(xlabel='Principal component ID', ylabel='Eigenvalue')\nsns.despine()\nplt.savefig('figures/08-03.png', dpi=300)\n\n\n\n\n\n\nFigure 13.3: png\n\n\n\nThis figure should be straightforward to understand. The first few components are more important than the others. In a scree plot, you are usually looking for an inflection point, i.e., a point where the slope of the line changes rather abruptly. Usually, that point is clear, but we can also inspect the eigenvalues themselves if we want a bit more precision, just remember that the eigenvalues are 0-indexed, so 0 represents the first component, 1 represents the second component, and so on.\neigenvalues.head(10)\nWe might use the fact that dropping from 3.14 to 1.48 (a decrease of more than 50%) is significantly greater than the drop from 1.48 to 1.12, and from 1.12 to 1.01, to identify an inflection point at 1.48. The general rule is that you extract the components to the left of the inflection point, excluding the component at the inflection point itself. However, there are debates about whether it is best to keep all components with eigenvalues higher than some threshold, such as 1, the idea being that this is still quite a lot of variation even if less than the other components. In this example, cutting at the inflection point would be the third component, which means we would extract the first two. On the other hand, if we go with a threshold of 1, then we would take the first 5.\nWhen different rules suggest different courses of action, the best solution is the one most aligned with your goals. One reason why researchers perform PCA is because they want to do some sort of regression analysis but have a bad multi-collinearity problem. In that case, keep lots of components! It is better to keep information than throw it away unless you really need to throw some away. If, conversely, you are trying to visualize a high-dimensional dataset by collapsing it down to two significant dimensions, then you should only extract those two components provided they contain a lot of variance.\nHere, we will extract the first two because they preserve a lot of variance, and because the next thing I want to do is create a simple visualization of where the countries in our analysis are positioned in terms of these latent dimensions, and creating informative visualizations in three or more dimensions is a fool’s errand.\ncomponent_1 = pca_results[:, 0]\ncomponent_2 = pca_results[:, 1]\n\nPC12 = pd.DataFrame(zip(component_1, component_2), columns=['PC1', 'PC2'])\nWe can now easily visualize how the countries in our dataset are positioned in relation to these two principal components. Let’s grab the country names from our metadata variables to use in the visualization, which will be a simple density plot with country names indicating where each country is given these two components.\nPC12['Country'] = countries\nax = sns.kdeplot(data=PC12, x='PC1', y='PC2', alpha=.8, fill=True)\nfor i, country in enumerate(PC12['Country']):\n    ax.text(PC12['PC1'][i],\n            PC12['PC2'][i],\n            country,\n            horizontalalignment='left',\n            size=3,\n            color='black',\n            weight='normal')\nax.set(xticklabels=[], yticklabels=[])\nax.set(\n    xlabel=\n    f'$\\longleftarrow$ PC1 (eigenvalue: {np.round(eigenvalues.loc[0], 2)}) $\\longrightarrow$',\n    ylabel=\n    f'$\\longleftarrow$ PC2 (eigenvalue: {np.round(eigenvalues.loc[1], 2)}) $\\longrightarrow$'\n)\n\nplt.savefig('figures/08-04.png', dpi=300)\n\n\n\n\n\n\nFigure 13.4: png\n\n\n\nWhile the text is dense in Figure 13.4 (a high resolution version is available in the online supplement), careful inspection should lead to noticing several patterns. The first principal component is defined by the opposition between countries like Norway, Denmark, Switzerland, Luxembourg, and Germany on the one hand, and by Burundi, Tukmenistan, Syria, Eritrea, and North Korea on the other. The second principal component is defined by the opposition of countries like Haiti, Dominican Republic, Nigeria, Gabon, and Honduras on the one hand, and Laos, Eritrea, United Arab Emirates, China, and Singapore on the other. The eigenvalue is much higher for the first principal component, suggesting that the interpretation of the differences between countries on the left and the right of the graph is most important.\nThis is not a factor analysis. We have not guided the PCA towards this solution. Instead, we have obtained these two latent dimensions mathematically, through matrix decomposition, and projected the countries onto that latent space. These two dimensions only represent 68% of the variance in the dataset, but when you think about it, that’s a lot of information for just two variables. The challenge, given that this is computationally inductive, is to do the qualitative and historical work necessary to interpret this representation of the latent structure in the data. However, don’t forget that the only information the PCA has to work with comes from our original variables, so those variables are a great place to start.",
    "crumbs": [
      "**DISCOVERY**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Reduction and latent dimensions</span>"
    ]
  },
  {
    "objectID": "reduction-and-latent-dimensions.html#conclusion",
    "href": "reduction-and-latent-dimensions.html#conclusion",
    "title": "13  Reduction and latent dimensions",
    "section": "13.4 CONCLUSION",
    "text": "13.4 CONCLUSION",
    "crumbs": [
      "**DISCOVERY**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Reduction and latent dimensions</span>"
    ]
  },
  {
    "objectID": "reduction-and-latent-dimensions.html#key-points",
    "href": "reduction-and-latent-dimensions.html#key-points",
    "title": "13  Reduction and latent dimensions",
    "section": "13.5 Key Points",
    "text": "13.5 Key Points\n\nWe learned about latent variables and the differences between theory-driven and data-driven dimensionality reduction\nDiscussed distinctions between Factor Analysis and Principal Component Analysis\nConducted a PCA\n\n\n\n\n\n\nBrienza, Justin, Franki Kung, Henri Santos, Ramona Bobocel, and Igor Grossmann. 2018. “Wisdom, Bias, and Balance: Toward a Process-Sensitive Measurement of Wisdom-Related Cognition.” Journal of Personality and Social Psychology 115 (6): 1093.\n\n\nCoppedge, Michael, John Gerring, Adam Glynn, Carl Henrik Knutsen, Staffan Lindberg, Daniel Pemstein, Brigitte Seim, Svend-Erik Skaaning, and Jan Teorell. 2020. Varieties of Democracy: Measuring Two Centuries of Political Change. Cambridge University Press.\n\n\nField, Andy, Jeremy Miles, and Zoë Field. 2012. Discovering Statistics Using r. Sage publications.\n\n\nGéron, Aurélien. 2019. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O’Reilly Media.\n\n\nMüller, Andreas, and Sarah Guido. 2016. Introduction to Machine Learning with Python: A Guide for Data Scientists. \" O’Reilly Media, Inc.\".\n\n\nTabachnick, Barbara G, Linda S Fidell, and Jodie B Ullman. 2007. Using Multivariate Statistics. Vol. 5. Pearson Boston, MA.",
    "crumbs": [
      "**DISCOVERY**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Reduction and latent dimensions</span>"
    ]
  },
  {
    "objectID": "reduction-and-latent-dimensions.html#footnotes",
    "href": "reduction-and-latent-dimensions.html#footnotes",
    "title": "13  Reduction and latent dimensions",
    "section": "",
    "text": "If you are looking for the eigenvectors, Sklearn stores them in the .components_ attribute.↩︎",
    "crumbs": [
      "**DISCOVERY**",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Reduction and latent dimensions</span>"
    ]
  },
  {
    "objectID": "iteration.html",
    "href": "iteration.html",
    "title": "6  Sequential iterative modelling",
    "section": "",
    "text": "In Progress\n\n\n\nThis is a “shitty first draft,” with some sections being shittier than others. Normally I wouldn’t share work so early in it’s development, but I’m interested in your feedback. Feel free to skim, but know that I am actively developing this chapter and it will be going through some very extensive changes in the coming weeks.",
    "crumbs": [
      "**WORKFLOW**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>~~Sequential~~ iterative modelling</span>"
    ]
  },
  {
    "objectID": "relational-data.html",
    "href": "relational-data.html",
    "title": "11  Relational data",
    "section": "",
    "text": "11.1 WHAT ARE SOCIAL NETWORKS?",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Relational data</span>"
    ]
  },
  {
    "objectID": "relational-data.html#what-are-social-networks",
    "href": "relational-data.html#what-are-social-networks",
    "title": "11  Relational data",
    "section": "",
    "text": "11.1.1 From Independent Individuals to Networks\nLin Freeman’s (2004) history of social network analysis starts with a colorful (and slightly unsettling) quote from an American sociologist in the late-1960s that gets straight to the core of the difference between network analysis and the traditional quantitative social science of his day:\n\nFor the last thirty years, empirical social research has been dominated by the sample survey. But as usually practiced, using random sampling of individuals, the survey is a sociological meatgrinder, tearing the individual from his (sic.) social context and guaranteeing that nobody in the study interacts with anyone else in it. It is a little like a biologist putting his (sic.) experimental animals through a hamburger machine and looking at every hundredth cell through a microscope; anatomy and physiology get lost, structure and function disappear, and one is left with cell biology… If our aim to is to understand people’s behaviour rather than simply to record it, we want to know about primary groups, neighbourhoods, organizations, social circles, and communities; about interaction, communication, role expectations, and social control.\nAllen Barton (1968)\n\nWhile many people still practice this kind of “meatgrinder” research, Barton’s distinction is much less salient now than it used to be. Mainstream quantitative social science has changed a lot since he wrote that in 1968, and again since he was quoted by Freeman in 2004. For one thing, network analysis is no longer just an obscure undertaking of mathematically-inclined sociologists, social psychologists, and other social scientists; it’s well within the mainstream of applied quantitative science across dozens of disciplines and is an important research area in contemporary statistics.\n\nTODO: Cite the new edition of the handbook here.\n\nNetwork analysis is one of several major developments in quantitative data analysis that attempts to model interdependent relationships and institutional contexts. Another, multilevel analysis (or hierarchical modelling), will be covered in Chapter 29. Both seek to explicitly model the complex interdependencies between entities (e.g., people) by emphasizing their shared contexts, relationships, and interactions. However, in network analysis, an entity’s context is typically their connections to other entities and their structural position in a network (a concept we will discuss briefly here and again in later chapters). In multilevel analysis, an entity’s context is typically some sort of institutional environment that is shared with other entities, such as classrooms in a school, teams in a league, provinces or states in a country, or countries in the world polity (see Buhari-Gulmez 2010; Meyer, Krücken, and Drori 2009 on world polity theory and Stanford school institutionalism). In a multilevel network analysis (see Lazega and Snijders 2015), the context would be the entities’ concrete connections with one another nested in one of many possible shared institutional contexts, such as networks of informal relations between employees nested in the specific firms they work for (e.g., Brailly et al. 2016).\nWe care about these network connections and shared institutional contexts for many reasons. Perhaps the most obvious is that we think complex interdependencies have important effects on specific outcomes that we care about. For example, we might hypothesize that whether someone believes misinformation that vaccines cause autism depends in part on the beliefs of the people they interact with frequently or whom they trust the most. Or we might hypothesize that one’s overall health and wellbeing depends in part on the health and wellbeing of the people they are connected to. The logic is similar for multilevel analysis, but what follows “depends in part on” would refer to some sort of shared institutional context or environment rather than a complex network of concrete relationships and interactions.\nNot all hypotheses about how networks influence individual outcomes are based on direct connections, however. My friends influence me, but their friends (including those whom I am not friends with) influence them, and my friends’ friends’ friends influence them, and so on. Each step out in such a friendship network usually brings new, but diminished, influences. Networks are complex systems; what happens in one region of the network can affect disparate regions, and seemingly small differences in micro-level interaction processes (e.g., norms around who you interact with, how you interact with them, and what they mean to you) can have dramatic macro-level outcomes. For example, we might design a network study to better understand how micro-level social norms generate macro-level structures that shape disease transmission dynamics (Bearman, Moody, and Stovel 2004), or how network structures differently impact the spread of an infectious disease through a population and the adoption of health behaviours necessary to mitigate the spread of that disease (Centola 2018).\nWe also care about networks and institutional contexts because, as social scientists, we want to understand networks and institutions for their own sakes, inclusive of the social and political processes that generate different kinds of structural configurations. This might be because we are interested in advancing scientific knowledge by doing rigorous theory-driven research, or because we want to leverage that knowledge for some applied reason, such as intervening in a network to mitigate the effects of misinformation and disinformation, to disrupt the diffusion of a violent political ideology, or to improve health and wellbeing.\nThis is what makes networks so interesting and beautiful: we are all linked together in a vast and dense web of intersecting, meaningful, and mutually-influential relationships. But this complexity can quickly get out of hand. There’s a reason why old fashioned quantitative social science worked like a meatgrinder: it was extremely difficult to do much of anything else. Consequently, the history of network analysis is full of fascinating stories of intrepid sociologists, social psychologists, anthropologists, and other social scientists coming up with clever new mathematical models to describe and analyze interdependent relationships, and developing research software to use those models in applied research. Now that network analysis is being practiced across so many scientific fields, methods and models are improving at breakneck speed.\nJust about anything that a social scientist might be interested in can be usefully described in network terms, and just about any research question you might ask could be cast as a question about networks, where the network or some network-based variable might be:\n\npart of an explanation for something else, such as why some people practice safe sex while others don’t, or\nthe thing we are trying to explain, such as why some schools have racially segregated friendship networks while others do not.\n\nDoes this mean you should model everything as a network and pose every question as a question about networks? No (though as a network scientist and sociologist I’m tempted to say yes). Recall Box’s loop: your model should be whatever will best answer your question.\nFreeman didn’t include it in his excerpt of Barton’s 1968 article, but just a wee bit further down the page, Barton poses an important question: “what do we want?” Different things, of course. Network analysis is diverse and interdisciplinary, and you can find meaningful divisions between different groups of network analysts that use different tools to answer different questions. But what unites network scientists of virtually every social science, now and in the past, is a paradigmatic preference for holistic research strategies that focus on people and groups embedded in complex interdependent relationships. Let’s turn towards some of the various ways network analysts have conceptualized those relationships.\n\n\n11.1.2 What is a Network?\nThe spread of network analysis to and from so many scientific fields, often with developments occurring independently of one another, has produced a variety of synonymous terms in network analysis. These interchangeable terms can be confusing when you first run into them. For clarity, we will say that networks consist of a set of entities, which we will usually call nodes (also called vertices or actors), and the relationships between those entities, edges (also called ties or arcs). In theory, a node can be any kind of entity and an edge can be any kind of relationship or interaction between such entities. In the social sciences, we typically work with social networks where the nodes have some sort of agency, such as individual people or groups of people (e.g., an organization), and the edges represent some sort of meaningful relationship between them. Following Kitts (2014) and Kitts and Quintane (2020), we can categorize these common types of edges as defined by:\n\npatterns of sentiment, such as who likes or dislikes whom;\nsocially-constructed role relations, such as friendships, research collaborations, romantic partnerships, doctoral student and supervisor relationships, family;\nbehavioural interactions and contact over time, such as who messages whom;\nproviding access to resources, support, information, and opportunities, such as who contacts whom for advice in a personal crisis, to discuss a problem at work, or to pick up your groceries while quarantining in a global pandemic.\n\nThese four types of edges provide us with a high-level framework for talking about types of networks based on the relational information they encode and, importantly, the questions we want to answer and the theories we are using. In many respects, the type of relation that defines the edges in a network is the most important thing in determining what type of network you have, and what you can reasonably do with it. I emphasize that one of the most common mistakes that novices make is trying to answer a research question, or apply a theory, that is a poor match for the type of network, like trying to answer a question about power and inequality with data on a collaboration network. It can work with the right data and a bit of mental gymnastics, but it shouldn’t have to. As with any other type of research, this is a match that you want to ensure you get right. This is, once again, a matter of good research design.\nGraphically, we can represent nodes as points and edges as lines that connect those nodes. Figure 11.1 is a hypothetical network with five nodes (Chen, Nate, Anika, Anvita, and Patrick) and the relationships between them. When two nodes are connected, such as Chen and Anvita, we say they are adjacent. If we choose to focus on a specific node, we refer to it as the ego, and the nodes ego is directly connected to can be referred to as alters. Together, an ego’s alters represent a neighbourhood. For example, if we are focused on Patrick, Patrick is “ego,” and their neighbourhood would consist of the alters Anvita, Chen, and Anika.\nWe’ll come up with a fictitious story about information sharing in this network later in the chapter. For now, let’s just focus on learning some technical vocabulary and understanding how we can represent social networks with relational data structures.\n\n\n\n\n\n\nFigure 11.1: Cap\n\n\n\nThe edges have arrows because the network is directed as opposed to undirected. For example, there is an arrow pointing from Anika to Chen because Anika sends something to Chen, or initiates something with Chen, that Chen may not reciprocate. Many interactions and relationships can be represented this way. Email and SMS communication can be modelled as directed interactions: “Anika emails Chen” becomes an edge from Anika to Chen. Requests for support or advice can be modelled as coming from one person (“help, please!”) and the support or advice being sent back in return (“Here you go!”). In a network defined by patterns of sentiment, one person may like another who may or may not like them back. Other types of relationship don’t make sense to represent as directed. While one node might nominate another node as a “friend” and not be nominated in return (tragic!), this really shouldn’t be the case in coauthorship. If Anika wrote a book with Chen, Chen must also have written a book with Anika.\nNetworks are simply relational mathematical objects and have no inherent visual form. The image above is just a common and convenient way of representing relational data for small networks. However, just as a scatterplot is a representation of data, rather than the data itself, so too is the above image. With networks, as with text, it’s matrices all the way down. In this case, it’s a square adjacency matrix. Consider Figure 11.2, which shows how the graphical network representations of the network (directed on the left, undirected on the right) align with two different adjacency matrices.\n\n\n\n\n\n\nFigure 11.2: Cap\n\n\n\nFirst, for both the directed network (left) and the undirected network (right), many cells are empty, which represents 0, or the absence of an edge. The diagonals are highlighted in gray, but this is just to emphasize them. The diagonals are 0 because the nodes in these networks are not permitted to connect to themselves, which means that there are no self-loops. In other types of networks, such as email communication networks, self-loops might be possible, and a self-loop could be created by an action such as a node emailing themselves.\nIf you look closely at the undirected network’s adjacency matrix, you will notice that the data above and below the diagonal are mirror images of one another, but not in the directed network. That’s because you can have relational asymmetry in a directed network (Anika can send a tie to Chen that Chen does not reciprocate) but not in an undirected network.\nBoth of these networks are binary; the adjacency matrices contain 0s (not shown) to represent the absence of an edge and 1s to represent their presence. However, we can populate these cells with plenty of other information, often interpreted as some sort of edge weight. For example, in an interaction network we might populate cells with count data representing frequency of interaction within a given time frame. In a sentiment network, we might populate the cells with numbers that indicate different types or levels of sentiment, such as a Likert scale from 1 - 5 or a set of distinctions such as “strong” and “weak.” There is a lot of flexibility in how these data are collected, and it is largely up to you are to make decisions that make sense in the context of your research project.\nTraditionally, researchers have focused on positive ties like friendship, support and sharing, or collaboration and collective action. But as Harrigan, Labianca, and Agneessens (2020) and others have pointed out, “some relationships harm. Others are characterised by avoidance, dislike, or conflict” (page 1). These negative ties are (hopefully) less common, but are disproportionately impactful in our lives. They also operate in ways in that are fundamentally different than positive ties. Networks that incorporate data on the positive and negative ties are called signed graphs, and are a major area of theoretical and methodological development in contemporary network analysis.\nLet’s make two final distinctions. First, the network we are working with here is unipartite, which means there is only one type of node (people) and the matrix storing the data is square, with the same set of nodes in the rows and the columns. However, it is also possible to consider networks with two types of nodes, such as between people and organizational affiliations, or between people and events. This kind of network is bipartite, because there are two types of nodes, and the underlying matrix is a rectangular incidence matrix (or affiliation matrix) with one node type represented on the rows and the other in the columns. There are fascinating theoretical and methodological literatures on bipartite networks (for some foundational ideas, see Breiger 1974; Mützel and Breiger 2020), but regrettably we don’t have the space to discuss bipartite networks here.\nFinally, the example we are working with is a whole network, in contrast to an ego network. As I mentioned, we can think of each node in a network as “ego” with a neighbourhood composed of their direct connections (alters). If the network data is collected to capture all of the relevant relationships within some network boundary (e.g. all students in a classroom), then we are working with a whole network, and the main data collection tasks include specifying the boundaries of the network (e.g., the classroom) within which we want to record relationships. If, however, we collect some sample of people and then collect data on their individual relationships, then we are working with a collection of ego networks, one for each node in the study. Ego networks, fascinating though they are, are also out of scope for this book. If you are interested in learning more about ego network analysis, I recommend Crossley et al. (2015) and Small et al. (2021), as well as the 2020 special issue of Network Science on ego network analysis edited by Perry et al. (2020).\n\nFurther Reading\nThere is no shortage of outstanding conceptual introductions to network analysis. Crossley et al. (2015) provide a great introduction to ego-network analysis, which regrettably is not covered in this book. Christina Prell’s (2012) Social Network Analysis, John Scott’s (2017) Social Network Analysis, Garry Robins (2015) Doing Social Networks Research, and Borgatti, Everett, and Johnson’s (2018) Analyzing Social Networks are all great general introductions to network analysis. If you want to know more about data collection in network analysis, I recommend jimi adam’s (2020) Gathering Social Network Data.",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Relational data</span>"
    ]
  },
  {
    "objectID": "relational-data.html#working-with-relational-data",
    "href": "relational-data.html#working-with-relational-data",
    "title": "11  Relational data",
    "section": "11.2 WORKING WITH RELATIONAL DATA",
    "text": "11.2 WORKING WITH RELATIONAL DATA\nWith these generalities out of the way, let’s start getting into the details of working with relational data.\n\n11.2.1 Edgelists and Nodelists\nAs I mentioned earlier, matrices are the heart of network analysis, but they are not ideal ways to enter, store, or manage network data for anything other than small networks. In contemporary practice, most network data is stored in the form of edgelists and nodelists. In Figure 11.3, the same relational data that is represented in the graph, and encoded in the adjacency matrix above, is encoded as an edgelist. The first two columns of the edgelist is where the relational data itself is stored. The columns are labelled source and target because the network is directed; the node in the source column sends an edge to the node in the target column.\n\n\n\n\n\n\nFigure 11.3: Cap\n\n\n\nWhile an edgelist only requires pairs of nodes, we can also include additional columns that provide data about the relationship. There is nothing special or unique about data that describes these edges except for the fact that they describe characteristics of the relationship between two entities rather than characteristics of the entities themselves. Just as we carefully specify variables for describing the attributes of entities, we can carefully specify variables for describing the attributes of relationships between entities. For example, we might have a variable that categorizes edges by the type of relationship (e.g., family, friend, foe, professional, romantic, people who dance together, people who do intravenous drugs with one another) or by it’s sentiment (positive, neural, negative), to suggest just a couple of possibilities. Just as we can with an adjacency matrix, we can record edge weight (such as interaction frequency) as a variable in the edgelist. In longitudinal or dynamic networks (discussed in later chapters), we might also record the wave that a relationship was observed in, or perhaps a timestamp of when the interaction occurred (e.g., when Chen sent an email to Anvita). It may be a simple point, but it’s very important to understand: we can record any empirical observations of the attributes of relationships. The same considerations about what to observe and record apply for edges as with nodes. There is nothing special about edge data except that it describes edges.\n\n\n11.2.2 Graph Objects from Edgelists (and Matrices and More)\nLet’s examine some actual data. In the code block below, we will import a couple of packages and load up some relational data collected from a group of French high school students in 2013. This dataset is one of several collected by the SocioPatterns research team (with collaborators the ISI Foundation in Turin, the Centre de Physique Théorique in Marseille, and Bitmanufactory in Cambridge). The particular dataset we will use describes a network of face-to-face contacts between high school students in Marseille over a five-day period in December 2013. This data was collected via contact diaries, in which students recorded who they came into contact with (restricted to other students in the study) and for how long. Similar data was also collected with wearable sensors, but we will just focus on the contact diaries for now.\nThe edge data is provided in a CSV file with three columns: i, j, and weight. Physical co-presence is, of course, naturally undirected. It is not possible to be physically co-present with someone who is not also physically co-present with you. Therefore, the edgelist names the columns with i and j instead of source and target. This also means that a tie from i to j is the same as a tie from j to i. Finally, edge weight data is stored in the weight column and is coded as follows:\n\nweight = 1 if i and j were co-present for at most 5 minutes\nweight = 2 if i and j were co-present for 5-15 minutes\nweight = 3 if i and j were co-present for 15-60 minutes\nweight = 4 if i and j were co-present for more than 60 minutes\n\nWe can load this edge data in a Pandas dataframe and perform any necessary cleaning before reading the edge data into the NetworkX package to create a graph object that we can analyze using network methods.\n\nTODO: Swap this content out with the introductory graph-tool content from the FCIT 607 tutorial?\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport pandas as pd\nimport seaborn as sns\n\nfrom dcss import set_style, download_dataset\n\nset_style()\ndownload_dataset(\n    \"https://www.dropbox.com/scl/fo/wbj6l2tyoc67o3vlonxp5/AERxFPRIhfgWG_MaVK_rjM4?rlkey=48x2taz2t5mru1j2ucjf670a8&st=e5qs5gw8&dl=0\",\n    save_path=\"data/SocioPatterns/\"\n)\ncontact_diaries = pd.read_csv(\"data/SocioPatterns/Contact-diaries-network_data_2013.csv\", sep=' ')\ncontact_diaries.head()\ncontact_diaries.info()\nAll three columns in this data are numeric: the nodes in columns i and j are represented by numerical IDs rather than the names of the participants in the study. There are 502 rows in this edgelist, which means there are 502 observed edges.\nWe can import this weighted edgelist into the Python package NetworkX, which will transform our edge data into a graph object that we can analyze using methods and models from network analysis. NetworkX provides a number of useful functions for doing this. We’ll use from_pandas_edgelist() because our data is stored in an edgelist format in a pandas dataframe. When we construct the network G, we’ll provide NetworkX with the names of the columns that contain the IDs for each node in an edge. Any other columns in the dataframe will be treated as edge attribute data. Finally, we will also tell NetworkX that this is an undirected graph by passing the argument create_using=nx.Graph(). This tells NetworkX that, when it is creating the network, an edge from i to j is the same as an edge from j to i. If we were working with a directed network, we could pass the argument create_using=nx.DiGraph() instead.\nG = nx.from_pandas_edgelist(contact_diaries, 'i', 'j', create_using=nx.Graph())\nG.name = 'Reported Contacts (Diary Data)'\nprint(G)\nYou might be wondering why the number of rows in the edgelist differs from the number of edges in the network object G. The reason is because different students report the same relation. i might say they spent between 15 and 30 minutes with j and j might later report the same contact with i. However, it seems that not all students reported all interactions (if they had, we would expect there to be 502 reports of 251 edges). Because we believe that students are more likely to forget to record an interaction than they are to fabricate one in their contact diary, we symmetrize the relationship, making it undirected (if i spent time with j, then the reverse must necessarily be true). We have informed NetworkX that this network should use undirected edges by specifying a Graph object rather than a DiGraph (directed graph).\nFinally, before moving on, let’s create a quick visualization of this network (Figure 11.4). This is an inherently challenging task as networks are very high-dimensional objects and we are limited to 2 dimensions. It’s best not to rely on visualizations such as these for any serious analytic work, but for relatively small networks they can still be informative.\n\nTODO: Critique the visualization; hint that we will do better in a couple of chapters.\n\nlayout = nx.nx_pydot.graphviz_layout(G)\nfig, ax = plt.subplots(figsize=(12, 12))\nnx.draw(G,\n        pos=layout,\n        node_color='gray',\n        edge_color='lightgray',\n        node_size=10,\n        width=.5)\nplt.savefig('figures/13_04.png', dpi=300)\n\n\n\n\n\n\nFigure 11.4: png\n\n\n\nYou’ll see similar visualizations one in the coming chapters. Each time you’ll develop a deeper understanding of what to look for, such as clusters of densely connected nodes, or pendants hanging off a dense cluster of nodes at the core of a network. For now, let’s keep moving.\nNetworkX has a variety of functions for reading network data stored in other formats as well, and you can find the one that works with the your data’s format by checking the documentation, which you can find at https://networkx.org/documentation/latest/. Some of the formats supported at the time of writing include edgelists and matrices stored in Python’s built in data structures (e.g., dictionaries) as well as numpy arrays and matrices, scipy matrices, and pandas dataframes. In addition to these edgelist and matrix data structures, NetworkX can also read and write network objects using file formats commonly used by other network analysis software, including graphml, GEXF, JSON, and Pajek files. Unless you are have a specific need for them, my recommendation is that you store your network data in the form of an edgelist and a nodelist in separate plain text CSV files.\nNow it’s time to turn to some important building blocks of network analysis. We will start by briefly discussing the “walk-structure” of a network and the notion of network flow. This is intended to get you to start thinking about networks in a particular way in preparation for content that appears in the next chapter. We will then turn to some of the basic micro-level building blocks of network structure, dyads and triads. Together, these two final sections provide a foundation for much of what follows in later chapters.",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Relational data</span>"
    ]
  },
  {
    "objectID": "relational-data.html#walk-structure-and-network-flow",
    "href": "relational-data.html#walk-structure-and-network-flow",
    "title": "11  Relational data",
    "section": "11.3 WALK-STRUCTURE AND NETWORK FLOW",
    "text": "11.3 WALK-STRUCTURE AND NETWORK FLOW\nOne of the most foundational ideas in social network analysis is that your position in a network affects your ability to access or shape the flow of things – such as resources, information, or knowledge – through that network. Those things are often called contagions, or social contagions, even when they are not literally contagious in the way an infectious disease is (and most contagions that social scientists are interested in do not spread like infectious diseases, as you will learn in Chapter 18). Depending on where a node is positioned in the network, they can control or otherwise manipulate how a contagion is flowing through a network. For example, they may prevent someone from learning about some important information by feeding them incorrect information or by betraying their trust and spilling their secrets to the rest of the office.\nThis notion of contagions flowing along the structure of a network enables disparate regions of that network to influence one another. In a friendship network, for example, we are influenced by our friends, by our friends’ friends, by our friends’ friends’ friends, and so on. But exactly how does that happen, and to what degree? Which of the many possible paths do contagions flow on? All of them at once? Some subset of paths? If so, which ones? Perhaps a single most optimal one?\n\n11.3.1 Walks, Trails, Paths, and Cycles\nEarlier I mentioned that we would spin up a fictitious story about our five person network (Chen, Patrick, Anika, Anvita, and Nate). We’ll do that now to help clarify some of the essential terminology, but we’ll start with a slightly smaller version – just Anika, Chen, and Patrick – and then add the others in.\nImagine a research lab with an upcoming team meeting with an ominous sounding item on the agenda. One member of the team, Chen, knows something about the context for this item and shares it with another member of the team, Patrick. In this scenario, Chen sends the information to Patrick (remember, this makes them adjacent).\nAs we’ve discussed, nodes are also linked to other nodes indirectly. Let’s say Chen’s information came from another team member, Anika. Anika and Patrick are not adjacent, but they are indirectly connected via Chen. How can we describe this relationship? And why does it matter whether we describe it at all?\nIn network analysis, we are often interested in whether something (e.g., information about the ominous sounding agenda item) can travel from node \\(i\\) (e.g., Anika) to node \\(j\\) (e.g., Patrick). If it is indeed possible for that to happen, how many people would it have to go through to get there? And is there more than one way it might get there? If so, are some ways more efficient than others? If so, which ones? We can answer questions such as these about any nodes in a network by invoking the concept of a walk, which also provides both general and specific terminology for describing a wide variety of indirect relationships (Borgatti and Everett 2020). Consider, the hypothetical network in Figure 11.5, which we just described with words.\n\n\n\n\n\n\nFigure 11.5: Cap\n\n\n\nA walk is simply any sequence of adjacent nodes and edges that start with some node and end with a node. They can even start and end with the same node. In fact, the same node can appear in a walk more than once, and so can the same edges! In short, a walk is a very general way of describing any way that you can go from one node to another by “walking” along the edges, even if what you want to do is get back to where you started. There are no restrictions provided the edges to walk on actually exist (or rather, are observed). This opens all kinds of useful ways of thinking about the distances between nodes, operationalized in terms of lengths of walks, which is defined in terms of the number of edges contained in the walk. In the above network, the walk from Anika to Patrick passes through 1 node, Chen, but has a length of 2 because it consists of the relationship between Anika and Chen, and between Chen and Patrick (2 edges).\nLet’s complicate this just a wee bit by introducing a few additional team members, our fictitious friends Anvita and Nate. Chen, especially anxious about the ominous agenda item, shares the information with Anvita and Nate. Anvita doesn’t know Patrick already knows, so shares the information with Patrick. Patrick doesn’t know the information ultimately came from Anika, so sends the information back to Anika. Nate prefers not to pass the information along because they aren’t sure how credible it really is. Figure 11.6 shows the structure of this network with the new information sharing relations. Note that you can’t read this representation left to right! The information flow process started with Anika in this hypothetical example.\n\n\n\n\n\n\nFigure 11.6: Cap\n\n\n\nOur initial walk from Anika to Patrick still exists, of course, but now we also have the possibility of many other walks. Anika to Chen to Nate is a walk. Anika to Chen to Anvita to Patrick and back to Anika is a walk. More specifically it is a closed walk because it starts and ends with the same node: Anika.\nIn empirical networks, the number of possible walks between any pair of nodes can be vast, but we can impose some order by grouping them into different kinds of walks, such as trails, paths, and cycles. A trail is type of walk where edges are not allowed to repeat themselves. For example, Anika to Chen to Anvita to Patrick to Anika is a trail but the exact same walk would not be a trail if we included another step to Chen (as that would be repeating an edge). The length of a trail is equal to the number of edges contained in the trail, which in the example above would be 4. A path is a type of walk where nodes are not allowed to be repeated. That means that the trail from Anika to Chen to Anvita to Patrick to Anika is not a path, because Anika is repeated twice, but Anika to Chen to Anvita to Patrick is. As with trails, the length of a path is equal to the number of edges it contains. Finally, cycles are types of closed walks that (a) involve a minimum of three nodes where the only node that is repeated is the node that starts and ends the walk, and (b) no edges are repeated. There are many other specific types of walks that we will not discuss here. For directed networks, the three-node minimum may be relaxed if you have two nodes that send an edge to each other. This is sometimes called a trivial cycle and its status as a cycle or not should depend on why you care about cycles.\nAll of these examples are walks. Some of those walks are trails, and some of those trails are paths and others are cycles. If there is a path between two nodes, say between Anika and Nate, then we say that those two are reachable.\nIn connected networks, there are typically many possible paths that connect any given pair of nodes in a network, but they are not all equally efficient. While information and other resources can certainly travel through a network via inefficient routes, the likelihood of actually going the distance is much greater when traveling on efficient paths. For that reason, we are commonly interested in focusing on the shortest paths between nodes. We will spend a good amount of time in the next chapter discussing shortest paths, followed by a discussion of some alternative assumptions about how contagions flow in a network.\nWe will leave our discussion of walk-structures for now. The key thing to emphasize right now is the general logic of traversing a social network this way, and to understand that the structure of the network affects the flow of contagions through it, meaning people in the network will be differentially exposed to those contagions, good or bad. We will return to this issue in the next chapter. For now, let’s turn to the some micro building blocks of network structure.",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Relational data</span>"
    ]
  },
  {
    "objectID": "relational-data.html#conclusion",
    "href": "relational-data.html#conclusion",
    "title": "11  Relational data",
    "section": "11.4 CONCLUSION",
    "text": "11.4 CONCLUSION\n\n\n11.4.1 Key Points\n\nRelational thinking provides new, different, and valuable ways of approaching social science\nDifferent types of ties change how we should think about a network\nLearned how to work with network files and datatypes in NetworkX\nDiscussed walks, paths, cycles, trails: ways of describing how things can move or traverse through a network\n\n\n\n\n\n\nadams, jimi. 2020. Gathering Social Network Data. SAGE Publications Incorporated.\n\n\nBarton, Allen. 1968. “Bringing Society Back in Survey Research and Macro-Methodology.” The American Behavioral Scientist 12 (2): 1.\n\n\nBearman, Peter, James Moody, and Katherine Stovel. 2004. “Chains of Affection: The Structure of Adolescent Romantic and Sexual Networks.” American Journal of Sociology 110 (1): 44–91.\n\n\nBorgatti, Stephen, and Martin Everett. 2020. Three Perspectives on Centrality. Edited by Ryan Light and James Moody. Oxford University Press.\n\n\nBorgatti, Stephen, Martin Everett, and Jeffrey Johnson. 2018. Analyzing Social Networks. Sage.\n\n\nBrailly, Julien, Guillaume Favre, Josiane Chatellet, and Emmanuel Lazega. 2016. “Embeddedness as a Multilevel Problem: A Case Study in Economic Sociology.” Social Networks 44: 319–33.\n\n\nBreiger, Ronald. 1974. “The Duality of Persons and Groups.” Social Forces 53 (2): 181–90.\n\n\nBuhari-Gulmez, Didem. 2010. “Stanford School on Sociological Institutionalism: A Global Cultural Approach.” International Political Sociology 4 (3): 253–70.\n\n\nCentola, Damon. 2018. How Behavior Spreads: The Science of Complex Contagions. Princeton University Press Princeton, NJ.\n\n\nCrossley, Nick, Elisa Bellotti, Gemma Edwards, Martin G Everett, Johan Koskinen, and Mark Tranmer. 2015. Social Network Analysis for Ego-Nets: Social Network Analysis for Actor-Centred Networks. Sage.\n\n\nFreeman, Linton. 2004. “The Development of Social Network Analysis.” A Study in the Sociology of Science 1 (687): 159–67.\n\n\nHarrigan, Nicholas, Giuseppe (Joe) Labianca, and Filip Agneessens. 2020. “Negative Ties and Signed Graphs Research: Stimulating Research on Dissociative Forces in Social Networks.” Social Networks 60: 1–10.\n\n\nKitts, James. 2014. “Beyond Networks in Structural Theories of Exchange: Promises from Computational Social Science.” In Advances in Group Processes. Emerald Group Publishing Limited.\n\n\nKitts, James, and Eric Quintane. 2020. “Rethinking Social Networks in the Era of Computational Social Science.” The Oxford Handbook of Social Networks, 71.\n\n\nLazega, Emmanuel, and Tom Snijders. 2015. Multilevel Network Analysis for the Social Sciences: Theory, Methods and Applications. Vol. 12. Springer.\n\n\nMeyer, John W, Georg Krücken, and Gili Drori. 2009. World Society: The Writings of John w. Meyer. Oxford University Press.\n\n\nMützel, Sophie, and Ronald Breiger. 2020. “Duality Beyond Persons and Groups.” The Oxford Handbook of Social Networks, 392.\n\n\nPerry, Brea, Bernice Pescosolido, Mario Small, and Ann McCranie. 2020. “Introduction to the Special Issue on Ego Networks.” Network Science 8 (2): 137–41.\n\n\nPrell, Christina. 2012. Social Network Analysis: History, Theory and Methodology. Sage.\n\n\nRobins, Garry. 2015. Doing Social Network Research: Network-Based Research Design for Social Scientists. Sage.\n\n\nScott, John. 2017. Social Network Analysis. Sage.\n\n\nSmall, Mario, Brea Perry, Bernice Pescosolido, and Ned Smith, eds. 2021. Personal Networks: Classic Readings and New Directions in Ego-Centric Analysis. Cambridge University Press.",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Relational data</span>"
    ]
  },
  {
    "objectID": "setting-up.html",
    "href": "setting-up.html",
    "title": "1  Setting up",
    "section": "",
    "text": "1.1 Open Source Software\nI’ve developed a specialized computing environment that you can use for this book, and your computational social science research more generally. Like all good research computing setups, this one starts with a good text editor: VS Code. Whether you work in the cloud using GitHub Codespaces or locally on your own computer, you’ll be using VS Code.\nA good text editor is an essential part of your research toolkit. You’ll use it primarily for writing code, but it’s also great for writing academic articles, chapters, theses, books, and so on. These days, most text editors have similar capabilities. While they may differ in appearance and feel, they can be customized extensively to suit your needs. Picking an editor might seem overwhelming, but it doesn’t need to be. For instance, you can check out Wikipedia’s article on the Editor War between Emacs and Vi to see how debates over relatively minor differences can evolve into multi-generational rivalries. To quote Brian Ward (2021) from How Linux Works, “Most UNIX wizards are religious about their choice of editor, but don’t listen to them. Just choose yourself” (pp. 24). You should try out a few and stick with what works best for you.\nIn what follows, I’ll assume you’re using Visual Studio Code (VS Code). It’s free, open-source, runs on all operating systems, has excellent support for computing on remote machines, and has excellent community extensions for writing code, working with data, and even drafting technical documents. It’s especially useful for Python. VS Code also integrates some great features for collaborative coding and working in containers, making it a perfect fit for everything you’ll tackle in this book, and computational social science projects more generally.\nWe’ll be using VS Code configured to act like an Integrated Development Environment (IDE) specifically for data science and computational social science. This setup ensures that all the tools you need to work with data, write code, and create reports are integrated into a single environment. You’ll be writing and executing Python code, generating plots, and authoring documents—all within VS Code. Instead of Jupyter Notebooks, we’ll use Quarto, a powerful tool that enables you to combine code, text, and visuals in one place, much like Jupyter but with more flexibility, especially for creating publications.\nThe key benefit of this setup is that you’ll be working in a containerized environment. Think of a container as a pre-configured computer within your own, complete with all the software and tools you need for this course. Inside the container, you’ll have access to Python, data analysis libraries, and Quarto—so there’s no need to install or manage these tools on your personal machine. We’ve already taken care of that in the course’s Docker image.\nThere are two main ways to use this computing setup: in the cloud using GitHub Codespaces, or locally on your own machine. If you are entirely new to this stuff, I would recommend starting in the cloud and setting up your local environment later. But select whatever approach you prefer!",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up</span>"
    ]
  },
  {
    "objectID": "setting-up.html#open-source-software",
    "href": "setting-up.html#open-source-software",
    "title": "1  Setting up",
    "section": "",
    "text": "1.1.1 Working in the Cloud with GitHub Codespaces\nIf you’re new to all of this and want to simplify the setup process, I recommend working in the cloud using GitHub Codespaces. First, you’ll need to sign up for a free GitHub account at github.com. GitHub Codespaces is a cloud-based development environment that allows you to run VS Code directly in your web browser, complete with all the tools and extensions you need. It’s especially convenient because it eliminates the need to install software on your own computer; everything runs on GitHub’s servers. As a student, you can access Codespaces for free. However, keep in mind that if your codespace remains inactive for a while, GitHub may prompt you to save your changes so that the codespace can be deleted to free up resources.\nTo get started, navigate to the supplementary materials repository for this book at https://github.com/UWNETLAB/dcss_supplementary. Once you’re there, you’ll see a green Code button near the top right corner of the repository page. Click on it, and in the dropdown menu, select Open with Codespaces. If you don’t see this option, it might be under Codespaces in the dropdown. This will launch a new Codespace instance in your browser, loading the repository’s contents and setting up the pre-configured development environment. Please note that the repository contents are being updated, so you might see new materials appear as they’re added.\nWhile working in the cloud with GitHub Codespaces is convenient, you might prefer to set up the environment on your own computer, especially if you want more control or if you anticipate working without a stable internet connection. In the next section, I’ll guide you through setting up the necessary software on your local machine so you can work offline and have all your tools directly at your fingertips.\n\n\n1.1.2 Working Locally on Your Own Computer\nIf you want to work locally on your own machine, instead of remotely in GitHub Codespaces, you’ll need to download and install Docker Desktop, Git, and VS Code on your computer. Once these are installed, you’ll be able to clone the course repository from GitHub, open it in VS Code, and start working in the container environment.\nTo get set up, install Docker Desktop and VS Code. Docker is the software that allows you to run the container, which contains the entire computing environment. To install Docker Desktop, go to the Docker Desktop page for your operating system. Download the installer and follow the instructions. Once Docker is installed, you’ll see its icon in your system’s toolbar or taskbar. Make sure it’s running before you proceed to the next steps.\nYou can download Visual Studio Code (VS Code) for free at code.visualstudio.com. After installing, open VS Code and head to the Extensions tab (on the left sidebar). Search for and install the following extensions:\n\nDev Containers (this lets you work in the container environment seamlessly)\nPython (for Python code development)\nQuarto (for authoring your Quarto files)\n\nThese extensions will make it easy to work with Python code and documents, as well as interact with the pre-configured container I’ve built for you.\nFinally, you’ll need Git, which will help you manage the course materials from GitHub. Install it from git-scm.com and follow the instructions for your operating system.\nOnce you have these tools installed, you’re ready to start working in the pre-configured environment. You don’t need to worry about installing Python, data science libraries, or Quarto—they’re all set up inside the container.\n\n\n1.1.3 Launching the Container and Getting Started\nNow that everything is installed, here’s how to set up the course materials and start working in the containerized environment:\n\n1.1.3.1 Clone the Course Repository\n\nOpen VS Code.\nClick on View in the top menu, then select Command Palette (or press Ctrl+Shift+P on Windows/Linux or Cmd+Shift+P on Mac).\nIn the command palette, type Git: Clone and select it.\nWhen prompted for the repository URL, enter https://github.com/UWNETLAB/dcss_supplementary.git. # update this\nChoose a local folder where you want to clone the repository.\n\n\n\n1.1.3.2 Open the Repository in VS Code\n\nAfter cloning, a prompt may appear asking if you want to open the cloned repository. Click Open.\nIf not, you can manually open it by going to File &gt; Open Folder and navigating to where you cloned the repository.\n\n\n\n1.1.3.3 Open in a Dev Container\n\nOnce the repository is open in VS Code, you might see a prompt at the bottom right corner asking if you want to Reopen in Container. Click on it.\nIf you don’t see the prompt, you can click on the green icon in the bottom-left corner of VS Code (the Dev Containers icon) and select Reopen in Container.\nVS Code will now build and open the container. This may take a few minutes the first time.\n\nThat’s it! You’re now set up to work within a complete computational social science environment, with everything you need installed and ready to go. In the rest of this chapter, I’ll introduce some basic knowledge and skills for command-line computing and using Git for version control. Then I’ll describe the differences between two types of virtualization used in this book: Docker containers and DevContainers on the one hand, and virtual environments managed by Poetry and/or Conda on the other. Feel free to skip this content and come back to it later if and when you need it.",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up</span>"
    ]
  },
  {
    "objectID": "setting-up.html#command-line-computing",
    "href": "setting-up.html#command-line-computing",
    "title": "1  Setting up",
    "section": "1.2 Command-Line Computing",
    "text": "1.2 Command-Line Computing\nDoing computational social science or data science often requires interacting with computers using a Command-Line Interface (CLI). This may feel unfamiliar and inefficient in the age of mobile computing, beautifully designed Graphical User Interfaces (GUIs), and touch screens, but it is what enables us to do computational research in the first place. A bit of knowledge unlocks a vast world of high-quality open-source tools and enables us to benefit from the expertise and experience of many other researchers, scientists, and engineers around the world. Among a great many other things, command-line computing also enables us to work on remote computers; organize our software, data, and models according to best practices; efficiently manage and track our research projects; and make collaboration much easier, and transparency, accountability, and reproducibility possible. The command line is our common point of departure, regardless of which operating system you happen to be using. In what follows, I’ll introduce some essentials of command-line computing, starting with an introduction to “the shell.”\n\n1.2.1 The Shell\nWhen I talk about working “on the command line” or “in a terminal,” what I really mean is that we tell our computer’s operating system what we want it to do by interacting with a program called the shell. We interact with the shell by typing commands into a terminal emulator, or “terminal.” Linux, macOS, and Windows all come with pre-installed terminal emulators, but for the sake of convenience we’ll use the one that’s built into VS Code. You can open it by clicking on the ‘View’ and then ‘Terminal’ menu items, or by using a keyboard shortcut.\nWhat you see when you first open a terminal window will depend on your machine and whatever theme is active in VS Code. By default, most will likely display your username and the name of the computer you’re working on (e.g., user@computer$). My setup—which you can see in Figure 1—uses the Nord theme for VS Code and customizes the terminal prompt using a tool called Starship. As I’ll explain shortly, I’ve used Starship to configure the terminal prompt in the pre-built computing environment that we’ll use throughout this book.\n\n\n\n\n\n\nFigure 1.1: The terminal in VS Code\n\n\n\nTo interact with the shell—that is, to tell our computer what we want to do—we type commands on the command line and then hit the Return key (i.e., Enter). To see how this works, type cal on the command line, and then hit Return.\ncal\ncal is a command-line program installed on your computer; when you type the command cal, your computer executes that program and prints a calendar to the screen with the current day highlighted. Other commands we pass to the shell are also programs, and as we will see below, they tend to be most useful when we provide those programs with additional information about what we want. If you happen to issue a command that your computer does not recognize, nothing bad will happen. Your computer will just tell you that it doesn’t understand what you want it to do. Go ahead and type something like Hi, shell! and hit return. We can do almost anything we want on the command line, but we have to use commands that our computer knows about.\n\n\n1.2.2 The Structure of Shell Commands\nGenerally, the commands you execute will have the same basic structure, though the components of a command may be called different things by different people. The first word you type is the command itself, or the name of the program you are going to execute. Often, the default behavior can be modified by using options, which can be specified using a “short” version (a letter following a -, such as ls -l) or a long version (a word following --, such as ls --long). When using the short version, you can string multiple options together following a single -, such as ls -ltS. In addition to the command itself and the options that follow, commands may take arguments, such as the name of a file or directory that a command should act on. This may seem very abstract now, but if you follow along with the commands below in your terminal, it will become more concrete.\n[COMMAND] [OPTIONS] [ARGUMENTS]\nWhile there are different terminal applications available, they all do the same thing: interact with the shell. If you’re using Windows, you’re going to end up running into the limitations of your default terminal emulator pretty quickly. One solution to this problem is to get a proper terminal emulator and configure it to best meet your needs. However, in this case, it’s unnecessary because you’ll be using the terminal in VS Code to interact with the shell inside the course’s pre-built computing environment, which is Linux!\nNow that you’re set up with a good terminal emulator that runs in a Linux environment, let’s discuss a few very simple but essential actions for getting work done on the command line. We’ll focus on a small subset of commands for navigating the file system and performing various operations with directories and files.\n\n\n1.2.3 Getting Around the File System\nThe commands you will use most frequently are those that enable you to navigate a computer’s file system and perform basic operations on directories (i.e., “folders”) and files. Let’s focus on those commands first. I recommend that you work through this chapter with your terminal open. You are likely to learn faster and more deeply if you execute these commands as you read, and if you take little breaks along the way to practice what you’ve learned.\n\n1.2.3.1 Directories and Files\nAlthough there are some differences across operating systems—chiefly between Windows and the *nix systems Linux and macOS—these operating systems all organize directories (or “folders”) as hierarchical trees. The root directory sits at the top of that tree; all files and subdirectories are contained within it. We call it ‘root’, but in most operating systems, it’ll just look like a single forward slash (/). In what follows, we’ll use the pre-built computing environment, which, again, is Linux.\nMost users navigate through their computer’s directory structure using a GUI, such as Finder on macOS or File Explorer on Windows. This process works a little differently on the command line, of course, but ultimately we want to do the same thing: get “into” the directories where our files are, as well as the directories above and below them in the filesystem. The directory we are “in” at any given time is the current working directory. To see where we are in the filesystem, we can print the path to that directory by typing the command pwd (print working directory).\npwd\nTo change directories, we use the cd command (change directory). For example, to move to the home directory, you can type:\ncd ~\nOr to move up one level to the parent directory:\ncd ..\nThe command cd is usually followed by an argument, which provides more information to the shell about where you want to go. For example, you could provide an absolute path, which starts with the root directory and goes to whatever directory you want to be in. For example, if I wanted to cd into a directory where I keep various files related to graduate supervision, I could type the absolute path:\ncd /users/johnmclevey/Documents/supervision/grad_students/\nThe path is essentially an ordered list of the nested directories you would have clicked through if you were navigating the file system using a GUI like Finder or File Explorer. Each step in the path is separated by a /.\nBecause this path starts at the root directory, I can execute it regardless of where in the filesystem I currently am. Alternatively, I can provide cd with a relative path, which tells the shell where to go relative to the current working directory. For example, if my current working directory was /users/johnmclevey/Documents/ and I wanted to get to grad_students/, I could use the following relative path:\ncd supervision/grad_students/\nIn this case, the command worked because we were “in” the Documents directory. But if /users/johnmclevey/Dropbox/ was my current working directory and I typed the same command, the shell would tell me that there is no such file or directory. When using relative paths, you have to provide the path from the current working directory to the directory where you want to be.\nTo list the files in a directory, you can use the command ls. If we execute ls without any arguments, it will default to printing the files in the current working directory, but if provided with an absolute or relative path, it will list the contents of that directory instead. Below, for example, we list the contents of the current working directory’s parent directory.\nls ..\nWe can provide ls with a number of options that modify what the program prints to screen. For example, we can print some metadata about our files—such as their access permissions, the name of the user who owns the file, the file size, the last time they were modified, and so on—if we add the option -l, which is short for “long output.”\nls -l\nWe can string together a number of these short options to change the behavior of the command. For example, adding the option t to our command (ls -lt) changes the order of the files printed to screen such that the most recently modified files are at the top of the list, whereas ls -lS prints them with the largest files on top. Using ls -a will display “hidden” files, some of which we will discuss below.\n\n\n\n1.2.4 Creating Files and Directories\nIt is also possible to create new directories and files from the command line. For example, to make a new directory inside the current working directory, we can use the command mkdir followed by the name of the directory we want to create. Once created, we can move into it using cd.\nmkdir learning_shell\ncd learning_shell\nTo create a new file, we use the touch command followed by the name of the file we want to create. For example, we could create a simple text file called test.txt.\ntouch test.txt\ntest.txt is an empty file. If we wanted to quickly add text to it from the command line, we could use a built-in command-line text editor, the most minimal of which is called nano. We can edit the file by calling nano and providing the name of the file we want to edit as an argument.\nnano test.txt\n\n1.2.4.1 Getting Help\nThe commands we’ve just learned, summarized in Table 1, are the ones that you will use the most often when working on the command line and are worth committing to memory. You can build out your knowledge from here on an as-needed basis. One way to do this is to look up information about any given command and the options and arguments it takes by pulling up its manual page using the man command. For example, to learn more about ls, you could type:\nman ls\nYou can then page through the results using your spacebar, and return to the command line by pressing the letter q.\nTable 1: Essential commands for working from the command line\n\n\n\n\n\n\n\n\nCommand\nAction\nExample\n\n\n\n\npwd\nPrint current working directory\npwd\n\n\ncd\nChange directory\ncd ..\n\n\nls\nList directory contents\nls -ltS\n\n\nmkdir\nMake new directory\nmkdir figures tables\n\n\ntouch\nMake a new text file\ntouch search_log.txt\n\n\nrm\nRemove file/directory\nrm output\n\n\ncp\nCopy file/directory\ncp manuscript_draft submitted_manuscripts\n\n\nmv\nMove file/directory\nmv manuscript_draft submitted_manuscripts\n\n\ngrep\nSearch files for a pattern\ngrep 'pattern' file.txt\n\n\nopen\nOpen a file or directory in the default application\nopen search_log.txt\n\n\nhistory\nPrint the history of commands issued to the shell\nhistory\n\n\n\nIf you are looking to expand your general command-line toolkit even further, there is an enormous number of high-quality tutorials online. Now that you have a basic foundation, learning will be faster. However, as mentioned earlier, I recommend against taking a completionist approach. Instead of trying to learn everything at once, get comfortable with this foundation and expand your knowledge in a problem-driven way. That way you will have time to practice and make interacting with your computer this way feel more natural, fast, and automatic.\n\n\n\n1.2.5 Further Reading\nIn general, the best way to deepen your knowledge of open-source computing, for research or otherwise, is to learn more about Linux and command-line computing. You’ll find plenty of excellent free resources about Linux and open-source computing online, but if you’re looking for a more guided and scaffolded tour that doesn’t throw you right into the deep end, I would recommend William Shotts’ The Linux Command Line or Brian Ward’s How Linux Works.\nNow let’s learn how to use version control software, specifically Git, in order to do our work in more transparent, auditable, and reproducible ways. I’ll focus on using a visual interface for Git (a “GUI”) inside VS Code and then I’ll explain how to use Git on the command line. Use whichever approach you prefer.",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up</span>"
    ]
  },
  {
    "objectID": "setting-up.html#version-control-tools",
    "href": "setting-up.html#version-control-tools",
    "title": "1  Setting up",
    "section": "1.3 Version Control Tools",
    "text": "1.3 Version Control Tools\nReal research projects can become complex messes very quickly. While there are organizational strategies you can adopt to keep the messiness to a minimum, a good version control system is essential. Version control systems watch your full project and record all the changes that happen everywhere, to every file (unless you tell it to ignore a file or subdirectory). It keeps logs of who did what, to what files, when, and if you use it properly, it can even keep log files and other notes associated with each change to the project. It eliminates the “need” for long descriptive file names with information about dates, authors’ comments, and revision stages, or emailing the “final version” of a .docx file back and forth between collaborators. At any given point, you can roll back in time to a previous state of the project. They also unlock enormous potential for collaboration—in fact, the version control system I recommend, Git, is used to manage what is arguably the largest and most complex software development project in the world, the Linux kernel, built by over 5,000 individual developers from over 400 organizations around the world.\nThe model for Git and other version control systems is that you store your project in a repository. Once your repository has been “initialized,” you work on your files as you normally would. From time to time—say each time you start and finish working on some piece of your project—you “add” or “stage” your changes and then “commit” them to the repository along with a brief log message about what you did and why. If your repository is linked to a remote server, such as GitHub, you can also “push” your changes to the remote repository.\nAs you do this, you build up a complete record of the history of your project, including who (your past self included) did what and why. You can roll back to any place in the history of your project, create experimental branches of the project to explore other lines of inquiry, and so on. Your collaborators can also have access to the remote repositories and can “clone” (i.e., download) the repository with the full history, make their own changes, add them, commit them, and push them back to the remote repository. You can get those changes yourself by “pulling” down new changes.\n\n1.3.1 Git from Inside VS Code\nUsing Git within VS Code provides a user-friendly way to manage version control without needing to remember all the command-line instructions. Here’s how you can use Git inside VS Code:\n\nAfter you’ve opened your project folder in VS Code, you’ll notice an icon on the left sidebar that looks like a branch with three nodes—that’s the Source Control tab. Click on it to access Git features.\nWhen you make changes to your files, VS Code will detect them and list them under Changes in the Source Control tab. You can click on each file to see what has changed.\nTo stage your changes, hover over the file and click on the + icon that appears, or right-click and select Stage Changes. Staged changes are ready to be committed.\nOnce you’ve staged the changes you want to commit, enter a commit message in the text box at the top (e.g., “Added data cleaning script”) and click the checkmark icon to commit them.\nIf your repository is linked to a remote repository on GitHub, you can push your commits by clicking on the ellipsis icon (...) in the Source Control tab and selecting Push. Similarly, you can pull changes from the remote repository by selecting Pull.\n\nUsing Git within VS Code makes version control more visual and can help you better understand what’s happening in your project. It also integrates well with other features of VS Code, such as the built-in terminal and extensions.\n\n\n1.3.2 Git from the Command Line\nIt’s also possible to use Git from the command line—in fact, it was designed for the command line! Let’s say you are starting a new web scraping project. You’ve written some code, all stored inside a directory called scraping_project. To use Git to manage your project, you would cd into the project directory and initialize Git. You only have to initialize Git once for a project.\ncd scraping_project\ngit init\nOnce you’ve made some changes to your project, you can “stage” the changes using git add. You can track individual file changes, but the easiest thing to do is to track any changes that have been made to the whole project directory. You can do this by specifying that you are adding changes for the full directory using the . (remember: . indicates the current directory).\ngit add .\nNext, you’ll want to write a commit message that briefly describes the changes you’ve made. For example, we might write:\ngit commit -m \"Drafted initial web scraping script\"\nGit provides a number of other useful commands that you’ll make frequent use of. For example, if you have an account on GitHub, GitLab, or some other service for managing Git repositories, you can push your changes to the remote version by using git push. Similarly, you could also “pull” down an up-to-date version of the project on another computer, once again making your work more portable. Other useful Git commands are provided in Table 2.\nTable 2: The subset of Git commands you need to do 99% of your work with\n\n\n\n\n\n\n\n\nCommand\nAction\nExample\n\n\n\n\ngit init\nInitialize a new Git repository\ngit init .\n\n\ngit add\nStage changes\ngit add ., git add article.md\n\n\ngit commit -m\nCommit changes with log message\ngit commit -m \"Drafted introduction\"\n\n\ngit push\nPush changes to remote repository\ngit push\n\n\ngit status\nCompare local repository with remote repository\ngit status\n\n\ngit pull\nUpdate your local repo with changes from remote\ngit pull\n\n\ngit clone\nClone a remote repository\ngit clone [URL]\n\n\n\nUsing version control software like Git has many benefits, not the least of which is that when you return to your work after a period of working on something else—say, when a chapter is being reviewed by your committee or a paper is under review—you will know exactly what you did and why later. When it comes to accountability, transparency, and reproducibility, this is perhaps the most important thing: you must always know what you did.\n\n\n1.3.3 Further Reading\nUsing tools like Git makes transparency, accountability, and reproducibility possible, but only if you use them properly. As you become more comfortable with these tools, you’ll want to seek out advice on how people use them for different types of projects. I recommend Patrick Ball’s (2016) “Principled Data Processing (PDP)” framework. Eric Ma’s (2021) Data Science Bootstrap: A Practical Guide to Getting Organized for Your Best Data Science Work also provides a lot of excellent advice for scientific computing more generally, and for setting up workflows for projects that don’t lend themselves well to the PDP framework.\nFinally, as a computational social scientist, you’ll need to understand and make use of virtualization tools. I’ve already set all of this up for you in our pre-built environment, so if all of this is new to you, it’s okay to skip over this next section and come back if and when you need it.",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up</span>"
    ]
  },
  {
    "objectID": "setting-up.html#virtualization-tools",
    "href": "setting-up.html#virtualization-tools",
    "title": "1  Setting up",
    "section": "1.4 Virtualization Tools",
    "text": "1.4 Virtualization Tools\nAs a computational social scientist, you’ll need to understand and utilize virtualization tools to ensure your work is consistent, reproducible, and portable across different computing environments. While this might seem complex at first, virtualization allows you to create isolated environments that encapsulate all the dependencies your projects require. This ensures that your code runs exactly as intended, regardless of where or how it’s executed.\nWe’ll focus on two primary types of virtualization: containers and virtual environments. Both serve the purpose of isolating your project’s dependencies but operate at different levels.\n\n1.4.1 Containers\nContainers are a form of operating system virtualization that allows you to run applications in isolated environments. They package up software with all its necessary components—code, runtime, system tools, libraries, and settings—so it runs consistently across different computing environments.\nThe most popular tool for working with containers is Docker. Docker enables you to create, deploy, and run applications inside containers. In this book, we’ve provided a pre-built Docker image that contains all the software and libraries you’ll need for computational social science research.\nDevContainers are a feature of VS Code that leverages Docker to provide a seamless development experience. When you open a project configured with a DevContainer, VS Code automatically builds and starts the Docker container defined for the project. This means you don’t have to worry about installing the correct versions of Python or any libraries on your local machine—the container handles all of that for you.\nUsing Docker and DevContainers has several advantages:\n\nConsistency: Everyone working on the project uses the same environment, eliminating “it works on my machine” problems.\nIsolation: Your development environment is isolated from your local machine, preventing conflicts between project dependencies.\nReproducibility: Containers can be versioned and shared, ensuring that others can reproduce your computational environment exactly.\n\nTo get started with containers in this book, all you need is Docker Desktop and the Dev Containers extension in VS Code (which we’ve covered earlier). When you open the course repository in VS Code, it will automatically detect the DevContainer configuration and prompt you to reopen the project inside the container. Once you do, you’ll be working inside the pre-configured environment we’ve provided.\n\n\n1.4.2 Virtual Environments\nWhile containers isolate the entire operating system environment, virtual environments allow you to isolate Python dependencies at the project level. This is particularly useful when you have multiple projects requiring different versions of the same packages.\nIn this book, we’ll occasionally use virtual environments managed by Conda or Poetry. These tools help you create and manage project-specific environments, ensuring your Python packages and dependencies are neatly organized.\n\n1.4.2.1 Using Conda for Virtual Environments\nConda is an open-source package and environment management system that works across Windows, macOS, and Linux. It’s especially popular in the data science community. We can create and activate a Conda environment from the command line within VS Code:\nFirst, Open the Terminal in VS Code. Click on ‘Terminal’ in the top menu and select ‘New Terminal’, or use the shortcut Ctrl+Shift+ (Windows/Linux) or Cmd+Shift+ (macOS). Then Create a New Environment. Navigate (cd) to your project directory and create a new environment with\nconda create --name dcss\nwhere dcss is the name of the environment. (You can use whatever name you like.)\nOnce you’ve done that, you can Activate the Environment with\nconda activate dcss\nYou’ll notice your terminal prompt now starts with (dcss), indicating the environment is active. You can Install Packages (e.g., numpy and pandas) into your environment using conda install:\nconda install numpy pandas\n\n\n1.4.2.2 Using Poetry for Virtual Environments\nPoetry is a tool that manages Python dependencies and virtual environments, simplifying the process of setting up and maintaining your project. It’s installed in the Docker container we’re working in, and we’re using it to manage our dependencies right now! I won’t get into all the details of using Poetry here, but there are a couple of useful commands you should know about.\nFirst, to install Python packages (e.g., bambi) into the course environment, you can run\npoetry add bambi\nSecond, to install or update all packages, run\npoetry install\n\n\n\n\n\n\nPlanned Updates\n\n\n\nI am going to add a lot of content on Poetry at some point soon. Most likely in fall 2024.\n\n\n\n\n1.4.2.3 Activating Virtual Environments in VS Code\nVS Code is adept at recognizing and working with virtual environments. After creating and activating a virtual environment, you might need to tell VS Code to use it for running Python code:\n\nSelect the Python Interpreter: Press Ctrl+Shift+P (Windows/Linux) or Cmd+Shift+P (macOS) to open the Command Palette. Type Python: Select Interpreter and select it.\nChoose Your Environment: From the list of available interpreters, select the one corresponding to your virtual environment.\n\nThis ensures that when you run or debug your code in VS Code, it uses the correct environment.\n\n\n1.4.2.4 When to Use Virtual Environments\nWhile our pre-built Docker environment covers most of the computational needs for this book, there are instances—like when we explore generative models for networks—where you’ll need to create and manage virtual environments. Virtual environments are particularly useful when:\n\nWorking on Multiple Projects: Isolating dependencies prevents conflicts between projects.\nRequiring Specific Package Versions: Some projects might need specific versions of libraries that differ from others.\nCollaborating with Others: Sharing a pyproject.toml or environment.yml file allows collaborators to replicate your environment exactly.\n\nIf you’re new to this, don’t worry. You can rely on the container setup for now and revisit virtual environments when we reach those chapters.",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up</span>"
    ]
  },
  {
    "objectID": "setting-up.html#conclusion",
    "href": "setting-up.html#conclusion",
    "title": "1  Setting up",
    "section": "1.5 Conclusion",
    "text": "1.5 Conclusion\nStarting with computational social science might seem overwhelming due to the array of tools and technologies involved. However, each tool serves a specific purpose in streamlining your workflow, enhancing collaboration, and ensuring reproducibility.\nWhile the initial setup requires effort, investing time in learning these tools will pay off immensely. You’ll find that they not only make your research more efficient but also open up possibilities for collaboration and innovation that aren’t feasible with traditional methods.\nRemember, it’s perfectly normal to feel a bit lost when diving into something new. Take it step by step, refer back to this chapter as needed, and don’t hesitate to seek out additional resources or ask questions. Soon enough, using these tools will become second nature, and you’ll be well-equipped to tackle any computational challenge in your research.",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up</span>"
    ]
  },
  {
    "objectID": "setting-up.html#key-points",
    "href": "setting-up.html#key-points",
    "title": "1  Setting up",
    "section": "1.6 Key Points",
    "text": "1.6 Key Points\n\nCommand Line Proficiency: Understanding how to navigate and operate in the shell is foundational for computational social science.\nVS Code: A versatile text editor and IDE that integrates seamlessly with tools like Git, Docker, and virtual environments.\nGit for Version Control: Essential for tracking changes, collaborating with others, and ensuring reproducibility.\nContainers with Docker and DevContainers: Provide consistent and isolated environments, ensuring your code runs the same everywhere.\nVirtual Environments with Conda and Poetry: Allow for project-specific dependency management without interfering with system-wide settings.\nIntegration in VS Code: All these tools work together within VS Code, providing a unified and efficient workflow.\nReproducibility and Collaboration: Using these tools promotes transparency, accountability, and ease of collaboration in research.\nContinuous Learning: Embrace a problem-driven approach to expand your skills as you encounter new challenges.\n\n\n\n\n\n\nBall, Patrick. 2016. “Principled Data Processing.” Data & Society.\n\n\nMa, Eric. 2021. Data Science Bootstrap: A Practical Guide to Getting Organized for Your Best Data Science Work. LeanPub.\n\n\nWard, Brian. 2021. How Linux Works: What Every Superuser Should Know. no starch press.",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up</span>"
    ]
  },
  {
    "objectID": "survey-data.html#surveys-in-the-21st-century",
    "href": "survey-data.html#surveys-in-the-21st-century",
    "title": "6  Processing Structured Data Survey Data",
    "section": "7.1 Surveys in the 21st century",
    "text": "7.1 Surveys in the 21st century",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>~~Processing Structured Data~~ Survey Data</span>"
    ]
  },
  {
    "objectID": "survey-data.html#sample-surveys-in-the-21st-century",
    "href": "survey-data.html#sample-surveys-in-the-21st-century",
    "title": "6  Processing Structured Data Survey Data",
    "section": "7.1 Sample surveys in the 21st-century",
    "text": "7.1 Sample surveys in the 21st-century",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>~~Processing Structured Data~~ Survey Data</span>"
    ]
  },
  {
    "objectID": "survey-data.html#common-issues-with-survey-data",
    "href": "survey-data.html#common-issues-with-survey-data",
    "title": "6  Processing Structured Data Survey Data",
    "section": "7.2 Common issues with survey data",
    "text": "7.2 Common issues with survey data",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>~~Processing Structured Data~~ Survey Data</span>"
    ]
  },
  {
    "objectID": "survey-data.html#processing-survey-data",
    "href": "survey-data.html#processing-survey-data",
    "title": "7  Survey Data",
    "section": "7.3 Processing Survey Data",
    "text": "7.3 Processing Survey Data\nNow that we’ve covered the broader context of survey methodology, past and present, let’s dive into the practical side of working with survey data. Before you can even begin to analyze your data, there are a number of essential tasks you’ll need to complete to ensure your data is clean and ready for analysis. This data processing work is a crucial part of any research project; talk to anyone with experience doing this kind of work and they’ll tell you how important, and time consuming, this work can be.\nSome of these tasks are common to all datasets, while others are specific to survey data, such as dealing with missing responses or applying survey weights. In this chapter, we’ll go through these steps one by one, using a panel survey from Canada as our working example.\nAs always, we’ll start by importing packages.\n\nimport pandas as pd\nimport dcss\n\ndcss.set_style()\n\n\n7.3.1 The Canadian Election Studies\nFor this chapter, we’ll be using data from the 2021 Canadian Election Study (CES-2021). The CES consists of two main surveys: the Campaign Period Survey (CPS), which was conducted during the 2021 federal election campaign, and the Post-Election Survey (PES), which followed shortly after the election.\nThe CPS ran from August 17 to September 19, 2021, and was divided into three waves:\n\nThe first wave, the CPS, was the initial survey.\nThe CPS Modules included additional questions and ran later in the campaign.\nThe CPS Oversample wave was designed to collect responses from regions that were underrepresented in the initial waves.\n\nAfter the election, all CPS participants were re-contacted for the Post-Election Survey (PES). The response rate for this follow-up was 72%, resulting in 15,069 completed surveys. The CES is a great example of a large, complex survey dataset, with multiple waves and extensive data cleaning procedures.\nOne of the key features of the CES is the use of survey weights, which are designed to make the sample more representative of the Canadian population. The weights adjust for differences in the likelihood of respondents being selected based on factors like region, gender, and age. In a later section, we’ll explore how to apply these weights in your analysis to ensure your results are valid.\n\n\n7.3.2 Download the data\nYou can download the datasets as zip files from their respective Dataverse repositories.1\n\n2019 Phone Survey\n\nStore in data/canadian_election_studies/Canadian-Election-Study-2019\n\n2019 Online Survey\n\nStore in data/canadian_election_studies/Canadian-Election-Study-2019\n\n2021 Surveys\n\nStore in data/canadian_election_studies/Canadian-Election-Study-2021\n\n\nAlternatively, you can use the download_dataset function from dcss, which will download and file the datasets for you.\n\nces_url = \"https://www.dropbox.com/scl/fo/hdqfapeqaiwk47t885iwg/ANjnpc8u4rnuMQ83TXExzQ4?rlkey=t6olw4xekzjm46mx8ndjgqrit&st=dlunronk&dl=0\"\n\ndcss.download_dataset(\n    ces_url, 'data/canadian_election_studies/'\n)\n\nFile already exists at 'data/canadian_election_studies/'. Skipping download.\n\n\n\n\n7.3.3 Load the Data\nIn this case, the researchers CITE have made the data available as .dta files, which are a format used by the proprietary software Stata. We can read these files into memory using Pandas read_stata() function, which we do for the 2021 survey data below.\n\nces21 = pd.read_stata(\n    'data/canadian_election_studies/Canadian-Election-Study-2021/2021 Canadian Election Study v2.0.dta', convert_categoricals=False)\n\nOnce the data is loaded, one of the first things you’ll want to check is the size of the dataset—how many rows and columns are there? This will give you a sense of how large the survey is and help you plan your analysis. We can do this using the .shape attribute for dataframes.\n\nces21.shape\n\n(20968, 1059)\n\n\nWe can also get a quick overview of the dataset by using the .info() function, which will show us the number of columns, their data types, and whether there are any missing values.\n\nces21.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20968 entries, 0 to 20967\nColumns: 1059 entries, cps21_StartDate to pes21_weight_general_restricted\ndtypes: datetime64[ns](7), float32(22), float64(753), int16(2), int32(2), int8(180), object(93)\nmemory usage: 142.1+ MB\n\n\nIn this dataset, we have a wide variety of data types. Some variables are stored as integers, while others are stored as floats or strings. Note that Pandas infers these dtypes when it loads the data. As we shall soon see, some may not be ideal for our purposes. It’s important that the data types are appropriate for each variable before proceeding with analysis, as misclassified types can cause problems later on. We’ll do that shortly.",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Survey Data</span>"
    ]
  },
  {
    "objectID": "survey-data.html#get-started",
    "href": "survey-data.html#get-started",
    "title": "6  Processing Structured Data Survey Data",
    "section": "7.3 Get Started",
    "text": "7.3 Get Started",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>~~Processing Structured Data~~ Survey Data</span>"
    ]
  },
  {
    "objectID": "survey-data.html#foundations-of-survey-methodology-the-total-survey-error-framework",
    "href": "survey-data.html#foundations-of-survey-methodology-the-total-survey-error-framework",
    "title": "7  Survey Data",
    "section": "",
    "text": "Figure 7.1: Remake this.\n\n\n\n\n\n\n\n\n\n\nFigure 7.2: Remake this.\n\n\n\n\n\n\n\n\n\n\nFigure 7.3: Remake this.\n\n\n\n\n\nCoverage error happens when the sampling frame doesn’t fully cover the target population, leading to undercoverage or overcoverage.\nSampling error occurs due to random chance when selecting a sample from the population.\nNonresponse error is when certain individuals from the sample do not respond, potentially leading to biased results if non-respondents differ significantly from respondents.\nAdjustment error can arise during weighting or imputation when researchers attempt to correct for nonresponse or other biases.\n\n\n\n\n\n\n\nFigure 7.4: Remake this.",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Survey Data</span>"
    ]
  },
  {
    "objectID": "survey-data.html#how-survey-research-has-evolved",
    "href": "survey-data.html#how-survey-research-has-evolved",
    "title": "7  Survey Data",
    "section": "7.2 How survey research has evolved",
    "text": "7.2 How survey research has evolved\nWhile the classic foundation of survey methodology – as articulated in the Total Survey Error Framework – remains important, survey methodology has evolved significantly along the two lines of representation and measurement (via question asking). Let’s consider some of those changes now.\n\n7.2.1 Sampling and representation\nIn the early days of survey research, political pollsters often used quota sampling, a non-probability sampling method where researchers try to match their sample to the population by selecting respondents based on certain demographic characteristics (like age, gender, and income). The aim was to create a microcosm of the broader population by setting quotas for each demographic group, making sure to include a proportional number of individuals from each category. This method was considered state-of-the-art at the time, and, in many cases, it worked reasonably well. However, it was not without its flaws, as the 1936 U.S. presidential election famously made obvious.\nIn 1936, the American magazine Readers Digest conducted one of the largest surveys ever attempted up to that time: 2.4 million people. Based on that survey, Readers Digest confidently predicted that the Republican candidate, Alf Landon, would win the presidential election in a landslide, beating the Democratic incumbent Franklin D. Roosevelt. Yet Roosevelt won with over 60% of the popular vote, carrying all but two states in one of the most decisive victories in U.S. history.\nWhat went wrong with the Readers Digest poll? Easy: They had a very biased sample. Although the poll attempted to match the population demographically via quota sampling, it relied heavily on lists of telephone subscribers, car owners, and magazine subscribers to do so. In the 1930s, during the Great Depression, all three of these groups were disproportionately wealthy, white, and conservative. Since many Americans could not afford to own telephones or cars, the less affluent were severely underrepresented in the sample, and they who more likely to support Roosevelt over Landon. Quota sampling, while well-intentioned, could not overcome the bias introduced by the faulty sampling frame and resulted in wildly inaccurate predictions.\nThis famous failure highlighted the dangers of non-probability samples (where every member of a population does not have a known, equal chance of being selected) and drove innovations in sampling, chiefly the development of probability sampling techniques that produce more representative samples.\nContemporary survey research faces some similar challenges due to the rise of online panels and non-probability samples. Online panels are often based on voluntary participation; rather than randomly sampling people from a population, people opt into a survey panel, introducing potential biases. Like the 1936 Readers Digest poll, these modern non-probability samples may fail to represent certain segments of the population. People who spend time answering online questionnaires for survey panels differ from people who don’t. However, contemporary statistical techniques like post-stratification and Bayesian multilevel modeling have made non-probability samples much more useful.\n\nTODO: Update the text below.\n\nOne of the most striking examples of this is cite Gelman’s Xbox study. In 2012, Gelman and his collaborators used a non-representative sample of Xbox users—mostly younger men who were much more conservative than the general population—to predict voting behavior in the 2012 U.S. presidential election. Xbox users are not a random cross-section of the population; they tend to be younger, more male, and more politically conservative, characteristics that could introduce severe bias if the sample were analyzed without adjustments. However, using advanced techniques, Gelman’s team was able to produce remarkably accurate predictions. Here’s how they did it:\n\nThe researchers used post-stratification to adjust the data to better reflect the known characteristics of the U.S. voting population. Post-stratification works by dividing the population into different demographic groups (called “strata”) based on factors like age, gender, race, and education. After gathering the data, weights are applied to each group based on its proportion in the actual population. For example, since young, conservative men were overrepresented in the Xbox sample, their responses were down-weighted, while responses from older women were up-weighted to correct for their underrepresentation.\nThe researchers also used Bayesian multilevel regression to improve their predictions. This method allows for partial pooling, meaning that estimates for smaller or underrepresented groups (e.g., older women in the Xbox sample) are “borrowed” from similar groups to avoid overfitting and to stabilize the estimates. In this way, the model could leverage patterns across different groups to make better predictions even when some strata had fewer respondents.\n\nDespite using a highly non-representative sample, these techniques allowed Gelman’s team to accurately predict the outcome of the election. This study marked a significant shift in how researchers think about survey sampling, showing that non-probability samples—if adjusted properly with techniques like post-stratification—can still yield valuable insights.\nTo fully understand these developments, it’s helpful to clarify a few key concepts:\n\nQuota Sampling: In quota sampling, the researcher sets quotas for certain demographic categories (e.g., gender, age, income) to ensure that the sample reflects the population’s composition on these variables. However, within each category, respondents are selected non-randomly, which can lead to bias if certain subgroups are over- or under-sampled (as happened in the 1936 Readers Digest poll).\nNon-Probability Samples: A non-probability sample is one where not every individual in the population has a known or equal chance of being selected. This includes convenience samples (e.g., surveying people who are easily accessible), voluntary response samples (e.g., online panels where participants opt in), and snowball samples (e.g., respondents recruit others to participate). The challenge with non-probability samples is that they may not accurately represent the population, leading to biased results.\nPost-Stratification: This is a technique used to correct for biases in non-probability samples by weighting the data according to known population characteristics. It’s often used in conjunction with Bayesian methods to adjust for the over- or under-representation of certain groups in the sample.\n\nBy combining non-probability samples with advanced techniques like post-stratification and Bayesian modeling, contemporary researchers can make surprisingly accurate inferences about the broader population, even from biased or incomplete samples. This represents a major evolution from the quota sampling methods of the early 20th century, highlighting the power of modern statistical methods to overcome the limitations of non-probability sampling.\nIn sum, while early attempts at survey sampling, like the 1936 Readers Digest poll, illustrated the pitfalls of non-representative samples, modern techniques have transformed the way we handle survey data. Non-probability samples, once considered unreliable, can now produce accurate results through methods like post-stratification and Bayesian multilevel modeling, as shown in the Xbox study. These advances mark a major shift in survey research, allowing us to better navigate the challenges of representation and make more informed inferences about public opinion and behavior. We will learn about both in this book, but not yet! There’s a good amount of foundational work to do first.\n\n\n7.2.2 The science of asking questions: past and present\nThe second major change in survey methodology is related to how we ask questions. The way questions are phrased can dramatically influence responses—a phenomenon known as question wording effects. Even small changes in phrasing can lead to different answers. For example, asking “Do you approve of the government’s handling of the economy?” might yield different results than “Do you think the government has done a good job managing the economy?” This is partly due to social desirability bias, where respondents may choose answers they think are socially acceptable rather than ones that reflect their true thoughts. Moreover, context effects, such as the order in which questions are asked or the types of response options available, can further bias responses by subtly guiding respondents toward certain answers.\nThe structure of response options has long played a critical role in survey design. Researchers have known for decades that the placement of response options can lead to order effects (respondents are more likely to choose options that appear at the beginning or end of a list). Similarly, the way response options are framed, such as presenting extreme versus moderate choices, can nudge respondents toward particular answers. For example, if a question offers responses ranging from “terrible” to “excellent,” respondents may gravitate toward moderate choices. Effective survey design must account for these influences to minimize bias and elicit more reliable and valid responses. Crafting survey questions is both an art and a science.\nWhile the fundamental challenges of question design have persisted over time, the science of asking questions has evolved significantly. In a recent review article, Schaeffer and Dykema (2020) highlighted how survey design and methodology has moved toward decision-based frameworks that seek to identify question characteristics that influence respondent behavior. This marks a shift from earlier research that often focused on isolated elements like question wording, toward a more comprehensive approach that accounts for the co-occurrence of question characteristics. (TODO: Provide an example here.)\nOne significant change is the expanded use of fully labelled options for response categories. In the past, it was common to rely on numeric scales or partial labeling, but recent research shows that using fully labeled response categories, like “not at all” to “extremely,” increases reliability across diverse modes of data collection, particularly in self-administered surveys. The use of five response categories has become a standard, as this number strikes a balance between providing enough granularity for nuanced responses without overwhelming the respondent.\nAnother important shift in recent years is that item-specific (IS) response formats that ask respondents about the topic in question directly (e.g., “How satisfied are you with your job?”) are now heavily favoured over traditional agree-disagree (AD) scales that pose a statement and ask the respondent whether they agree (e.g. “I am satisfied with my job” — agree or disagree?).\nAS scales such as these are falling out of favour for several reasons, most importantly their susceptibility to biases like acquiescence (the tendency to agree with statements) and extreme responding. IS formats have been shown to providing more accurate data by reducing these biases. This shift represents an important development in the field, addressing long-standing issues with response quality.\nThe rise of self-administered web and mobile surveys has also brought new challenges and innovations in question design. In earlier decades, most surveys were administered face-to-face or via telephone, which allowed interviewers to clarify questions and guide respondents through the survey process. With the shift toward self-administered surveys, researchers have had to design questions that work across diverse modes and platforms, including small mobile screens where question length, complexity, and presentation can significantly impact data quality.\nOne response to this challenge has been the development of dynamic filtering, which allows follow-up questions to be triggered based on respondents’ initial answers. This technique reduces cognitive load and improves data quality by preventing respondents from having to answer irrelevant questions. Similarly, yes-no checklists are now preferred over traditional check-all-that-apply (CATA) formats, which tend to under-report responses. Yes-no checklists prompt respondents to actively consider each option, leading to more complete and accurate answers.\n\nTODO Add some screenshots of old and new designs here to help clarify these points.\n\nAdditionally, the shift to mobile-first design has led to a reconsideration of grid formats and question batteries. While early research suggested that grids, which present multiple related questions in a single visual format, could save time and reduce respondent burden, more recent research shows that grids can lead to straightlining, where respondents provide the same answer across all questions to expedite the process. This is particularly problematic in mobile surveys, where the small screen size makes grids harder to navigate. Presenting questions individually has been found to improve data quality, especially in mobile contexts.\nAs researchers continue to refine methods for assessing data quality, new challenges will emerge, particularly in an era of mixed-mode and mobile-first surveys. The current state of the art includes innovations like item-specific formats, fully labeled response scales, and adaptive technologies such as dynamic filtering, all of which reflect an ongoing effort to optimize survey design across modes and respondent groups. These advancements show a clear trajectory toward more respondent-friendly surveys that reduce bias and improve the validity of the data collected.\nIn sum, the field of survey question design has moved from focusing on individual elements of question structure to a holistic, decision-based approach that accounts for the complexities of modern data collection environments. This evolution reflects broader changes in the landscape of survey research, where technological advancements and an increasingly diverse respondent base demand ever-more sophisticated tools for capturing reliable, valid data.\n\n\n7.2.3 Linked data\nA third and final development in modern survey research is a greater reliance on linking survey data with other datasets.\nAnother exciting development in recent years has been the integration of social media data into surveys. Researchers like Chris Bail have pioneered methods for linking social media behavior with survey responses, allowing us to explore how online activity shapes opinions and behaviors. In my own work with David, we’ve taken this a step further by asking survey respondents to provide their social media handles, allowing us to combine readymade and custommade data sources in innovative ways. This combination of direct and passive data collection opens up new possibilities for understanding complex social phenomena.\n\nHarper government and the census, recordlinkage in government\nHenrickson and the socio-ecological systems stuff\nBail and the social media surveys",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Survey Data</span>"
    ]
  },
  {
    "objectID": "survey-data.html#study-the-codebook",
    "href": "survey-data.html#study-the-codebook",
    "title": "7  Survey Data",
    "section": "7.4 Study the Codebook",
    "text": "7.4 Study the Codebook\nBefore diving deeper into the data, it’s essential to take a close look at the codebook. The codebook is your guide to understanding what each variable represents, how the data was collected, and any quirks you should be aware of (such as how missing data is coded or how questions were worded). You can find the codebook for this dataset in the Technical Report and Codebook PDF file.\nIn particular, pay attention to the survey design and sample design sections, as they explain how the data was collected and the rationale behind the survey weights. After that, you’ll want to spend some time familiarizing yourself with the variables in the dataset, especially any you expect to be working with.\n\n7.4.1 Understanding the sample design and survey weights\n\nTODO: Add content on the CES design here instead? Or not?\n\nOne of the most important features of survey data is the use of survey weights. These weights are used to adjust the sample so that it more accurately reflects the population from which it was drawn. For example, if women are overrepresented in the sample, the weights will adjust their contribution to the analysis to prevent bias.\nThere are different types of weights, including base weights, which correct for unequal probabilities of selection, and post-stratification weights, which adjust for nonresponse and other factors. In the CES, we have a variety of weights to choose from, depending on the specific analysis we want to conduct.\n\n\n7.4.2 Applying Survey Weights\nApplying weights is straightforward in most statistical software, and Pandas is no exception. Let’s say we want to compare the weighted and unweighted mean of a particular variable. We can do this by applying the appropriate weight variable to our analysis.\n\nTODO: Add an example to illustrate how applying weights can shift survey results. For instance, show a comparison between unweighted vs. weighted means or proportions in a simple analysis, using the CES dataset.\n\n\n# Example here\n\n\n\n7.4.3 Assessing Data Quality\nOnce you’ve familiarized yourself with the sample design and applied the necessary weights, your next step should be to carefully assess the quality of the survey data. There are a few common problems to look out for, including inconsistencies, outliers, and missing data.\nSurvey data often contains inconsistencies, particularly in panel surveys where the same respondents are surveyed multiple times. For example, a respondent’s age or region of residence might be reported differently across waves. It’s important to identify and correct these inconsistencies before proceeding with analysis.\n\n# Example here\n\nIn addition to inconsistencies, you should also check for outliers—extreme values that might skew your results. In survey data, these can sometimes be caused by respondents rushing through the survey or providing inaccurate answers. One way to detect this is by checking the distribution of survey completion times. Respondents who completed the survey in an unusually short time may not have been paying full attention.\n\n# Example here\n\nSpeeders—respondents who rush through the survey—can introduce bias into your data. There are a number of techniques for identifying speeders, including setting a minimum threshold for survey completion time or using attention-check questions to detect inattentiveness. If you suspect that certain respondents weren’t paying attention, you may want to exclude them from your analysis.\n\n# Example here\n\nAnother common issue in survey data is response bias, where respondents may answer questions in a way that reflects social desirability rather than their true opinions. For example, people might be reluctant to express unpopular views or admit to certain behaviors. While it’s difficult to eliminate response bias entirely, being aware of it and using techniques like randomized response – DEFINE – can help mitigate its effects.",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Survey Data</span>"
    ]
  },
  {
    "objectID": "survey-data.html#handling-missing-data-bad-less-bad-and-good-imputation-practices",
    "href": "survey-data.html#handling-missing-data-bad-less-bad-and-good-imputation-practices",
    "title": "7  Survey Data",
    "section": "7.5 Handling missing data: Bad, less bad, and good imputation practices",
    "text": "7.5 Handling missing data: Bad, less bad, and good imputation practices\nSurvey data often contains missing responses, and how you handle them can have a big impact on your analysis. There are three main types of missing data, all fairly poorly named in my opinion:\n\nMissing Completely at Random (MCAR), where the missingness is entirely random and unrelated to any other variable;\nMissing at Random (MAR), where the missingness is related to some observed variable but not to the missing data itself; and finally\nMissing Not at Random (MNAR), where the missingness is related to the value of the missing data.\n\nIn most cases, we assume that data is MAR, and there are several methods that we might use in response, including:\n\nlistwise deletion (removing all cases with missing data),\nmean imputation (replacing missing values with the mean),\nmultiple imputation (define), and\nBayesian imputation (define).\n\nWhile listwise deletion and mean imputation are easy to implement, they are generally poor choices. Mean imputation, for example, ignores variability in the data and can lead to biased estimates. A better approach is to use multiple imputation, which accounts for uncertainty in the missing data by creating several different datasets with imputed values and then combines the results. However, there are problems with this approach as well, such as limitations. The gold standard is Bayesian imputation, which is fully integrated into Bayesian models. Since I haven’t even defined “Bayesian” yet, we’ll set this method aside for now and return to it in later chapters. When we do, you’ll be delighted to learn that we don’t have to do anything special; Bayesian inference gives it to us for free!\n\nTODO: Missing data in this chapter specifically…\n\n\n# Example here\n\n\n7.5.1 Recoding Variables\nOne of the most common tasks in survey data processing is recoding variables. This is especially important for categorical variables, where you might need to collapse categories or create dummy variables. For example, if you have a variable with multiple levels of education, you might want to recode it into a simpler variable with just two categories: “college degree” vs. “no college degree.”\nIn addition to recoding categories, you may also need to reverse score some variables. This is common with Likert-scale questions, where higher values indicate more agreement or approval. If some items are negatively worded, you’ll need to reverse the scale so that all items point in the same direction.\n\n# Example here",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Survey Data</span>"
    ]
  },
  {
    "objectID": "survey-data.html#grid-questions",
    "href": "survey-data.html#grid-questions",
    "title": "7  Survey Data",
    "section": "7.6 Grid Questions",
    "text": "7.6 Grid Questions\nAs I mentioned earlier, grid questions are falling out of favour in survey methodology. However, you’ve very likely to encounter them, especially if you are working with longitudinal data or older surveys. With these questions, respondents are asked to rate a series of items on the same scale (e.g., a series of agree/disagree statements). These questions can present some challenges when it comes to data processing, particularly if there are missing responses. One approach is to collapse related questions into indices or standardize scales so that the responses are comparable across different items.\nThe CES survey data contains several such questions, including…\n\n# Example here\n\n\n7.6.1 Categorical Data\nMost surveys include quite a lot of categorical data, which should be handled very carefully in any modeling.\nWhen working with categorical data in Pandas, it’s important to make sure that our categorical variables are ordered properly and that “Don’t know / prefer not to answer” responses – which don’t fit neatly into ordinal data – are handled consistently. You’ll also need to decide how to represent your categorical variables in your analysis. In nearly all cases you’ll want to treat categories as index variables, but in some cases you may wish to do things the old-fashioned way and use one-hot encoding (i.e., creating dummy variables). The implications of this choice will become clear a little later in the book, when we get to multilevel models. For now, we’ll …\n\n# Example here",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Survey Data</span>"
    ]
  },
  {
    "objectID": "survey-data.html#reproducibility-and-non-destructive-workflows",
    "href": "survey-data.html#reproducibility-and-non-destructive-workflows",
    "title": "7  Survey Data",
    "section": "7.7 Reproducibility and Non-destructive Workflows",
    "text": "7.7 Reproducibility and Non-destructive Workflows\nBefore we conclude, I want to emphasize the importance of reproducibility and transparency in survey data work. It’s essential to work non-destructively, meaning that you should never modify your original data files. Instead, document all of your data cleaning and recoding decisions in your scripts and notebooks. This might seem tedious at first, but with practice, it becomes second nature and ensures that your work is both reproducible and ethical.",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Survey Data</span>"
    ]
  },
  {
    "objectID": "survey-data.html#footnotes",
    "href": "survey-data.html#footnotes",
    "title": "7  Survey Data",
    "section": "",
    "text": "Dataverse provides an API for accessing data programatically. You might want to experiment with it after working through the chapter Web data (APIs)↩︎",
    "crumbs": [
      "**DATA**",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Survey Data</span>"
    ]
  },
  {
    "objectID": "causality.html#a-brief-history-and-overview-of-causal-inference",
    "href": "causality.html#a-brief-history-and-overview-of-causal-inference",
    "title": "20  Causality",
    "section": "",
    "text": "20.1.1 Experiments, counterfactuals, and the potential outcomes framework\nCausal Inference, Experimental Design, and the Potential Outcomes Framework\nTo understand the potential outcomes framework and its role in causal inference, it’s important to start with the foundation of causal inference through experimental design. In the social sciences, we often aim to answer questions like: What is the effect of a policy change? or How does an intervention affect outcomes for different groups of people? These are fundamentally causal questions, and answering them requires methods that go beyond correlational analysis.\nExperimental design is one of the most powerful tools for answering causal questions. The randomized controlled trial (RCT) is the gold standard in experimental design because it uses random assignment to balance both observed and unobserved confounders across treatment and control groups. This randomization makes it possible to attribute differences in outcomes directly to the treatment, eliminating concerns about bias from confounding variables. The potential outcomes framework, developed through work by Jerzy Neyman and later formalized by Donald Rubin, builds on this tradition of experimental design.\nThe Potential Outcomes Framework and Counterfactual Reasoning\nThe potential outcomes framework is a conceptual and statistical tool for understanding causal effects, particularly in experimental and quasi-experimental settings. The core idea is to define causality in terms of counterfactuals: what would have happened to a subject if they had received a different treatment?\nLet’s say we are evaluating the effect of a new education program on student achievement. For each student, there are two potential outcomes:\n•   Y(1): The outcome if the student receives the program (treatment).\n•   Y(0): The outcome if the student does not receive the program (control).\nThe causal effect for each student is the difference between these two potential outcomes: Y(1) – Y(0). Of course, in reality, we can only observe one of these outcomes (the one corresponding to the treatment the student actually received). The unobserved outcome is the counterfactual—what would have happened under a different treatment.\nThe potential outcomes framework helps us estimate the average treatment effect (ATE) by comparing the outcomes across treated and untreated groups. In a randomized experiment, where subjects are randomly assigned to treatment and control groups, we can confidently estimate this causal effect because randomization ensures that the two groups are comparable. In non-experimental settings, methods like propensity score matching are often used to mimic the randomization process and control for confounders.\nIs the Potential Outcomes Framework More Frequentist or Bayesian?\nThe potential outcomes framework is typically associated with frequentist statistics, particularly because of its close ties to experimental design and hypothesis testing. However, this association is not due to any inherent limitation of the framework itself—it’s more a reflection of its intellectual development. Since the framework was formalized within the frequentist tradition, particularly through the work of Neyman and Rubin, its most common applications involve frequentist methods like randomization tests, hypothesis tests, and confidence intervals.\nBut there’s no reason why the potential outcomes framework has to be exclusively frequentist. In fact, Bayesian methods can be applied to the potential outcomes framework just as effectively. In a Bayesian perspective, we could place priors on the treatment effects (the differences between potential outcomes) and update these priors based on the data we collect. This would lead to a posterior distribution over the treatment effects, allowing us to quantify uncertainty in a more nuanced way than frequentist confidence intervals typically allow.\nWhat distinguishes the potential outcomes framework is not its frequentist or Bayesian orientation, but its experimental nature. It’s rooted in the logic of comparing what did happen with what could have happened under different conditions. Whether we analyze the resulting data using frequentist or Bayesian methods depends more on the philosophical and methodological preferences of the researcher than on the framework itself.\nCausal Inference in the Social Sciences: Examples of the Potential Outcomes Framework\nThe potential outcomes framework has been widely applied across the social sciences, particularly in fields like economics, political science, and education. Here are some examples of how it has been used:\n1.  Education Policy: In evaluating the effects of new educational interventions, such as charter schools or teacher training programs, researchers often use the potential outcomes framework to estimate the impact of the treatment on student achievement. For example, in studies where some students attend charter schools and others attend traditional public schools, the potential outcomes framework helps estimate what would have happened to charter school students if they had instead attended public schools.\n2.  Labor Economics: The potential outcomes framework is frequently used to estimate the effects of labor market interventions, such as job training programs or wage subsidies. By comparing the outcomes of individuals who received the intervention with similar individuals who did not, researchers can estimate the average treatment effect on employment outcomes, wages, or job satisfaction.\n3.  Political Science: In studying the effects of campaign spending or media exposure on voter behavior, political scientists use the potential outcomes framework to estimate how changes in one variable (e.g., campaign spending) influence another (e.g., voter turnout). When randomization isn’t feasible, methods like propensity score matching or instrumental variables are often used to estimate causal effects in non-experimental data.\nCounterfactuals vs. Interventions: Are They the Same?\nThe terms counterfactuals and interventions are often used interchangeably in causal inference, but there is a subtle distinction between them.\n•   Counterfactuals refer to hypothetical outcomes that represent what would have happened under a different condition or treatment. In the potential outcomes framework, counterfactual reasoning is used to define causal effects. The key question is: What would the outcome have been if the subject had received a different treatment?\n•   Interventions, on the other hand, refer to active changes made to a system—such as applying a treatment or policy change. In Judea Pearl’s framework, interventions are formalized using do-calculus, which allows researchers to reason about how an intervention changes the causal structure of a system. The central question in this approach is: What happens to the outcome when we intervene on a particular variable?\nWhile the logic of counterfactuals and interventions is similar in that both involve considering alternate scenarios, the key difference is that interventions are about actively changing a system, whereas counterfactuals are about comparing what happened with what could have happened.\nIn Pearl’s structural causal models (SCMs), interventions are represented graphically using directed acyclic graphs (DAGs), while counterfactuals are used to reason about different potential outcomes under these interventions. Essentially, interventions change the causal graph by manipulating one or more variables, while counterfactuals compare the different possible outcomes based on that change.\nConclusion: Integrating the Potential Outcomes Framework into Causal Inference\nThe potential outcomes framework is a powerful tool for understanding causal effects, particularly in experimental and quasi-experimental settings. While it is often associated with frequentist methods due to its origins in experimental design, it is compatible with both frequentist and Bayesian approaches. The framework emphasizes counterfactual reasoning, asking what would have happened under different treatment conditions, and has been widely applied in fields such as education policy, labor economics, and political science.\nThe distinction between counterfactuals and interventions—while subtle—helps clarify the differences between the potential outcomes framework and Pearl’s causal models. Both approaches aim to understand causality, but interventions are about actively changing a system, while counterfactuals are used to reason about the different possible outcomes of such changes.\nBy bringing together insights from path models, SEMs, DAGs, and the potential outcomes framework, we can develop a more nuanced understanding of causality that incorporates both experimental and observational data, as well as both counterfactual reasoning and intervention analysis. These complementary methods offer a broad toolkit for answering the complex causal questions that drive much of social science research.\n\n\n20.1.2 From path analysis to structural causal models\nLet’s return to some ideas / themes that originally came up in Mindful modeling and Sequential iterative modelling…\nSequential Modeling: From Sewall Wright and Lazarsfeld to Structural Equation Models and Mediation Analysis\nSewall Wright and the Foundations of Path Analysis\nThe origins of path analysis—a key precursor to structural equation models (SEM)—can be traced back to the work of Sewall Wright, a geneticist and evolutionary biologist. In the early 20th century, Wright developed path analysis as a method to study the complex causal relationships between traits in genetics. His landmark 1921 paper introduced the notion of path coefficients, which could quantify the direct and indirect effects of one variable on another in a series of interconnected variables.\nWright’s approach provided a way to model causal relationships between variables by representing them as paths in a graph. His idea was that any complex system, such as biological traits influenced by multiple genes and environmental factors, could be broken down into a series of linear relationships, each contributing to an overall understanding of the causal structure. Wright introduced a formal method to calculate causal pathways and partition variance, helping to explain how much of the variation in a particular outcome could be attributed to different factors along the path.\nWright’s work on path analysis was foundational because it formalized the idea that we could model causal relationships using statistical techniques. This was a critical step in the development of structural equation modeling (SEM) and paved the way for later work on causal modeling in the social sciences. Although Wright’s work initially focused on biology, his methods were quickly adopted and extended by social scientists, particularly at Columbia University, where Paul Lazarsfeld and others would build on these ideas.\nPaul Lazarsfeld and the Extension of Sequential Modeling in Sociology\nWhile Sewall Wright’s path analysis was originally developed to study genetics, Paul Lazarsfeld and his colleagues at Columbia University extended these methods to the study of social phenomena. Lazarsfeld was instrumental in the development of quantitative sociology, applying statistical models to understand the complex relationships between social variables.\nLazarsfeld’s survey methodology and his emphasis on breaking down social phenomena into sequential, step-by-step processes fit naturally with Wright’s path analysis. Lazarsfeld adopted and expanded these ideas to study how variables like social class, education, and media exposure influenced individual behavior. The famous two-step flow of communication model, which posited that media influenced opinion leaders who then influenced the public, was an example of how Lazarsfeld applied Wright’s ideas in a sociological context.\nLazarsfeld’s work at Columbia was pivotal in formalizing the use of path analysis in the social sciences. By treating social behavior as a series of causal paths, Lazarsfeld helped lay the groundwork for the development of structural equation models (SEMs), which would come to dominate quantitative research in fields like sociology, psychology, and economics.\nThe Development of Structural Equation Models (SEM)\nBuilding on the foundations laid by Sewall Wright and Paul Lazarsfeld, structural equation models (SEM) emerged as a powerful tool for modeling both observed and latent variables and their causal relationships. SEM combined the path analysis developed by Wright with the factor analysis techniques that Lazarsfeld and others were pioneering at Columbia.\nSEM allows researchers to model complex relationships between variables, incorporating both direct and indirect effects while accounting for measurement error. By linking observed variables to underlying latent constructs—such as attitudes, beliefs, or social status—SEM provides a way to study concepts that cannot be directly measured. This was especially important for the social sciences, where many of the variables of interest (e.g., intelligence, political ideology) are not directly observable.\nOne of the key strengths of SEM is that it provides a formal structure for modeling causal relationships. Researchers can specify both the measurement model (how latent variables are measured by observable indicators) and the structural model (the causal paths between latent and observed variables). However, like Wright’s path analysis, SEM often requires that researchers specify the causal paths in advance, limiting the flexibility to revise models as new data or insights become available.\nMediation Analysis in Psychology: Sequential Models in a Different Form\nWhile SEM gained prominence across many fields, a related technique known as mediation analysis became particularly influential in psychology. Baron and Kenny formalized mediation analysis in their 1986 paper, which outlined how researchers could study the relationship between an independent variable (X) and a dependent variable (Y) through an intermediary variable (M).\nMediation models were essentially sequential causal models—X affects M, which then affects Y. This approach was appealing because it provided a way to test for indirect effects, giving researchers insight into the mechanisms by which one variable influences another. For example, psychologists could study how stress affects health outcomes, mediated by coping mechanisms.\nMediation analysis shared common roots with path analysis and SEM, as all these approaches were built on the idea of sequential causality. Researchers would specify a causal path upfront, estimate the parameters of each step, and then interpret the direct and indirect effects of the variables involved.\nThe Limitations of Sequential and Mediation Models\nDespite the widespread adoption of path analysis, SEM, and mediation models, these approaches suffer from several key limitations:\n1.  Underdetermination: Multiple models can explain the same data. For example, in a mediation analysis, several different pathways might explain the relationship between stress, coping mechanisms, and health. Similarly, in a path analysis, many different causal sequences could be consistent with the observed data. This problem of underdetermination means that even a well-fitting model may not reflect the true underlying causal structure.\n2.  Fixed Causal Assumptions: Like Wright’s original path analysis, both SEM and mediation models require researchers to specify causal paths in advance. This can be problematic when the true causal relationships are more complex or when researchers don’t have enough theoretical knowledge to specify the correct model. Once the paths are set, there is limited opportunity for revision.\n3.  Lack of Iteration: Traditional mediation models and SEM often lack the iterative refinement that is key to modern approaches like Bayesian modeling. Once a model is specified and tested, it’s typically considered final, leaving little room for the kind of ongoing revision and testing that might reveal better or more accurate models.\nFrom Path Analysis and SEM to Pearl’s DAGs\nThe ideas underlying path analysis, structural equation modeling, and mediation analysis directly influenced the development of directed acyclic graphs (DAGs) by Judea Pearl. Pearl’s work on causal inference built on these earlier methods but extended them in crucial ways.\nPearl introduced DAGs as a way to represent and reason about causal structures graphically. Each node in a DAG represents a variable, and the directed arrows show the causal relationships between them. While path analysis and SEM also use arrows to depict causal paths, Pearl’s DAGs added several innovations:\n•   Flexibility: Unlike traditional sequential models, DAGs allow for greater flexibility in specifying and revising causal models. Researchers can use DAGs to represent a wide range of potential causal structures and can test these structures using data. If the model needs to be revised based on new evidence, this can be done iteratively, unlike the fixed paths in SEM.\n•   Do-Calculus: Pearl’s development of do-calculus provided a formal method for reasoning about interventions in causal models. This was a significant step forward from SEM and path analysis, which did not have formal tools for studying how interventions on one variable would affect others.\nDAGs and Pearl’s work on causal inference represent a direct evolution from the ideas pioneered by Sewall Wright, Paul Lazarsfeld, and those working on SEM and mediation analysis. But Pearl’s framework is more dynamic and iterative, allowing for continuous model refinement based on both prior knowledge and new data.\nMcElreath’s Iterative Approach: Structural Causal Models in Action\nRichard McElreath’s iterative Bayesian approach builds on the foundation laid by Pearl’s DAGs and extends it further through the integration of Bayesian inference. In McElreath’s framework, we begin with a set of prior beliefs about the causal structure of the world, build models based on those priors, and then iteratively revise those models as we gather more data and insight.\nThis iterative process represents a departure from the fixed paths of traditional SEM, path analysis, and mediation models. Instead of assuming we know the correct causal structure upfront, McElreath’s approach encourages us to compare multiple models, refine them based on new evidence, and remain open to revising our assumptions.\nThe flexibility and dynamism of this approach make it better suited for understanding the latent constructs that are central to much of social science research. Latent variable models in McElreath’s approach are treated like instruments that evolve and improve with use, much like the way gravitational wave detectors evolved iteratively in the physical sciences (as explored earlier in Collins’ work).\nConclusion: A Move Toward Iteration and Structural Causal Models\nThe evolution from Sewall Wright’s path analysis to Lazarsfeld’s sequential models, structural equation modeling, and mediation analysis represents a rich tradition of causal modeling in the social sciences. However, these methods often assume fixed causal sequences, which limit their flexibility and may lead to misspecified models.\nThe development of DAGs by Judea Pearl and the rise of Bayesian iterative modeling, as championed by McElreath, offer a more robust and flexible approach to causal inference. By allowing for model comparison, refinement, and iteration, these modern approaches overcome many of the limitations of traditional sequential models.\nIn shifting from the fixed paths of early models to the dynamic, iterative nature of structural causal models, we move closer to capturing the true complexity of social phenomena. This iterative approach better reflects the uncertainty and complexity of real-world data, allowing researchers to build and refine models in a way that leads to deeper and more accurate understanding.\nThis revision includes Sewall Wright’s foundational role in developing path analysis, integrates it into the historical development of SEM and mediation models, and connects these methods to Pearl’s DAGs and McElreath’s iterative approach. This provides a comprehensive view of how causal modeling has evolved across disciplines and highlights the advantages of modern, flexible methodologies.",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Causality</span>"
    ]
  },
  {
    "objectID": "causality.html#structural-causal-models-scms",
    "href": "causality.html#structural-causal-models-scms",
    "title": "20  Causality",
    "section": "20.2 Structural causal models (SCMs)",
    "text": "20.2 Structural causal models (SCMs)\n\nTODO: Pull some content from above when revising",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Causality</span>"
    ]
  },
  {
    "objectID": "mindful-modelling.html#the-underdetermination-of-theory-by-evidence",
    "href": "mindful-modelling.html#the-underdetermination-of-theory-by-evidence",
    "title": "5  Mindful modelling",
    "section": "",
    "text": "5.1.1 Empirical Equivalence: More Than One Theory Can Fit the Data\nOne of the fundamental ideas behind underdetermination is that competing theories can often explain the same set of data equally well. This is called empirical equivalence.\nDavid Bloor, a leading figure from the Edinburgh School, was one of the key proponents of this view, arguing that scientific knowledge is not purely dictated by empirical evidence but also by sociological factors. He introduced the symmetry principle in his foundational work Knowledge and Social Imagery (1976), which posits that the same sociological mechanisms that explain “true” beliefs should also be applied to explain “false” ones. The underlying assumption here is that the distinction between true and false is not always clear-cut and is often shaped by factors beyond data alone.\nFor example, in the early development of quantum mechanics, both the wave theory of light and particle theory were empirically adequate for certain experimental results. Scientists had to navigate this empirical equivalence, and it wasn’t solely the data that drove the eventual preference for one theory over the other—it was also shaped by intellectual, institutional, and even personal preferences among physicists of the time.\n\n\n5.1.2 Social Factors in Theory Choice: Why Some Theories Prevail\nIf empirical equivalence holds, why does one theory become dominant over another? This is where the sociological dimension enters. Sociologists like Harry Collins and Trevor Pinch have illustrated that the closure of scientific debates often has as much to do with social and cultural factors as it does with the data itself.\nCollins, in his classic study The Sociology of Scientific Knowledge (1983), explored how different groups of scientists could interpret the same data differently depending on their prior assumptions, institutional affiliations, and even personal biases. He famously studied the debate over the detection of gravitational waves, where multiple research groups used the same instruments but came to different conclusions. He showed that the decision to accept or reject a discovery was not a straightforward application of empirical data but involved social negotiation within the scientific community.\nRob Evans and Collins further extended this argument in their work on interactional expertise—the idea that expertise in science is not only about technical knowledge but also about participating in the social practices and language of a scientific community. This notion reinforces the view that the resolution of scientific debates cannot be reduced to data alone, as social networks and linguistic practices play a significant role in shaping consensus.\n\n\n5.1.3 Interpretive Flexibility: Data Is Not Neutral\nThe concept of interpretive flexibility is another key idea in the sociology of scientific knowledge. It means that the same data can be interpreted in multiple ways depending on the theoretical framework or background assumptions of the scientist.\nThis concept is most famously explored by Trevor Pinch and Harry Collins in their work The Golem: What You Should Know About Science (1993). Pinch and Collins use historical case studies to show that scientific facts are not “given” by the data—they are interpreted and constructed by scientists within a particular theoretical and cultural context. For example, in the case of the discovery of pulsars, the initial data could have been interpreted as noise or as an artifact of instrumentation. It took a theoretical leap, shaped by the expectations and assumptions of the scientists involved, to interpret that data as evidence of a new kind of astronomical object.\nThis notion of interpretive flexibility challenges the idea that data “speaks for itself.” Instead, it suggests that scientific facts are the result of negotiation, interpretation, and the application of theoretical lenses that vary across different groups of scientists.\n\n\n5.1.4 Knowledge as Socially Constructed: The Symmetry Principle\nAt the heart of the Edinburgh School’s approach is the idea that scientific knowledge is socially constructed. David Bloor’s symmetry principle demands that both successful and unsuccessful theories be explained by the same sociological mechanisms. Whether a theory is later considered true or false doesn’t matter for sociological analysis; both outcomes are seen as the product of social processes.\nThis principle is particularly important because it rejects the idea that true scientific knowledge is exempt from sociological scrutiny. In practice, this means that scientists’ choices, preferences, and institutional environments can be just as influential in shaping a scientific theory as the data itself.\nA classic example comes from the history of phrenology, a now-discredited science that purported to explain human behavior by examining the shape of the skull. For much of the 19th century, phrenology was considered legitimate science by many researchers. According to Bloor, the eventual rejection of phrenology wasn’t purely due to new data; instead, changing social and intellectual climates, as well as shifts in institutional power, played a critical role in its downfall.\n\n\n5.1.5 Examples and Case Studies\n\nTODO: Are there better case studies? I think I want to lean on Collin’s GW work because it emphasizes iteration and integrity so much. Weave these or other examples into the text above.\nGravitational Waves: Collins’ work on the detection of gravitational waves is a vivid illustration of how different groups of scientists can interpret the same data differently based on social and institutional contexts. Even with the same instrumentation, conclusions varied, showing how data does not dictate a single outcome.\nQuantum Mechanics: The early 20th-century debates between the particle and wave theories of light provide another classic case of underdetermination. Both theories explained the data adequately for certain experimental conditions, but the resolution of this debate was influenced by the broader intellectual currents and the personal inclinations of key figures in the field.\nPulsars: Pinch and Collins’ study of pulsars shows that interpreting new astronomical data wasn’t simply a matter of observation. The interpretation of pulsars as a new kind of star was contingent on the theoretical frameworks and prior assumptions of the astronomers involved.\n\n\n\n5.1.6 Underdetermination and Modern Modeling Practices\nThis sociological lens on science aligns with the modern approach to statistical modeling, especially in fields like Bayesian inference, where uncertainty, model comparison, and multiple potential explanations are explicitly acknowledged. In Bayesian statistics, as advocated by Richard McElreath, modelers must confront the fact that the data doesn’t automatically generate a single “true” model. Instead, multiple models can explain the same data, and choosing between them involves a combination of theoretical assumptions, prior knowledge, and external validation criteria.\nMuch like the sociologists of scientific knowledge argue, scientific knowledge and statistical modeling are underdetermined by data alone. This makes the process inherently interpretive, driven by a combination of evidence, judgment, and social context.",
    "crumbs": [
      "**WORKFLOW**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mindful modelling</span>"
    ]
  },
  {
    "objectID": "mindful-modelling.html#bayesian-modeling-and-underdetermination",
    "href": "mindful-modelling.html#bayesian-modeling-and-underdetermination",
    "title": "5  Mindful modelling",
    "section": "5.2 Bayesian Modeling and Underdetermination",
    "text": "5.2 Bayesian Modeling and Underdetermination\nIn the previous section, we explored how SSK scholars argue that data is often underdetermined by theory—meaning that multiple theories can explain the same set of data. This theme resonates with modern Bayesian statistical modeling, particularly in the framework Richard McElreath develops in Statistical Rethinking (2015). Bayesian statistics provides a formal structure for handling uncertainty and acknowledges the role of assumptions and prior beliefs in shaping the interpretation of data, echoing the interpretive flexibility discussed by sociologists of science.\nHere are key parallels between the two approaches:\n\n5.2.1 Priors and Empirical Equivalence: Data Doesn’t Speak for Itself\nOne of the central features of Bayesian inference is that it combines prior beliefs (or knowledge) with observed data to form a posterior distribution—a probabilistic estimate of the parameters or outcomes of interest. In this framework, priors represent what we know (or assume) before seeing the data, and they can strongly influence the resulting posterior distribution, especially when the data is sparse or noisy.\nThis is closely related to the SSK concept of empirical equivalence. When different theories (or models) can explain the same data, it’s the assumptions and priors that distinguish them. In Bayesian terms, two models might fit the data equally well, but their differing priors reflect different theoretical commitments or preconceptions, which is analogous to how scientists with different theoretical frameworks might interpret the same empirical evidence differently.\nTake the example of early quantum mechanics from the SSK discussion. Just as the particle and wave theories of light could both account for the experimental data, in a Bayesian framework, we might have two models that predict the same likelihood of observing certain outcomes. The choice between them could depend on the priors we bring to the problem—e.g., the simplicity of one theory, its coherence with other areas of physics, or historical precedents.\nIn McElreath’s approach, modeling starts with the acknowledgment that data does not “speak for itself,” but is interpreted through the lens of priors and assumptions. This is exactly the kind of underdetermination the Edinburgh School talks about—data alone cannot resolve competing models; it must be interpreted with context.\n\n\n5.2.2 Multiple Models and the Role of Social Factors in Theory Choice\nIn Bayesian modeling, one of the key practices is model comparison—the idea that we build and test multiple models to explore different hypotheses or theoretical explanations. McElreath’s method often involves specifying multiple models with different assumptions and comparing them using tools like posterior predictive checks, information criteria (like WAIC or LOO), and model averaging. The key here is that the data alone isn’t dictating which model is “right”; rather, model comparison becomes a nuanced process that includes prior beliefs, empirical fit, and theoretical coherence.\nThis has a clear connection to the SSK’s focus on the social factors involved in theory choice. Just as the decision about which scientific theory prevails is influenced by non-empirical factors (such as institutional affiliations, intellectual traditions, or social negotiations), the choice between models in a Bayesian framework isn’t determined by data alone. McElreath’s framework acknowledges that theory choice is as much about parsimony, interpretability, and prior knowledge as it is about empirical adequacy.\n\nNOTE: I can connect to Tiago here as well. MDL.\n\nLet’s recall the example from Harry Collins about the detection of gravitational waves. Different research groups arrived at different conclusions using the same data, and these differences were not simply about empirical evidence—they involved judgments about which models were more plausible based on prior expectations and institutional practices. In Bayesian terms, this is akin to different groups working with different priors or theoretical assumptions, leading to different posterior beliefs even when confronted with the same likelihood function (data).\n\n\n5.2.3 Interpretive Flexibility: Priors and Posteriors in Bayesian Thinking\nThe concept of interpretive flexibility, discussed by Pinch and Collins, holds that data can be interpreted in different ways depending on the theoretical framework in use. Bayesian modeling formalizes this flexibility through the use of priors, which are updated in light of new evidence but do not necessarily converge on a single “truth.” The flexibility remains in the system: different priors lead to different posteriors, even if the data is identical.\nIn the Bayesian framework, this flexibility is not a flaw but a feature. McElreath teaches that our models are simplifications, and multiple valid models can coexist based on different sets of assumptions or data-generating processes. Rather than seeking a singular correct model, McElreath encourages exploration of alternative models and emphasizes understanding the assumptions embedded in each.\nReturning to the pulsar discovery case from the SSK section: astronomers had to decide whether their data represented a new celestial object or an instrument error. This decision wasn’t simply dictated by the data; it depended on their theoretical commitments and assumptions. Similarly, in a Bayesian framework, our priors guide us in interpreting ambiguous data and help us decide between competing explanations, even when the data might be equally compatible with both.\n\n\n5.2.4 Knowledge as Socially Constructed: Priors Reflect Judgment\nBloor’s symmetry principle, which posits that both true and false scientific beliefs are subject to sociological analysis, dovetails with the idea that Bayesian priors represent subjective judgment. In Bayesian inference, priors are not objective truths—they reflect what the modeler (or community) believes before seeing the data. This means that different communities or scientists might use different priors, based on their theoretical or social contexts.\nMcElreath’s Bayesian approach mirrors this concept by explicitly acknowledging the role of subjective judgment in model construction. He advocates for transparency in how priors are chosen, and his approach encourages modelers to be aware of the implications of their prior choices. The Bayesian process doesn’t claim to eliminate subjectivity but rather to incorporate it into the analysis in a formalized way.\n\nNOTE: Maximum-entropy priors, etc.\n\nBloor’s work on the downfall of phrenology provides a useful analogy here. In the 19th century, phrenology was considered legitimate science by many researchers, despite its later rejection. A Bayesian analysis might suggest that the priors of 19th-century scientists—shaped by their social, cultural, and intellectual context—were very different from modern priors. When new data came in, it was filtered through these prior beliefs, and the eventual shift away from phrenology can be seen as a change in the scientific community’s priors over time, driven as much by social change as by empirical evidence.\n\n\n5.2.5 Connecting the Dots: Underdetermination, Bayesian Modeling, and the Sociology of Knowledge\nBoth SSK and Bayesian statistics, as articulated by McElreath, embrace the idea that data alone cannot settle scientific debates. Instead, both fields recognize that assumptions, priors, and social context shape how data is interpreted and which theories (or models) are favored. Let’s synthesize these connections:\n\nTheoretical Flexibility: Just as the sociology of scientific knowledge sees multiple theories as potentially explaining the same data (empirical equivalence), Bayesian modeling builds multiple models with different assumptions to explain the same data. Neither approach assumes that one “true” theory or model will automatically emerge from the data alone.\nRole of Assumptions: In both SSK and Bayesian thinking, assumptions are central. For the sociologist of science, these assumptions may be social or institutional (e.g., the credibility of a research group or the cultural context in which a theory is accepted). For McElreath’s Bayesian approach, the assumptions are mathematical and encoded in the priors. But in both cases, it’s clear that data does not operate in isolation from the assumptions used to interpret it.\nModel Comparison as Social Process: Just as the resolution of scientific debates in the sociology of science involves social negotiation, the choice between Bayesian models involves multiple criteria beyond empirical fit—such as parsimony, coherence with prior knowledge, and theoretical plausibility. McElreath’s approach recognizes this and advocates for a holistic view of model comparison, just as SSK scholars advocate for understanding theory choice within its broader social context.\nUncertainty and Multiple Valid Explanations: Both approaches embrace uncertainty and resist the temptation to find a singular answer. In Bayesian modeling, uncertainty is explicitly modeled, and competing models are compared probabilistically. Similarly, SSK scholars argue that scientific theories are underdetermined by data and must be evaluated in light of broader social and intellectual contexts. In both cases, the aim is not to eliminate uncertainty but to understand and work with it.\n\n\n\n5.2.6 Two Sides of the Same Coin\nIn the end, Bayesian statistics and the sociology of scientific knowledge are both concerned with understanding the contingent, context-dependent nature of scientific knowledge. Both fields reject the notion that data alone can settle debates and instead focus on how assumptions, priors, and social context shape the production of knowledge.\nWhether through Bayesian modeling’s explicit treatment of uncertainty and priors or the SSK’s analysis of the social factors shaping theory choice, both fields highlight that science is as much about judgment as it is about data. And in that judgment lies the connection between McElreath’s Bayesian thinking and the ideas of underdetermination developed by Bloor, Collins, and others.",
    "crumbs": [
      "**WORKFLOW**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mindful modelling</span>"
    ]
  },
  {
    "objectID": "mindful-modelling.html#why-start-with-scientific-models-causal-thinking-before-statistical-estimation",
    "href": "mindful-modelling.html#why-start-with-scientific-models-causal-thinking-before-statistical-estimation",
    "title": "5  Mindful modelling",
    "section": "5.3 Why Start with Scientific Models? Causal Thinking Before Statistical Estimation",
    "text": "5.3 Why Start with Scientific Models? Causal Thinking Before Statistical Estimation\nRichard McElreath, in Statistical Rethinking, emphasizes that modeling is not simply about fitting statistical models to data—it’s about articulating a scientific model that reflects your understanding of the underlying causal processes. This distinction is crucial because it shifts our focus from mere pattern recognition to causal explanation. In other words, statistical models help us estimate relationships, but it’s our scientific models that tell us what those relationships mean.\nThis idea connects directly to broader issues in the philosophy of science, where the distinction between describing data and explaining phenomena has long been central. Nancy Cartwright and Judea Pearl’s work on causality further sharpens this distinction by arguing that if we want to understand how the world works, we must go beyond statistical associations and grapple directly with causal structures.\n\n5.3.1 Statistical Models Describe, Scientific Models Explain\nLet’s start with McElreath’s main point: the idea that statistical models are tools for quantifying relationships, but they don’t tell us why those relationships exist. For example, imagine we have data showing a strong correlation between the number of ice creams sold and the number of drownings at the beach. A purely statistical model might capture this association beautifully, but it would tell us nothing about the actual causal mechanism behind it.\nHere’s where scientific models come into play. A scientific model based on our understanding of the world would suggest a common cause (hot weather) driving both ice cream sales and beachgoers’ risk of drowning. By articulating this causal structure, we avoid mistaking correlation for causation. This, in essence, is what McElreath advocates: build a causal model of the world first, and then construct your statistical model to estimate the parameters of that causal model. Otherwise, you risk falling into the trap of overfitting patterns without understanding their meaning.\n\n\n5.3.2 Nancy Cartwright and the Importance of Mechanisms\nNancy Cartwright, a philosopher of science, extends this argument by emphasizing the importance of mechanisms in scientific modeling. In her book Nature’s Capacities and Their Measurement (1989), she argues that statistical regularities alone are not enough. What we care about in science are the capacities and mechanisms that produce those regularities. For Cartwright, a good scientific model isn’t just one that fits the data—it’s one that represents the causal mechanisms that generate the phenomena we observe.\nFor example, when studying the impact of a new drug, it’s not enough to say that people who take the drug recover more often than those who don’t. A proper scientific model would also try to capture the mechanism of action—how the drug interacts with biological processes to produce the observed effects. This resonates with McElreath’s approach: don’t just describe the data; think about the underlying causal process and represent that in your model.\n\nTODO: Integrate the sociological content on mechanisms here too (Gross, etc.)\n\n\n\n5.3.3 Judea Pearl: Causal Graphs and Do-Calculus\nJudea Pearl’s work, particularly his development of causal diagrams and do-calculus, complements Cartwright’s focus on mechanisms. Pearl argues that causality isn’t just about statistical associations (correlations, regressions, etc.)—it’s about understanding the causal pathways that connect variables. His famous use of directed acyclic graphs (DAGs) offers a visual and formal way to represent these causal relationships.\nPearl’s DAGs are not just a tool for visually organizing relationships—they are a formal system that helps clarify causal assumptions. By mapping out which variables are causes and which are effects, and how they are linked, DAGs let us distinguish between correlation and causation. This is essential for avoiding common statistical pitfalls, like confusing a common cause with a direct effect (as in our ice cream-drowning example). Pearl formalizes this in his do-calculus, which allows us to mathematically reason about interventions and causality in ways that go beyond standard statistical techniques.\nConsider a classic example from Pearl’s work: the relationship between smoking and lung cancer. A statistical model might find a strong association between smoking and cancer, but how do we know it’s causal? Using a DAG, we can represent the possible causal structure and adjust for confounding factors like genetic predisposition. This is exactly the kind of thinking that McElreath encourages when he advises us to start with a scientific model first. Pearl’s work provides a robust formalism to represent and interrogate those scientific models.\n\n\n5.3.4 Connecting to the Sociology of Science: Underdetermination and Causal Models\nIt’s worth pausing here to connect this back to the earlier discussion of underdetermination in the sociology of scientific knowledge (SSK). Just as we saw with David Bloor and Harry Collins, data alone often underdetermines theory—there can be multiple explanations for the same pattern in the data. Scientific models, particularly causal models, help us navigate this ambiguity by giving us a framework to make testable predictions about the world. Cartwright and Pearl’s emphasis on mechanisms and causal pathways can be seen as providing tools to resolve some of the underdetermination we see in data analysis.\nWhereas SSK emphasizes the social processes by which scientific theories are chosen, Cartwright and Pearl offer formal tools for testing different causal models and refining our understanding of the mechanisms at play. Still, as Cartwright would remind us, even these tools cannot remove the role of judgment—our models remain simplifications, and they rely on assumptions about the world.\n\n\n5.3.5 Examples: Causality in Action\nLet’s revisit some of the earlier examples to see how this thinking applies:\n\nGravitational Waves (Collins): The discovery of gravitational waves involved competing interpretations of noisy data. From a Bayesian modeling perspective, different scientific teams might have had different priors or causal models about what the data represented. Judea Pearl’s DAGs would encourage us to map out the causal assumptions behind each interpretation, clarifying the distinctions between signal and noise. Cartwright’s emphasis on mechanisms would further push us to think about the physical processes underlying those signals—what’s the actual mechanism producing the waves?\nQuantum Mechanics (Bloor): When choosing between particle and wave theories of light, scientists weren’t just fitting statistical models to data—they were building causal models based on their understanding of how light behaves. Pearl’s DAG framework would allow scientists to formally represent different causal structures and ask: under what circumstances would we expect to see these particular data patterns? Cartwright would remind us to look for the mechanisms that could generate the observed phenomena, rather than just focusing on the fit of the statistical models.\n\n\n\n5.3.6 Setting the Stage for Simulation and Validation\nThis brings us to the next phase in building good models: simulating data and validating models. Once we’ve articulated our causal scientific model, we can use tools like DAGs to represent it and simulations to test it. This is a key part of the model-building process that McElreath emphasizes—good models don’t just fit the data we have; they help us make predictions about new data. By simulating data, we can check whether our models capture the underlying causal processes or whether they’re merely overfitting to the data at hand.\n\n\n5.3.7 The Role of Directed Acyclic Graphs (DAGs)\nFinally, DAGs—first introduced by Judea Pearl—will be a key tool in the next chapter as we dive into model validation. DAGs allow us to visually represent our scientific modeling assumptions and clarify the causal structure we’re working with. By drawing out the relationships between variables, we can test our assumptions, simulate data, and evaluate whether our models are consistent with the scientific story we’re trying to tell.\n\n\n5.3.8 Scientific Models First, Statistics Second\nIn sum, the argument for starting with scientific models before moving to statistical models is not just a practical suggestion from McElreath—it’s grounded in deep philosophical principles about how we understand the world. Nancy Cartwright’s focus on mechanisms reminds us that a good model must capture the causal processes at play, not just statistical regularities. Judea Pearl’s DAGs and do-calculus provide formal tools for representing and testing these causal models, helping us distinguish correlation from causation.\nAs we move forward into model validation and data simulation, this philosophical grounding will be essential. We’re not just fitting data—we’re testing our understanding of the world, refining our causal models, and ensuring that they can generate predictions about new situations. By keeping our focus on scientific models first, we ensure that our statistical models remain tools for discovery, not just tools for description.",
    "crumbs": [
      "**WORKFLOW**",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mindful modelling</span>"
    ]
  },
  {
    "objectID": "iteration.html#sequential-modelling",
    "href": "iteration.html#sequential-modelling",
    "title": "6  Sequential iterative modelling",
    "section": "",
    "text": "6.1.1 How Sequential Modeling Works:\n\nInitial Model (Baseline Model): Typically, a simple model is built first, including only key independent variables (e.g., demographic variables like age, gender, education).\nAdding Layers: In subsequent steps, additional variables (e.g., social or economic factors) are added in a sequence, often based on theory or prior research. Each step reveals how the introduction of these new variables changes the coefficients and relationships found in the initial model.\nAssessment and Comparison: At each stage, the new model is compared to the previous one to assess how the additional variables impact the model’s explanatory power (e.g., using measures like adjusted R² or AIC/BIC in regression models).\nInterpreting Changes: The focus is not just on the final model but on understanding the changes in relationships between variables as new layers of complexity are introduced. For example, a variable that is significant in the initial model may lose its significance once another variable is added, suggesting that the initial relationship was spurious or confounded.\n\n\n\n6.1.2 Connection to Paul Lazarsfeld and Columbia University:\nSequential modeling has deep roots in the survey research tradition pioneered by Paul Lazarsfeld and his colleagues at Columbia University in the mid-20th century. Lazarsfeld, a leading figure in quantitative sociology, developed several methods for analyzing survey data that emphasized careful, stepwise modeling and interpretation.\nLazarsfeld and his collaborators were known for developing sequential or stepwise approaches to modeling that aimed to uncover causal pathways in social behavior, especially using path analysis and latent variable modeling techniques. They were particularly interested in how different variables mediated or moderated relationships between social phenomena, such as how media exposure influenced voting behavior or how social class affected educational outcomes.\nThis tradition, especially in the Columbia School of sociology, emphasized: - Incremental Analysis: Lazarsfeld’s approach involved testing hypotheses in stages, starting with basic models and progressively adding more nuanced variables to see how they influenced the relationships observed. This mirrors the contemporary idea of sequential modeling. - Mediation and Moderation: Lazarsfeld’s work frequently examined how intermediary variables (mediators) or external factors (moderators) altered the direct effects observed in survey research. This layered modeling approach is aligned with the modern practice of sequentially adding variables to better understand complex causal relationships. - Survey Research Tradition: Sequential modeling in sociology is closely tied to survey data analysis, a method that Lazarsfeld and his colleagues pioneered. Surveys provided rich data that could be analyzed in layers, allowing sociologists to isolate the impact of various social forces on individual behavior.\n\n\n6.1.3 Example of Sequential Modeling in Sociology:\nConsider a study on educational attainment. A sequential modeling approach might work like this:\n\nModel 1: Start with a model including only basic demographic variables (age, gender, race).\nModel 2: Add socio-economic status (SES) variables like parental education and income to see how much they contribute to explaining differences in educational attainment beyond demographics.\nModel 3: Introduce school-related factors (school quality, peer effects) to see if the impact of SES diminishes, suggesting that these school-related factors mediate the effect of SES on educational outcomes.\nModel 4: Include neighborhood characteristics or individual motivations to further unpack how these additional variables explain differences in outcomes.\n\nThe results from each step provide insights into the different layers of influence on educational attainment, helping to disentangle whether SES exerts a direct effect or whether its impact is channeled through other factors like school quality or individual motivation.\n\n\n6.1.4 Summary:\nSequential modeling is a systematic, layered approach to building and interpreting models, which allows sociologists to analyze how different factors influence an outcome in stages. Its roots can be traced back to the survey research tradition of Paul Lazarsfeld at Columbia University, where stepwise modeling helped illuminate complex causal relationships in social behavior. This method remains a powerful tool in quantitative sociology for disentangling the effects of multiple, interacting social factors.\n\nhow does this compare to contemporary workflows, especially iterarative bayesian approaches?\n\nContemporary workflows, particularly in iterative Bayesian approaches, share similarities with traditional sequential modeling but diverge in significant ways, particularly in how uncertainty is handled and how models are updated in a more dynamic, flexible manner. Let’s explore the key differences and similarities between sequential modeling in the classical (frequentist) tradition, rooted in the work of Lazarsfeld and the Columbia University school of sociology, and more modern Bayesian workflows.\n\n\n6.1.5 Similarities between Sequential Modeling and Bayesian Approaches\n\nIncremental Model Building:\n\nBoth sequential modeling and Bayesian workflows emphasize building models step by step, incorporating variables progressively.\nIn both approaches, the goal is to understand how each set of variables (e.g., demographic, socio-economic, psychological) affects the outcome, and how adding more information refines the relationships between these variables.\n\nMediation and Moderation:\n\nThe emphasis on mediators and moderators in sequential modeling mirrors what happens in Bayesian workflows when you assess how including latent variables or hierarchical structures refines your understanding of causal mechanisms.\nIn both frameworks, there is an interest in how adding more complexity—whether via additional variables or hierarchical structures—changes the relationships and pathways observed.\n\nInterpretation of Changes:\n\nIn both methods, understanding how models evolve as you add layers is crucial. You’re not only interested in the final model but in how relationships change, coefficients shift, or whether variables lose significance once others are included.\nBayesian approaches, like sequential modeling, often involve iteratively revising models as new data or variables are introduced.\n\n\n\n\n6.1.6 Key Differences between Sequential Modeling and Bayesian Workflows\n\nHandling of Uncertainty:\n\nFrequentist Sequential Modeling: In classical sequential modeling, each model step generates a fixed estimate for coefficients and standard errors. Uncertainty is typically represented in terms of confidence intervals or p-values, and model comparisons often rely on point estimates and goodness-of-fit statistics like AIC/BIC or adjusted R².\nBayesian Iterative Modeling: In contrast, Bayesian approaches fully incorporate uncertainty at every stage of the modeling process. Rather than producing fixed parameter estimates, Bayesian models generate posterior distributions for each parameter, reflecting not just the best guess (mean or median of the posterior) but also the uncertainty around it. This allows for a more nuanced understanding of how much we “know” about the relationships in the model, based on the data and prior knowledge.\n\nPriors and Incorporation of Existing Knowledge:\n\nFrequentist Sequential Models: In traditional approaches, no prior information is used unless explicitly modeled (e.g., in mixed-effects models). You rely entirely on the data to estimate parameters, and there is no direct mechanism for incorporating previous knowledge.\nBayesian Approaches: Bayesian workflows allow for the inclusion of prior knowledge through the use of priors. Each model iteration can refine these priors based on new data, progressively updating beliefs. In a Bayesian framework, you can explicitly model your uncertainty before even adding data, and adjust it dynamically as more data or model layers are added.\n\nModel Comparison and Convergence:\n\nFrequentist Sequential Models: Sequential modeling often involves formal hypothesis testing between steps (e.g., nested model tests, likelihood ratio tests) or the comparison of AIC/BIC values to determine which model fits best. These comparisons are static, in the sense that you test one model at a time and move forward with it.\nBayesian Models: In Bayesian workflows, model comparison is typically done via posterior predictive checks, WAIC, or LOO-CV (Leave-One-Out Cross-Validation), which offer more dynamic ways to assess how well a model predicts new data. Bayesian models are often more flexible and iterative in that you can continue to refine models, incorporating more complexity as new data or insights arise, without discarding the uncertainty of previous models.\n\nIncorporating Hierarchies and Multilevel Structures:\n\nSequential Models: While classical approaches can incorporate multilevel or hierarchical models, they often do so as a fixed structure once decided (e.g., a mixed-effects model). Hierarchies are incorporated once the basic relationships have been modeled, and they are often added toward the end of the modeling process.\nBayesian Hierarchical Models: In Bayesian workflows, hierarchical modeling is more naturally integrated from the start. Hierarchical structures can be part of the initial model specification, and Bayesian methods excel at modeling partial pooling, which allows for more flexible handling of group-level effects. Bayesian methods also handle uncertainty in these group-level estimates in ways that frequentist methods cannot.\n\nFlexibility and Iteration:\n\nFrequentist Sequential Models: Once a model is estimated and fitted, there is often a point at which no further modification is made. Models are compared at specific checkpoints, but past this point, you don’t revisit prior steps or include new evidence without re-estimating models from scratch.\nBayesian Workflows: Bayesian modeling is inherently iterative. Models are constantly refined based on new data, and it’s easier to revisit previous steps, incorporate new knowledge, and even compare models with the same or different priors to see how robust the results are. This flexibility allows for a more dynamic workflow, as you can continually refine models as new data, theory, or even model diagnostics evolve.\n\nPhilosophy of Inference:\n\nFrequentist Sequential Models: The sequential modeling approach often relies on null hypothesis testing at each stage. The goal is to assess whether the additional variables provide statistically significant improvements or changes.\nBayesian Models: Bayesian approaches, on the other hand, focus on probabilistic inference rather than hypothesis testing. The emphasis is on estimating credible intervals and the posterior distributions of parameters rather than testing a null hypothesis at each stage. In this framework, every stage of the model contributes to a clearer picture of the data, without the rigid framework of significance testing.\n\n\n\n\n6.1.7 Example: Education and Income\nLet’s say we’re interested in modeling the effect of education on income, using additional covariates like age and gender.\n\n6.1.7.1 In a Frequentist Sequential Model:\n\nModel 1: Start with education as the sole predictor of income.\nModel 2: Add gender to control for gender differences in income.\nModel 3: Add age to control for generational effects.\nModel Comparison: Use AIC or p-values to assess whether the model fit improves with each added variable.\n\n\n\n6.1.7.2 In a Bayesian Workflow:\n\nModel 1: Start with education, with priors reflecting previous knowledge of education’s effect on income.\nModel 2: Add gender, updating the posterior distributions from Model 1, and refining prior beliefs if necessary.\nModel 3: Add age, incorporating uncertainty at every step.\nModel Comparison: Use WAIC or LOO-CV to evaluate model fit, while retaining the flexibility to refine priors and revisit earlier steps.\n\nIn this Bayesian example, you maintain a more dynamic process, where you constantly update beliefs based on the evidence from the data, rather than locking in each stage of the model with hypothesis tests or fixed point estimates.\n\n\n\n6.1.8 Conclusion\nSequential modeling in the Lazarsfeld tradition provides a powerful framework for building models incrementally, particularly in survey-based research. However, contemporary iterative Bayesian workflows take this a step further by incorporating uncertainty at each stage, allowing for flexible updates of prior knowledge, and focusing on probabilistic inference. Both approaches share the core principle of systematically building complexity into models, but Bayesian methods offer more nuanced handling of uncertainty and a dynamic, iterative process that is well-suited to the complexities of modern data analysis.",
    "crumbs": [
      "**WORKFLOW**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>~~Sequential~~ iterative modelling</span>"
    ]
  },
  {
    "objectID": "iteration.html#iteration",
    "href": "iteration.html#iteration",
    "title": "6  Sequential iterative modelling",
    "section": "6.2 Iteration",
    "text": "6.2 Iteration\nHarry Collins’ Gravity’s Shadow: The Search for Gravitational Waves provides a detailed ethnographic account of the decades-long process that led to one of the most significant scientific discoveries of the 21st century. The story of gravitational waves offers a powerful analogy to latent variable modeling in the social sciences, especially when we focus on the iterative nature of both endeavors and the role of instruments in refining our understanding of unseen phenomena.\nIterative Work: Learning and Refining through Constant Revision\nCollins’ Gravity’s Shadow emphasizes the iterative nature of scientific work. The detection of gravitational waves was not a one-time event, nor was it a simple matter of waiting for better instruments to arrive. It was the result of decades of trial and error, refinement, and revision. Collins meticulously documents the process by which scientists developed increasingly sensitive instruments, created models to interpret their data, and collaborated across institutions and disciplines to reach the point of detection.\nOne of the central themes of Gravity’s Shadow is how scientists continually revised their models and instruments based on new findings. Each new iteration brought them closer to their goal, but none was considered final. Even after the construction of LIGO (the Laser Interferometer Gravitational-Wave Observatory), the project did not immediately lead to a discovery. Instead, it involved years of calibration, data collection, and refinement of both instruments and the models used to interpret the data. Scientists had to deal with noise, ambiguity, and the possibility of false detections, all of which demanded constant revision and refinement of both their physical instruments and their conceptual models of gravitational waves.\nThis iterative process is strikingly similar to the way Richard McElreath describes model-building in the social sciences. McElreath advocates for an iterative approach to statistical modeling: we build a model, test it, refine it based on the evidence, and repeat. Just as gravitational wave scientists couldn’t expect LIGO to detect waves on the first try, social scientists can’t expect their first model to perfectly capture the latent constructs they’re studying. The process is one of continual refinement—improving our models, testing their fit with data, and revising them as new evidence and insights emerge.\nExample: The Iterative Development of LIGO\nOne of the most powerful examples from Gravity’s Shadow is the iterative development of LIGO itself. Collins documents how early versions of the interferometer faced significant challenges—there was too much noise in the data, the sensitivity wasn’t high enough, and there were countless external factors (like seismic activity) that had to be accounted for. Each iteration of LIGO improved upon the previous one, with more advanced technology, better calibration methods, and more precise models of what gravitational wave signals would look like.\nIn the social sciences, latent variable models undergo a similar process of iteration. When studying constructs like populism or class structures, our first models are often too simplistic. They fail to account for important factors, or they overfit the data. Like the early versions of LIGO, these models don’t work perfectly, but they provide a foundation to build upon. We refine them based on new data, adjust our assumptions, and incorporate new variables that better capture the complexity of the latent construct. The iterative process is essential to both fields—whether we’re detecting waves in space-time or estimating abstract social constructs, progress comes from constant refinement.\nLatent Variable Models as Telescopes: Better Instruments for Understanding\nBuilding on the iterative theme, let’s take the analogy of latent variable models as instruments—or even as telescopes—further. Just as LIGO and other telescopes allowed scientists to “see” previously unobservable phenomena, latent variable models are the social scientist’s instruments for making abstract concepts visible.\nIn Gravity’s Shadow, Collins highlights how the development of better instruments was key to the detection of gravitational waves. LIGO didn’t detect gravitational waves on its first day of operation—scientists had to design and continually improve a device that was sensitive enough to pick up the incredibly faint ripples in space-time. Each iteration of LIGO improved its ability to detect these waves, much like how the development of more powerful telescopes allowed us to observe distant galaxies that were once invisible to the human eye.\nSimilarly, in the social sciences, latent constructs like populism, belief systems, or social class are not directly observable. We can’t “see” populism in the same way we can observe a physical object. Instead, we build models that act as telescopes—tools that allow us to make these hidden constructs visible through their indirect manifestations in observable variables. Survey responses, voting behavior, political rhetoric—these are the “signals” that we observe, but they are only proxies for the latent construct we’re trying to understand. Just as LIGO detects the effects of gravitational waves on space-time, latent variable models detect the effects of latent constructs on the data we can observe.\nExample: Detecting Populism through Latent Variable Models\nLet’s take an example from the social sciences: the latent construct of populism. Much like gravitational waves, populism doesn’t have a material form—it’s an abstract concept that exists as a set of beliefs, attitudes, and behaviors. We can’t directly observe populism in the same way we can observe a physical event. Instead, we detect its presence through indirect indicators: speech patterns in political leaders, voting behavior in certain segments of the population, or responses to survey questions about trust in elites and institutions.\nA latent variable model is our equivalent of LIGO in this case. It allows us to detect the presence of populism by modeling the relationships between the observable indicators (survey responses, speeches, voting patterns) and the latent construct itself. Just as LIGO had to filter out noise and refine its detection methods, our latent variable models need to account for measurement error, confounding variables, and ambiguity in the data.\nAs we refine these models, much like the iterative development of LIGO, we improve our ability to “see” populism more clearly. Each new dataset, each new refinement of the model, helps us to sharpen the lens through which we view this abstract construct, much like how each new version of LIGO allowed scientists to detect ever fainter gravitational waves.\nIteration as a Core Theme: From Gravitational Waves to Social Constructs\nThe iterative process described in Gravity’s Shadow resonates deeply with the way McElreath and other modelers approach latent variable modeling. Iteration is at the heart of both scientific discovery and model-building. In both fields, we begin with imperfect models or instruments, refine them based on new data and insights, and move incrementally closer to uncovering the truth about latent phenomena.\nThis iterative process has two key components:\n\nModel-building and refinement: In both the physical sciences and social sciences, our first models are rarely correct. Collins describes how gravitational wave scientists continually refined their models of what a gravitational wave signal would look like, just as social scientists refine their latent variable models based on new data and theoretical insights.\nValidation through indirect evidence: Whether it’s gravitational waves or populism, we rely on indirect evidence to validate our models. In Collins’ case, scientists used complex simulations and comparisons with existing physical theories to validate their models of gravitational waves. In social sciences, we use simulation, fit indices, and external validation (like cross-study comparisons) to ensure that our latent variable models are capturing the underlying construct we’re interested in.\n\nWhere This Fits: An Epistemological Bridge\nThis section would fit well in a chapter on latent variable modeling and scientific inference—it could serve as a philosophical bridge between discussions of measurement and validity and the iterative nature of scientific work. By connecting the iterative process in both gravitational wave detection and latent variable modeling, you can emphasize the theme of gradual, methodical improvement in our understanding of unseen constructs.\nThis could also be an introduction to a more detailed discussion on latent variable modeling techniques. By positioning latent variable models as instruments for detecting the unseen, you provide a compelling motivation for the use of these techniques, linking back to the larger philosophical discussion about how we build and refine our scientific models over time.\nConclusion: Instruments for the Invisible\nBoth gravitational wave detection and latent variable modeling demonstrate that scientific discovery is an iterative process—we start with imperfect models and instruments, revise them based on new data, and move closer to understanding phenomena that are invisible to direct observation. Just as LIGO allowed physicists to “see” gravitational waves, latent variable models allow social scientists to detect abstract constructs like populism or belief systems.\nBy positioning latent variable models as the social science equivalent of telescopes or instruments, we underscore their importance in refining our understanding of the world. And, just as the detection of gravitational waves required constant iteration and refinement, so too does the process of detecting latent constructs in the social sciences. This iterative, instrument-based approach helps us navigate the complexities of unseen phenomena, whether they exist in the fabric of space-time or in the abstract structures of human society.",
    "crumbs": [
      "**WORKFLOW**",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>~~Sequential~~ iterative modelling</span>"
    ]
  },
  {
    "objectID": "causality.html#counterfacturals-versus-interventions",
    "href": "causality.html#counterfacturals-versus-interventions",
    "title": "20  Causality",
    "section": "20.3 Counterfacturals versus interventions",
    "text": "20.3 Counterfacturals versus interventions\n\nTODO: Pull some content from above when revising",
    "crumbs": [
      "**PREDICTION & INFERENCE**",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Causality</span>"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Who Is This Book For?\nI have tried to write this book for as broad and diverse a readership as possible, but even with 200,000 words it can’t cover everything, and it can’t be a book for everyone. The type of person I have primarily kept in mind is a scientific researcher—regardless of their specific disciplinary background or research interests—with little to no computational experience or expertise. I have no specific career stage in mind; rather than writing for students of a particular level, I have tried to write for anyone who is new, or relatively new, to computational social science. That could be as true for a tenured professor as for a graduate student, or an advanced undergraduate student.\nWhat do I mean by “scientific researcher” or “scientist”? I use those terms a lot, and pretty casually, so it’s worth taking a moment to clarify what I mean by them. I mean “scientist” in a very general sense, inclusive of all the social, cognitive, and communication sciences; applied health sciences; environmental sciences; and interdisciplinary fields like network science that cut across nearly every branch of science. While it is easy to enumerate dozens of things that make sociologists different from epidemiologists, economists different from biologists, and everyone different from physicists, my preference in writing this book has been to emphasize what we have in common: we all engage in efforts to honestly and systematically advance the state of general knowledge in our fields using a combination of empirical data, theory, and models. Given that you are currently reading this book, I assume you are interested in furthering your ability to do so using computational approaches.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#who-is-this-book-for",
    "href": "introduction.html#who-is-this-book-for",
    "title": "Introduction",
    "section": "",
    "text": "Assumed Knowledge and Skills\nIn this edition, I assume you have little to no previous experience with programming, scientific computing, or quantitative data analysis. The early chapters are designed to give you a foundation in these areas, which we will build on as we progress through more advanced methods.\nWhen it comes to methodological background, I no longer assume that you are familiar with research design or conventional quantitative research methods in the social sciences. Instead, the first half of the book has been thoroughly revised to teach this content from the ground up, building up your skills incrementally. This shift marks a key difference from the print edition, where I expected readers to have already completed a course in quantitative research methods. Now, my goal is to guide you through the foundations, with a focus on concepts like causal inference and probabilistic modeling. While it may be a bit of an unconventional introduction to quantitative methods, I believe it will better prepare you for the present and future landscape of social science research. We’ll focus on the present as well as the probable and preferable future instead of being beholden to the past.\nPeople enter computational social science with vastly different intellectual backgrounds. Some might have strong experience in quantitative methods, while others may come from a more qualitative or interpretive background. These differences can shape your learning process in important and perhaps unexpected ways.\nFirst, if you have some experience with quantitative data analysis—perhaps from a course in statistics or econometrics—you’re in a great position to build on that foundation. You may find the transition into computational methods easier because you’re already comfortable with probabilistic thinking and model-based approaches. This prior knowledge will be an asset, especially as we delve into more complex models.\nHowever, if your background is more qualitative or interpretive, you also bring valuable skills to the table, even if they might not seem immediately relevant. Many computational methods, especially those used in fields like cultural sociology or political science, are designed to engage with the kinds of unstructured data—such as text and images—that qualitative researchers often work with. Moreover, your experience with abstract theoretical frameworks will help you develop a unique perspective on how to approach computational models. Over time, you’ll learn to represent these theories within the models, turning your theoretical strengths into a modeling advantage.\nFinally, there is the matter of learning to program in the first place. While some quantitative researchers may enjoy an advantage here, the benefits are not ubiquitous. Having to write at least some code does not mean professional quantitative researchers adhere to best programming or scientific computing practices. Most of the time, the scripts they write get the job done in ugly and inefficient ways because they have acquired what programming knowledge they have in quantitative methods courses and/or by trying to solve specific data analysis problems rather than starting from the basics. I speak from personal experience.\nWhile problem-driven learning is excellent in general (it features heavily in this book), it can develop many bad habits that—all aesthetic judgments aside—are bad for science. Even the most impressive stats nerds tend to be pretty bad programmers and could benefit from learning some basics. This is not so easily done, though, because some things that are good practice in programming and software development are not necessarily good practice in science. While you don’t have to become a professional programmer to do great computational social science, it is very important to fix bad habits.\n\n\nAdvice on Using This Book to Learn Efficiently\nWhen you are first learning computational social science or data science, it can feel like you are drinking from a firehose. If that’s the case for you, I have a few pieces of general advice that might help.\nFirst, I recommend using a simplified version of Bloom’s taxonomy of learning outcomes (just a web search away) to determine (a) where you are right now and (b) where you want to be. Don’t skip steps in the taxonomy. If there are multiple steps between where you are and where you want to be, that’s OK, but you need to take those steps one at a time. Work through the book slowly and deliberately. After each chapter, take stock of your cumulative knowledge. What have you learned, and how does it relate to what you learned previously? Can you explain what you’ve learned in your own words? Can you do the things that are introduced in each chapter on your own? In doing this, you may find that you understand something when you see it on the page but are unable to clearly explain that thing or do that thing yourself. You are experiencing the fuzzy threshold between different levels of competence: understanding (level 1 of Bloom’s taxonomy), explanation (level 2), and application (level 3). If you have no previous experience of any kind, then your learning goals should focus on understanding before all else. Just make sense of what’s going on. Once you have acquired understanding, then you can work on being able to correctly and clearly explain. The third step is competent application of your new knowledge and skills.\nThe book supports this scaffolded approach to learning by design (inspired by Weinstein, Sumeracki, and Caviglioli 2018; Brown, Roediger, and McDaniel 2014; Doyle and Zakrajsek 2018). I recommend you read each chapter twice. Read it the first time on its own. Focus on understanding the concepts and see how each part of the chapter fits together into a coherent whole. Read the code and take notes. If something is unclear to you, write a reminder about what you don’t understand but keep reading. Once you get to the end of the chapter, stop. If you have time, sleep on it. Give your brain a chance to process everything you’ve exposed it to. Let it move some of that content from short-term memory to long-term memory overnight! Then, without letting too much time pass, go back to the start of the chapter, but this time, make sure you are sitting in front of a computer. Type out the code in the chapter as you go, changing little things here and there to see how things work. Then, try to work through the problems and online learning materials associated with the chapter (described below). Whenever you encounter a gap in your knowledge, (i) explicitly make note of it and (ii) fill it by going back to the relevant part of the chapter or by searching online.\nMy second piece of advice is to consider your learning process in computational social science as a kind of enculturation process (even though computational social science has many different communities and cultures). This is very different from the learning-with-a-checklist mindset. Checklists are helpful when they break complex things down into small manageable chunks, which is why each chapter starts with a (check)list of learning objectives. That said, checklists make it easy to forget that the individual items themselves are not really the point. The way I present computational social science is heavily focused on the underlying principles and practices of the field, as well as the shared conceptual underpinnings of a wide variety of methods and models (e.g., Bayesian inference).\nLearning to do computational social science in line with these principles usually requires slowing down at first to unlearn some deeply ingrained habits and replace them with others. It’s less about “learning Python” or other specific skills than it is about enculturation into a culture of transparent, auditable, and reproducible scientific computing. To align yourself with these core principles, you must know how to write all of your data collection, cleaning, and analysis code in well-organized scripts or notebooks, managed using virtual environments and version control software, and executed from the command line. Knowing how to use each tool is just the first step; to subsume principle and practice is the end goal. Most social scientists are never taught this, and many well-intentioned efforts to share important “how-to” knowledge have the unintended consequence of disorienting and overwhelming highly capable newcomers who, through no fault of their own, can’t see the point of doing things in such a seemingly byzantine fashion.\nMy third piece of advice is to be kind to yourself. Regardless of how challenging you find the material, there is a lot to learn, and you can’t learn it all at once. The knowledge and skills that are required to develop deep expertise are scattered throughout the entire book. Most of the foundational technical stuff you need to know to get started, however, is introduced in the earliest chapters. Everything else is about gradually layering more specific computing skills on top of the general foundation. As you progress, regularly and honestly assess the state of your knowledge: you will see progress over time.\n\n\nUsing Large Language Models to Enhance Your Learning\nIn the few years since this book was published, Large language models (LLMs) like OpenAI’s ChatGPT, GitHub’s Copilot, Anthropic’s Claude, Google’s Gemini, and Meta’s Llama (which is open source) have burst onto the scene. Everything is different now. If you choose to use them, these tools can be an invaluable learning resource or a major liability. Perhaps most importantly, you should use LLMs as learning aids rather than crutches. Here are a few examples of how you might do that:\n\nClarifying Concepts: When you encounter a concept that is challenging to grasp, you can ask an LLM to provide explanations with more or less technical detail, with or without analogies, or by drawing connections to other concepts you already understand.\nClarifying Code: When you are reading someone else’s code – mine, for example – and something isn’t quite making sense, you can ask an LLM to provide a detailed line-by-line explanation. After reviewing the explanation, you can ask follow up questions to clear up any lingering confusion.\nParsing Errors: Debugging code is an important skills, but can be one of the most frustrating aspects of programming, especially when you’re starting out. LLMs can help you interpret error messages and suggest potential fixes. For example, if you run some code and get a TypeError, you can input the error message into an LLM and ask for assistance: “I’m getting a ‘TypeError: unsupported operand type(s) for +: ’int’ and ‘str’’ in my code. What does this mean, and how can I fix it?” Instead of mindlessly copying code until it seems to work, treat the LLM like a tutor and make a sincere effort to understand the problem and proposed solution.\nExploring Alternative Approaches: If you’re thinking through ways to solve a problem, you can ask an LLM to critique your ideas or outline alternative methods.\nDeveloping Tests: As you develop as a computational social scientist, you’ll reach a point where it is important to ensure your code works as intended by putting it through a battery of tests. However, writing tests can be tedious. LLMs can help you write better tests more efficiently, and can prompt you to consider scenarios that you may otherwise overlook.\nDrafting Documentation: Similarly, writing clear documentation is essential for both your future self and others who might read your code. LLMs can assist in drafting and updating documentation to explain exactly what your code is designed to do.\n\nThere are just a few examples of how LLMs can be a helpful tool in learning computational social science, but it’s crucial to maintain some healthy skepticism. Always critically evaluate the responses you receive, cross-reference with reliable sources, and test any code or suggestions provided. Remember, the goal is not to have the LLM do the work for you but to use it as a tool to enhance your own understanding and problem-solving abilities.\nBy thoughtfully integrating LLMs into your learning process, you can overcome hurdles more efficiently and deepen your understanding of computational social science. But it’s very important to use LLMs responsibly, and to disclose when and how you use them in your work. Be cautious about sharing sensitive data when using these models, especially if they are cloud-based services. Always adhere to ethical guidelines and data protection regulations relevant to your field and region.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#sec-roadmap",
    "href": "introduction.html#sec-roadmap",
    "title": "Introduction",
    "section": "Roadmap",
    "text": "Roadmap\n\n\n\n\n\n\nThis Content Will Change!\n\n\n\nThe roadmap below will change. Skim it if you want a general sense of what’s to come, but know that I won’t put much effort into changing this roadmap until the revisions are finished, or close to finished.\n\n\nThis book is carefully structured so that a complete newcomer can start at the beginning and work through it step by step. It’s also flexible enough to allow readers with some background to jump to specific chapters depending on their needs. Here’s a preview of what you can expect, highlighting some important changes from the print edition.\n\nIntroduction\nThe introduction sets the stage for your journey into computational social science. It includes:\n\n\nIntroduction\n\n\n\n\n\n\n🏠\nAbout this edition\n\n\n00\nLearning to do computational social science\n\n\n\n\nI’ve updated this chapter to reflect changes in assumed backgrounds and added a new section offering guidance on using large language models (LLMs) like ChatGPT and GitHub Copilot for learning and doing computational social science.\n\n\nPart I | Research Computing\nIn Part I, we dive right into programming with Python, laying the groundwork for all the computational methods to come. These chapters have been revised to reflect current best practices, while keeping the core content similar to previous versions.\n\n\n\n01\nGetting started with Python\n\n\n02\nPython 101\n\n\n03\nPython 102\n\n\n\nIn Chapter 1: Getting started with research computing, we set up the computational environment and go over essential tools for research computing, such as version control and basic command-line skills. Chapters 2 and 3 cover Python fundamentals, including basic data types, control flow, functions, data structures, and working with files.\n\n\nPart II | Obtaining Data\nPart II focuses on acquiring and working with different types of data. This section includes updates and new chapters, making the learning experience more streamlined.\n\n\n\n04\nSampling and survey data\n\n\n05\nWeb data (APIs)\n\n\n06\nWeb data (Scraping)\n\n\n07\nAudio, image, and document data\n\n\n\nChapter 4 is a new chapter that introduces essential knowledge and skills for working with real world survey data using Pandas. We’ll use several public opinion surveys, including the Eurobarometer and European Values surveys and a Canadian survey on attitudes about harm reduction initiatives for people who use drugs (PWUD).\nIn Chapter 5, you’ll learn how to collect data from APIs, with practical examples like The Guardian for news data and the YouTube Data API for social media data. Chapter 6 focuses on web scraping, with updates to reflect changes in web technologies and ethical considerations. Chapter 7 is a new chapter that introduces working with multimedia data, such as audio, images, and documents—an increasingly important skill in modern computational social science. This chapter is currently in development and should be publically available in winter 2025.\n\n\nPart III | Exploring Data\nPart III delves into exploratory data analysis (EDA) and introduces a central theme in this book: latent factors, components, and variables. This content has been extensively revised to focus on the process of exploratory data analysis (EDA) with different types of data and research problems (rather than focusing on how to use Python for EDA, which was an unintended consequence of the approach I took in the print edition).\n\n\n\n08\nProcessing structured data\n\n\n09\nExploratory data analysis and visualization\n\n\n10\nAssociation and latent factors\n\n\n11\nText as data\n\n\n12\nText similarity and latent semantic space\n\n\n13\nSocial networks and relational thinking\n\n\n14\nStructural similarity and latent social space\n\n\n\nChapter 8 covers processing structured data like survey or readymade datasets. Chapter 9: Exploratory data analysis and visualization focuses on purposeful EDA as part of a broader workflow. Chapter 10: Association and latent factors introduces latent factors and components, preparing you for more complex models in later parts. Chapters 11-14 introduce text analysis and network modeling, with an emphasis on latent structures in both text and social networks.\n\n\nPart IV | Prediction and Inference\nThis section has undergone significant changes, focusing more deeply on prediction in supervised machine learning, followed by probabilistic reasoning, Bayesian inference, and causal inference.\n\n\n\n15\nMachine and statistical learning\n\n\n16\nPrediction\n\n\n17\nProbability\n\n\n18\nCredibility\n\n\n19\nCausality\n\n\n\nChapter 15: Machine and statistical learning introduces supervised learning and prediction. Chapters 16-18 develop a strong foundation in probabilistic reasoning and Bayesian inference. Chapter 19: Causality introduces causal inference with observational data using directed acyclic graphs (DAGs) and Bayesian causal models.\n\n\nPart V | Generative Modeling\nPart V builds on the previous part by introduces generative models, including linear models, network models, text models, and agent-based simulations. All of these chapters have been extensively revised and explicitly build on one another.\n\n\n\n20\nLinear regression\n\n\n21\nMultilevel regression with post-stratification\n\n\n22\nGeneralized linear models\n\n\n23\nCausal analysis\n\n\n24\nLatent structure in networks\n\n\n25\nLatent topics in text (LDA/STM)\n\n\n26\nComplex adaptive systems\n\n\n27\nDeveloping agent-based models\n\n\n\nChapter 20-22 guide you through core statistical models in a Bayesian framework, emphasizing Bambi and PyMC integration. Chapter 23 explains how to use these kinds of models for causal analysis, and Chapters 24-27 take you into advanced generative modeling for networks, topics, and agent-based models.\n\n\nPart VI | Deep Learning Demystified\nPart VI introduces neural networks, natural language processing (NLP), and transformer-based models.\n\n\n\n\n\n\n\n28\nArtificial neural networks (FNN, RNN, CNN)\n\n\n29\nProcessing natural language data (SpaCy and embeddings)\n\n\n30\nTransformers and self-attention\n\n\n31\nLatent topics in text and images using transformers\n\n\n\nIn Chapter 28, we cover neural networks with traditional feed-forward, recurrent, and convolutional architectures, which sets the stage for Chapter 29 on processing natural language data and understanding word vectors / embeddings. Chapter 30 introduces the attention architecture used by transformer models and demonstrates how to use transformers from HuggingFace for senitment analysis and named entity recognition. Chapter 31 introduces transformer-based topic models for text and image data.\n\n\nPart VII | Professional Responsibilities and Ethical Practice\nThe final section of the book addresses our professional responsibilities as computational social scientists, specifically doing ethical research in reproducible, transparent, and accountable ways.\n\n\n\n32\nResearch ethics and politics\n\n\n33\nNext steps\n\n\n\nChapter 32 retains it’s original content, but has been expanded to include debates about LLMs, “alignment,” and “uncensored” models. Chapter 33 offers guidance on further developing your skills in computational social science.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#datasets-used-in-this-book",
    "href": "introduction.html#datasets-used-in-this-book",
    "title": "Introduction",
    "section": "Datasets Used in This Book",
    "text": "Datasets Used in This Book\nAs I mentioned previously, the examples in this book are based around a variety of real-world datasets that are likely more similar to what you would work with on a daily basis than the convenient toy datasets that are often used in other learning materials. These datasets generally fall into one of three categories:\n\n“Structured” datasets, for lack of a better term. If you’ve worked with real-world statistical data before, the format of these structured datasets is likely to be familiar to you: their rows represent observations (or cases), and their columns represent variables. We will make frequent use of four such datasets, each described in the subsection below.\n\nVarieties of Democracy\nEuropean Values Study (EVS)\nFreedom House “Freedom on the Net”\nUS 2020 Election Dataset\n\nRelational/network datasets. These are also “structured” data, but they differ from the structured data listed above in that they describe meaningful relationships between entities (e.g., people). We will make frequent use of four relational datasets, also described below.\n\nSocioPatterns friendship networks\nThe Copenhagen Networks Study data\nThe Enron email communication network\nA series of networks constructed by parsing information from text data\n\nText datasets. We will make use of a number of text datasets throughout the book, but the two most important by far are datasets of millions of political speeches by Canadian and British politicians.\n\nThe Canadian Hansards, 1867–2020\nThe British Hansards, 1802–2020\n\n\nBelow, I provide a general overview of these datasets and explain where to go if you want to learn more about them. You may want to come back to these descriptions as you work through the book.\n\n“Structured” Datasets\nThe Varieties of Democracy (V-Dem) dataset (Coppedge et al. 2020) is the result of a massive project with collaborators from nearly every country in the world, headquartered at the V-Dem Institute at the University of Gothenburg, Sweden. It contains a dizzying array of data points that are, in aggregate, used to measure key aspects of political regimes for countries around the world along a continuum of democratic and autocratic, grounded in five major theoretical traditions in political science, political sociology, and political theory and philosophy. The dataset includes over 4,000 variables per country-year, including a set of five high-level scales used to assess the extent of electoral, liberal, participatory, deliberative, and egalitarian democracy in a given country per year, stretching back to the 1800s. We will be using subsets of the larger V-Dem dataset extensively, especially in the first half of the book. You can learn a lot about the V-Dem project, and everything you would ever want to know about this dataset and more, from Coppedge et al. (2020), and from the codebook for Version 11 of the dataset (Coppedge et al. 2021).\nThe European Values Study (EVS) (EVS 2017), housed at the Data Archive for the Social Sciences of GESIS – Leibniz Institute in Cologne, is a set of standardized surveys of participants across Europe on topics including religion, national identity, morality, politics, family, work, society, and the environment, among other things. Each survey dataset includes over 400 variables spanning demographics and the aforementioned focal areas. They are administered in the context of one-hour face-to-face interviews with an additional questionnaire. Participation in all EVS surveys is on the basis of informed consent and is completely voluntary. Participation in the study is confidential, all data is anonymized, and direct identifiers are never added to the EVS database.\nThe Freedom on the Net dataset is created and maintained by Freedom House (2020), a U.S. nonprofit headquartered in Washington, D.C. Unlike the two massive datasets preceding this one, the Freedom on the Net dataset consists of five substantive variables for each of the 65 countries included. Three of those variables are sector scores, tracking ‘Obstacles to Access’, ‘Limits on Content’, and ‘Violations of User Rights’. The final two are an overall numerical score measuring internet freedom and a categorical label derived from the overall numerical score that labels countries as having either ‘Free’, ‘Partly Free’, or ‘Not Free’ access to the internet. We primarily use the Freedom House dataset as a companion to the V-Dem dataset to see if it’s possible to predict a country’s internet freedoms using other (non-internet-related) democratic indices.\nThe final “structured” dataset we will use in this book is a US 2020 Election Dataset, created by my PhD student Pierson Browne specifically for this book. The dataset was built from components of three different datasets:\n\n‘Individual Contributions’, from the U.S. Government’s Federal Election Commission (2020),\nThe 2017 Cook Partisan Voting Index (Wasserman and Flinn 2020), and\nRaphael Fontes’ “US Election 2020” dataset, publicly available on Kaggle (Fontes 2020).\n\nThe dataset covers Campaign Spending Differential, Vote Differential, Cook Partisan Voting Index, Republican Incumbency, and Democratic Incumbency, for each of the 435 Federal Congressional Districts electing Voting Representatives contested in the 2020 U.S. General Election. We will use this dataset extensively throughout our chapters on Bayesian Regression and Bayesian Hierarchical Linear Regression.\n\n\nRelational/Network Datasets\nThe Copenhagen Networks Study dataset was created by Piotr Sapiezynski, Arkadiusz Stopczynski, David Dreyer Lassen, and Sune Lehmann (2019). It consists of a multi-layered relational network based on digital interactions between 700 undergraduate students from the Technical University of Denmark. We will use this dataset in the chapters that discuss contagion dynamics on social networks. The data was primarily collected from questionnaires, Facebook, and participants’ smartphones. It includes measures of digital interaction, physical proximity, and online ‘friendship.’ There are too many details to fully recount here, but Sapiezynski et al. (2019) provide extensive details in their Nature (Scientific Data) article. All participants gave free and informed consent and were aware of their ability to withdraw from the study at any time and/or to have their data deleted. The authors took great pains to ensure participant privacy throughout. All of the automatically logged data was anonymized.\nThe Enron email communication network dataset was collated by my PhD student Tyler Crick specifically for this book, once again by doing extensive work cleaning and augmenting existing datasets. The base download of the data came from a version with corrections made by Arne Ruhe (Ruhe 2016). This version was later found to have inconsistencies with other available versions, such as the many available from EnronData.org under a Creative Commons Attribution 3.0 United States license. A significant number of job titles were still missing from these datasets, so thorough searches of LinkedIn, Google’s web cache, and the Internet Archive were used to either verify the identified job titles and correct missing or vague ones (“Employee,” for example, quite often was actually a trader). The data was used here only for social network analysis, so only the relational aspects (sender and receiver email address) were retained from the emails—no text content from the email bodies is reproduced here.\nThe SocioPatterns dataset (Mastrandrea, Fournet, and Barrat 2015) is the result of a collaborative research project run by the ISI Foundation in Turin, Italy; the Centre de Physique Théorique in Marseille, France; and Bitmanufactory in Cambridge, United Kingdom. There are a number of datasets contained therein, but we will only use two:\n\nA directed self-reported friendship network between high-school students in Marseille, France, in December 2013\nA directed contact network constructed from students’ contact diaries\n\nAll participants were over 18 at the time of study deployment and offered free and informed consent. The Commission Nationale de l’Informatique et des Libertés approved the study, including its privacy measures.\n\n\nText Datasets\nNearly all of the text analysis we do in this book will focus on examples from two massive text datasets: The Canadian Commons Hansard and the British Commons Hansard. Both are very similar but are unique to their national contexts. The British Commons Hansard is created by the British Government (T. U. Parliament 2021) and contains transcripts (not verbatim, but close) of recorded speeches in British Parliament, dating back to 1802. It consists of all of the speeches made by politicians in Parliamentary sessions, recorded, transcribed, and entered into public record. Similarly, the Canadian Commons Hansard (T. C. Parliament 2021) is created by the Canadian Government and consists of transcripts (not verbatim, but close) of recorded speeches in Canadian Parliament, dating back to 1867.\nThere is, of course, much more to say about these datasets than what is included here, or in the specific chapters where we use these datasets. I encourage you to consult the citations for each dataset to learn more. There are also additional details available in the online supplementary materials (described below).",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#learning-materials",
    "href": "introduction.html#learning-materials",
    "title": "Introduction",
    "section": "Learning Materials",
    "text": "Learning Materials\n\nLearning Objectives, Key Concepts, Recommended Content, and Key Points\nEach chapter in this book follows a few conventions to help you learn. First, each chapter starts with a set of itemized learning objectives and ends with a bulleted set of key points in the chapter. Of course, these are not meant to cover everything. The learning objectives highlight some of the key things you should ensure you understand before moving on to the next chapter. The key points that conclude each chapter are intended to help connect the end of each chapter with the start of the next; note that they are not detailed enough to stand in for carefully working your way through the chapter (by design).\nThroughout each chapter, you will find the first mention of all key concepts in bold text. That’s where I define many key terms, so you’ll want to be especially attentive to that content. Finally, each chapter also contains boxes that provide some additional advice and recommendations. Most of the time, I’ll point you to other readings and resources that you can use to further develop your knowledge and skills.\n\n\nOnline Supplementary Learning Materials\n\nTODO: Update this for the new and revised supplementary learning materials.\n\nThe central design decision that has guided all other decisions in the creation of this book—to show you how to do computational social science—means that many of the additional learning materials are not well-suited to the printed page. Every chapter in this book is accompanied by a wide variety of supplementary materials created by myself, Pierson Browne, Tyler Crick, Alexander Graham, Jillian Anderson, and a number of other computational social scientists and data scientists. These materials are all provided as an online supplement because (1) it makes them vastly more useful to you, and (2) it frees up an astonishingly large number of words in the book that can be put to other uses, like teaching more methods and models in greater depth than otherwise possible.\nAll of the supplementary learning materials for this book are provided in a git repository (which you will learn about shortly) available at github.com/UWNETLAB/doing_computational_social_science. Among other things, you will find:\n\nA set of carefully scaffolded problem sets accompanying each chapter in the book. These problem sets are much more extensively developed than what you would typically find at the end of each chapter in books such as this one. These are the problems that I use in my own classes at the University of Waterloo. An answer key for instructors is available upon request.\nA copy of the datasets we use in this book, though filtered and subsetted to include only the portions I actually use in each chapter. You will also find instructions to secure the full datasets if you wish.\nA set of perspectives and practical advice from other computational social scientists and data scientists. Many of these were initially part of the book manuscript itself, but the combination of life interrupted via COVID-19 and with more than 100,000 words to cut from the penultimate draft, they’ve been moved to the online materials. An unintended benefit of this change is that more perspectives can be included afterwards. Expect this part of the online materials to grow over time.\nHigh-resolution color images of every one of the figures in this book, with file names that are easily matched back to images in the book.\nInstructions on how to download and use dozens of large-scale pre-trained language models trained by Tyler Crick and myself at the University of Waterloo.\nA wealth of additional materials on scientific computing that will take you well beyond the basics introduced in Chapter 2.\nCourse syllabi.\nA DCSS virtual environment (explained below and in Chapter 2).\nAnd more…\n\nThese supplementary materials are intended to help you work interactively through every chapter of the book, to test your knowledge and practice your skills, to share important views and experiences other than my own, and to provide some additional chapter-specific material that is worthwhile but doesn’t ‘fit’ in this version of the book for one reason or another.\nChapter 2 explains how to download these materials and get your scientific computing environment set up. Once you’ve done that, you’ll be able to make extensive use of all of the accompanying materials as you work through the book.\n\n\nThe DCSS Computing Setup and Python Package\nFinally, this book also ships with its very own pre-built computing environment that ensures you will be able to use all the exact packages (with the exact same versions) that are used in this book no matter which operating system you are using, and no matter what changes occur between the time this book goes to the printers and when you pick it up to read it. It will make your life a lot more convenient, not to mention that of instructors who may assign this book in a course while looking to spend less time on technical support. Everything you need to know about this carefully crafted environment, including how to access it and use it, is provided in the next chapter. Note that this is setup is completely different than the one that was designed for the print edition.\nI’ve also created a Python package, appropriately called dcss, to accompany this book. It’s been extensively redesigned and refactored since the initial release. It’s included in the installations managed by the virtual environment, so you don’t need to do anything special to install it.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#conclusion",
    "href": "introduction.html#conclusion",
    "title": "Introduction",
    "section": "CONCLUSION",
    "text": "CONCLUSION\nNow that you have a sense of what this book is about, how it’s designed, and how you can get the most from it, it’s time to start doing computational social science! Let’s get started.\n\n\n\n\nBrown, Peter, Henry Roediger, and Mark McDaniel. 2014. Make It Stick: The Science of Successful Learning. Cambridge, MA.\n\n\nCoppedge, Michael, John Gerring, Adam Glynn, Carl Henrik Knutsen, Staffan Lindberg, Daniel Pemstein, Brigitte Seim, Svend-Erik Skaaning, and Jan Teorell. 2020. Varieties of Democracy: Measuring Two Centuries of Political Change. Cambridge University Press.\n\n\nCoppedge, Michael, John Gerring, Carl Knutsen, Staffan Lindberg, Jan Teorell, David Altman, Michael Bernhard, et al. 2021. “V-Dem Codebook V11.”\n\n\nDoyle, Terry, and Todd Zakrajsek. 2018. The New Science of Learning: How to Learn in Harmony with Your Brain. Stylus Publishing, LLC.\n\n\nEVS. 2017. “European Values Study 2017: Integrated Dataset (EVS 2017).”\n\n\nFederal Election Commission\", \"U.S. 2020. “Individual Contributions.”\n\n\nFontes, Raphael. 2020. “US Election 2020.”\n\n\nHouse, Freedom. 2020. “Freedom on the Net.”\n\n\nMastrandrea, Rossana, Julie Fournet, and Alain Barrat. 2015. “Contact Patterns in a High School: A Comparison Between Data Collected Using Wearable Sensors, Contact Diaries and Friendship Surveys.” PloS One 10 (9): e0136497.\n\n\nParliament, The Canadian. 2021. “The Canadian Commons Hansard.”\n\n\nParliament, The UK. 2021. “The UK Commons Hansard.”\n\n\nRuhe, Arne Hendrik. 2016. “Enron Data.” Http://Www.ahschulz.de/Enron-Email-Data/.\n\n\nSapiezynski, Piotr, Arkadiusz Stopczynski, David Dreyer Lassen, and Sune Lehmann. 2019. “Interaction Data from the Copenhagen Networks Study.” Scientific Data 6 (1): 1–10.\n\n\nWasserman, David, and Ally Flinn. 2020. “Introducing the 2017 Cook Political Report Partisan Voter Index.”\n\n\nWeinstein, Yana, Megan Sumeracki, and Oliver Caviglioli. 2018. Understanding How We Learn: A Visual Guide. Routledge.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "processing-data.html",
    "href": "processing-data.html",
    "title": "4  Processing Data",
    "section": "",
    "text": "4.1 Imports",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Processing Data</span>"
    ]
  },
  {
    "objectID": "processing-data.html#imports",
    "href": "processing-data.html#imports",
    "title": "4  Processing Data",
    "section": "",
    "text": "import os\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom dcss import download_dataset\nfrom dcss import set_style\nset_style()",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Processing Data</span>"
    ]
  },
  {
    "objectID": "processing-data.html#practical-pandas-first-steps",
    "href": "processing-data.html#practical-pandas-first-steps",
    "title": "4  Processing Data",
    "section": "4.2 PRACTICAL PANDAS: FIRST STEPS",
    "text": "4.2 PRACTICAL PANDAS: FIRST STEPS\n\n4.2.1 Getting Data into Pandas\nThe Pandas package makes it easy to load data from an external file directly into a dataframe object. It uses one of many reader functions that are part of a suite of I/O (input / output, read / write) tools. I’ve listed some common examples in Table 4.1. Information on these and other reader functions can be found in the Pandas documentation, which also provides useful information about the parameters for each method (e.g. how to specify what sheet you want from an Excel spreadsheet, or whether to write the index to a new CSV file).\n\n\n\nTable 4.1: I/0 Methods for Pandas\n\n\n\n\n\nData Description\nReader\nWriter\n\n\n\n\nCSV\nread_csv()\nto_csv()\n\n\nJSON\nread_json()\nto_json()\n\n\nMS Excel and OpenDocument (ODF)\nread_excel()\nto_excel()\n\n\nStata\nread_stata()\nto_stata()\n\n\nSAS\nread_sas()\nNA\n\n\nSPSS\nread_spss()\nNA\n\n\n\n\n\n\nI will focus on the read_csv() function to demonstrate the general process. The only required argument is that we provide the path to the file location, but there are many useful arguments that you can pass, such as the file encoding. By default, Pandas assumes your data is encoded with UTF-8. If you see an encoding error or some strange characters in your data, you can try a different encoding, such as latin1.\nThis chapter will use data from the Varities of Democracy (VDEM) dataset. VDEM is an ongoing research project to measure the level of democracy in governments around the world and updated versions of the dataset are released on an ongoing basis. The research is led by a team of over 50 social scientists who coordinate the collection and analysis of expert assessments from over 3,200 historians and Country Experts (CEs). From these assessments, the VDEM project has created a remarkably complex array of indicators designed to align with five high-level facets of democracy: electoral, liberal, participatory, deliberative, and egalitarian. The dataset extends back to 1789 and is considered the gold standard of quantitative data about global democratic developments. You can find the full codebook online, and I strongly recommend that you download it and consult it as you work with this data. You can find the full dataset at (https://www.v-dem.net/en/data/data/v-dem-dataset-v11/) and the codebook here (https://www.v-dem.net/figures/filer_public/e0/7f/e07f672b-b91e-4e98-b9a3-78f8cd4de696/v-dem_codebook_v8.pdf). The filtered and subsetted version we will use in this book can be downloaded using the download_dataset() function. Note that this will also download additional VDEM materials, including the codebook.\nLet’s load the CSV file into a Pandas dataframe.\nvdem_data_url = \"https://www.dropbox.com/scl/fo/6ay4x2qo4svyo92wbvlxt/ACtUxCDoLYxLujkekHdXiJ4?rlkey=lhmhiasjkv3ndvyxjxapi24sk&st=2p76a0dw&dl=0\"\n\ndownload_dataset(\n    vdem_data_url,\n    save_path='data/vdem'\n)\ndf = pd.read_csv(\n    'data/vdem/V-Dem-CY-Full+Others-v10.csv',\n    low_memory=False\n)\nOnce you have your data loaded, one of the first things you will want to know is how many rows and columns there are. You can do this using the .shape attribute of the dataframe.\ndf.shape\nThis is a fairly large dataset. It has 27,013 observations and 4,108 variables! First, I will construct a new dataframe from this one that contains only the columns I want.\n\n\n4.2.2 What Do You Need? Selecting Columns\nI will create a list of the variable names I want to retain, and call the original dataframe followed by the name of the list in square brackets. In this case, I will retain the following variables:\n\nthe country name,\nthe country ID,\nthe geographic region,\nthe year,\nthe polyarchy index,\nthe liberal democracy index,\nthe participatory democracy index,\nthe deliberative democracy index, and\nthe egalitarian democracy index,\nwhether Internet users’ privacy and their data is legally protected,\nhow polarized the country is on political issues, and\nlevels of political violence.\nwhether or not the country is a democracy\n\nI will call the new dataframe sdf, for ‘subsetted dataframe.’ Of course, you can call it anything you like. If you are going to be working with multiple dataframes in the same script or notebook, then it’s a good idea to give them much more descriptive names. For now, I am only working with two, so I will use df for the full dataframe and sdf for the dataframe with a subset of the original variables. I will make careful note of any dataframes I add.\nsubset_vars = ['country_name', 'country_text_id', 'e_regiongeo', 'year', 'v2x_polyarchy', 'v2x_libdem', 'v2x_partipdem', 'v2x_delibdem', 'v2x_egaldem', 'v2smprivex', 'v2smpolsoc', 'v2caviol', 'e_boix_regime']\nsdf = df[subset_vars]\nsdf.shape\nWe’ve created a new dataframe called sdf. It still has 27,013 rows, but only 13 variables. We can print their names using the .columns attribute for the dataframe.\nlist(sdf.columns)\n\n4.2.2.1 What’s in Your dataframe?\nWe can use the .info() method to see: the total number of observations, the total number of columns, the names of the columns, the number of non-missing observations for each, the datatype for each variable, the number of variables that contain data of each type (e.g. integers and floats), and the total amount of memory used by the dataframe.\nsdf.info()\nThe datatypes in this dataframe are float64 (numbers with decimals), int64 (integers), and object. In Pandas, object refers to columns that contain strings, or mixed types, such as strings and integers (object encompasses many more things, too: it’s a catchall category). Pandas can also work with booleans (True or False), categorical variables, and some specialized datetime objects. Recall how we selected columns to make our dataset. In the code below, I use the same idea to show only a few variables, rather than all 35, to save space. We will explain this a little more later in the chapter.\nWe can also use the .describe() method to get summary information about the quantitative variables in our dataset, including the number of non-missing information, the mean and standard deviation, and a five number summary:\nsdf[['e_regiongeo', 'year', 'v2x_polyarchy']].describe()\n\n\n\n4.2.3 Heads, Tails, and Samples\nWe can also inspect the “head” or the “tail” of our dataframe using the .head() and .tail() methods, which default to the first or last 5 rows in a dataframe unless you provide a different number as an argument, such as .head(10).\nsdf[['country_name', 'year', 'v2x_libdem']].head()\n\n\nTesting tufte-style margin notes…\nsdf[['country_name', 'year', 'v2x_libdem']].tail(3)\nIf you would prefer a random sample of rows, you can use the .sample() method, which requires you to specify the number of rows you want to sample.\nsdf[['country_name', 'year', 'v2x_libdem']].sample(15)\n\n\n4.2.4 What Do You Need? Filtering Rows\nWhen we executed the .describe() method earlier, you may have noticed that the range for the year variable is 1789-2019. Let’s say we have a good reason to focus on the years from 1900-2019. We will have to filter the data to have only the rows that meet my needs.\nThere are several ways to filter rows, including slices (e.g. all observations between index \\(i\\) and index \\(j\\)), or according to some sort of explicit condition, such as “rows where the year &gt;= 1900.” Note that when we filter or slice a dataframe, the new object is just a view of the original and still refers to the same data. Pandas will warn us if we try to modify the filtered object, so a lot of the time, things are smoother if we make a new copy.\nrowfilter = sdf['year'] &gt;= 1900\nfsdf = sdf[rowfilter].copy()\nfsdf.info()\nWe could also do this using the .query() method, which accepts a boolean expression as a string.\nalternate_fsdf = sdf.query('year &gt;= 1900').copy()\nalternate_fsdf.info()\nOur final dataframe – which I have called fsdf for filtered and subsetted dataframe – now has 13 columns (from 4,108) and 18,787 observations (from 27,013).\n\n\n4.2.5 Writing Data to Disk\nJust as I read our initial CSV file into Pandas using the read_csv() function, I can write this new dataframe to disk using the write_csv() function.\nfsdf.to_csv('data/vdem_filtered_subset.csv', index=False)",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Processing Data</span>"
    ]
  },
  {
    "objectID": "processing-data.html#understanding-pandas-data-structures",
    "href": "processing-data.html#understanding-pandas-data-structures",
    "title": "4  Processing Data",
    "section": "4.3 UNDERSTANDING PANDAS DATA STRUCTURES",
    "text": "4.3 UNDERSTANDING PANDAS DATA STRUCTURES\nNow let’s discuss Pandas’ main data structures, Series and DataFrames, and how they relate to one another.\n\n4.3.1 The Series\nEach column in a dataframe is an object called a Series. A Series is a one-dimensional object (e.g. a vector of numbers) with an index, which is itself a vector, or array, of labels.\nFor example, the column v2x_delibdem in fsdf is a Series containing floats and the index label for each observation. Printing a sample of 15 observations gives me a numerical index for each observation on the left and the actual value on the right. The index values are ordered in the Series itself, but they are out of sequence here because we pulled a random sample. As this is for demonstration purposes, I’ve included a random_state value to ensure you get the same sample that I do if you re-run this block.\nfsdf['v2x_delibdem'].sample(15, random_state = 42)\nIn most cases, the default index for a Series or dataframe is an immutable vector of integers:\nfsdf.index\nWe can easily modify an index so that it is made of up some other type of vector instead, including a string. Surprisingly, index values do not need to be unique. This enables some powerful techniques, but most of the time, you should avoid manually changing indexes.\n\n4.3.1.1 Accessing a Specific Row by its Index\nWe can use the index to retrieve specific rows from a dataframe or specific values from a Series, much as we would if we were selecting an element from a list, tuple, or array. The easiest way to do this is to pass the index value (e.g. 202) to .loc[]. As you can see below, the result is the observation-specific value for each variable in the dataframe.\nfsdf.loc[202]\nfsdf['v2x_delibdem'].loc[202]\nfsdf['v2x_delibdem'].loc[20000]\nNote that .loc does not refer to the 202nd row of the dataframe. If you were looking closely at the .index command above, you might have noticed the dataframe only contains 18,787 rows but .loc can still return row 20,000 - the index didn’t change when you removed a bunch of rows from the dataframe. Think of .loc as accessing a dictionary of the index values - it will even give a KeyError if you ask for an element that doesn’t exist.\nInstead, if we want the access the n-th row of a dataframe, we can use .iloc[n]. Think of the index as a list and you’re referring to an element of that list by its list index. Let’s use .iloc to select the last element in the dataframe. Note that the index position for the last element will be 18,786 even though the dataframe length is 18,787, because Python data structures are almost always 0-indexed. Here you see the index of the row, which was formerly the row number, as the Name at the bottom.\nfsdf.iloc[18786]\nIf there isn’t a reason to retain the original indexing of the unfiltered dataframe, it’s usually a good idea to reset the index.\nfsdf.reset_index(inplace = True, drop = True)\nfsdf.loc[18786]\nAfterwards, .loc and .iloc become fairly interchangeable, with a few exceptions: .loc has dictionary-like capabilities whereas .iloc is more list-like. Now, let’s take a closer look at the dataframe.\n\n\n\n4.3.2 Dataframes\nDataframes in Pandas are really just collections of Series that are aligned on the same index values. In other words, the Series we worked with previously have their own indices when we work with them as standalone Series, but in the fsdf dataframe, they share an index.\nAs you’ve already seen, dataframes are organized with variables in the columns and observations in the rows, and you can grab a single Series from a dataframe using square brakets – let’s do that now, using the fsdf dataframe:\ndeliberative = fsdf['v2x_delibdem']\nNote that we can also use dot notation to select columns. fsdf.v2x_delibdem is functionally equivalent to fsdf['v2x_delibdem'], and may be used interchangeably.\nWe are not limited to selecting columns that already exist in our dataset. You can also create and add new ones. For example, you can create a new column called “21 Century” and assign Boolean value based on whether the observation is in the 2000s.\nfsdf['21 Century'] = fsdf['year'] &gt;= 2000\nfsdf[['21 Century']].value_counts().reset_index()\nSometimes, the new columns created are transformations of a Series that already exists in the dataframe. For example, you can create a new missing_political_violence_data column which will be True when the v2caviol Series (levels of political violence) is empty and False otherwise.\nfsdf['missing_political_violence_data'] = fsdf['v2caviol'].isna()\nfsdf['missing_political_violence_data'].value_counts().reset_index()\nAs you can see from executing value_counts(), there is missing data on levels of political violence for 6042 observations.\n\n\n4.3.3 Missing Data\nIt’s important to understand how missing data is handled. Missing data is common in real-world datasets, and it can be missing for multiple reasons! Generally, Pandas uses the np.nan value to represent missing data. NumPy’s np.nan value is a special case of a floating point number representing an unrepresentable value. These kinds of values are called NaNs (Not a Number).\nimport numpy as np\n\ntype(np.nan)\nnp.nan cannot be used in equality tests, since any comparison to a np.nan value will evaluate as False. This includes comparing np.nan to itself.\nn = np.nan\nn == n\nnp.nan values do not evaluate to False or None. This can make it difficult to distinguish missing values. You can use the np.isnan() function for this purpose, and it is especially useful in control flow.\nif np.nan is None:\n    print('NaN is None')\nif np.nan:\n    print('NaN evaluates to True in control flow')\nif np.isnan(np.nan):\n    print('NaN is considered a NaN value in NumPy')\nAdditionally, np.nan values are generally excluded from Pandas functions that perform calculations over dataframes, rows, or columns. For example, documentation often stipulates that a calculation is done over all values, excluding NaN or NULL values.\ntotal = len(fsdf['v2caviol'])\ncount = fsdf['v2caviol'].count()\nprint(f'Total: {total}')\nprint(f'Count: {count}')\nprint(f'Diff: {total-count}')\nThe total number of items in the v2caviol column (political violence) is much higher than the counts received from the count() function. If what we learned above is correct, this difference should be accounted for when we discover how many items in this column are NaNs.\nnans = fsdf['v2caviol'].isna().sum()\nprint(' NaNs: {}'.format(nans))\nAs you can probably tell, the .isna() method, which is similar to np.isnan() but covers additional cases, can be very useful in transforming and filtering data.",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Processing Data</span>"
    ]
  },
  {
    "objectID": "processing-data.html#aggregation-grouped-operations",
    "href": "processing-data.html#aggregation-grouped-operations",
    "title": "4  Processing Data",
    "section": "4.4 AGGREGATION & GROUPED OPERATIONS",
    "text": "4.4 AGGREGATION & GROUPED OPERATIONS\nData analysis projects often involve aggregation or grouped operations. For example, we might want to compute and compare summary statistics for observations that take different values on a categorical variable. It can be helpful to be able to carve up the dataset itself, performing operations on different subsets of data. We’re going to do that using the .groupby() method, which partitions the dataframe into groups based on the values of a given variable. We can then perform operations on the resulting groups. Let’s group our countries into geographic regions using the e_regiongeo variable.\ngrouped = fsdf.groupby('e_regiongeo')\nThe above code returns a grouped object that we can work with. Let’s say we want to pull out a specific group, like South East Asia, which is represented in the data using the numerical ID 13. I know this because the relevant information is provided in the VDEM codebook, which I suggest you keep open whenever you are working with the VDEM data.\nWe can use the get_group() method to pull a group from the grouped object. (Note that the .get_group() code below is equivalent to fsdf[fsdf['e_regiongeo'] == 13].)\nsouth_east_asia = grouped.get_group(13)\nsouth_east_asia[['country_name', 'year', 'e_boix_regime']].head()\nThe data stored in south_east_asia are all of the observations of South East Asian countries in the VDEM data, stored now in their own dataframe. .get_group() is yet another way to extract a subset of a dataframe (by way of a groupby object), and is especially useful when the subset of data you want to work with is only observations with a particular value for a categorical variable in your data.\nGenerally speaking, when we group a dataset like this it’s because we want to compute something for a group within the dataset, or for multiple groups that we want to compare. We can do this by specifying the grouped object, the Series we want to perform an operation on, and finally the operation we want to perform. For example, let’s compute the median polyarchy score for countries in each of the regions in the dataset.\npoly = grouped['v2x_polyarchy'].median()\npoly.head()\nIt would be more useful to see the name of the region rather than its numeric label. We can do this by creating a dictionary that maps the numeric IDs to the region name, and then use the .map() method to tell Pandas were to lookup the values it needs to create a new column with the country names. First, the dictionary:\nregions = {\n    1:'Western Europe',\n    2:'Northern Europe',\n    3:'Southern Europe',\n    4:'Eastern Europe',\n    5:'Northern Africa',\n    6:'Western Africa',\n    7:'Middle Africa',\n    8:'Eastern Africa',\n    9:'Southern Africa',\n    10:'Western Asia',\n    11:'Central Asia',\n    12:'East Asia',\n    13:'South-East Asia',\n    14:'South Asia',\n    15:'Oceania', # (including Australia and the Pacific)\n    16:'North America',\n    17:'Central America',\n    18:'South America',\n    19:'Caribbean' # (including Belize Cuba Haiti Dominican Republic)\n}\nAnd now we can pass this dictionary into the .map() method applied to the fsdf['e_regiongeo'] Series, creating a new Series called fsdf['Region']\nfsdf['Region'] = fsdf['e_regiongeo'].map(regions)\nIt is also possible to group by multiple variables, such as geographic region and year, and then perform an operation on those slightly more fine-grained groups. This will result in 2,211 groups, so we will preview a random sample of 10.\ngrouped = fsdf.groupby(['Region', 'year'])\npoly = grouped['v2x_polyarchy'].median()\npoly.reset_index()\npd.DataFrame(poly).reset_index().sample(10)\nWe can perform other types of operations on the grouped object itself, such as computing the number of observations in each group (equivalent to value_counts()).\ngrouped.size().sort_values(ascending=False)\nFinally, we can perform multiple operations on a grouped object by using the agg() method. The agg() method will apply one or more aggregate functions to a grouped object, returning the results of each.\n#| warning: false\nwith_agg = grouped['v2x_polyarchy'].agg([min, np.median, 'max', 'count'])\nwith_agg.reset_index().sample(10)\nWe can even define our own function for agg() to use! If we’re willing to pass a dictionary, .agg() also lets us apply different functions to multiple variables at the same time! Instead of passing one list per function, you can use a dictionary where the column names are the keys and the functions are the values (you can also pass a list of functions) to perform some truly involved aggregration all in one line of code.",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Processing Data</span>"
    ]
  },
  {
    "objectID": "processing-data.html#working-with-time-series-data",
    "href": "processing-data.html#working-with-time-series-data",
    "title": "4  Processing Data",
    "section": "4.5 WORKING WITH TIME SERIES DATA",
    "text": "4.5 WORKING WITH TIME SERIES DATA\nMany real world datasets include a temporal component. This is especially true if you are working with data that comes from the web, which may have precise timestamps for things like the time an email was sent, or a news story was published. Strings are often used to store dates and times, but this is not ideal because strings don’t take advantage of the unique properties of time. It is difficult to sort dates if they are stored in strings with strange formats, for example.\n\"Monday Mar 2, 1999\" &gt; \"Friday Feb 21, 2020\"\nExtracting features like day, month, or timezone from strings can be time-consuming an error-prone. This is why Pandas and Python have implemented special types for date/time objects, called [Timestamp](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html) and [Datetime](https://docs.python.org/2/library/datetime.html), respectively. These are essentially equivalent to one another.\nThe VDEM data contains an enormous amount of temporal data, but all at the level of the year. Let’s switch over to a different dataset that has more fine-grained temporal data, and more closely resembles data that you would obtain from the web. In this case, we are going to use some data on Russian information operations targeting the 2016 American Presidential Election. You can read a bit about this data on the FiveThirtyEight blogpost Why We’re Sharing 3 Million Russian Troll Tweets.\nUnlike the VDEM data, the Russian Troll Tweets come as a collection of csv files. We will use a clever little trick to load up all the data in a single dataframe. The code block below iterates over each file in the russian-troll-tweets/ subdirectory in the data directory. If the file extension is csv, is reads the csv into memory as a dataframe. All of the dataframes are then concatenated into a single dataframe containing data on ~ 3M tweets.\nrussian_troll_data_url = \"https://www.dropbox.com/scl/fo/a3uxioa2wd7k8x8nas0iy/AH5qjXAZvtFpZeIID0sZ1xA?rlkey=p1471igxmzgyu3lg2x93b3r1y&st=xvhtn8gi&dl=0\"\n\ndownload_dataset(\n    russian_troll_data_url,\n    save_path='data/russian_troll_tweets'\n)\ndata_dir = os.listdir(\"data/russian_troll_tweets/\")\n\nfiles = [f for f in data_dir if 'csv' in f]\n\ntweets_df = pd.concat((pd.read_csv(\n    f'{\"data/russian_troll_tweets/\"}/{f}',\n    encoding='utf-8', low_memory=False) for f in files), ignore_index=True)\n\ntweets_df.info()\nAs you can see, we have two datatypes in our dataframe: object and int64. Remember that Pandas uses object to refer to columns that contain strings, or which contain mixed types, such as strings and integers. In this case, they refer to strings.\nOne further thing to note about this dataset: each row is a tweet from a specific account, but some of the variables describe attributes of the tweeting accounts, not of the tweet itself. For example, followers describes the number of followers that the account had at the time it sent the tweet. This makes sense, because tweets don’t have followers, but accounts do. We need to keep this in mind when working with this dataset.\nWe can convert date strings from a column or Series into Timestamps using the to_datetime function. We will do that here, assigning the new datetime objects to new variables. Note that this code will take a bit of time to run when executed on all 3 million tweets (if your computer isn’t the strongest, you might want to consider first using tweets_df.sample() to reduce the size of the dataframe).\ntweets_df['dt_publish_date'] = pd.to_datetime(tweets_df['publish_date'])\ntweets_df['dt_harvested_date'] = pd.to_datetime(tweets_df['harvested_date'])\ntweets_df[['author', 'content', 'publish_date']].sample(5)\nIn order, the datetime object fields are as follows: year-month-day hour:minute:second:microsecond. To retrieve an integer corresponding to the month when the tweet was published:\ntweets_df['dt_publish_date'].dt.month\nWhen our date and time variables are stored as datetime objects, we can access many time-specific attributes using dot notation. The Pandas documentation includes many examples of the kinds of temporal units and other functionality.\nWe can also sort our dataframe based on publish_date because Pandas knows that it is working with datetime objects.\nsorted_df = tweets_df.sort_values(['dt_publish_date'])\nWe can also add and subtract datetime columns to create new columns.\ntweets_df['days_until_harvest'] = tweets_df['dt_harvested_date'] - tweets_df['dt_publish_date']\ntweets_df['days_until_harvest'].sample(10)\nLet’s create new variables for the Year, Month, and Day each tweet was created on. We can do this by using the year, month, and day attributes on the datetime object.\ntweets_df['Year'] = tweets_df['dt_publish_date'].dt.year\ntweets_df['Month'] = tweets_df['dt_publish_date'].dt.month\ntweets_df['Day'] = tweets_df['dt_publish_date'].dt.day\nPandas offers specialized tools for grouping data into various segments of time. This involves converting a time series at one level into another (e.g. from days to weeks), and is known as resampling. Within resampling broadly, upsampling aggregates dates / times and downsampling disaggregates dates / times. Let’s upsample our data to plot the number of Tweets per day.\nThe first thing we will do is use the datetime object dt_publish_date as an index. This will let us easily group observations by resampling dates.\ntweets_df = tweets_df.set_index('dt_publish_date')\nWe can now use the .resample() method with the argument D to specify that we want to group by day. The table below provides some other options you can use when resampling dates.\n\nUnits of time in Pandas. You can use any of these units to upsample or downsample temporal data.\n\n\nValue\nDescription\n\n\n\n\nB\nbusiness day frequency\n\n\nC\ncustom business day frequency (experimental)\n\n\nD\ncalendar day frequency\n\n\nW\nweekly frequency\n\n\nM\nmonth end frequency\n\n\nBM\nbusiness month end frequency\n\n\nCBM\ncustom business month end frequency\n\n\nMS\nmonth start frequency\n\n\nBMS\nbusiness month start frequency\n\n\nCBMS\ncustom business month start frequency\n\n\nQ\nquarter end frequency\n\n\nBQ\nbusiness quarter endfrequency\n\n\nQS\nquarter start frequency\n\n\nBQS\nbusiness quarter start frequency\n\n\nA\nyear end frequency\n\n\nBA\nbusiness year end frequency\n\n\nAS\nyear start frequency\n\n\nBAS\nbusiness year start frequency\n\n\nBH\nbusiness hour frequency\n\n\nH\nhourly frequency\n\n\nT\nminutely frequency\n\n\nS\nsecondly frequency\n\n\nL\nmilliseonds\n\n\nU\nmicroseconds\n\n\nN\nnanosecondsa\n\n\n\nWe will also use the .size() method to determine the number of tweets that were produced each day.\ngrouped_cal_day = tweets_df.resample('D').size()\ngrouped_cal_day\nAt this point, we are going to visualize the results of our work with a line plot. We are going to do this with the Seaborn and matplotlib packages, which we will discuss in the next chapter. For now, focus on the visualization and ignore the code. The code blocks below produces Figures Figure 8.1 and Figure 8.2.\n#| output: false\nsns.lineplot(data=grouped_cal_day, color='#32363A')\nsns.despine()\nplt.savefig('figures/06_01.png', dpi=300)\n\n\n\n\n\n\nFigure 4.1: caption…\n\n\n\nDays may not be the best unit of time to work with in this case. We can, of course, upsample from days to weeks instead, and produce the same plot.\nweekly = tweets_df.resample('W').size()\nweekly.head()\n#| output: false\nax = sns.lineplot(data=weekly, color='#32363A')\nax.set_xlabel('\\nWeekly observations')\nax.set_ylabel('Number of Tweets\\n')\nsns.despine()\nplt.savefig('figures/06_02.png', dpi=300)\n\n\n\n\n\n\nFigure 4.2: caption…\n\n\n\nThe plot is much cleaner when we count at the level of weeks rather than days.",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Processing Data</span>"
    ]
  },
  {
    "objectID": "processing-data.html#combining-dataframes",
    "href": "processing-data.html#combining-dataframes",
    "title": "4  Processing Data",
    "section": "4.6 COMBINING DATAFRAMES",
    "text": "4.6 COMBINING DATAFRAMES\nCombining dataframes is a very common task. In fact, though it might not seem obvious, combining datasets is one of the most valuable skills you can have when doing computational social science. Here, we will consider some of the most common approaches: concatenating and merging, and we will briefly describe a more advanced set of methods commonly referred to as record linkage.\nConcatenating a dataframe is conceptually pretty simple - think of it like attaching the rows or columns of one dataframe below/to the right of the last row/column of another dataframe. For this to be useful, the two dataframes should have at least one row or column in common, but usually you would only concatenate if there were many such overlapping entries.\nfull_df =  pd.read_csv(\"data/vdem/filtered_subset.csv\")\ndf_australia = full_df.query(\"country_name == 'Australia'\")\nlen(df_australia)\ndf_sa = full_df.query(\"country_name == 'South Africa'\")\nlen(df_sa)\nThe default behaviour for pd.concat() is to perform a row-wise join, which it refers to as axis=0. We can override this default by specifying axis=1, which will produce a column-wise join:\nconcatenated = pd.concat([df_australia, df_sa], axis=1)\nlen(concatenated)\nWhen we concatenate the two dataframes the number of columns stays the same but the number of rows increases, accounting for the rows in both the original dataframes. Normally, this kind of concatenation would result in a different number of columns, but in this case, the two dataframes we joined had the exact same columns (which makes sense, given that they were both extracted from the same parent dataframe).\n\n4.6.1 Merging\nAn alternative way to combine datasets is to merge them. If you want to create a dataframe that contains columns from multiple datasets but is aligned on rows according to some column (or set of columns), you probably want to use the merge() function. To illustrate this, we will work with data from two different sources. The first is the VDEM data we used in first part of this chapter (fsdf). The second is a dataset from Freedom House on levels of internet freedom in 65 countries. More information is available at https://freedomhouse.org/countries/freedom-net/scores.\ndownload_dataset(\n    'https://www.dropbox.com/scl/fo/fnw5yrslxza9plhqnqhxr/AA7997oGIdd3k3EjluHyLBc?rlkey=hr93qtcdp6uh7d3lsfzbc6nr6&st=bz0xzw41&dl=0',\n    'data/freedom_house/'\n)\n\nfreedom_df = pd.read_csv( \"data/freedom_house/internet_freedoms_2020.csv\")\nTo merge these dataframes we need to find a column which can be used to match rows from one dataframe to the rows in the other. The columns don’t need to have the same name, just values that can be matched with one another. Whatever columns we choose will be called “keys” in our merge. In our case this will be the country name columns from each dataset.\nfsdf.columns\nfreedom_df.columns\nWe will use the merge function to combine these two dataframes using ‘country_name’ and ‘Country’. We’re going to do an inner merge, which is the default if the option isn’t set, and will keep only the keys (ie. countries) that appear in both dataframes.\nmerged = pd.merge(fsdf, freedom_df, how='inner', left_on='country_name', right_on='Country')\nprint('merged has {} rows and {} columns'.format(len(merged), len(merged.columns)))\nlen(fsdf) + len(freedom_df)\nYou should see 5 new columns in the merged dataframe compared to the fsdf one. Notice how many rows each of the dataframes have: many fewer rows than the original VDEM dataframe but many more than the Freedom House dataframe. So in our case, if a row’s country doesn’t appear in the other dataset, that row will not be included in the merged dataframe.\nThis can be adjusted using the how parameter. There are five ways of merging dataframes in Pandas: left, right, outer, inner, and cross. Check out the documentation to see how the other four methods work.\nThere are ways to improve the matching, either manual methods or semi-automated methods such as record linkage, described below. Let’s see which countries aren’t common between the dataframes, using a set operation ^ (XOR), which returns a set of elements from the combination of set1 and set2 that are either not in set1 or not in set2.\nfsdf_set = set(fsdf['country_name'].tolist())\nfreedom_set = set(freedom_df['Country'].tolist())\n\nunmatched = fsdf_set ^ freedom_set\n\nprint('Total countries: ' + str(len(fsdf_set) + len(freedom_set)))\nprint('Unmatched countries: ' + str(len(unmatched)))\nWe can then use the & set operator to see which of the missing countries are present in each of the country sets. If the data is small enough, we can print the two sets as sorted lists in a dataframe. The most obvious manual change we could do here is make “United States” and “United States of America” consistent but we would also expect Myanmar to be in the VDEM data. We could also make this change manually by knowing that Myanmar was referred to as Burma until 1989. However, it just so happens that at the top of the south_east_asia aggregated group dataframe from earlier, “Burma/Myanmar” was the name used, rather than Burma alone. For a more complex but automated solution to disambiguating different versions of country names, we would have to use some form of record linkage, discussed briefly below.\nfsdf_missing = list(fsdf_set & unmatched)\nfsdf_missing.sort()\nfreedom_missing = list(freedom_set & unmatched)\nfreedom_missing.sort()\npd.DataFrame({'VDEM': pd.Series(fsdf_missing), 'Freedom': pd.Series(freedom_missing)})\n\n\n4.6.2 Record Linkage\nThe merge function works great when you can make exact matches between columns. It also works really well because checking for exact matches has been optimized in Pandas. However, it’s often the case that we need to combine datasets which cannot be merged based on exact matches.\nInstead, we often have to use inexact matching (aka “fuzzy matching” or “approximate matching”) to combine datasets. Typically, this involves using some similarity metric to measure how close two keys are to one another. Then a match is made based on thresholds, rules, or a nearest-neighbour approach. However, naively calculating similarity between all possible key combinations results in incredibly lengthy compute times. Instead, there are ways to exclude some key pairs from the beginning. This allows you to drastically reduce the number of comparisons you need to make. Additionally, inexact matching can leverage machine learning techniques which uses human curated examples to learn to predict whether two rows should be matched with one another.\nIf this “more advanced” approach to combining datasets is of interest, I highly suggest looking into the recordlinkage Python package.\n\nFurther Reading\nMuch of what I introduce in this chapter is foundational; you’ll build on that foundation in later chapters. But if you are looking for a slower and more comprehensive introduction to Pandas and Numpy, then I would recommend VanderPlas’ (2016) Python Data Science Handbook.",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Processing Data</span>"
    ]
  },
  {
    "objectID": "processing-data.html#conclusion",
    "href": "processing-data.html#conclusion",
    "title": "4  Processing Data",
    "section": "4.7 CONCLUSION",
    "text": "4.7 CONCLUSION",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Processing Data</span>"
    ]
  },
  {
    "objectID": "processing-data.html#key-points",
    "href": "processing-data.html#key-points",
    "title": "4  Processing Data",
    "section": "4.8 Key Points",
    "text": "4.8 Key Points\n\nIn this chapter, we expanded into the world of processing structured data using Pandas; these are critical skills for computational social scientists\nWe covered the basic Pandas data structures, Series and dataframes, and the index and datetime objects\nWe discussed how to subset dataframes by selecting columns and filtering rows, followed by a discussion of how to do systematic comparisons by performing operations on grouped dataframes\nWe then discussed how to combine multiple dataframes using merge and concatenate and introduced the general idea of record linkage.\n\n\n\n\n\n\nVanderPlas, Jake. 2016. Python Data Science Handbook: Essential Tools for Working with Data. \" O’Reilly Media, Inc.\".",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Processing Data</span>"
    ]
  },
  {
    "objectID": "processing-data.html#footnotes",
    "href": "processing-data.html#footnotes",
    "title": "4  Processing Data",
    "section": "",
    "text": "Here too!↩︎",
    "crumbs": [
      "**COMPUTING**",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Processing Data</span>"
    ]
  }
]