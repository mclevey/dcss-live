<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>32&nbsp; Processing Natural Language Data ‚Äì Doing Computational Social Science&lt;br&gt;[The **Continuous Development** Edition]{.small}</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./transformer-revolution.html" rel="next">
<link href="./artificial-neural-networks-fnn-rnn-cnn.html" rel="prev">
<link href="./figures/logo.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="custom.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./artificial-neural-networks-fnn-rnn-cnn.html"><strong>DEEP LEARNING</strong></a></li><li class="breadcrumb-item"><a href="./language-models-and-embeddings.html"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Processing Natural Language Data</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./figures/logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Doing Computational Social Science<br><span class="small">The <strong>Continuous Development</strong> Edition</span></a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/UWNETLAB/dcss_supplementary/tree/master/book/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
              <i class="bi bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
    </div>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">üè†</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text"><strong>RESEARCH COMPUTING</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./getting-started.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Getting Started</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./python-101.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Python 101</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./python-102.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Python 102</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text"><strong>MINDFUL MODELING</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./metaphor-map-reduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Modeling as metaphor, map, and reduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./iterative-workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Iterative workflows</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text"><strong>OBTAINING DATA</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sampling-and-survey-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title"><del>Processing Structured Data</del> Sampling and Survey Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./web-data-apis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Web data (APIs)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./web-data-scraping.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Web data (Scraping)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./audio-files-and-documents.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Audio files and documents</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text"><strong>EXPLORING</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exploratory-data-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Exploratory data analysis (EDA) <!-- Exploring with purpose --></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./association-and-latent-variables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Association and latent variables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./text-as-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Text as Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mapping-text.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Text similarity and latent semantic space</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./networks-as-not-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Networks and relational thinking</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./centrality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Centrality</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mapping-network-structure.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Mapping network structure</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text"><strong>PREDICTION &amp; INFERENCE</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supervised-learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Supervised Machine Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./prediction-and-classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Prediction and classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./causality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Causality</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Probability 101</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./credibility.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Credibility</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./measurement-and-missingness.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Measurement and missingness</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text"><strong>MODELING</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linear-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Bayesian Regression Models with Probabilistic Programming</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./multilevel-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Multilevel regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./generalized-linear-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Generalized Linear Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./structural-causal-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Structural causal models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./modeling-texts-lda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Modeling text with LDA topic models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./modeling-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Latent structure in networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./agent-based-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Agent-based models (ABMs)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./diffusion-opinion-cultural-cognition.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Diffusion, opinion dynamics, and cultural cognition</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text"><strong>DEEP LEARNING</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./artificial-neural-networks-fnn-rnn-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Artificial neural networks 101</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./language-models-and-embeddings.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Processing Natural Language Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./transformer-revolution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">The transformer revolution</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./modeling-text-transformer-topic-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Modeling text: transformer topic models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text"><strong>PROFESSIONAL RESPONSIBILITIES</strong></span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ethical-css.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Research Ethics, Politics, and Practices</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./open-css.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Open computational social science</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./future-css.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Future computational social science</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./acknowledgements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgements</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./courses.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Courses and Workshops</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives"><span class="header-section-number">32.1</span> LEARNING OBJECTIVES</a></li>
  <li><a href="#learning-materials" id="toc-learning-materials" class="nav-link" data-scroll-target="#learning-materials"><span class="header-section-number">32.2</span> LEARNING MATERIALS</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction"><span class="header-section-number">32.3</span> INTRODUCTION</a>
  <ul class="collapse">
  <li><a href="#package-imports" id="toc-package-imports" class="nav-link" data-scroll-target="#package-imports"><span class="header-section-number">32.3.1</span> Package Imports</a></li>
  </ul></li>
  <li><a href="#text-processing" id="toc-text-processing" class="nav-link" data-scroll-target="#text-processing"><span class="header-section-number">32.4</span> TEXT PROCESSING</a>
  <ul class="collapse">
  <li><a href="#getting-to-know-spacy" id="toc-getting-to-know-spacy" class="nav-link" data-scroll-target="#getting-to-know-spacy"><span class="header-section-number">32.4.1</span> Getting to Know SpaCy</a></li>
  </ul></li>
  <li><a href="#normalizing-text-via-lemmatization" id="toc-normalizing-text-via-lemmatization" class="nav-link" data-scroll-target="#normalizing-text-via-lemmatization"><span class="header-section-number">32.5</span> NORMALIZING TEXT VIA LEMMATIZATION</a></li>
  <li><a href="#part-of-speech-tagging" id="toc-part-of-speech-tagging" class="nav-link" data-scroll-target="#part-of-speech-tagging"><span class="header-section-number">32.6</span> PART-OF-SPEECH TAGGING</a></li>
  <li><a href="#syntactic-dependency-parsing" id="toc-syntactic-dependency-parsing" class="nav-link" data-scroll-target="#syntactic-dependency-parsing"><span class="header-section-number">32.7</span> SYNTACTIC DEPENDENCY PARSING</a>
  <ul class="collapse">
  <li><a href="#noun-chunks" id="toc-noun-chunks" class="nav-link" data-scroll-target="#noun-chunks"><span class="header-section-number">32.7.1</span> Noun Chunks</a></li>
  <li><a href="#extracting-words-by-dependency-labels-subject-verb-object-triplets" id="toc-extracting-words-by-dependency-labels-subject-verb-object-triplets" class="nav-link" data-scroll-target="#extracting-words-by-dependency-labels-subject-verb-object-triplets"><span class="header-section-number">32.7.2</span> Extracting Words by Dependency Labels: Subject, Verb, Object Triplets</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">32.8</span> CONCLUSION</a>
  <ul class="collapse">
  <li><a href="#key-points" id="toc-key-points" class="nav-link" data-scroll-target="#key-points"><span class="header-section-number">32.8.1</span> Key Points</a></li>
  </ul></li>
  <li><a href="#word-vectors" id="toc-word-vectors" class="nav-link" data-scroll-target="#word-vectors"><span class="header-section-number">33</span> Word vectors</a>
  <ul class="collapse">
  <li><a href="#learning-objectives-1" id="toc-learning-objectives-1" class="nav-link" data-scroll-target="#learning-objectives-1"><span class="header-section-number">33.1</span> LEARNING OBJECTIVES</a></li>
  <li><a href="#learning-materials-1" id="toc-learning-materials-1" class="nav-link" data-scroll-target="#learning-materials-1"><span class="header-section-number">33.2</span> LEARNING MATERIALS</a></li>
  <li><a href="#introduction-1" id="toc-introduction-1" class="nav-link" data-scroll-target="#introduction-1"><span class="header-section-number">33.3</span> INTRODUCTION</a></li>
  <li><a href="#can-we-model-meaning" id="toc-can-we-model-meaning" class="nav-link" data-scroll-target="#can-we-model-meaning"><span class="header-section-number">33.4</span> CAN WE MODEL MEANING?</a>
  <ul class="collapse">
  <li><a href="#the-distributional-hypothesis" id="toc-the-distributional-hypothesis" class="nav-link" data-scroll-target="#the-distributional-hypothesis"><span class="header-section-number">33.4.1</span> The Distributional Hypothesis</a></li>
  </ul></li>
  <li><a href="#what-are-neural-word-embeddings" id="toc-what-are-neural-word-embeddings" class="nav-link" data-scroll-target="#what-are-neural-word-embeddings"><span class="header-section-number">33.5</span> WHAT ARE NEURAL WORD EMBEDDINGS?</a>
  <ul class="collapse">
  <li><a href="#learning-embeddings-with-word2vec" id="toc-learning-embeddings-with-word2vec" class="nav-link" data-scroll-target="#learning-embeddings-with-word2vec"><span class="header-section-number">33.5.1</span> Learning Embeddings with Word2Vec</a></li>
  </ul></li>
  <li><a href="#cultural-cartography-getting-a-feel-for-vector-space" id="toc-cultural-cartography-getting-a-feel-for-vector-space" class="nav-link" data-scroll-target="#cultural-cartography-getting-a-feel-for-vector-space"><span class="header-section-number">33.6</span> CULTURAL CARTOGRAPHY: GETTING A FEEL FOR VECTOR SPACE</a>
  <ul class="collapse">
  <li><a href="#king---man-woman-neq-queen" id="toc-king---man-woman-neq-queen" class="nav-link" data-scroll-target="#king---man-woman-neq-queen"><span class="header-section-number">33.6.1</span> King - Man + Woman <span class="math inline">\(\neq\)</span> Queen</a></li>
  </ul></li>
  <li><a href="#learning-embeddings-with-gensim" id="toc-learning-embeddings-with-gensim" class="nav-link" data-scroll-target="#learning-embeddings-with-gensim"><span class="header-section-number">33.7</span> LEARNING EMBEDDINGS WITH GENSIM</a>
  <ul class="collapse">
  <li><a href="#data" id="toc-data" class="nav-link" data-scroll-target="#data"><span class="header-section-number">33.7.1</span> Data</a></li>
  </ul></li>
  <li><a href="#comparing-embeddings" id="toc-comparing-embeddings" class="nav-link" data-scroll-target="#comparing-embeddings"><span class="header-section-number">33.8</span> COMPARING EMBEDDINGS</a>
  <ul class="collapse">
  <li><a href="#imports" id="toc-imports" class="nav-link" data-scroll-target="#imports"><span class="header-section-number">33.8.1</span> Imports</a></li>
  <li><a href="#aligning-your-vector-spaces" id="toc-aligning-your-vector-spaces" class="nav-link" data-scroll-target="#aligning-your-vector-spaces"><span class="header-section-number">33.8.2</span> Aligning Your Vector Spaces!</a></li>
  <li><a href="#step-1-train-the-compass" id="toc-step-1-train-the-compass" class="nav-link" data-scroll-target="#step-1-train-the-compass"><span class="header-section-number">33.8.3</span> Step 1: Train the Compass</a></li>
  <li><a href="#step-2-train-a-series-of-aligned-embedding-models" id="toc-step-2-train-a-series-of-aligned-embedding-models" class="nav-link" data-scroll-target="#step-2-train-a-series-of-aligned-embedding-models"><span class="header-section-number">33.8.4</span> Step 2: Train a Series of Aligned Embedding Models</a></li>
  </ul></li>
  <li><a href="#conclusion-1" id="toc-conclusion-1" class="nav-link" data-scroll-target="#conclusion-1"><span class="header-section-number">33.9</span> CONCLUSION</a>
  <ul class="collapse">
  <li><a href="#key-points-1" id="toc-key-points-1" class="nav-link" data-scroll-target="#key-points-1"><span class="header-section-number">33.9.1</span> Key Points</a></li>
  </ul></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/UWNETLAB/dcss_supplementary/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./artificial-neural-networks-fnn-rnn-cnn.html"><strong>DEEP LEARNING</strong></a></li><li class="breadcrumb-item"><a href="./language-models-and-embeddings.html"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Processing Natural Language Data</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Processing Natural Language Data</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<ul>
<li>Note the word embeddings chapter is below this one; integrate them</li>
</ul>
<section id="learning-objectives" class="level2" data-number="32.1">
<h2 data-number="32.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">32.1</span> LEARNING OBJECTIVES</h2>
<p>By the end of this chapter, you should be able to:</p>
<ul>
<li>Describe the main components of SpaCy‚Äôs natural language processing pipeline</li>
<li>Effectively use SpaCy‚Äôs <code>doc</code>, <code>token</code>, and <code>span</code> data structures for working with text data</li>
<li>Describe why normalizing text data can improve the quality of downstream analyses</li>
<li>Describe the difference between stemming and lemmatization</li>
<li>Use part-of-speech labels to select and filter tokens from documents</li>
<li>Examine noun chunks (ie. phrases) that are detected by SpaCy‚Äôs pipeline</li>
<li>Examine Subject, Verb, Object Triplets</li>
</ul>
</section>
<section id="learning-materials" class="level2" data-number="32.2">
<h2 data-number="32.2" class="anchored" data-anchor-id="learning-materials"><span class="header-section-number">32.2</span> LEARNING MATERIALS</h2>
<p>You can find the online learning materials for this chapter in <code>doing_computational_social_science/Chapter_10</code>. <code>cd</code> into the directory and launch your Jupyter Server.</p>
</section>
<section id="introduction" class="level2" data-number="32.3">
<h2 data-number="32.3" class="anchored" data-anchor-id="introduction"><span class="header-section-number">32.3</span> INTRODUCTION</h2>
<p>In this chapter, we will shift our focus from working with structured quantitative data to natural language data stored in the form of unstructured text. We will begin by learning how to use the package SpaCy for common natural language processing (NLP) tasks, such as cleaning and normalizing text data, followed by a discussion of labeling words by their part-of-speech, manipulating syntactic dependencies betweens words, and using all of this to create a rough 3-word summary of the content in a sentence. Later, we will put this knowledge to use for custom text pre-processing functions to use for downstream tasks in other chapters of the book.</p>
<section id="package-imports" class="level3" data-number="32.3.1">
<h3 data-number="32.3.1" class="anchored" data-anchor-id="package-imports"><span class="header-section-number">32.3.1</span> Package Imports</h3>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spacy <span class="im">import</span> displacy</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer, TfidfVectorizer</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> TruncatedSVD</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dcss <span class="im">import</span> set_style</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>set_style()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="text-processing" class="level2" data-number="32.4">
<h2 data-number="32.4" class="anchored" data-anchor-id="text-processing"><span class="header-section-number">32.4</span> TEXT PROCESSING</h2>
<p>With the exception of some recent neural network and embedding-based text methods that we will consider later in this book, the quality of most text analyses can be dramatically improved with careful text processing prior to any modelling. For data cleaning, common text processing tasks include removing punctuation, converting to lower case, normalizing words using techniques like stemming or lemmatization, and selecting some subset of terms to use in the analysis. When selecting the subset of terms, it is possible to use a vocabulary that you curate yourself (in which case it is referred to as a <strong>dictionary</strong>) or to select terms based on some sort of criteria, such as their frequency or part-of-speech (e.g., noun, adjectives).</p>
<p>You can process your text data any number of ways in Python, but my advice is that you use a package called SpaCy. SpaCy is, to put it plainly, head and shoulders above the rest when it comes to processing natural language data in Python, or in any other language for that matter. If you are interested in natural language processing, SpaCy alone is reason to do your work entirely in Python. In this first part of the chapter, we will introduce SpaCy with an emphasis on its built in data processing pipelines and data structures, and then we will practice using it to process a data set consisting of political speeches.</p>
<section id="getting-to-know-spacy" class="level3" data-number="32.4.1">
<h3 data-number="32.4.1" class="anchored" data-anchor-id="getting-to-know-spacy"><span class="header-section-number">32.4.1</span> Getting to Know SpaCy</h3>
<p>One of the major benefits of using SpaCy is that it is tightly integrated with state-of-the-art statistical language models, trained using deep learning methods that you will start learning in later chapters.</p>
<p>We are not yet ready to get into the details of pre-trained statistical language models, but we will briefly touch on them here since knowing a <em>bit</em> about them is an important part of learning how to use SpaCy to process natural language data.</p>
<p>Some recent advances have begun revolutionizing natural language processing. To grossly oversimplify things, <strong>transfer learning</strong> means that the output of a machine learning model that was trained in one context is reused in another context. In fields like computer vision and natural language processing, we are almost always talking about deep learning models that take an enormous amount of time and energy to train. In NLP, the basic idea is to train such a model on truly massive datasets (e.g., crawls of the entire open web). In doing so, the model learns a lot about language <em>in general</em>, but perhaps not much about any specific domain. The output from the pre-trained model can be made available to researchers, who can update it using annotated data from the specific domain they are interested in, such as news stories reporting on the Black Lives Matter movement. For most tasks, this transfer learning approach outperforms models that have been trained on a massive dataset but have not been updated with domain-specific data, or models trained the other way around.</p>
<p>While we haven‚Äôt actually gotten into the machine learning (let alone deep neural networks and transfer learning), it is useful to keep this general idea of reusing models in a transfer learning framework in mind. In this chapter, for example, all of the methods you learn how to use are <em>informed</em> by a statistical language model that has been pre-trained on a massive general text corpus, including web data from <a href="https://commoncrawl.org">commoncrawl.org</a> and the <a href="https://catalog.ldc.upenn.edu/LDC2013T19">OntoNotes 5</a> corpus, which contains data from telephone conversations, newswire, newsgroups, broadcast news, broadcast conversation, and weblogs. The pre-trained language models that SpaCy provides can be used as is, or they can be updated with domain-specific annotated data. In the rest of this chapter, we will not update the pre-trained models.</p>
<p>SpaCy‚Äôs pre-trained models come in three sizes ‚Äì small, medium, and large. Each is available in multiple languages<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> and follows a simple naming convention: language + model name (which is the type of model + genre of text it was trained on + the model size). The medium core English model trained on news data is <code>en_core_news_md</code>, and the large English core model trained on web data (blogs, comments, and online news) is <code>en_core_web_lg</code>.</p>
<p>These models vary in what they do, how they do it, how fast they work, how much memory they require, and how accurate they are for various types of tasks. As we now know, it is important to pick the model that is best suited to the specific research application. The smaller models are of course faster and less memory-intensive but they tend to be a bit less accurate. For most general-purpose tasks they work fine, but your case is probably not ‚Äúgeneral purpose‚Äù ‚Äì it is probably fairly domain specific, in which case you may want to work with a larger model, or a model that you can train and update yourself.</p>
<p>Models are not installed with SpaCy, so you will need to download them to your machine. You can do this on the command line with the following command:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> <span class="at">-m</span> spacy download en_core_web_sm</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> <span class="at">-m</span> spacy download en_core_web_md</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> <span class="at">-m</span> spacy download en_core_web_lg</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>One they have been downloaded, we can use SpaCy‚Äôs pre-trained models by loading them into memory using the <code>.load()</code> method and assigning the model to a language object, which is SpaCy‚Äôs NLP ‚Äúpipeline‚Äù. As we will see below, this object contains everything needed to process our raw text. You can call it whatever you want, but the convention is to call it <code>nlp</code>. Once we have imported SpaCy and loaded one of the ‚Äúcore‚Äù models, we are ready to start processing text. We don‚Äôt need the named entity recognition or syntactic dependency parser for this part, so we‚Äôll disable those components of the pipeline.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># nlp = spacy.load("en_core_web_sm", disable=['ner', 'parser'])</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>python <span class="op">-</span>m spacy download en_core_web_sm</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We‚Äôve now created an instance of spaCy‚Äôs text processing pipeline. Let‚Äôs put it to use!</p>
<section id="the-spacy-nlp-pipeline" class="level4" data-number="32.4.1.1">
<h4 data-number="32.4.1.1" class="anchored" data-anchor-id="the-spacy-nlp-pipeline"><span class="header-section-number">32.4.1.1</span> The SpaCy NLP Pipeline</h4>
<p>Once the language model has been loaded (<code>nlp</code>), we can start processing our raw text by passing it through SpaCy‚Äôs default text processing pipeline, which is illustrated in <span class="quarto-unresolved-ref">?fig-10_01</span>. This is often the slowest part of a natural language processing workflow because SpaCy does a <em>lot</em> of heavy lifting right at the start. The result of this process will be something called a <code>Doc</code> object, which we will discuss momentarily; for now, let‚Äôs focus on the big picture and then circle back and fill in the details on each pipeline component and data structure later.</p>
<p><embed src="figures/spacy.pdf" class="img-fluid"></p>
<p>As shown in Figure XXX, as soon as our original text enters SpaCy‚Äôs pipeline, it encounters the <strong>tokenizer</strong>, which identifies the boundaries of words and sentences. Most of the time, punctuation makes it relatively simple for computers to detect sentence boundaries but periods in abbreviations and acronyms (e.g.&nbsp;U.K., U.S.A) can complicate this simple approach. Even tokenizing individual words can be tricky, as this process involves making decisions like whether to convert contractions to one token or two (e.g.&nbsp;it‚Äôs vs.&nbsp;it is), or whether to tokenize special characters like emoji. SpaCy tokenizes text using language-specific rules, differentiating between punctuation marking the end of a sentence and punctuation used in acronyms and abbreviations. It will also use pre-defined language-specific rules to split tokens like <code>don't</code> into <code>do</code> and <code>n't</code>. Although these rules are language-specific, if SpaCy doesn‚Äôt already have a tokenizer for a language you need, it is possible to add new languages. (Instructions on how to do this are available in the SpaCy documentation.)</p>
<p>In the second step of the pipeline, SpaCy assigns each a tag based on its <strong>part-of-speech</strong> using its pre-trained statistical models. In doing so, SpaCy combines rules-based expertise from linguistics with supervised machine learning models. The third step maps syntactic dependencies between words (e.g., which words in a sentence depend on or modify other words in a sentence) using its neural network model. At its most basic, dependency parsing is the basis for accurate sentence segmentation in SpaCy, but it also enables more complex analysis typical to the field of linguistics. The fourth step in the processing pipeline is to recognize <strong>named entities</strong>. This is a very useful and important task for computational social scientists but is relatively complex and tends to be highly-dependent on the data used to train the model. Therefore, we will set named entity recognition aside until later, where we can explore it in more depth and learn how to train models that are customized to work best for our specific research applications (see Chapter 33). Note that when we loaded the pre-trained language model and initialized the <code>nlp</code> pipeline, we disabled the <code>ner</code> component. Since we are not going to use it here, disabling it in the pipeline speeds up text processing a noticeable amount because it means SpaCy won‚Äôt spend time executing that part of the pipeline.</p>
<p>The general processing pipeline I have just described is summarized in the Figure below, which is reproduced from <a href="https://spacy.io/usage/spacy-101">the spaCy documentation</a>. Note the ‚ÄúCustom Pipeline Components‚Äù on the right side of the processing pipeline. This indicates the option of adding additional steps to the pipeline, such as categorizing texts based on some pre-defined set of labels, assigning customized attributes to the <code>Doc</code>, <code>Token</code>, and <code>Span</code> objects, merging noun chunks or named entities into single tokens, and so on. Technically you can add your own custom steps to any part of the SpaCy pipeline, not just the end. These custom steps are beyond the scope of this chapter, but now you know it‚Äôs possible to add them.</p>
<p>Now that we understand how to download, load, and use pre-trained statistical models as part of SpaCy‚Äôs default text processing pipeline, it‚Äôs time to learn about SpaCy‚Äôs containers: <code>Doc</code>s, <code>token</code>s, and <code>span</code>s.</p>
</section>
<section id="the-spacy-containers" class="level4" data-number="32.4.1.2">
<h4 data-number="32.4.1.2" class="anchored" data-anchor-id="the-spacy-containers"><span class="header-section-number">32.4.1.2</span> The SpaCy Containers</h4>
<p>We‚Äôll use a simple example to illustrate SpaCy‚Äôs containers. We start by passing some raw input text into the processing pipeline and then demonstrate how to work with the containers that store the output of that pipeline.</p>
<p>As an example, let‚Äôs consider the abstract for Bart Bonikowski‚Äôs <span class="citation" data-cites="bonikowski2017ethno">(<a href="references.html#ref-bonikowski2017ethno" role="doc-biblioref">2017</a>)</span> journal article ‚ÄúEthno-nationalist populism and the mobilization of collective resentment‚Äù published in <em>The British Journal of Sociology</em>. Here is the raw text of the abstract:</p>
<blockquote class="blockquote">
<p>Scholarly and journalistic accounts of the recent successes of radical-right politics in Europe and the United States, including the Brexit referendum and the Trump campaign, tend to conflate three phenomena: populism, ethno-nationalism and authoritarianism. While all three are important elements of the radical right, they are neither coterminous nor limited to the right. The resulting lack of analytical clarity has hindered accounts of the causes and consequences of ethno-nationalist populism. To address this problem, I bring together existing research on nationalism, populism and authoritarianism in contemporary democracies to precisely define these concepts and examine temporal patterns in their supply and demand, that is, politicians‚Äô discursive strategies and the corresponding public attitudes. Based on the available evidence, I conclude that both the supply and demand sides of radical politics have been relatively stable over time, which suggests that in order to understand public support for radical politics, scholars should instead focus on the increased resonance between pre-existing attitudes and discursive frames. Drawing on recent research in cultural sociology, I argue that resonance is not only a function of the congruence between a frame and the beliefs of its audience, but also of shifting context. In the case of radical-right politics, a variety of social changes have engendered a sense of collective status threat among national ethnocultural majorities. Political and media discourse has channelled such threats into resentments toward elites, immigrants, and ethnic, racial and religious minorities, thereby activating previously latent attitudes and lending legitimacy to radical political campaigns that promise to return power and status to their aggrieved supporters. Not only does this form of politics threaten democratic institutions and inter-group relations, but it also has the potential to alter the contours of mainstream public discourse, thereby creating the conditions of possibility for future successes of populist, nationalist, and authoritarian politics.</p>
</blockquote>
<p>I have the abstract saved in a text file called ‚Äúbonikowski_2017.txt‚Äù. To feed this abstract into the SpaCy pipeline we‚Äôll read it into memory, assign it to a variable, and then call our <code>nlp()</code> object on it.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'data/bonikowski_2017.txt'</span>, <span class="st">'r'</span>) <span class="im">as</span> f:</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    abstract <span class="op">=</span> f.read()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="docs" class="level5" data-number="32.4.1.2.1">
<h5 data-number="32.4.1.2.1" class="anchored" data-anchor-id="docs"><span class="header-section-number">32.4.1.2.1</span> <code>Doc</code>s</h5>
<p>In SpaCy, the first data structure to understand is the <strong><code>Doc</code></strong> object returned from the default processing pipeline indicated in the Figure above. The <code>Doc</code> object contains the linguistic annotations that we will use in our analyses, such as information about parts-of-speech. As indicated in the Figure above, we create the <code>Doc</code> object by running our data through the NLP pipeline. We‚Äôll call the <code>Doc</code> object <code>doc</code>, but of course we could call it pretty much anything we want.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>doc <span class="op">=</span> nlp(abstract)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'There are </span><span class="sc">{</span><span class="bu">len</span>(doc)<span class="sc">}</span><span class="ss"> tokens in this document.'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>SpaCy‚Äôs <code>Doc</code> object is designed to facilitate <em>non-destructive</em> workflows. It‚Äôs built around the principle of always being able to access the original input text. In SpaCy, no information is ever lost and the original text can always be reconstructed by accessing the <code>.text</code> attribute of a <code>Doc</code>, <code>Sentence</code>, or <code>Token</code> object. For example, <code>doc.text</code> recreates the exact text from the <code>abstract</code> object that we fed into the pipeline. Note that although we access <code>.text</code> as we would an attribute of an object, as though the text is stored plainly as a variable attached to it, <code>.text</code> is actually a class method that retrieves the original text from SpaCy‚Äôs underlying C storage structure.</p>
<p>Each <code>Doc</code> object includes information about all of the individual <code>sentence</code>s and <code>token</code>s that are used in the raw text. For example, we can print each individual sentence in the <code>Doc</code>. In the code block below, we print each sentence from the abstract. I won‚Äôt print the full text here, but you will see it on your screen if you follow along with the code.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> sent <span class="kw">in</span> doc.sents:</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(sent, <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Similarly, we can iterate over the <code>Doc</code> object and print out each <code>token</code>. Iterating tokens is the default behaviour of a <code>Doc</code> object, so we don‚Äôt need to use <code>.tokens</code> to access them.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> token <span class="kw">in</span> doc:</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(token)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The ability to iterate over tokens greatly simplifies the process of cleaning and extracting relevant information from our text data. In the sections below, we‚Äôll iterate over tokens for a variety of important text processing tasks, including normalizing text and extracting words based on their part-of-speech, two tasks we turn to shortly.</p>
<p>The <code>Doc</code> object itself can be stored on disk and reloaded later, which can be very useful when working with large collections of text that take non-trivial amounts of time to pass through the default processing pipeline. This can be done a few different ways, including the new <code>DocBin</code> class for serializing and holding the contents of multiple <code>Doc</code> objects, which can then be saved as a <code>.spacy</code> file using <code>DocBin.to_disk()</code>. The <code>to_array()</code> method exports an individual <code>Doc</code> object to an <code>ndarray</code> (from <code>numpy</code>), where each token occupies a row and each token attribute is a column. These arrays can also be saved to disk using numpy, but the <code>DocBin</code> method is the most convenient.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> spacy.tokens <span class="im">import</span> DocBin</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>doc_export <span class="op">=</span> DocBin()</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>doc_export.add(doc)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>doc_export.to_disk(<span class="st">'data/bart_bonikowski_doc.spacy'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Of course, it is possible to read these <code>Doc</code>s back into memory using methods like <code>DocBin.from_disk()</code>, or loading the saved <code>ndarray</code> and using <code>Doc.from_array()</code>. Loading from <code>DocBin</code> is the most convenient, but keep in mind that you need a vocabulary from an <code>nlp()</code> object to recreate the <code>Doc</code> objects themselves.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>doc_import <span class="op">=</span> DocBin().from_disk(<span class="st">'data/bart_bonikowski_doc.spacy'</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>docs <span class="op">=</span> <span class="bu">list</span>(doc_import.get_docs(nlp.vocab))</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>doc <span class="op">=</span> docs[<span class="dv">0</span>]</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'There are </span><span class="sc">{</span><span class="bu">len</span>(doc)<span class="sc">}</span><span class="ss"> tokens in this document.'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>There are 346 tokens in this document.</code></pre>
</section>
<section id="token" class="level5" data-number="32.4.1.2.2">
<h5 data-number="32.4.1.2.2" class="anchored" data-anchor-id="token"><span class="header-section-number">32.4.1.2.2</span> <code>Token</code></h5>
<p>The second type of object to know about is the <strong><code>Token</code></strong>. A token is each individual element of the raw text, such as words and punctuation. The <code>Token</code> object stores information about lexical types, adjacent whitespace, the parent <code>Doc</code> that a token belongs to, and ‚Äúoffsets‚Äù that index precisely where the token occurs within the parent <code>Doc</code>. As we will see in subsequent chapters, all of this <code>Token</code> metadata can be used to accomplish specific natural language processing tasks with a high-degree of accuracy, such as the information extraction tasks covered in later chapters.</p>
<p><code>Tokens</code> are stored as hash values to save memory, but just as we can access the raw input text of a <code>Doc</code> object using <code>.text</code>, we can see the textual representation of a given token using <code>.text</code>. We can also access each token by specifying its index position in the <code>Doc</code> or by iterating over the <code>Doc</code>.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> token <span class="kw">in</span> doc:</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(token.text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>An enormous amount of information is stored about each <code>Token</code>, most of which can be retrieved using methods discussed extensively in the documentation. We‚Äôll cover examples of some fairly important ones, including methods for accessing the normalized forms of the token such as a lemma, its part-of-speech, the dependency relations it‚Äôs embedded in, and in some cases, even an estimate of the token‚Äôs sentiment.</p>
<section id="span" class="level6" data-number="32.4.1.2.2.1">
<h6 data-number="32.4.1.2.2.1" class="anchored" data-anchor-id="span"><span class="header-section-number">32.4.1.2.2.1</span> <code>Span</code></h6>
<p>The final data structure to understand before moving on is the <strong><code>Span</code></strong>, which is a slice of a <code>Doc</code> object that consists of multiple <code>tokens</code> but is smaller than the full <code>Doc</code>. When you iterate of sentences in a document, each of those is actually a <code>Span</code>. Knowing how spans work can be very useful for data exploration, as well as programmatically gathering contextual words that are adjacent to a target type of token, such as a type of named entity. We can specify a span by using slice notation. For example, we could define a <code>Span</code> by providing the range of token indexes from 5 to 15. Note that this span will include token 5 but not token 15!</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>a_span <span class="op">=</span> doc[<span class="dv">5</span>:<span class="dv">15</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Given a span, we can use many of the same methods available for <code>Docs</code> and <code>Tokens</code>, as well as merging and splitting <code>Spans</code>, or copying them into their own <code>Doc</code> objects.</p>
<p>Now that we have a solid foundational understanding of SpaCy‚Äôs statistical models, processing pipeline, and containers, we can take a closer look at two important components of the text processing pipeline that are <em>very</em> useful when pre-processing text data for the type of analyses we will perform in this chapter: (a) normalizing text via lemmatization, and (b) part-of-speech tagging.</p>
</section>
</section>
</section>
</section>
</section>
<section id="normalizing-text-via-lemmatization" class="level2" data-number="32.5">
<h2 data-number="32.5" class="anchored" data-anchor-id="normalizing-text-via-lemmatization"><span class="header-section-number">32.5</span> NORMALIZING TEXT VIA LEMMATIZATION</h2>
<p>When we work with natural language data, we have to decide how to handle words that mean more or less the same thing but have different surface forms (e.g.&nbsp;compute, computing). On the one hand, leaving words as they appear preserves nuances in language that may be useful. However, those words are tokenized and counted separately, as if they had no semantic similarity. An alternative approach is to normalize the text by grouping together words that mean more or less the same thing and reducing them to the same token. The idea, in short, is to define classes of equivalent words and treat them as a single token. Doing so loses some of the nuance but can dramatically improve the results of most text analysis algorithms. The two most widely-used approaches to text normalization are stemming and lemmatization.</p>
<p><strong>Stemming</strong> is a rule-based approach to normalizing words regardless of what role the word plays in a sentence (e.g.&nbsp;noun or verb), or of the surrounding context. For example, the Snowball stemmer takes in each individual word and follows rules about what parts of the word (e.g.&nbsp;‚Äúing‚Äù) should be cut off. As you might imagine, the results you get back are usually not themselves valid words.</p>
<p>Rather than chopping off parts of tokens to get to a word stem, <strong>lemmatization</strong> normalizes words by reducing them to their dictionary form. As a result, it always returns valid words, which makes it considerably easier to interpret the results of almost any text analysis. In addition, lemmatization can be done either with a simple language-specific lookup table or in a rule-based way that considers a token‚Äôs part-of-speech (discussed below), which enables it to differentiate between ways of using the same word (e.g.&nbsp;‚Äúmeeting‚Äù as a noun, ‚Äúmeeting‚Äù as a verb) and identical words that have different normalisation rules in different contexts. Lemmatization is extremely accurate and is almost always going to be a better choice than stemming. It is also more widely used.</p>
<p>Keeping in mind that our most common goal with computational text analysis is to see the shape and structure of the forest, not any individual tree, you can probably see why this is useful in the context of analyzing natural language data. Although we lose some nuance by normalizing the text, we improve our analysis of the corpus (i.e.&nbsp;the ‚Äúforest‚Äù) itself.</p>
<p>As mentioned earlier, SpaCy‚Äôs <code>nlp()</code> does most of the heavy computing up front. As a result, our <code>Doc</code> object already includes information about the lemmas of each token in our abstract. By default, the latest (3.0+) version of SpaCy uses the simpler lookup lemmatizer. To use the newer rule-based one that incorporates part-of-speech information, we‚Äôll install the additional data and modify the pipeline component to use the rule-based one.</p>
<p>You can install the <code>spacy-lookups-data</code> package in a virtual environment with</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install spacy-lookups-data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Alternatively, if you are not using a virtual environment for some reason, you can run:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode sh code-with-copy"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="at">--user</span> spacy-lookups-data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This new lemmatizer needs to replace the existing one, but it <em>also</em> needs to come after the other default pipeline components that assign part-of-speech tags. Unfortunately, simply using <code>nlp.replace()</code>, puts the new lemmatizer after the parser but before the tags are mapped by the <code>AttributeRuler</code> part of the pipeline. It‚Äôs unclear whether this is intentional or a minor bug due to the fact that SpaCy is in the middle of a major transition to Version 3. The easiest approach currently is to exclude the default lemmatizer during loading, then add the new one back in at the end. The lemmatizer also needs to be initialized in order to load the data from <code>spacy-lookups-data</code>.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># nlp = spacy.load('en_core_web_sm', disable=['ner'], exclude = ['lemmatizer'])</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="co"># lemmatizer = nlp.add_pipe('lemmatizer', config = {'mode': 'rule'})</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># lemmatizer.initialize()</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">'en_core_web_sm'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We can iterate over each token in the <code>Doc</code> and add its lemma to a list. It‚Äôs worth noting that using <code>.lemma_</code> on a token returns only the lemmatized text, not the original token, so the <code>lemmas</code> object we create here is a standard python list of strings. To do additional SpaCy-specific operations, we have to return to the original <code>doc</code> object.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>doc <span class="op">=</span> nlp(abstract)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>lemmatized <span class="op">=</span> [(token.text, token.lemma_) <span class="cf">for</span> token <span class="kw">in</span> doc]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The list we just created contains all the tokens in our original document as well as their lemmas <em>where appropriate</em>. If not appropriate, the same token is added twice. To get a sense of the difference between the original tokens and their lemmas, and how minimal (and yet helpful) this normalization can be, let‚Äôs take a peek at the lemmas from the first 100 words of the abstract:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> each <span class="kw">in</span> lemmatized[:<span class="dv">100</span>]:</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> each[<span class="dv">0</span>].lower() <span class="op">!=</span> each[<span class="dv">1</span>].lower():</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>each[<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>each[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">)'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>accounts (account)
successes (success)
politics (politic)
including (include)
phenomena (phenomenon)
are (be)
elements (element)
are (be)
resulting (result)
has (have)
hindered (hinder)
accounts (account)
causes (cause)
consequences (consequence)
existing (exist)</code></pre>
<p>This simple process of iterating over tokens and selecting some, but not all, is something we will do again and again in this chapter. There are more efficient ways to do this kind of pre-processing work ‚Äì specifically by writing a custom function ‚Äì but we will put that task on hold until we‚Äôve covered each of the individual pieces.</p>
</section>
<section id="part-of-speech-tagging" class="level2" data-number="32.6">
<h2 data-number="32.6" class="anchored" data-anchor-id="part-of-speech-tagging"><span class="header-section-number">32.6</span> PART-OF-SPEECH TAGGING</h2>
<p>In some research applications, you may want to restrict the subset of words that you include in your text analysis. For example, if you are primarily interested in understanding <em>what</em> people are writing or talking about (as opposed to <em>how</em> they are talking about something), then you may decide to include only nouns and proper nouns, or noun chunks (discussed below) in your analysis. In our example abstract, nouns and noun chunks like ‚ÄúEurope,‚Äù ‚Äúradical-right politics,‚Äù ‚ÄúBrexit referendum,‚Äù ‚ÄúTrump campaign,‚Äù ‚Äúcauses and consequences,‚Äù ‚Äúethno-nationalist populism,‚Äù and so on tell us far more about the <em>content</em> of this abstract than words such as ‚Äúand,‚Äù ‚Äúhas,‚Äù ‚Äúrecent,‚Äù or ‚Äúavailable.‚Äù We can do this by filtering words based on their <strong>part-of-speech</strong>.</p>
<p>If you‚Äôre a little lost at this point, that‚Äôs a good thing; it means you‚Äôre paying attention, and are justifiably struggling to conceptualize the reconstitution of language we‚Äôve covered in the last few paragraphs. At this point, an example might help show how these processes play out in action. Returning to our example abstract, we‚Äôll start by examining each word and its part of speech.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> item <span class="kw">in</span> doc[:<span class="dv">20</span>]:</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>item<span class="sc">.</span>text<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>item<span class="sc">.</span>pos_<span class="sc">}</span><span class="ss">)'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Scholarly (ADJ)
and (CCONJ)
journalistic (ADJ)
accounts (NOUN)
of (ADP)
the (DET)
recent (ADJ)
successes (NOUN)
of (ADP)
radical (ADJ)
- (PUNCT)
right (NOUN)
politics (NOUN)
in (ADP)
Europe (PROPN)
and (CCONJ)
the (DET)
United (PROPN)
States (PROPN)
, (PUNCT)</code></pre>
<p>SpaCy classifies each word into one of 19 different parts-of-speech, each of which is defined in the <a href="https://spacy.io/api/annotation#pos-tagging">documentation</a>. However, if you are uncertain about what a part-of-speech tag is, you can also ask SpaCy to <code>explain()</code> it to you. For example, <code>spacy.explain('ADJ')</code> will return <code>adjective</code>, and <code>spacy.explain('ADP')</code> will return <code>adposition</code>. Because the part-of-speech a word plays can vary depending on the sentence ‚Äì ‚Äòmeeting‚Äô can be a noun or a verb, depending on the context ‚Äì SpaCy‚Äôs approach to part-of-speech tagging combines language-based rules and statistical knowledge from its trained models that can be used to estimate the best part-of-speech for a word given the words that appear before and after it.</p>
<p>If these 19 parts-of-speech are not sufficient for your purposes, it is possible to access fine-grained parts-of-speech that include additional information, including verb tenses and specific types of pronouns. These fine-grained parts-of-speech can be accessed using the <code>.tag</code> attribute rather than <code>.pos_</code>. As you likely expect, there are far more fine-grained parts-of-speech than coarse-grained. Their meanings can all be found online in the SpaCy documentation.</p>
<p>Because SpaCy assigns a part-of-speech to each token when we initially call <code>nlp()</code>, we can iterate over the tokens in our abstract and extract those that match the part-of-speech we are most interested in. For example, the following code will identify the nouns in our abstract.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>nouns <span class="op">=</span> [item.text <span class="cf">for</span> item <span class="kw">in</span> doc <span class="cf">if</span> item.pos_ <span class="op">==</span> <span class="st">'NOUN'</span>]</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(nouns[:<span class="dv">20</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>['accounts', 'successes', 'right', 'politics', 'referendum', 'campaign', 'phenomena', 'populism', 'ethno', 'nationalism', 'authoritarianism', 'elements', 'right', 'right', 'lack', 'clarity', 'accounts', 'causes', 'consequences', 'ethno']</code></pre>
<p>We can do the same for other parts of speech, such as adjectives, or for multiple parts of speech.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>adjectives <span class="op">=</span> [item.text <span class="cf">for</span> item <span class="kw">in</span> doc <span class="cf">if</span> item.pos_ <span class="op">==</span> <span class="st">'ADJ'</span>]</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>adjectives[:<span class="dv">20</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>['Scholarly',
 'journalistic',
 'recent',
 'radical',
 'important',
 'radical',
 'coterminous',
 'limited',
 'analytical',
 'nationalist',
 'contemporary',
 'temporal',
 'discursive',
 'public',
 'available',
 'radical',
 'stable',
 'public',
 'radical',
 'pre']</code></pre>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>parts <span class="op">=</span> [<span class="st">'NOUN'</span>, <span class="st">'ADJ'</span>]</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>words <span class="op">=</span> [item.text <span class="cf">for</span> item <span class="kw">in</span> doc <span class="cf">if</span> item.pos_ <span class="kw">in</span> parts]</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>words[:<span class="dv">20</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>['Scholarly',
 'journalistic',
 'accounts',
 'recent',
 'successes',
 'radical',
 'right',
 'politics',
 'referendum',
 'campaign',
 'phenomena',
 'populism',
 'ethno',
 'nationalism',
 'authoritarianism',
 'important',
 'elements',
 'radical',
 'right',
 'coterminous']</code></pre>
<p>The accuracy of the part-of-speech tagger in version 3 of SpaCy is 97% for the small English core model and 97.4% for the large English core models, both of which are trained using convolutional neural networks. As mentioned earlier, you will only see modest gains in accuracy by switching to a larger statistical model. Ultimately, as you will soon learn, the accuracy of these kinds of models depends in large part on the data they‚Äôre trained on. The good news is that the accuracy rates for part-of-speech tagging are consistently high regardless of the corpus used for training, and for researchers like us who are more interested in applying these algorithms, rather than developing them, have nothing to gain from trying to beat 97% accuracy.</p>
</section>
<section id="syntactic-dependency-parsing" class="level2" data-number="32.7">
<h2 data-number="32.7" class="anchored" data-anchor-id="syntactic-dependency-parsing"><span class="header-section-number">32.7</span> SYNTACTIC DEPENDENCY PARSING</h2>
<p>The third component of the SpaCy processing pipeline (see Figure 1) is the syntactic dependency parser. This rule-based parser rests on a solid foundation of linguistic research and, when combined with machine learning models, greatly increases the accuracy of a variety of important text processing tasks. It also makes it possible to extract meaningful sequences of words from texts, such as short phrases, or components of larger narratives and frames. We will consider the power of this approach by looking at how SpaCy extracts <strong>noun chunks</strong> from text, setting aside more complex manipulations of the dependency tree until later.</p>
<p>When we communicate in natural languages such as English, we follow sets of commonly held rules that govern how we arrange words, clauses, and phrases in sentences. For the most part, we learn these rules ‚Äì <strong>grammar</strong> ‚Äì implicitly via socialization as children, and then more explicitly later in life. For non-linguists, some explicit forms of instruction about the ‚Äúcorrect‚Äù and ‚Äúincorrect‚Äù way of doing things in a language is what probably comes to mind when we think about grammar, but from a linguistic point of view grammatical rules should <em>not</em> be seen as proscriptive but rather as cultural and evolving in populations over time. Grammatical ‚Äúrules‚Äù are about dominant patterns in usage in a population (linguists use the word ‚Äòrule‚Äô in the way sociologists and political scientists do, not the way physicists do). They are one of the best examples of shared culture and implicit cultural rules we have! Rather than proscription, linguists are focused on <em>description</em> and <em>explanation</em> of grammatical rules, and there is an enormous amount of formal linguistic theory and research on modelling grammar. In fact, P√£nini‚Äôs study of the grammatical structure of Sanskrit was written in the 4th-century and is still discussed today <span class="citation" data-cites="jurafsky2000speech">(<a href="references.html#ref-jurafsky2000speech" role="doc-biblioref">Jurafsky and Hand 2009</a>)</span>!</p>
<p>One of the most enduring ways of modelling grammar is <strong>dependency parsing</strong>, which has its origins in ancient Greek and Indian linguistics <span class="citation" data-cites="jurafsky2000speech">(<a href="references.html#ref-jurafsky2000speech" role="doc-biblioref">Jurafsky and Hand 2009</a>)</span>. Dependency parsing is a rules-based approach that models the relationships between words in a sentence as a directed network. The edges in the network represent various kinds of grammatical relationships between pairs of words. You may already be able to think of some important grammatical relations, such as <strong>clausal argument relations</strong> (e.g.&nbsp;a word can be a <em>nominal subject</em> of another word, a <em>direct</em> or <em>indirect object</em>, or a <em>clausal complement</em>), <strong>modifier relations</strong> (e.g.&nbsp;<em>adjectives</em> that modify a noun, <em>adverbs</em> that modify a verb), or others such as <strong>coordinating conjunctions</strong> that connect phrases and clauses in sentences. Linguists have documented many important grammatical relations and have systematically compared how they operate across different languages <span class="citation" data-cites="nivre2017universal">(e.g. <a href="references.html#ref-nivre2017universal" role="doc-biblioref">Nivre and Fang 2017</a>)</span>. SpaCy combines this rules-based dependency parsing with machine learning models, which results in extremely high levels of accuracy for a broad range of NLP tasks, such as part-of-speech tagging, discussed earlier.</p>
<p>There are some rules around how these dependency-relation networks are constructed that are helpful to understand. First, every sentence has one root word (i.e.&nbsp;node) that is not dependent on any other words. It‚Äôs the starting point for our sentence from which <em>all</em> other words ‚Äúgrow‚Äù. Second, with the single exception of the root word, every word has one and only one dependency relationship with another word. Finally, there is a path that starts at the root word and connects to every other word in the tree. This directed acyclic network is usually represented with the text written horizontally left to right, with arcs connecting and labeling specific dependency relationships between words.</p>
<p>The syntactic dependency parser built into SpaCy is powerful, accurate, and relatively fast. SpaCy also simplifies the process of understanding these syntactic dependencies by using a visualization tool called displacy, which is especially useful for researchers with little background knowledge of formal linguistic theory. For example, let‚Äôs use displacy to visualize the syntactic dependencies in a short sentence. Below, we do this for a short and simple sentence. If you‚Äôre executing code from a script, you should use the <code>.serve()</code> method. If you‚Äôre in a Jupyter Notebook, you should use <code>.render()</code> instead.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>sentence <span class="op">=</span> nlp(<span class="st">"This book is a practical guide to computational social science"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><embed src="figures/displacy.pdf" class="img-fluid"></p>
<figcaption>A visualization of syntactic dependency relationships between words.</figcaption>
</figure>
</div>
<p>The dependency relations that SpaCy identified in this simple sentence are shown in the Table below and in <span class="quarto-unresolved-ref">?fig-10_02</span> (produced using displacy). As you can see, SpaCy has mapped each word in our document to another word, based on a specific type of dependency relationship. Those dependency types are actually labeled on the arcs in the visualization. In Figure XXX and Table XXX, each word has a ‚Äúhead‚Äù (which sends a directed link to the word as a ‚Äúchild‚Äù) but only some have ‚Äúchildren‚Äù (which receive an incoming link from a word if they depend on it).</p>
<table class="caption-top table">
<caption>A table view of the syntactic dependencies shown in <span class="quarto-unresolved-ref">?fig-10_02</span>.</caption>
<colgroup>
<col style="width: 22%">
<col style="width: 8%">
<col style="width: 15%">
<col style="width: 13%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">TEXT</th>
<th style="text-align: left;">DEP</th>
<th style="text-align: left;">HEAD TEXT</th>
<th style="text-align: left;">HEAD POS</th>
<th style="text-align: left;">CHILDREN</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">this</td>
<td style="text-align: left;">det</td>
<td style="text-align: left;">book</td>
<td style="text-align: left;">NOUN</td>
<td style="text-align: left;">[]</td>
</tr>
<tr class="even">
<td style="text-align: left;">book</td>
<td style="text-align: left;">nsubj</td>
<td style="text-align: left;">is</td>
<td style="text-align: left;">AUX</td>
<td style="text-align: left;">[this]</td>
</tr>
<tr class="odd">
<td style="text-align: left;">is</td>
<td style="text-align: left;">ROOT</td>
<td style="text-align: left;">is</td>
<td style="text-align: left;">AUX</td>
<td style="text-align: left;">[book, guide, .]</td>
</tr>
<tr class="even">
<td style="text-align: left;">a</td>
<td style="text-align: left;">det</td>
<td style="text-align: left;">guide</td>
<td style="text-align: left;">NOUN</td>
<td style="text-align: left;">[]</td>
</tr>
<tr class="odd">
<td style="text-align: left;">practical</td>
<td style="text-align: left;">amod</td>
<td style="text-align: left;">guide</td>
<td style="text-align: left;">NOUN</td>
<td style="text-align: left;">[]</td>
</tr>
<tr class="even">
<td style="text-align: left;">guide</td>
<td style="text-align: left;">attr</td>
<td style="text-align: left;">is</td>
<td style="text-align: left;">AUX</td>
<td style="text-align: left;">[a, practical, to]</td>
</tr>
<tr class="odd">
<td style="text-align: left;">to</td>
<td style="text-align: left;">prep</td>
<td style="text-align: left;">guide</td>
<td style="text-align: left;">NOUN</td>
<td style="text-align: left;">[science]</td>
</tr>
<tr class="even">
<td style="text-align: left;">computational</td>
<td style="text-align: left;">amod</td>
<td style="text-align: left;">science</td>
<td style="text-align: left;">NOUN</td>
<td style="text-align: left;">[]</td>
</tr>
<tr class="odd">
<td style="text-align: left;">social</td>
<td style="text-align: left;">amod</td>
<td style="text-align: left;">science</td>
<td style="text-align: left;">NOUN</td>
<td style="text-align: left;">[]</td>
</tr>
<tr class="even">
<td style="text-align: left;">science</td>
<td style="text-align: left;">pobj</td>
<td style="text-align: left;">to</td>
<td style="text-align: left;">ADP</td>
<td style="text-align: left;">[computational, social]</td>
</tr>
<tr class="odd">
<td style="text-align: left;">.</td>
<td style="text-align: left;">punct</td>
<td style="text-align: left;">is</td>
<td style="text-align: left;">AUX</td>
<td style="text-align: left;">[]</td>
</tr>
</tbody>
</table>
<p>For now, what‚Äôs important to understand is that SpaCy does this dependency parsing as part of the default processing pipeline (and like other parts of the pipeline, it is possible to disable it if you don‚Äôt need it). However, we can extract information about these dependency relations directly from the syntactic tree, which in turn enables us to extract a variety of useful information from text with a very high degree of precision, and makes it possible to partially automate methods such as quantitative narrative analysis, briefly discussed below, which are otherwise very laborious and time consuming.</p>
<section id="noun-chunks" class="level3" data-number="32.7.1">
<h3 data-number="32.7.1" class="anchored" data-anchor-id="noun-chunks"><span class="header-section-number">32.7.1</span> Noun Chunks</h3>
<p>One substantial benefit of dependency parsing is the ability to extract coherent phrases and other sub-sentence chunks of meaning from text. We will learn a bit about how to navigate the dependency tree shortly, but for now we can get a sense of the power of dependency parsing by looking at the example of noun phrases, which SpaCy calls ‚Äúnoun chunks.‚Äù</p>
<p><strong>Noun chunks</strong> consist of a single word (the noun) or a string of words including a noun and the words that modify that noun. These are usually ‚Äúpre-modifiers,‚Äù meaning words (e.g.&nbsp;adjectives) that appear <em>before</em> the focal noun, not after. A <strong>base noun phrase</strong> is a phrase that has a noun as its head, and which does not itself contain another noun phrase.</p>
<p>Below, we iterate over the <code>doc</code> containing the text of Bonikowski‚Äôs article and print each noun chunk:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> item <span class="kw">in</span> <span class="bu">list</span>(doc.noun_chunks)[:<span class="dv">10</span>]:</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(item.text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Remember, the computer doesn‚Äôt actually know the meaning of any of these words or phrases. Given that, the results are surprisingly accurate; it should be clear how useful this kind of simplification could be for working with large volumes of text! In a later chapter, we will take a closer look at detecting noun chunks, using a machine learning approach designed specifically for this task.</p>
</section>
<section id="extracting-words-by-dependency-labels-subject-verb-object-triplets" class="level3" data-number="32.7.2">
<h3 data-number="32.7.2" class="anchored" data-anchor-id="extracting-words-by-dependency-labels-subject-verb-object-triplets"><span class="header-section-number">32.7.2</span> Extracting Words by Dependency Labels: Subject, Verb, Object Triplets</h3>
<p>Earlier, you learned how to process a large collection of <code>Doc</code>s and extract <code>Token</code>s from each based on several criteria, including their part-of-speech. We can also extract tokens from documents based on other criteria, such as their dependency relationships with other words. For example, if we wanted to extract a very small representation of an action-object narrative from a sentence (e.g., ‚Äú<strong>Kat</strong> (subject) <strong>plays</strong> (verb) <strong>bass</strong> (object).‚Äù), we could extract the transitive verb (i.e., a verb that takes an object, ‚Äúplays‚Äù) and the direct object of that transitive verb (i.e., ‚Äúbass‚Äù). To do this, we simply check the <code>.dep_</code> tags for each token rather than the <code>.pos_</code> tags. For example, the loops below creates a list of tuples containing the transitive verbs and direct objects for each sentence in <code>doc</code>.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> sent <span class="kw">in</span> doc.sents:</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    tvdo <span class="op">=</span> [(token.head.text, token.text) <span class="cf">for</span> token <span class="kw">in</span> sent <span class="cf">if</span> token.dep_ <span class="op">==</span> <span class="st">'dobj'</span>]</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(tvdo)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>When analyzing text in terms of these semantic dependencies, we are often looking to extract information in the form of a <strong>semantic triplet</strong> of subject-verb-object, also known as an <strong>SVO</strong>. In social scientific text analysis, these triplets are most closely associated with the quantitative narrative analysis framework developed by Roberto Fransozi <span class="citation" data-cites="franzosi2004words">(<a href="references.html#ref-franzosi2004words" role="doc-biblioref">2004</a>)</span>. The idea, in short, is that these SVOs contain crucial information about <em>who</em> did <em>what</em> to <em>whom</em>. We will see examples of working with this kind of data in later chapters, but let‚Äôs take a preliminary look at what the kind of think we can expect when extracting SVOs.</p>
<p>Walking through the linguistic technicalities of a fully functional SVO workflow is outside the scope of this chapter, but we can use the <code>subject_verb_object_triples()</code> function included in the <code>dcss</code> package to see the results of a reasonably complex implementation of the basic idea, as outlined by researchers such as Fransozi.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dcss.text <span class="im">import</span> subject_verb_object_triples</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(subject_verb_object_triples(doc))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Some of these look pretty good, but others leave a little to be desired. As you can probably imagine, there are an enormous number of challenges involved in automating this kind of language processing. To get things <em>just right</em>, you have to consider how people write and speak in different contexts, how sentence construction varies (active, passive; formal, informal), how statements differ from questions, and so on. It is possible to get very high-quality results by building complex logic into the way you walk through the dependency trees, but in general you can expect to find that the signal-to-noise ratio in automated SVO analyses typically means you have to do a good amount of manual work to clean up the results.</p>
<blockquote class="blockquote">
<p><strong>Further Reading</strong></p>
<p><span class="citation" data-cites="vasiliev2020natural">Vasiliev (<a href="references.html#ref-vasiliev2020natural" role="doc-biblioref">2020</a>)</span> provides a fairly deep dive into spaCy for a variety of natural language processing tasks. The spaCy documentation is itself also <em>very</em> good, although some parts of it might be a bit challenging to fully understand until you know a bit more about neural networks and large-scale pre-trained language models. Those topics are covered later in the book.</p>
</blockquote>
</section>
</section>
<section id="conclusion" class="level2" data-number="32.8">
<h2 data-number="32.8" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">32.8</span> CONCLUSION</h2>
<section id="key-points" class="level3" data-number="32.8.1">
<h3 data-number="32.8.1" class="anchored" data-anchor-id="key-points"><span class="header-section-number">32.8.1</span> Key Points</h3>
<ul>
<li>We discussed a variety of common text processing tasks and demonstrated how to use them on a small text dataset and a very large one.</li>
<li>Learned about how SpaCy‚Äôs text processing pipeline is organized, and how to use its data structures</li>
<li>We used SpaCy‚Äôs pipeline and data structures to normalize text via lemmatization</li>
<li>Filtered and selected words based on the part-of-speech and their syntactic dependencies</li>
<li>Learned how to approximately identify the subject, verb, and object in a sentence</li>
</ul>
</section>
</section>
<section id="word-vectors" class="level1" data-number="33">
<h1 data-number="33"><span class="header-section-number">33</span> Word vectors</h1>
<!-- Can We Model Meaning? Contextual Representation and Neural Word Embeddings -->
<section id="learning-objectives-1" class="level2" data-number="33.1">
<h2 data-number="33.1" class="anchored" data-anchor-id="learning-objectives-1"><span class="header-section-number">33.1</span> LEARNING OBJECTIVES</h2>
<ul>
<li>Learn what word embeddings models are, and what they can be used for</li>
<li>Learn what Word2Vec is and how the CBOW and Skip-gram architectures differ</li>
<li>Understand why we should not trust intuitions about complex high-dimensional vector spaces</li>
</ul>
</section>
<section id="learning-materials-1" class="level2" data-number="33.2">
<h2 data-number="33.2" class="anchored" data-anchor-id="learning-materials-1"><span class="header-section-number">33.2</span> LEARNING MATERIALS</h2>
<p>You can find the online learning materials for this chapter in <code>doing_computational_social_science/Chapter_32</code>. <code>cd</code> into the directory and launch your Jupyter Server.</p>
</section>
<section id="introduction-1" class="level2" data-number="33.3">
<h2 data-number="33.3" class="anchored" data-anchor-id="introduction-1"><span class="header-section-number">33.3</span> INTRODUCTION</h2>
<p>The text analysis models we‚Äôve been working with to this point in the book have primarly been focused on fairly traditional content analytic tasks, such as describing and comparing the thematic content contained in a collection of text documents. Nearly all of these models have been based on long and sparse vector representations of text data, otherwise known as a ‚Äúbag-of-words.‚Äù In this chapter, we will learn how to represent text data with short dense vectors, otherwise known as word embeddings. Embeddings have interesting implications if used to understand how different words are used in similar contexts, giving us insights into patterns of language use. There is a tendency to think of embeddings are modelling meaning, but for reasons that will become clear in this chapter, we should be careful to avoid imputing meaning to embeddings.</p>
<p>In what follows, we‚Äôll discuss some of the challenges involved with modelling meaning in general, followed by an introduction to using neural word embedding models. As always, we‚Äôll break the models down to better understand how they work, and we‚Äôll spend a bit of time working with pre-trained embeddings to deepen your understanding of embeddings, and to get a feel for vector space. We‚Äôll emphasize fairly simple vector math operations with these embeddings to help you understand why we should <em>not</em> assume that embeddings are good proxies for meaning, and why we need to be very careful with how we interpret the results of analyses that bundle together many vector operations to construct larger ‚Äúdimensions‚Äù of cultural meaning. Finally, I‚Äôll close the chapter by showing you how to train your own word embedding models, including how to train multiple models in a way that facilitates valid cross-sectional comparisons and historical / temporal analysis.</p>
</section>
<section id="can-we-model-meaning" class="level2" data-number="33.4">
<h2 data-number="33.4" class="anchored" data-anchor-id="can-we-model-meaning"><span class="header-section-number">33.4</span> CAN WE MODEL MEANING?</h2>
<p>Word embeddings have received a lot of interest as quantitative representations of what words ‚Äúmean.‚Äù It‚Äôs an astoundingly complex problem, and we need to tread very carefully. So, before we get into the specifics of the models, let‚Äôs take a moment to briefly consider some of the relevant theoretical background here.</p>
<p>Questions about <em>meaning</em> and its consequences for social scientific inquiry have been at the center of some of the biggest theoretical and methodological debates and divides in sociology and other social sciences since at least the early 20th century <span class="citation" data-cites="mohr2020measuring">(see <a href="references.html#ref-mohr2020measuring" role="doc-biblioref">Mohr et al. 2020</a> for a fascinating discussion in the context of contemporary cultural sociology)</span>. Researchers primarily concerned with understanding meaning have tended to prefer more qualitative and interpretivist approaches. Historically, this complexity has led many quantitatively-minded researchers to concede serious efforts to understand meaning to interpretivists, and to focus instead on describing and comparing <em>content</em>.</p>
<p>Despite this longstanding paradigmatic and methodological fault line, these have never been the only two options for social scientific text analysis. For example, relational sociologists working at the intersection of cultural sociology and social network analysis have developed a wide variety of formal and mathematical models of the cultural-cognitive dimensions of institutions, and for inductively exploring and modelling ‚Äúmeaning structures‚Äù <span class="citation" data-cites="mohr1998measuring mohr2015toward edelmann2018formal mohr2013introduction">(<a href="references.html#ref-mohr1998measuring" role="doc-biblioref">Mohr 1998</a>; <a href="references.html#ref-mohr2015toward" role="doc-biblioref">Mohr, Wagner-Pacifici, and Breiger 2015</a>; <a href="references.html#ref-edelmann2018formal" role="doc-biblioref">Edelmann and Mohr 2018</a>; <a href="references.html#ref-mohr2013introduction" role="doc-biblioref">Mohr and Bogdanov 2013</a>)</span>. Much of the theoretical and methodological considerations guiding text-analytic work in ‚Äúrelational sociology‚Äù have evolved in lockstep with network analysis <span class="citation" data-cites="emirbayer1997manifesto crossley2010towards mische2011relational">(see <a href="references.html#ref-emirbayer1997manifesto" role="doc-biblioref">Emirbayer 1997</a>; <a href="references.html#ref-crossley2010towards" role="doc-biblioref">Crossley 2010</a>; <a href="references.html#ref-mische2011relational" role="doc-biblioref">Mische 2011</a>)</span>, and in particular with the evolution of network analytic methods that are focused on understanding relational identities, the cultural-cognitive dimensions of institutions, and the dynamics of socio-semantic networks (which combine network analysis with various kinds of natural language processing). These developments are interesting in part because much of 1970s and 80s-era network analysis energetically eschewed all questions of culture and meaning, considered intractable and unscientific, in pursuit of establishing a thoroughly <em>structural</em> paradigm. But from the 1990s onward, even the most fervent structuralists were taking culture and meaning seriously <span class="citation" data-cites="white1992identity">(e.g., <a href="references.html#ref-white1992identity" role="doc-biblioref">White 1992</a>)</span>, in search of a deeper understanding of the <strong>co-constitution</strong> of social structure (networks) and culture (meanings, practices, identities, etc.). Much has happened since then.</p>
<p>As part of this larger effort to integrate relational theory and methods, we‚Äôve seen a proliferation of new methodological tools and approaches for text analysis ‚Äì some developed ‚Äúin-house,‚Äù others imported ‚Äì that try to avoid counter-productive dichotomies (e.g., quantitative and qualitative, inductive and deductive, exploratory and confirmatory). The embedding methods I introduce in this chapter and the next can be seen as another contribution to efforts to measure and model meaning structures. They have opened up new discussions in computational research on culture, knowledge, and ideology <span class="citation" data-cites="kozlowski2019geometry linzhuo2020social stoltz2019concept taylor2020concept rheault2020word mclevey2021embeddings">(<a href="references.html#ref-kozlowski2019geometry" role="doc-biblioref">Kozlowski, Taddy, and Evans 2019</a>; <a href="references.html#ref-linzhuo2020social" role="doc-biblioref">Linzhuo, Lingfei, and James 2020</a>; <a href="references.html#ref-stoltz2019concept" role="doc-biblioref">Stoltz and Taylor 2019</a>; <a href="references.html#ref-taylor2020concept" role="doc-biblioref">Taylor and Stoltz 2020</a>; <a href="references.html#ref-rheault2020word" role="doc-biblioref">Rheault and Cochrane 2020</a>; <a href="references.html#ref-mclevey2021embeddings" role="doc-biblioref">McLevey et al. 2021</a>)</span>, including deeply embedded cultural stereotypes and collective biases <span class="citation" data-cites="garg2018word bolukbasi2016man jones2020stereotypical papakyriakopoulos2020bias">(<a href="references.html#ref-garg2018word" role="doc-biblioref">Garg et al. 2018</a>; <a href="references.html#ref-bolukbasi2016man" role="doc-biblioref">Bolukbasi et al. 2016</a>; <a href="references.html#ref-jones2020stereotypical" role="doc-biblioref">Jones et al. 2020</a>; <a href="references.html#ref-papakyriakopoulos2020bias" role="doc-biblioref">Papakyriakopoulos et al. 2020</a>)</span>. There are also ongoing efforts to develop new methodological tools for using word embeddings to conduct research, informed by intersectionality theory <span class="citation" data-cites="collins2020intersectionality collins2015intersectionality crenshaw1989demarginalizing">(<a href="references.html#ref-collins2020intersectionality" role="doc-biblioref">Collins and Bilge 2020</a>; <a href="references.html#ref-collins2015intersectionality" role="doc-biblioref">Collins 2015</a>; <a href="references.html#ref-crenshaw1989demarginalizing" role="doc-biblioref">Crenshaw 1989</a>)</span>, on the social categories and institutions that intersect to create and maintain social inequality and systems of oppression <span class="citation" data-cites="nelson2021leveraging">(e.g., <a href="references.html#ref-nelson2021leveraging" role="doc-biblioref">Nelson 2021</a>)</span>. We will briefly discuss these and other applications below. It is important to keep in mind my earlier statement: these issues are <em>complex</em>. We need to be careful to exercise caution when presented with ‚Äúeasy‚Äù answers that draw the connection between embeddings and meaning.</p>
<p>Of course, sociologists and other social scientists are not the only ones who‚Äôve struggled long and hard with the problem of measuring and modelling meaning. The dominant way of modelling meaning in the field of computational linguistics has deep affinities with social scientific paradigms. The branch of linguistics concerned with meaning is called <strong>semantics</strong>, and in many respects, its starting point is the failure of dictionary-based approaches for defining the meaning of words. Paul Elbourne‚Äôs <span class="citation" data-cites="elbourne2011meaning">(<a href="references.html#ref-elbourne2011meaning" role="doc-biblioref">2011</a>)</span> book <em>Meaning: A Slim Guide to Semantics</em> starts with a thorough debunking of the dictionary approach to meaning, showing the limitations of everyday dictionary definitions when applied to the laborious work done by philosophers over thousands of years to define the meaning of specific words like ‚Äúknowledge.‚Äù Many social scientists who gripe about the lack of broadly-shared definitions of core concepts in our field ‚Äì e.g.&nbsp;culture, network, field, habitus, system, identity, class, gender, and so on ‚Äì will be comforted to know that similar concerns are raised in other sciences and in engineering, like metallurgists being unable to reach a consensus on an acceptable definition of metal <span class="citation" data-cites="elbourne2011meaning">(<a href="references.html#ref-elbourne2011meaning" role="doc-biblioref">Elbourne 2011, 9</a>)</span>.</p>
<p>We are used to the idea that dictionaries are an authority on meaning, but of course dictionary definitions change over time in response to how language is used. For example, Merriam-Webster recently added ‚Äòthey‚Äô as a personal pronoun, reflecting large-scale social changes in how we think and talk about gender identities. Other new words, phrases, and concepts from popular culture have also been added, such as the Bechdel test, swole, on point, page view, screen time, cybersafety, bottle episode, go cup, gig economy, and climate change denial. Culture and language evolve.</p>
<p><span class="citation" data-cites="elbourne2011meaning">Elbourne (<a href="references.html#ref-elbourne2011meaning" role="doc-biblioref">2011</a>)</span> provides many examples of the ‚Äúmind-boggling complexity‚Äù involved in giving adequate definitions to the meanings of words. His larger point is that any definition-based approach to assigning meanings to words (including dictionaries) will <em>always</em> be unsatisfactory. A serious theory or approach to modelling meaning needs much more than definitions. His comparison of different cognitive and linguistic theories are well worth reading but are beyond the scope of this chapter, but one of the key take-aways is that meanings are <em>not definitions</em> and they are not determined by the characteristics of the things they refer to. Instead, meanings are concepts that reside in our heads and are generally attached to low-level units like words, which are strongly modified by the contexts in which they‚Äôre used and scale up to higher-level units like sentences. These meanings are shared but not universal. When it comes to any given thing ‚Äì say the word ‚Äúpopulist‚Äù ‚Äì the concept in my head is not <em>identical</em> to the concept in your head, but communication does not break down because our concepts are qualitatively similar. We might not mean <em>exactly</em> the same thing by the word populist, but our concepts overlap sufficiently enough that we can have a meaningful conversation and our interactions don‚Äôt descend into conceptual chaos.</p>
<p>The core sociological idea here is grounded in a critique of two extremes. Traditional philosophical approaches to cognition and meaning have been overly-focused on individual thinking and meaning. Conversely, neuroscience primarily focuses on processes presumed to be more-or-less universal, such as understanding the biological and chemical mechanisms that enable thought <em>in general</em> rather than explaining specific thoughts. But as Karen Cerulo and many others have pointed out, even if the cognitive <em>processes</em> are universal, cognitive <em>products</em> are not <span class="citation" data-cites="lizardo2019can cerulo2010mining cerulo2002culture ignatow2009culture">(<a href="references.html#ref-lizardo2019can" role="doc-biblioref">Lizardo et al. 2019</a>; <a href="references.html#ref-cerulo2010mining" role="doc-biblioref">K. A. Cerulo 2010</a>; <a href="references.html#ref-cerulo2002culture" role="doc-biblioref">K. Cerulo 2002</a>; <a href="references.html#ref-ignatow2009culture" role="doc-biblioref">Ignatow 2009</a>)</span>. There is variation in how groups of people ‚Äì societies, subcultures, whatever ‚Äì perceive the world, draw boundaries, employ metaphors and analogies, and so on <span class="citation" data-cites="dimaggio1997culture brekhus2019oxford">(<a href="references.html#ref-dimaggio1997culture" role="doc-biblioref">DiMaggio 1997</a>; <a href="references.html#ref-brekhus2019oxford" role="doc-biblioref">Brekhus and Ignatow 2019</a>)</span>. These meaning structures are not reducible to concepts in individual people‚Äôs heads; they are embedded in different cultural systems that are external to any individual person, and are <em>shared but not universal.</em> Given this variability and the staggering complexity of meaning <em>in general</em>, we can best understand <em>meaning</em> by understanding how people use language in context.</p>
<p>The idea that we could best understand shared but not necessarily universal meanings by studying how groups of people use language was, surprisingly, a revolutionary idea as recently as the 1950s. It was most famously posited by the famed philosopher Ludwig <span class="citation" data-cites="anscombe1953philosophical">Wittgenstein (<a href="references.html#ref-anscombe1953philosophical" role="doc-biblioref">1953</a>)</span>, whose argument that ‚Äòmeaning resides in use‚Äô was the inspiration behind the <em>specific</em> linguistic hypothesis that informs embedding models and is a common theme underlying many of the recent breakthroughs in natural language processing: the distributional hypothesis.</p>
<section id="the-distributional-hypothesis" class="level3" data-number="33.4.1">
<h3 data-number="33.4.1" class="anchored" data-anchor-id="the-distributional-hypothesis"><span class="header-section-number">33.4.1</span> The Distributional Hypothesis</h3>
<p>Wittgenstein‚Äôs <span class="citation" data-cites="anscombe1953philosophical">(<a href="references.html#ref-anscombe1953philosophical" role="doc-biblioref">1953</a>)</span> proposal that empirical observations of how people actually use language could reveal far more about meaning than formal rules derived through logical analysis was taken up by a group of linguists in the 1950s <span class="citation" data-cites="joos1950description harris1954distributionalstructure firth1957synopsis">(especially <a href="references.html#ref-joos1950description" role="doc-biblioref">Joos 1950</a>; <a href="references.html#ref-harris1954distributionalstructure" role="doc-biblioref">Harris 1954</a>; <a href="references.html#ref-firth1957synopsis" role="doc-biblioref">Firth 1957</a>)</span> who first proposed <strong>the distributional hypothesis</strong>, which has informed approaches to measuring and modelling meaning in language data ever since.</p>
<p>According to the <strong>distributional hypothesis</strong>, words that appear in similar semantic contexts, or ‚Äúenvironments,‚Äù will tend to have similar meanings.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> In one of the foundational statements of the idea, Zellig <span class="citation" data-cites="harris1954distributionalstructure">Harris (<a href="references.html#ref-harris1954distributionalstructure" role="doc-biblioref">1954</a>)</span> defined a word‚Äôs context in terms of the other words that it <em>co-occurs</em> with, given some boundary such as a phrase or a sentence. For example, we can infer that ‚Äúphysician‚Äù and ‚Äúdoctor‚Äù mean similar things if we see that they tend to be used interchangeably in sentences like ‚ÄúAlondra is looking for a <code>[physician, doctor, ...]</code> specializing in pain management.‚Äù Across many texts, we might also learn that ‚Äúdoctor‚Äù and ‚Äúprofessor‚Äù are also more or less interchangeable but in different types of context. While the former pair of words might co-occur in contexts shared with words such as ‚Äúpain‚Äù, ‚Äúmedicine‚Äù, ‚Äúnurse‚Äù, and ‚Äúinjury‚Äù, the latter pair may co-occur in contexts shared with words like ‚Äúuniversity‚Äù, ‚Äústudents‚Äù, ‚Äúresearch‚Äù, ‚Äúteaching‚Äù, and ‚Äúknowledge‚Äù. ‚ÄúProfessor‚Äù and ‚Äúphysician‚Äù may also co-occur, but more rarely. In any instance, the meaning of the words depends on the other words surrounding it. Words that have identical, or nearly identical contexts are synonyms. In fact, the distributional hypothesis bears a striking resemblance to the idea of structural equivalence in social network analysis, which was introduced in Chapter 30. (Like synonyms, people that are structurally similar tend to be connected to the same alters.)</p>
<p>Distributionalists like Harris and Firth believe that formal theories of language should be kept to a minimum and knowledge should be produced by rigorous analysis of empirical data on language use. Given enough data on natural language use (e.g.&nbsp;in everyday interactions, in email messages and social media posts, in news stories and scientific publications, etc.), we can learn an enormous amount about the contextual relationships between words as they are actually used. In practice, this idea is operationalized in terms of <strong>vector semantics</strong>, and is the foundation of all modern natural language processing that is concerned with understanding <em>meaning</em> <span class="citation" data-cites="jurafsky2000speech">(<a href="references.html#ref-jurafsky2000speech" role="doc-biblioref">Jurafsky and Hand 2009</a>)</span>.</p>
<p>With that briefest of context introduced, let‚Äôs turn our attention to word embeddings.</p>
</section>
</section>
<section id="what-are-neural-word-embeddings" class="level2" data-number="33.5">
<h2 data-number="33.5" class="anchored" data-anchor-id="what-are-neural-word-embeddings"><span class="header-section-number">33.5</span> WHAT ARE NEURAL WORD EMBEDDINGS?</h2>
<p>In previous chapters, we used bag-of-words models to represent individual documents as <em>long</em> and <em>sparse</em> vectors, and document collections as <em>wide</em>, <em>sparse</em> matrices (i.e., DTMs). These matrices are long because each feature represents a unique word in the vocabulary, and each cell represents something like presence / absence, frequency, or some sort of weight such as TF-IDF for each word in each document. They are sparse because most words do not appear in most documents, which means that most cells have values of 0. This approach can be very powerful for modelling latent distributions of topical content, but we actually gain more insight into what words mean by using shorter, denser vector representations, generally referred as <strong>word embeddings</strong>. Words are just the beginning, though. They provide a foundation we can build on to explore and model meaning and larger cultural systems in ways that were not possible just a short time ago.</p>
<p>In bag-of-word models, we represent <em>documents</em> with long sparse vectors indicating the presence or absence, frequency, or weight of a word in each document. Embeddings differ in that they represent <em>words</em> with short dense vectors that define the local semantic contexts within wich words are used. <span class="quarto-unresolved-ref">?fig-32_01</span> illustrates this idea of local semantic contexts using a window of 5 words that slides over each word in sentence from <span class="citation" data-cites="neblo2018politics">Neblo, Esterling, and Lazer (<a href="references.html#ref-neblo2018politics" role="doc-biblioref">2018</a>)</span>. This sliding window approach gives us much deeper insight into how words relate to other words, but it comes at the cost of fine-grained information about how each word relates to the documents in which they appeared.</p>
<p><embed src="figures/word2vec.pdf" class="img-fluid"></p>
<p>In addition to (1) assigning vectors to words instead of documents, (2) observing co-occurrences within small local contexts rather than entire documents, and (3) using short dense vectors instead of long spare vectors, embeddings are also different in that (4) the vector representation for any given word is <em>learned</em> by a neural network trained on positive and negative examples of co-occurrence data. (In fact, we could have extracted embeddings from the neural networks we trained in Chapter 24!) Words that tend to appear in the same contexts, but rarely with one another, tend to share meanings. The learned word embeddings put words with similar meanings close to one another in vector space.</p>
<!-- ## What Can We Do with Embeddings?

> **NOTE**: SAME CONTENT BUT WITH A DIFFERENT TONE

- **TODO**: discussion of the social scientific literature, emphasis on GoC, bias, etc. 
- **PIERSON: REVIEW CONTENT SHOULD BE WORKED IN HERE.**
- **TODO: This tone will be different than I imagined originally. Instead of "the point is to do these larger things," it will be more like "there are some interesting proposals, but let's be careful and not leap to big interpretations here; this is some tricky shit. don't jump into just adding and subtracting vectors and pretend you've got a perfect mathematical model of meaning.** -->
<section id="learning-embeddings-with-word2vec" class="level3" data-number="33.5.1">
<h3 data-number="33.5.1" class="anchored" data-anchor-id="learning-embeddings-with-word2vec"><span class="header-section-number">33.5.1</span> Learning Embeddings with Word2Vec</h3>
<p>Now that we have some context for understanding embeddings, let‚Äôs discuss one of the most important recent breakthroughs in <em>learning</em> word embeddings from text data ‚Äì <strong>word2vec</strong>. As with previous chapters, the goal here is mainly to clarify the way the models work at a relatively high-level.</p>
<p>The development of <strong>word2vec</strong> by Tomas Mikolov and a team of researchers at Google <span class="citation" data-cites="mikolov2013distributed mikolov2013efficient">(<a href="references.html#ref-mikolov2013distributed" role="doc-biblioref">Tomas Mikolov, Sutskever, et al. 2013</a>; <a href="references.html#ref-mikolov2013efficient" role="doc-biblioref">Tomas Mikolov, Chen, et al. 2013</a>)</span> was a transformative development in natural language processing. As we‚Äôve already discussed, word embedding models in general are focused on the local semantic contexts that words are used in rather than the documents they appear in; they <em>learn</em> these short dense representations from the data rather than relying on count-based features. Let‚Äôs break down the modelling process, as we have with previous models in the book.</p>
<p>Word2vec has two different architectures: <strong>Continuous Bag-of-Words (CBOW)</strong> and <strong>Skip-gram</strong>. Both use word co-occurrence data generated from local semantic contexts, such as a moving window of 5 words around a focal word as shown in the example in Figure XXX. However, CBOW and Skip-gram use this co-occurrence data differently; CBOW uses the <em>context words</em> (within each thin crimson box) in a shallow neural network model trained to predict the target word (in each thick crimson box), whereas skip-gram uses the target word to predict the context words. The interesting thing about the neural network model used in these two architectures is that <em>we don‚Äôt actually care about their predictions</em>. What we care about are the feature weights that the neural networks learn in the context of figuring out how to make their predictions well. <em>Those feature weights are our word embeddings!</em> We only train the neural network models to obtain the embeddings. That‚Äôs their raison d‚Äôetre.</p>
<p>The shallow neural network models that word2vec use to learn the embeddings, illustrated in Figure XXX (which is adapted from <span class="citation" data-cites="mikolov2013distributed">Tomas Mikolov, Sutskever, et al. (<a href="references.html#ref-mikolov2013distributed" role="doc-biblioref">2013</a>)</span>), have a few clever modifications. In CBOW, the <em>non-linear</em> hidden layer is replaced by a much simpler <em>linear</em> projection. For each token in the data, the feature vector of the underlying token is the target, while the input to the neural network is the average of the vectors for each of the individual context tokens (ie. the vectors of the surrounding words). It‚Äôs worth mentioning briefly that the Gensim implementation of word2vec gives the option to sum the context vectors rather than average them. After the neural network tries to predict the target word, the resulting probabilities are used to update the feature weights for both the target token <em>and</em> the vectors of the context tokens that were averaged. Once training is done, these updated weights provide each word with a single, dense vector of feature weights. For Skip-gram, rather than sending an averaged context vector to the neural network objective function, the input is the token under consideration and the output is error (probability) vectors for <em>each</em> context word that are then added together, before being used to update feature weights.</p>
<p>Second, the functions used for the prediction in the output layer are different than what we might typically use in such a neural network. The CBOW architecture replaces the traditional softmax (log-linear) classifier for the output layer with a binary logistic regression classifier, and the Skip-gram architecture replaces it with a much more efficient hierarchical softmax variant. In <span class="quarto-unresolved-ref">?fig-32_02</span>, <span class="math inline">\(T\)</span> represents the target word, and the indices represent word position in relation to the target word.</p>
<p><embed src="figures/word2vec_models.pdf" class="img-fluid"></p>
<p>For CBOW, the second innovation is especially valuable, as using a softmax classifier to make and evaluate predictions would require updating feature weights (i.g., embeddings) for every word in the vocabulary every time a prediction is made for a word in the corpus. Instead, word2vec uses a clever innovation called <strong>negative sampling</strong>, in which the target word is evaluated as either co-occurring with the context words from the moving window or not. This enables the use of binary logistic regression for the output layer.</p>
<p>If you‚Äôre thinking ‚Äú<em>hold up, won‚Äôt the context words all have a score of 1?</em>,‚Äù you‚Äôre right! To deal with this problem, the model randomly selects the required number of negative samples from the rest of the corpus (i.e., not from the local semantic context) and assigns them all 0s for that particular batch. As a result, the weights (again, embeddings) only need to be slightly increased for the target and context words and slightly decreased for the words from the negative sample.</p>
<p>The CBOW architecture is a variant of bag-of-words in that word sequence <em>within the local semantic context</em> does not have an impact on the prediction task described above. The similarities end there. Rather than creating one large static matrix, the ‚Äòcontinuous‚Äô part of CBOW refers to how the sliding window moves through the whole corpus, creating and then discarding a bag-of-words for each target word. Since the embeddings for each of the context words are averaged for the prediction task, the semantic context is flattened to a single vector regardless of the number of words in the semantic context. For this reason, it‚Äôs better to keep the semantic contexts fairly small. Otherwise the summing of embeddings can result in a non-descript vector soup, with the subtleties of each word increasingly diminished by the inclusion of more distant words. The authors of word2vec report that a window size of 4 on each side of the target word produced the best results for them.</p>
<p>In the Skip-gram architecture, the input and prediction tasks are basically the inverse of CBOW. Rather than using the average of the embeddings of words around the target word to try to predict the target word, Skip-gram uses the target word to try to predict the co-occurring words in its semantic context. There is no <strong>averaging</strong> of vectors before training, and the training process focuses on the relationship between the target word and many different context words, so the embeddings learned by Skip-gram tend to be more subtle and lossless. Skip-gram has a much longer training time, though, because each word under consideration is used to predict multiple context words before the prediction vectors for each of those words are added together and then used to update the feature weight vectors for the context words. As with CBOW, we can improve the training runtime by using negative sampling.</p>
<p>Unlike CBOW, where the summing of embeddings prior to prediction can result in a less informative vector soup if the semantic contexts are too large, Skip-gram actually <em>benefits</em> from larger window sizes (at the expense of increased runtime). One benefit is that the lack of summing means any updates to the weights are specific to that word, and are therefore more precise. Second, there are far more updates to the embeddings, as each word is used in far more model predictions than would be the case in CBOW. Finally, Skip-gram models do consider word ordering <em>a bit</em>, in that they weight relationships between the target word and context words based on how far away they are within the semantic context, so a window of 10, for example, is a pretty good balance.</p>
<p>A discussion of the hierarchical softmax variant that word2vec uses is outside the scope of this chapter, but the simplified version is that words and their outputs are arranged in a tree-like pattern, such that many words (leaves) are often connected to the same output and their weights can all be updated from a single calculation. The more important thing to know is that hierarchical softmax tends to perform better on infrequent words whereas negative sampling performs better on frequent words. Either of these classifiers can be used for both the CBOW and Skip-gram options, and can actually be used at the same time.</p>
<p>Both model architectures, then, have their strengths and weaknesses. The CBOW architecture is a bit better at learning syntactic relationships, so is likely to produce embeddings where word pairs like ‚Äòneuron‚Äô and ‚Äòneurons‚Äô or ‚Äòbroken‚Äô and ‚Äòbroke‚Äô will be very similar. CBOW also tends to better represent frequently appearing words and is faster to train, so is well-suited to large corpuses. Skip-gram models produce more precise word embeddings in general, and especially for rare words. The embeddings it produces can be especially good at finding words that are near-synonyms. The cost of these improvements are increases in runtime, but in cases where that is less of a concern (e.g., working with smaller datasets), the improvements can certainly be worth the wait. The differences between these architectures are less significant given the specific model parameters used and given enough iterations.</p>
</section>
</section>
<section id="cultural-cartography-getting-a-feel-for-vector-space" class="level2" data-number="33.6">
<h2 data-number="33.6" class="anchored" data-anchor-id="cultural-cartography-getting-a-feel-for-vector-space"><span class="header-section-number">33.6</span> CULTURAL CARTOGRAPHY: GETTING A FEEL FOR VECTOR SPACE</h2>
<p>Word embeddings are very powerful for many applications and sometimes the results are astonishing. But there are some very important caveats to keep in mind when using word2vec-style embeddings. We will illustrate those caveats with perhaps the most iconic and oft-referenced example of word embedding ‚Äúanalogies‚Äù.</p>
<section id="king---man-woman-neq-queen" class="level3" data-number="33.6.1">
<h3 data-number="33.6.1" class="anchored" data-anchor-id="king---man-woman-neq-queen"><span class="header-section-number">33.6.1</span> King - Man + Woman <span class="math inline">\(\neq\)</span> Queen</h3>
<p>Recall that part of the CBOW training process is to average (or just sum) the context vectors. <span class="citation" data-cites="mikolov2013linguistic">Tom√°≈° Mikolov, Yih, and Zweig (<a href="references.html#ref-mikolov2013linguistic" role="doc-biblioref">2013</a>)</span> found that if you take the word embedding vector for ‚Äúking‚Äù, add it to the vector for ‚Äúwoman‚Äù, and then subtract the vector for ‚Äúman‚Äù, the resulting vector is ‚Äúvery close‚Äù to the vector for ‚Äúqueen‚Äù. This example has been referenced countless times, from package documentation to social science papers that aim to measure and compare complex cultural concepts.</p>
<p>We will use the very convenient <code>whatlies</code> package to plot the iconic word embedding example.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> whatlies <span class="im">import</span> Embedding</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> whatlies.embeddingset <span class="im">import</span> EmbeddingSet</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> whatlies.language <span class="im">import</span> SpacyLanguage</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>lang <span class="op">=</span> SpacyLanguage(<span class="st">'en_core_web_md'</span>)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">"display.notebook_repr_html"</span>, <span class="va">False</span>)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dcss.utils <span class="im">import</span> list_files, IterSents, mp_disk</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dcss.text <span class="im">import</span> bigram_process</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> multiprocessing <span class="im">import</span> Process, Manager</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.utils <span class="im">import</span> simple_preprocess</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dcss <span class="im">import</span> set_style</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>set_style()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Using the <code>plot()</code> function, we can plot either a single word vector, or some mathematical combination of vectors enclosed in brackets (as shown in <span class="quarto-unresolved-ref">?fig-32_03</span>). If you call <code>plot()</code> multiple times in the same cell, all of the requested vectors will show up in the figure.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>(lang[<span class="st">'queen'</span>] <span class="op">-</span> lang[<span class="st">'king'</span>]).plot(kind<span class="op">=</span><span class="st">'arrow'</span>, color<span class="op">=</span><span class="st">'lightgray'</span>, show_ops<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>(lang[<span class="st">'king'</span>] <span class="op">+</span> lang[<span class="st">'woman'</span>] <span class="op">-</span> lang[<span class="st">'man'</span>]).plot(kind<span class="op">=</span><span class="st">'arrow'</span>, color<span class="op">=</span><span class="st">'lightgray'</span>, show_ops<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>lang[<span class="st">'man'</span>].plot(kind<span class="op">=</span><span class="st">'arrow'</span>, color<span class="op">=</span><span class="st">'crimson'</span>)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>lang[<span class="st">'woman'</span>].plot(kind<span class="op">=</span><span class="st">'arrow'</span>, color<span class="op">=</span><span class="st">'crimson'</span>)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>lang[<span class="st">'king'</span>].plot(kind<span class="op">=</span><span class="st">'arrow'</span>, color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>lang[<span class="st">'queen'</span>].plot(kind<span class="op">=</span><span class="st">'arrow'</span>, color<span class="op">=</span><span class="st">'black'</span>)</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)<span class="op">;</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><embed src="chapter_32_neural_word_embeddings_files/chapter_32_neural_word_embeddings_12_0.pdf" class="img-fluid"></p>
<figcaption>png</figcaption>
</figure>
</div>
<p>The combination vector appears to be virtually identical to the vector for ‚Äúqueen‚Äù. But there is more to this than meets the eye. Let‚Äôs look at a few comparisons between the vectors with some useful vector combination and comparison functions built-in to whatlies.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Queen and King: "</span> <span class="op">+</span> <span class="bu">str</span>(lang[<span class="st">'queen'</span>].distance(lang[<span class="st">'king'</span>])))</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Man and Woman: "</span> <span class="op">+</span> <span class="bu">str</span>(lang[<span class="st">'man'</span>].distance(lang[<span class="st">'woman'</span>])))</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Man and King: "</span> <span class="op">+</span> <span class="bu">str</span>(lang[<span class="st">'man'</span>].distance(lang[<span class="st">'king'</span>])))</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Woman and King: "</span> <span class="op">+</span> <span class="bu">str</span>(lang[<span class="st">'woman'</span>].distance(lang[<span class="st">'king'</span>])))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Queen and King: 0.27473903
Man and Woman: 0.2598256
Man and King: 0.59115386
Woman and King: 0.7344341</code></pre>
<p>Take note that ‚Äúqueen‚Äù and ‚Äúking‚Äù aren‚Äôt very distant from each other (this is cosine distance). Neither are ‚Äúman‚Äù and ‚Äúwoman‚Äù. This is because they actually share a lot of the same semantic contexts; that is, they are used, conversationally, in very similar ways. With that said, ‚Äúman‚Äù is definitely a bit closer to ‚Äúking‚Äù than ‚Äúwoman‚Äù is. Let‚Äôs do the vector math.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>king_woman_no_man <span class="op">=</span> lang[<span class="st">'king'</span>] <span class="op">+</span> lang[<span class="st">'woman'</span>] <span class="op">-</span> lang[<span class="st">'man'</span>]</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"King and combo-vector:"</span> <span class="op">+</span> <span class="bu">str</span>(lang[<span class="st">'king'</span>].distance(king_woman_no_man)))</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Queen and combo-vector: "</span> <span class="op">+</span> <span class="bu">str</span>(lang[<span class="st">'queen'</span>].distance(king_woman_no_man)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>King and combo-vector:0.19757414
Queen and combo-vector: 0.21191555</code></pre>
<p>The combined vector <em>that should be almost the same as ‚Äúqueen‚Äù</em> is actually still closer to the vector for ‚Äúking‚Äù. Given the plot above, how is this possible? This is the first caveat: word embedding vectors are <em>multi-dimensional</em> space - in this case, 300 dimensions. The best we can really plot is 3-dimensional space and the plot above is 2-dimensional. In either case, there is a LOT of data-reduction happening.</p>
<p>Let‚Äôs get a different perspective on things by using <code>plot_interactive()</code> (a screenshot of which is shown in <span class="quarto-unresolved-ref">?fig-32_04</span>). First, add the vectors to an <code>EmbeddingSet()</code> class instance. Then it‚Äôs as simple as adding <code>.plot_interactive()</code> to that object, along with a few parameters, including the distance metric to use for the axes (cosine distance).</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co">## RENAME THE COMBINATION VECTOR BECAUSE THE ORIGINAL ('MAN') WOULD BE USED FOR THE PLOT</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>king_woman_no_man.orig <span class="op">=</span> king_woman_no_man.name </span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>king_queen_man_woman_plus <span class="op">=</span> EmbeddingSet(lang[<span class="st">'king'</span>], lang[<span class="st">'queen'</span>], </span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>                                         lang[<span class="st">'man'</span>], lang[<span class="st">'woman'</span>], king_woman_no_man)</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>king_queen_man_woman_plus.plot_interactive(x_axis<span class="op">=</span>lang[<span class="st">"king"</span>], </span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>                                           y_axis<span class="op">=</span>lang[<span class="st">"queen"</span>], </span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>                                           axis_metric <span class="op">=</span> <span class="st">'cosine_similarity'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><embed src="figures/king_queen.pdf" class="img-fluid"></p>
<p>This helps put things into perspective. The combination vector is clearly a shift towards queen and away from king, but not dramatically considering that it‚Äôs been influenced by <em>two</em> vectors, so the ‚Äòking‚Äô vector is actually only 1/3rd of the combination one. Recall how these words, which you might be tempted to consider opposites, actually share a lot of contexts. Their embeddings are all wrapped up with each other. When you remove the vector for ‚Äòman‚Äô from the vector for ‚Äòwoman‚Äô, you are actually taking some defining details away from the vector for ‚Äòwoman‚Äô because you‚Äôve removed parts of the contexts that they share! Here‚Äôs an illustrative example.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Woman and Queen: "</span> <span class="op">+</span> <span class="bu">str</span>(lang[<span class="st">'woman'</span>].distance(lang[<span class="st">'queen'</span>])))</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Woman and Queen without man: "</span> <span class="op">+</span> <span class="bu">str</span>((lang[<span class="st">'woman'</span>]<span class="op">-</span>lang[<span class="st">'man'</span>]).distance(lang[<span class="st">'queen'</span>])))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Woman and Queen: 0.5933935
Woman and Queen without man: 0.7745669</code></pre>
<p>The distance between ‚Äúwoman‚Äù and ‚Äúqueen‚Äù actually increases by about 18% if you subtract the vector for ‚Äúman‚Äù! You can see why we need to be <em>extremely</em> careful and methodical in any research that relies on complex combinations of vectors, and in fact this may be a research path to avoid entirely. If you find yourself in a situation where one term is more central to the concept you‚Äôre examining than others, for example, the other terms will outweigh the important one. However, remember that these vectors are just arrays ‚Äì you can weight the entire array if you want to change its contribution to the combined vector.</p>
<p>In the next cell, we access the raw vectors for ‚Äòwoman‚Äô and ‚Äòman‚Äô, multiplying the latter by 0.5, before making them <code>Embedding</code> class objects again.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Woman and Queen without man: "</span> <span class="op">+</span> <span class="bu">str</span>(Embedding(<span class="st">'halfway'</span>, lang[<span class="st">'woman'</span>].vector<span class="op">-</span>lang[<span class="st">'man'</span>].vector<span class="op">*</span><span class="fl">0.5</span>).distance(lang[<span class="st">'queen'</span>])))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>Woman and Queen without man: 0.61308193</code></pre>
<p>As you can see, removing only half of the vector for ‚Äòman‚Äô dramatically reduces the amount of extra distance between ‚Äòwoman‚Äô and ‚Äòqueen‚Äô. In the section that follows, we‚Äôll take a bit of time to look at groups of embeddings for words that are not quite so universally used in most life contexts.</p>
<p>You might also be wondering why the <code>king+woman-man</code> example has been used so frequently in the literature, given this issue. You will find that, in some implementations of word2vec, ‚Äòqueen‚Äô will be returned as the ‚Äúmost similar‚Äù word to the combined vector. Typically, when you use an in-built function to combine words and then return the most similar words, the results returned will <em>not</em> include the constituent words! If they didn‚Äôt, those functions would always return the word itself as the top similar word! This is understandable for a convenience function, but also important to be aware of when using embeddings for research. This issue has been noted and discussed in more detail previously, with some heavy caution about the use of word analogy tasks for any serious purposes <span class="citation" data-cites="nissim2020fair">(<a href="references.html#ref-nissim2020fair" role="doc-biblioref">Nissim, Noord, and Goot 2020</a>)</span>. The authors also reference the introductory paper for transformer models, which we‚Äôll cover in detail in the next chapter, noting that they‚Äôve completely eliminated the concept of analogy as either a training task, or model evaluation method.</p>
<p>With that said, if we‚Äôre careful about how we use embeddings and the claims we make about them, it‚Äôs hard to argue against the results of the embeddings, without manipulation, as indicators of patterns of text use.</p>
</section>
</section>
<section id="learning-embeddings-with-gensim" class="level2" data-number="33.7">
<h2 data-number="33.7" class="anchored" data-anchor-id="learning-embeddings-with-gensim"><span class="header-section-number">33.7</span> LEARNING EMBEDDINGS WITH GENSIM</h2>
<p>Now that you have some understanding of what word embeddings are, what they can be used for, and how the models that learn them work, let‚Äôs get our hands dirty by actually training some models with Gensim.</p>
<section id="data" class="level3" data-number="33.7.1">
<h3 data-number="33.7.1" class="anchored" data-anchor-id="data"><span class="header-section-number">33.7.1</span> Data</h3>
<p>We‚Äôll use the Canadian Hansard dataset for the rest of this chapter (and the next).</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>datasets <span class="op">=</span> list_files(<span class="st">"data/canadian_hansards/lipad/"</span>, <span class="st">'csv'</span>)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(datasets)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>3401</code></pre>
<p>Training good word embeddings requires a lot of text, and we want to avoid loading all that text into memory at once. Gensim‚Äôs algorithm expects only a single sentence at a time, so a clever way to avoid consuming a lot of memory is to store each sentence from the data on its own line in a text file, and then read that enormous text file into memory one line at a time, passing just the sentence to Gensim. That way, we never have to hold all of our data in memory at the same time.</p>
<p>This requires some pre-processing. The Canadian Hansard data is provided as a large collection of CSV files, each containing a single <code>Series</code> with full text for a given speech. We want to get each sentence from each speech in each dataset, while working as efficiently as possible and minimizing the amount of data held in memory.</p>
<p>The function below is one way to do this. It will take some time to run, but perhaps not as long as you would think, given how much data we are working with here, and given that we can use the <code>mp_disk</code> utility for multiprocessing to take advantage of available CPU cores. A less general version of the <code>mp</code> utility, <code>mp_disk</code>, accepts an iterable (e.g.&nbsp;a list) of the data that needs processing, the function you‚Äôll use to process it, a filename to write the results to, and any other arguments that the processing function needs.</p>
<p>You may notice the unexplained <code>q</code> object at the end of this function call. Although a full discussion of the ins and outs of multiprocessing is beyond the scope of this chapter, it is useful to understand what is going on here. The <code>q</code> and the <code>m</code> objects are specific instances of general classes in python‚Äôs <code>multiprocessing</code> module that allow us to write to a text file from multiple parallel processes without having to worry about file access locks or file corruption. The iterable with the data in it will also be divided into multiple lists, so that each CPU core can work on its own subset, so it‚Äôs important that the function is prepared to deal with a list of data and also return that data in a list.</p>
<p>The next block of text iterates over each of the dataframes in the batch, adding the speeches from each to a list. The batch of speeches is sent to the <code>bigram_process</code> function, which is a convenience wrapper for Gensim‚Äôs n-gram pipeline and some text pre-processing using SpaCy. This function expects a flat list of documents, then handles breaking each document into sentences and creating the flat list of sentences that Gensim expects for bigram model training. The utility function returns a list of untokenized sentences, with bigram pairs of words joined by <code>_</code>.</p>
<p>To cap off the process, we send each batch of results to the multiprocessing Queue object so that each sentence can be written onto a new line of the file <code>speeches.txt</code>. Before sending the sentences to the file writing queue, we join them into a single string with a new line character in between, because this is much faster than having the multiprocessing queue write each line to the output file individually.</p>
<p>Whew. Let‚Äôs do it.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_sentences(dataset):</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>    dfs <span class="op">=</span> [pd.read_csv(df) <span class="cf">for</span> df <span class="kw">in</span> dataset]  </span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>    speeches <span class="op">=</span> []</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> df <span class="kw">in</span> dfs:</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>        speeches.extend(df[<span class="st">'speechtext'</span>].tolist())</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>    speeches <span class="op">=</span> [<span class="bu">str</span>(s).replace(<span class="st">'</span><span class="ch">\n</span><span class="st">|</span><span class="ch">\r</span><span class="st">'</span>, <span class="st">' '</span>) <span class="cf">for</span> s <span class="kw">in</span> speeches]     </span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>    _, sentences <span class="op">=</span> bigram_process(speeches, n_process <span class="op">=</span> <span class="dv">1</span>)    </span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>    sentences <span class="op">=</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>.join(sentences)  </span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>    q.put(sentences)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Below, we use the above <code>get_sentences()</code> function to process the data in our <code>datasets</code> object, writing the results out to <code>speeches.txt</code>, with each sentence from each speech getting it‚Äôs own line in the file. It will take some time to run, but perhaps not as long as you would think given how much data we are working with here.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> Manager()</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> m.Queue()</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>mp_disk(datasets, get_sentences, <span class="st">'data/txt_files/can_hansard_speeches.txt'</span>, q)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let‚Äôs do a quick count to see how many words our dataset contains.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'data/txt_files/can_hansard_speeches.txt'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> <span class="bu">file</span>.read()</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> data.split()</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="bu">len</span>(words))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This file has roughly 180 million words after processing.</p>
<p>With our data re-organized in <code>speeches.txt</code>, we can iterate over the file to train a CBOW or Skipgram classification model, while using as little memory as possible. We will use a custom class that does the iteration for us, yielding one sentence at a time, which we can pass into <code>gensim.models.Word2Vec()</code>. Once again, you can expect this process to take some time but it‚Äôll be sped up by setting the <code>workers</code> parameter to the number of CPU cores you have.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>sentences <span class="op">=</span> IterSents(<span class="st">'data/txt_files/can_hansard_speeches.txt'</span>)</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> gensim.models.Word2Vec(sentences, size <span class="op">=</span> <span class="dv">300</span>, window <span class="op">=</span> <span class="dv">4</span>, <span class="bu">iter</span> <span class="op">=</span> <span class="dv">5</span>, </span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>                               sg <span class="op">=</span> <span class="dv">0</span>, min_count <span class="op">=</span> <span class="dv">10</span>, negative <span class="op">=</span> <span class="dv">5</span>, workers <span class="op">=</span> <span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And with that, we‚Äôve learned our embeddings from a dataset of roughly 180 million words! We don‚Äôt want to have to relearn these embeddings needlessly (who has time for that?), so we‚Äôll write the model vocabulary to a text file called <code>model_vocabulary.txt</code> and then save the model itself to disk. That way, we can reload our trained model, rather than wasting time and energy re-training it.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>vocabulary <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(model.wv.vocab))</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'../models/model_vocabulary.txt'</span>, <span class="st">'w'</span>) <span class="im">as</span> f:</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> v <span class="kw">in</span> vocabulary:</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>        f.write(v)</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>        f.write(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>model.save(<span class="st">'../models/word2vec.model'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The model can be reloaded anytime, and if we don‚Äôt have to update it anymore, we can keep just the word vectors themselves, which is a leaner object.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> gensim.models.Word2Vec.load(<span class="st">'../models/word2vec.model'</span>)</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.wv</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="comparing-embeddings" class="level2" data-number="33.8">
<h2 data-number="33.8" class="anchored" data-anchor-id="comparing-embeddings"><span class="header-section-number">33.8</span> COMPARING EMBEDDINGS</h2>
<p>Everything we‚Äôve done so far can also be done comparatively, which makes things much more interesting from a social scientific perspective. The trouble with these sorts of extensions is that the word embedding training process is stochastic, so we can‚Äôt just learn embeddings for various different datasets and directly compare them. In fact, there‚Äôs no guarantee that two models trained on the exact same data will end up looking even remotely similar! While the relations between the words in vector space may be more or less consistent in the two models (in the sense that the angle between them will be similar) the random starting positions of those words in that vector space can produce wildly differing final states. To do anything comparative, cross-sectional or over time, we need our vector spaces to be aligned.</p>
<p>There have been a number of solutions proposed to solve this problem <span class="citation" data-cites="ruder2019survey artetxe2016learning mogadala2016bilingual di2019training">(e.g., <a href="references.html#ref-ruder2019survey" role="doc-biblioref">Ruder, Vuliƒá, and S√∏gaard 2019</a>; <a href="references.html#ref-artetxe2016learning" role="doc-biblioref">Artetxe, Labaka, and Agirre 2016</a>; <a href="references.html#ref-mogadala2016bilingual" role="doc-biblioref">Mogadala and Rettinger 2016</a>; <a href="references.html#ref-di2019training" role="doc-biblioref">Di Carlo, Bianchi, and Palmonari 2019</a>)</span>, but we will focus on the ‚Äúcompass‚Äù approach developed by <span class="citation" data-cites="di2019training">Di Carlo, Bianchi, and Palmonari (<a href="references.html#ref-di2019training" role="doc-biblioref">2019</a>)</span> because it‚Äôs well-implemented, efficient, and has Gensim at its core. It‚Äôs designed with temporal data in mind, but we handle cross-sectional comparisons in the exact same way. Below, I‚Äôll walk you through training a word embedding model ‚Äúanchor‚Äù (the compass) as a basis for comparison, and then we‚Äôll spend a bit of time working through a few temporal and cross-sectional comparisons.</p>
<blockquote class="blockquote">
<p>The compass functionality is available in the python package TWEC, which must be installed manually from the source code provided on GitHub. As the authors of the package note, TWEC requires a customized version of Gensim, so it‚Äôs advisable to make a virtual environment specifically for working with this package. As a reminder, you can do so with the following steps, all from the command line:</p>
<ol type="1">
<li>Clone the GitHub repository at <a href="https://github.com/valedica/twec.git">https://github.com/valedica/twec.git</a></li>
<li>Create a new conda vitual environment with <code>conda create -n twec_training</code></li>
<li>Activate your new conda environment with <code>conda activate twec_training</code></li>
<li><code>pip install cython</code></li>
<li>The author‚Äôs custom version of Gensim, <code>pip install git+https://github.com/valedica/gensim.git</code></li>
<li>cd into the twec repository</li>
<li>pip install ‚Äìuser .</li>
</ol>
<p>If you end up having a lot of trouble getting TWEC up and running, you can use any version of Gensim to load the models that have been pre-trained for this chapter. You can read more about our pre-trained models in the online supplementary materials.</p>
</blockquote>
<section id="imports" class="level3" data-number="33.8.1">
<h3 data-number="33.8.1" class="anchored" data-anchor-id="imports"><span class="header-section-number">33.8.1</span> Imports</h3>
<p>Since we are working in a new virtual environment (details provided in Box XXX) with a fresh new Python kernel. We‚Äôll continue to work with the Canadian Hansard data.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> twec.twec <span class="im">import</span> TWEC</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models.word2vec <span class="im">import</span> Word2Vec</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dcss.utils <span class="im">import</span> list_files, mp_disk</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tok <span class="im">import</span> Tokenizer</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.utils <span class="im">import</span> simple_preprocess</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> multiprocessing <span class="im">import</span> Process, Manager</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="aligning-your-vector-spaces" class="level3" data-number="33.8.2">
<h3 data-number="33.8.2" class="anchored" data-anchor-id="aligning-your-vector-spaces"><span class="header-section-number">33.8.2</span> Aligning Your Vector Spaces!</h3>
<p>The general process of using the TWEC approach to train a series of embedding models <em>that are aligned from the start</em> is as follows:</p>
<ol type="1">
<li>Train a word2vec model on the <em>entire</em> dataset in one go, retaining the position layer of the neural network model. This layer is called the <strong>compass</strong>. It computes a set of baseline embeddings that a series of embedding models (trained in Step 2) trained on every subset of the data (temporal slices, for example) can use as a common starting point, like a kind of ‚Äúreference model.‚Äù</li>
<li>Train a word2vec model for each subset of the data using the compass layer from Step 1 as the starting point. This ensures the vector spaces are properly aligned and lets the vector coordinates move around according to the embeddings of words in that subset of data.</li>
</ol>
<p>Once the reference model has been trained, the series of contextual models trained in the next step call all be trained with a common starting point (as opposed to a random one). Then the embeddings for each subset diverge from that common origin as appropriate and the differences and similarities between their vectors can be interpreted as meaningful differences. <span class="citation" data-cites="di2019training">Di Carlo, Bianchi, and Palmonari (<a href="references.html#ref-di2019training" role="doc-biblioref">2019</a>)</span> provide plenty of technical details on how TWEC works, if you are interested in going beyond what I introduce here.</p>
<p>Let‚Äôs perform both steps. We‚Äôll use the compass trained in Step 1 for a series of temporal and cross-sectional comparsons later in the chapter.</p>
</section>
<section id="step-1-train-the-compass" class="level3" data-number="33.8.3">
<h3 data-number="33.8.3" class="anchored" data-anchor-id="step-1-train-the-compass"><span class="header-section-number">33.8.3</span> Step 1: Train the Compass</h3>
<p>To train the compass, <code>TWEC</code> expects a text file where each sentence in our dataset is provided on a new line. Since we prepared this exact file in the previous chapter, we‚Äôll reuse it here. It‚Äôs stored in <code>speeches.txt</code>.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>compass_path <span class="op">=</span> <span class="st">'data/txt_files/can_hansard_speeches.txt'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Because <code>TWEC</code> uses a custom version of Gensim, it doesn‚Äôt automatically receive the many updates that Gensim has had in recent years. One of the package dependencies has been updated since the <span class="citation" data-cites="di2019training">Di Carlo, Bianchi, and Palmonari (<a href="references.html#ref-di2019training" role="doc-biblioref">2019</a>)</span> paper was published and now produces a warning about a function that will eventually be deprecated. To keep things a bit cleaner, we‚Äôll tell Python to suppress those warnings.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now we can inialize a TWEC class object, providing the parameters to pass to Gensim for training (note that the <code>negative=</code> argument for negative sampling is replaced by <code>ns=</code> here). We‚Äôll use this object to create the compass and when training the aligned temporal slices.</p>
<p>The results are automatically saved to a <code>model/</code> folder in the current working directory. This process will take the same amount of time as it took to train the Word2Vec model above, so it‚Äôs best to set ‚Äúoverwrite‚Äù to <code>False</code> so we don‚Äôt accidentally lose all of that processing time. Remember to set the number of workers to the number of cores you want to use - most personal computers have 4 cores. If you ever need to pick things back up after a restart (or a kernel crash) running the cell again will simply reload the trained compass.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>aligner <span class="op">=</span> TWEC(size <span class="op">=</span> <span class="dv">300</span>, siter <span class="op">=</span> <span class="dv">5</span>, diter <span class="op">=</span> <span class="dv">5</span>, window <span class="op">=</span> <span class="dv">10</span>, sg <span class="op">=</span> <span class="dv">0</span>, min_count <span class="op">=</span> <span class="dv">10</span>, ns <span class="op">=</span> <span class="dv">5</span>, workers <span class="op">=</span> <span class="dv">4</span>)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>aligner.train_compass(compass_path, overwrite<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="step-2-train-a-series-of-aligned-embedding-models" class="level3" data-number="33.8.4">
<h3 data-number="33.8.4" class="anchored" data-anchor-id="step-2-train-a-series-of-aligned-embedding-models"><span class="header-section-number">33.8.4</span> Step 2: Train a Series of Aligned Embedding Models</h3>
<p>Now that our reference model has been trained and stored in the <code>aligner</code> object, we can proceed with training a series of embedding models on various subsets of our data. In the examples that follow, we will train a series of models to show change over time, followed by a series of models to compare speeches by different political parties. We will use the same <code>aligner</code> object as the reference model for both.</p>
<section id="research-on-cultural-change-with-temporal-embeddings" class="level4" data-number="33.8.4.1">
<h4 data-number="33.8.4.1" class="anchored" data-anchor-id="research-on-cultural-change-with-temporal-embeddings"><span class="header-section-number">33.8.4.1</span> Research on Cultural Change with Temporal Embeddings</h4>
<p>Regardless of whether our comparison is cross-sectional or temporal, we need to subset our data <em>prior</em> to training any additional models. Since we are starting using embeddings to compare change over time, let‚Äôs divide our data into different temporal slices. We‚Äôll be training a gemsim word2vec model with each subset, so we will prepare the data with one sentence-per-line file for model training.</p>
<p>In this case, the CSV files in the Canadian Hansard dataset are organized into folders by year. We can use that to our advantage here. First, we‚Äôll load up the CSV files and create some lists to store the file paths for each decade.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>datasets <span class="op">=</span> list_files(<span class="st">"data/canadian_hansards/lipad/"</span>, <span class="st">'csv'</span>)</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(datasets)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>canadian_1990s <span class="op">=</span> []</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>canadian_2000s <span class="op">=</span> []</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>canadian_2010s <span class="op">=</span> []</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1990</span>,<span class="dv">1999</span>):</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>    year_data <span class="op">=</span> <span class="st">'data/canadian_hansards/lipad/'</span> <span class="op">+</span> <span class="bu">str</span>(i) <span class="op">+</span> <span class="st">'/'</span></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>    datasets_1990s <span class="op">=</span> list_files(year_data, <span class="st">'csv'</span>)</span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>    canadian_1990s.extend(datasets_1990s)</span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2000</span>,<span class="dv">2009</span>):</span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a>    year_data <span class="op">=</span> <span class="st">'data/canadian_hansards/lipad/'</span> <span class="op">+</span> <span class="bu">str</span>(i) <span class="op">+</span> <span class="st">'/'</span></span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a>    datasets_2000s <span class="op">=</span> list_files(year_data, <span class="st">'csv'</span>)</span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a>    canadian_2000s.extend(datasets_2000s)</span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2010</span>,<span class="dv">2019</span>):</span>
<span id="cb55-16"><a href="#cb55-16" aria-hidden="true" tabindex="-1"></a>    year_data <span class="op">=</span> <span class="st">'data/canadian_hansards/lipad/'</span> <span class="op">+</span> <span class="bu">str</span>(i) <span class="op">+</span> <span class="st">'/'</span></span>
<span id="cb55-17"><a href="#cb55-17" aria-hidden="true" tabindex="-1"></a>    datasets_2010s <span class="op">=</span> list_files(year_data, <span class="st">'csv'</span>)</span>
<span id="cb55-18"><a href="#cb55-18" aria-hidden="true" tabindex="-1"></a>    canadian_2010s.extend(datasets_2010s)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now that we have our data organized into temporal slices, we need to create our sentence-per-line files. To do that with multiprocessing, we‚Äôll re-use the <code>get_sentences()</code> function we used in the previous chapter.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> Manager()</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> m.Queue()</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>mp_disk(canadian_1990s, get_sentences, <span class="st">'data/txt_files/1990s_speeches.txt'</span>, q)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> Manager()</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> m.Queue()</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>mp_disk(canadian_2000s, get_sentences, <span class="st">'data/txt_files/2000s_speeches.txt'</span>, q)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> Manager()</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> m.Queue()</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>mp_disk(canadian_2010s, get_sentences, <span class="st">'data/txt_files/2010s_speeches.txt'</span>, q)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Finally, we can train individual models on the slices using the <code>aligner</code> object. As you may have guessed, this can take a bit of time and you probably want to process each in its own cell, setting ‚Äúsave‚Äù to <code>True</code> so that the model will be output to the <code>model/</code> directory, with a filename matching the name of the text file provided.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>model_1990s <span class="op">=</span> aligner.train_slice(<span class="st">'data/txt_files/1990s_speeches.txt'</span>, save<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>model_2000s <span class="op">=</span> aligner.train_slice(<span class="st">'data/txt_files/2000s_speeches.txt'</span>, save<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>model_2010s <span class="op">=</span> aligner.train_slice(<span class="st">'data/txt_files/2010s_speeches.txt'</span>, save<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>At this point we don‚Äôt need the compass model anymore, but it‚Äôs a good idea to keep it around. The contextual models we‚Äôve trained for each temporal slice are good to go, and unlike the compass model, can simply be loaded into Gensim for analysis. Note that although we used <code>sg=0</code> above because Skip-gram takes a long time to train compared to CBOW, the models you can load below were trained with Skip-gram.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>model_1990s <span class="op">=</span> Word2Vec.load(<span class="st">'../models/1990s_speeches.model'</span>)</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>model_2000s <span class="op">=</span> Word2Vec.load(<span class="st">'../models/2000s_speeches.model'</span>)</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>model_2010s <span class="op">=</span> Word2Vec.load(<span class="st">'../models/2010s_speeches.model'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now that we‚Äôve trained our aligned temporal embedding models, we can do all kinds of interesting and useful things, such as comparing the embeddings of terms in different decades. As a simple example, let‚Äôs look at the most similar words to ‚Äòclimate_change‚Äô across each decade. We should expect to see tokens such as ‚Äòglobal_warming‚Äô showing up, <em>but that‚Äôs what we want</em>; our model (which doesn‚Äôt actually know what words mean) is doing what it‚Äôs supposed to do. Below we can see that the similarity between these terms starts to decline a bit in the 2010s, when ‚Äòclimate_change‚Äô became the preferred term.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>model_1990s.wv.most_similar(positive <span class="op">=</span> <span class="st">'climate_change'</span>, topn <span class="op">=</span> <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>model_2000s.wv.most_similar(positive <span class="op">=</span> <span class="st">'climate_change'</span>, topn <span class="op">=</span> <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>model_2010s.wv.most_similar(positive <span class="op">=</span> <span class="st">'climate_change'</span>, topn <span class="op">=</span> <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="cross-sectional-comparisons-political-parties-on-climate-change" class="level4" data-number="33.8.4.2">
<h4 data-number="33.8.4.2" class="anchored" data-anchor-id="cross-sectional-comparisons-political-parties-on-climate-change"><span class="header-section-number">33.8.4.2</span> Cross-sectional Comparisons: Political Parties on Climate Change</h4>
<p>Sometimes our research goals are to compare culture and meaning across subgroups in a population, rather than change over time. For example, continuing with the examples we‚Äôve used in this chapter so far, we might be more interested in comparing how different political parties talk about climate change than how political discussions of climate change have evolved over time.</p>
<p>To make those comparisons, we need to organize our data by political party rather than by decade. To keep things relatively simple, we‚Äôll focus on the three major political parties: the Liberals, the New Democratic Party, and the Conservatives, keeping in mind that the latter is a relatively recent merger of the former Canadian Alliance, Progressive Conservative, and Reform parties. In this case, slicing the data isn‚Äôt quite as straightforward, so we‚Äôll create a modified version of <code>get_sentences()</code> that will accept lists of terms to mask (filter) the dataframes with.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>liberal <span class="op">=</span> [<span class="st">'Liberal'</span>]</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>conservative <span class="op">=</span> [<span class="st">'Conservative'</span>, <span class="st">'Canadian Alliance'</span>, <span class="st">'Progressive Conservative'</span>, <span class="st">'Reform'</span>]</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>ndp <span class="op">=</span> [<span class="st">'New Democratic Party'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_sentences_by_party(dataset, filter_terms):</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>    dfs_unfiltered <span class="op">=</span> [pd.read_csv(df) <span class="cf">for</span> df <span class="kw">in</span> dataset]</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>    dfs <span class="op">=</span> []  </span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> df <span class="kw">in</span> dfs_unfiltered:</span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>        temp_df <span class="op">=</span> df.dropna(subset <span class="op">=</span> [<span class="st">'speakerparty'</span>])</span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> temp_df[<span class="st">'speakerparty'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="bu">any</span>(party <span class="cf">for</span> party <span class="kw">in</span> filter_terms <span class="cf">if</span> party <span class="kw">in</span> x))</span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>        temp_df2 <span class="op">=</span> temp_df[mask]</span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(temp_df2) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>            dfs.append(temp_df2)</span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a>    speeches <span class="op">=</span> []</span>
<span id="cb67-14"><a href="#cb67-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb67-15"><a href="#cb67-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> df <span class="kw">in</span> dfs:</span>
<span id="cb67-16"><a href="#cb67-16" aria-hidden="true" tabindex="-1"></a>        speeches.extend(df[<span class="st">'speechtext'</span>].tolist())</span>
<span id="cb67-17"><a href="#cb67-17" aria-hidden="true" tabindex="-1"></a>    speeches <span class="op">=</span> [<span class="bu">str</span>(s).replace(<span class="st">'</span><span class="ch">\n</span><span class="st">|</span><span class="ch">\r</span><span class="st">'</span>, <span class="st">' '</span>) <span class="cf">for</span> s <span class="kw">in</span> speeches]   <span class="co"># make sure everything is a lowercase string, remove newlines    </span></span>
<span id="cb67-18"><a href="#cb67-18" aria-hidden="true" tabindex="-1"></a>    _, sentences <span class="op">=</span> u.bigram_process(speeches)    </span>
<span id="cb67-19"><a href="#cb67-19" aria-hidden="true" tabindex="-1"></a>    sentences <span class="op">=</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>.join(sentences)  <span class="co"># join the batch of sentences with newlines into 1 string</span></span>
<span id="cb67-20"><a href="#cb67-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb67-21"><a href="#cb67-21" aria-hidden="true" tabindex="-1"></a>    q.put(sentences)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> Manager()</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> m.Queue()</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>mp_disk(datasets, get_sentences_by_party, <span class="st">'data/txt_files/liberal_speeches.txt'</span>, q, liberal)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> Manager()</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> m.Queue()</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>mp_disk(datasets, get_sentences_by_party, <span class="st">'data/txt_files/conservative_speeches.txt'</span>, q, conservative)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> Manager()</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> m.Queue()</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>mp_disk(datasets, get_sentences_by_party, <span class="st">'data/txt_files/ndp_speeches.txt'</span>, q, ndp)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Now we can train an aligned model for each of the three parties, using the same <code>aligner</code> object we used earlier (trained on the full corpus).</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>model_liberal <span class="op">=</span> aligner.train_slice(<span class="st">'data/txt_files/liberal_speeches.txt'</span>, save<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>model_conservative <span class="op">=</span> aligner.train_slice(<span class="st">'data/txt_files/conservative_speeches.txt'</span>, save<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>model_ndp <span class="op">=</span> aligner.train_slice(<span class="st">'data/txt_files/ndp_speeches.txt'</span>, save<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>With our three aligned models, we can now compare how each of the three major parties talk about climate change. Remember that this is for <em>all</em> party-specific talk from 1990 onwards. We <em>could</em> train more models to disaggregate things even further (e.g., each party in each decade), but we‚Äôll keep things simple here.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>model_liberal <span class="op">=</span> Word2Vec.load(<span class="st">'../models/liberal_speeches.model'</span>)</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>model_conservative <span class="op">=</span> Word2Vec.load(<span class="st">'../models/conservative_speeches.model'</span>)</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>model_ndp <span class="op">=</span> Word2Vec.load(<span class="st">'../models/ndp_speeches.model'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>model_liberal.wv.most_similar(positive <span class="op">=</span> <span class="st">'climate_change'</span>, topn <span class="op">=</span> <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>model_conservative.wv.most_similar(positive <span class="op">=</span> <span class="st">'climate_change'</span>, topn <span class="op">=</span> <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>model_ndp.wv.most_similar(positive <span class="op">=</span> <span class="st">'climate_change'</span>, topn <span class="op">=</span> <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Of course, everything we did previously with the pre-trained embeddings can be applied and generalized with the models we‚Äôve trained here. Give it a shot!</p>
<blockquote class="blockquote">
<p><strong>Further Reading</strong></p>
<p>Adji Dieng, Francisco Ruiz, and David Blei <span class="citation" data-cites="dieng2020topic">(<a href="references.html#ref-dieng2020topic" role="doc-biblioref">2020</a>)</span> have developed a really interesting probabilistic topic model that uses embeddings to represent text rather than the DTM representations used in LDA topic models. They also generalize this model for dynamic data in <span class="citation" data-cites="dieng2019dynamic">(<a href="references.html#ref-dieng2019dynamic" role="doc-biblioref">A. Dieng, Ruiz, and Blei 2019</a>)</span>. If you are interested in the relationship between topic models and word embeddings, I recommend reading their articles.</p>
</blockquote>
</section>
</section>
</section>
<section id="conclusion-1" class="level2" data-number="33.9">
<h2 data-number="33.9" class="anchored" data-anchor-id="conclusion-1"><span class="header-section-number">33.9</span> CONCLUSION</h2>
<section id="key-points-1" class="level3" data-number="33.9.1">
<h3 data-number="33.9.1" class="anchored" data-anchor-id="key-points-1"><span class="header-section-number">33.9.1</span> Key Points</h3>
<ul>
<li>Word embeddings represent words with short dense vectors that describe the word‚Äôs local semantic contexts</li>
<li>Embeddings as a whole depict patterns of word usage and language structure</li>
<li>They are NOT ‚Äúmeaning,‚Äù and we should not trust intuitions built on low-dimensional representations</li>
<li>Constructed embeddings and aligned embeddings using Gensim</li>
</ul>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-artetxe2016learning" class="csl-entry" role="listitem">
Artetxe, Mikel, Gorka Labaka, and Eneko Agirre. 2016. <span>‚ÄúLearning Principled Bilingual Mappings of Word Embeddings While Preserving Monolingual Invariance.‚Äù</span> In <em>Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>, 2289‚Äì94.
</div>
<div id="ref-bolukbasi2016man" class="csl-entry" role="listitem">
Bolukbasi, Tolga, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. 2016. <span>‚ÄúMan Is to Computer Programmer as Woman Is to Homemaker? Debiasing Word Embeddings.‚Äù</span> <em>arXiv Preprint arXiv:1607.06520</em>.
</div>
<div id="ref-bonikowski2017ethno" class="csl-entry" role="listitem">
Bonikowski, Bart. 2017. <span>‚ÄúEthno-Nationalist Populism and the Mobilization of Collective Resentment.‚Äù</span> <em>The British Journal of Sociology</em> 68: S181‚Äì213.
</div>
<div id="ref-brekhus2019oxford" class="csl-entry" role="listitem">
Brekhus, Wayne, and Gabe Ignatow. 2019. <em>The Oxford Handbook of Cognitive Sociology</em>. Oxford University Press.
</div>
<div id="ref-cerulo2002culture" class="csl-entry" role="listitem">
Cerulo, Karen. 2002. <em>Culture in Mind: Toward a Sociology of Culture and Cognition</em>. Psychology Press.
</div>
<div id="ref-cerulo2010mining" class="csl-entry" role="listitem">
Cerulo, Karen A. 2010. <span>‚ÄúMining the Intersections of Cognitive Sociology and Neuroscience.‚Äù</span> <em>Poetics</em> 38 (2): 115‚Äì32.
</div>
<div id="ref-collins2015intersectionality" class="csl-entry" role="listitem">
Collins, Patricia. 2015. <span>‚ÄúIntersectionality‚Äôs Definitional Dilemmas.‚Äù</span> <em>Annual Review of Sociology</em> 41: 1‚Äì20.
</div>
<div id="ref-collins2020intersectionality" class="csl-entry" role="listitem">
Collins, Patricia, and Sirma Bilge. 2020. <em>Intersectionality</em>. John Wiley &amp; Sons.
</div>
<div id="ref-crenshaw1989demarginalizing" class="csl-entry" role="listitem">
Crenshaw, Kimberl√©. 1989. <span>‚ÄúDemarginalizing the Intersection of Race and Sex: A Black Feminist Critique of Antidiscrimination Doctrine, Feminist Theory and Antiracist Politics.‚Äù</span> <em>U. Chi. Legal f.</em>, 139.
</div>
<div id="ref-crossley2010towards" class="csl-entry" role="listitem">
Crossley, Nick. 2010. <em>Towards Relational Sociology</em>. Routledge.
</div>
<div id="ref-di2019training" class="csl-entry" role="listitem">
Di Carlo, Valerio, Federico Bianchi, and Matteo Palmonari. 2019. <span>‚ÄúTraining Temporal Word Embeddings with a Compass.‚Äù</span> In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, 33:6326‚Äì34. 01.
</div>
<div id="ref-dieng2020topic" class="csl-entry" role="listitem">
Dieng, Adji B, Francisco JR Ruiz, and David M Blei. 2020. <span>‚ÄúTopic Modeling in Embedding Spaces.‚Äù</span> <em>Transactions of the Association for Computational Linguistics</em> 8: 439‚Äì53.
</div>
<div id="ref-dieng2019dynamic" class="csl-entry" role="listitem">
Dieng, Adji, Francisco Ruiz, and David Blei. 2019. <span>‚ÄúThe Dynamic Embedded Topic Model.‚Äù</span> <em>arXiv Preprint arXiv:1907.05545</em>.
</div>
<div id="ref-dimaggio1997culture" class="csl-entry" role="listitem">
DiMaggio, Paul. 1997. <span>‚ÄúCulture and Cognition.‚Äù</span> <em>Annual Review of Sociology</em> 23 (1): 263‚Äì87.
</div>
<div id="ref-edelmann2018formal" class="csl-entry" role="listitem">
Edelmann, Achim, and John Mohr. 2018. <span>‚ÄúFormal Studies of Culture: Issues, Challenges, and Current Trends.‚Äù</span> <em>Poetics</em> 68: 1‚Äì9.
</div>
<div id="ref-elbourne2011meaning" class="csl-entry" role="listitem">
Elbourne, Paul. 2011. <em>Meaning: A Slim Guide to Semantics</em>. Oxford University Press.
</div>
<div id="ref-emirbayer1997manifesto" class="csl-entry" role="listitem">
Emirbayer, Mustafa. 1997. <span>‚ÄúManifesto for a Relational Sociology.‚Äù</span> <em>American Journal of Sociology</em> 103 (2): 281‚Äì317.
</div>
<div id="ref-firth1957synopsis" class="csl-entry" role="listitem">
Firth, John. 1957. <span>‚ÄúA Synopsis of Linguistic Theory, 1930-1955.‚Äù</span> <em>Studies in Linguistic Analysis</em>.
</div>
<div id="ref-franzosi2004words" class="csl-entry" role="listitem">
Franzosi, Roberto. 2004. <em>From Words to Numbers: Narrative, Data, and Social Science</em>. Cambridge University Press.
</div>
<div id="ref-garg2018word" class="csl-entry" role="listitem">
Garg, Nikhil, Londa Schiebinger, Dan Jurafsky, and James Zou. 2018. <span>‚ÄúWord Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes.‚Äù</span> <em>Proceedings of the National Academy of Sciences</em> 115 (16): E3635‚Äì44.
</div>
<div id="ref-harris1954distributionalstructure" class="csl-entry" role="listitem">
Harris, Zellig S. 1954. <span>‚ÄúDistributional Structure.‚Äù</span> <em>Word</em> 10 (2-3): 146‚Äì62.
</div>
<div id="ref-ignatow2009culture" class="csl-entry" role="listitem">
Ignatow, Gabriel. 2009. <span>‚ÄúCulture and Embodied Cognition: Moral Discourses in Internet Support Groups for Overeaters.‚Äù</span> <em>Social Forces</em> 88 (2): 643‚Äì69.
</div>
<div id="ref-jones2020stereotypical" class="csl-entry" role="listitem">
Jones, Jason, Mohammad Ruhul Amin, Jessica Kim, and Steven Skiena. 2020. <span>‚ÄúStereotypical Gender Associations in Language Have Decreased over Time.‚Äù</span> <em>Sociological Science</em> 7: 1‚Äì35.
</div>
<div id="ref-joos1950description" class="csl-entry" role="listitem">
Joos, Martin. 1950. <span>‚ÄúDescription of Language Design.‚Äù</span> <em>The Journal of the Acoustical Society of America</em> 22 (6): 701‚Äì7.
</div>
<div id="ref-jurafsky2000speech" class="csl-entry" role="listitem">
Jurafsky, Dan, and Martin Hand. 2009. <em>Speech &amp; Language Processing</em>. 2nd ed. Pearson Prentice Hall.
</div>
<div id="ref-kozlowski2019geometry" class="csl-entry" role="listitem">
Kozlowski, Austin, Matt Taddy, and James Evans. 2019. <span>‚ÄúThe Geometry of Culture: Analyzing the Meanings of Class Through Word Embeddings.‚Äù</span> <em>American Sociological Review</em> 84 (5): 905‚Äì49.
</div>
<div id="ref-linzhuo2020social" class="csl-entry" role="listitem">
Linzhuo, Li, Wu Lingfei, and Evans James. 2020. <span>‚ÄúSocial Centralization and Semantic Collapse: Hyperbolic Embeddings of Networks and Text.‚Äù</span> <em>Poetics</em> 78: 101428.
</div>
<div id="ref-lizardo2019can" class="csl-entry" role="listitem">
Lizardo, Omar, Brandon Sepulvado, Dustin S Stoltz, and Marshall A Taylor. 2019. <span>‚ÄúWhat Can Cognitive Neuroscience Do for Cultural Sociology?‚Äù</span> <em>American Journal of Cultural Sociology</em>, 1‚Äì26.
</div>
<div id="ref-mclevey2021embeddings" class="csl-entry" role="listitem">
McLevey, John, Tyler Crick, Browne Pierson, and Darrin Durant. 2021. <span>‚ÄúWord Embeddings and the Structural and Cultural Dimensions of Democracy and Autocracy, 1900-2020.‚Äù</span> <em>Canadian Review of Sociology</em> 7: 544‚Äì69.
</div>
<div id="ref-mikolov2013efficient" class="csl-entry" role="listitem">
Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. <span>‚ÄúEfficient Estimation of Word Representations in Vector Space.‚Äù</span> <em>arXiv Preprint arXiv:1301.3781</em>.
</div>
<div id="ref-mikolov2013distributed" class="csl-entry" role="listitem">
Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. <span>‚ÄúDistributed Representations of Words and Phrases and Their Compositionality.‚Äù</span> In <em>Advances in Neural Information Processing Systems</em>, 3111‚Äì19.
</div>
<div id="ref-mikolov2013linguistic" class="csl-entry" role="listitem">
Mikolov, Tom√°≈°, Wen-tau Yih, and Geoffrey Zweig. 2013. <span>‚ÄúLinguistic Regularities in Continuous Space Word Representations.‚Äù</span> In <em>Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, 746‚Äì51.
</div>
<div id="ref-mische2011relational" class="csl-entry" role="listitem">
Mische, Ann. 2011. <span>‚ÄúRelational Sociology, Culture, and Agency.‚Äù</span> <em>The SAGE Handbook of Social Network Analysis</em>, 80‚Äì97.
</div>
<div id="ref-mogadala2016bilingual" class="csl-entry" role="listitem">
Mogadala, Aditya, and Achim Rettinger. 2016. <span>‚ÄúBilingual Word Embeddings from Parallel and Non-Parallel Corpora for Cross-Language Text Classification.‚Äù</span> In <em>Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, 692‚Äì702.
</div>
<div id="ref-mohr1998measuring" class="csl-entry" role="listitem">
Mohr, John. 1998. <span>‚ÄúMeasuring Meaning Structures.‚Äù</span> <em>Annual Review of Sociology</em> 24 (1): 345‚Äì70.
</div>
<div id="ref-mohr2020measuring" class="csl-entry" role="listitem">
Mohr, John, Christopher Bail, Margaret Frye, Jennifer Lena, Omar Lizardo, Terence McDonnell, Ann Mische, Iddo Tavory, and Frederick Wherry. 2020. <em>Measuring Culture</em>. Columbia University Press.
</div>
<div id="ref-mohr2013introduction" class="csl-entry" role="listitem">
Mohr, John, and Petko Bogdanov. 2013. <span>‚ÄúIntroduction‚ÄîTopic Models: What They Are and Why They Matter.‚Äù</span> Elsevier.
</div>
<div id="ref-mohr2015toward" class="csl-entry" role="listitem">
Mohr, John, Robin Wagner-Pacifici, and Ronald Breiger. 2015. <span>‚ÄúToward a Computational Hermeneutics.‚Äù</span> <em>Big Data &amp; Society</em> 2 (2): 2053951715613809.
</div>
<div id="ref-neblo2018politics" class="csl-entry" role="listitem">
Neblo, Michael, Kevin Esterling, and David Lazer. 2018. <em>Politics with the People: Building a Directly Representative Democracy</em>. Vol. 555. Cambridge University Press.
</div>
<div id="ref-nelson2021leveraging" class="csl-entry" role="listitem">
Nelson, Laura. 2021. <span>‚ÄúLeveraging the Alignment Between Machine Learning and Intersectionality: Using Word Embeddings to Measure Intersectional Experiences of the Nineteenth Century US South.‚Äù</span> <em>Poetics</em>, 101539.
</div>
<div id="ref-nissim2020fair" class="csl-entry" role="listitem">
Nissim, Malvina, Rik van Noord, and Rob van der Goot. 2020. <span>‚ÄúFair Is Better Than Sensational: Man Is to Doctor as Woman Is to Doctor.‚Äù</span> <em>Computational Linguistics</em> 46 (2): 487‚Äì97.
</div>
<div id="ref-nivre2017universal" class="csl-entry" role="listitem">
Nivre, Joakim, and Chiao-Ting Fang. 2017. <span>‚ÄúUniversal Dependency Evaluation.‚Äù</span> In <em>Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</em>, 86‚Äì95.
</div>
<div id="ref-papakyriakopoulos2020bias" class="csl-entry" role="listitem">
Papakyriakopoulos, Orestis, Simon Hegelich, Juan Carlos Medina Serrano, and Fabienne Marco. 2020. <span>‚ÄúBias in Word Embeddings.‚Äù</span> In <em>Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</em>, 446‚Äì57.
</div>
<div id="ref-rheault2020word" class="csl-entry" role="listitem">
Rheault, Ludovic, and Christopher Cochrane. 2020. <span>‚ÄúWord Embeddings for the Analysis of Ideological Placement in Parliamentary Corpora.‚Äù</span> <em>Political Analysis</em> 28 (1): 112‚Äì33.
</div>
<div id="ref-ruder2019survey" class="csl-entry" role="listitem">
Ruder, Sebastian, Ivan Vuliƒá, and Anders S√∏gaard. 2019. <span>‚ÄúA Survey of Cross-Lingual Word Embedding Models.‚Äù</span> <em>Journal of Artificial Intelligence Research</em> 65: 569‚Äì631.
</div>
<div id="ref-stoltz2019concept" class="csl-entry" role="listitem">
Stoltz, Dustin, and Marshall Taylor. 2019. <span>‚ÄúConcept Mover‚Äôs Distance: Measuring Concept Engagement via Word Embeddings in Texts.‚Äù</span> <em>Journal of Computational Social Science</em> 2 (2): 293‚Äì313.
</div>
<div id="ref-taylor2020concept" class="csl-entry" role="listitem">
Taylor, Marshall, and Dustin Stoltz. 2020. <span>‚ÄúConcept Class Analysis: A Method for Identifying Cultural Schemas in Texts.‚Äù</span> <em>Sociological Science</em> 7: 544‚Äì69.
</div>
<div id="ref-vasiliev2020natural" class="csl-entry" role="listitem">
Vasiliev, Yuli. 2020. <em>Natural Language Processing with Python and SpaCy: A Practical Introduction</em>. No Starch Press.
</div>
<div id="ref-white1992identity" class="csl-entry" role="listitem">
White, Harrison. 1992. <em>Identity and Control: How Social Formations Emerge</em>. Princeton university press.
</div>
<div id="ref-anscombe1953philosophical" class="csl-entry" role="listitem">
Wittgenstein, Ludwig. 1953. <span>‚ÄúPhilosophical Investigations: The English Text of the Third Edition.‚Äù</span>
</div>
</div>
</section>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>At the time of writing, SpaCy provides these models for English, German, Spanish, Portuguese, French, Italian, Dutch, Norwegian, and Lithuanian. It is also capable of processing multilingual documents and tokenization for over 50 languages to allow model training. In the rest of this chapter and those that follow, we will use English-language models.<a href="#fnref1" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn2"><p>Context and environment can be used interchangeably in this case. (See what I did there?) For the sake of consistency, I will use the word context.<a href="#fnref2" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./artificial-neural-networks-fnn-rnn-cnn.html" class="pagination-link" aria-label="Artificial neural networks 101">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Artificial neural networks 101</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./transformer-revolution.html" class="pagination-link" aria-label="The transformer revolution">
        <span class="nav-page-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">The transformer revolution</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/UWNETLAB/dcss_supplementary/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>