# Text similarity and latent semantic space

## LEARNING OBJECTIVES

- Compute and interpret TF-IDF weights for terms in a corpus
- Represent documents in latent semantic space using matrix decomposition methods, more specifically Latent Semantic Analysis (LSA) via truncated Singular Value Decomposition (SVD)
- Compute the similarity between pairs of documents using cosine similarity

## LEARNING MATERIALS

You can find the online learning materials for this chapter in `doing_computational_social_science/Chapter_13`. `cd` into the directory and launch your Jupyter Server.

## INTRODUCTION

The previous chapters focused on (*i*) using SpaCy and gensim to process natural language data stored in the form of unstructured text, (*ii*) considered how various different types of text processing and modelling fit together into larger pipelines, and (*iii*) discussed the differences between two ways of creating quantitative representations of text data: coding (or "labelling" / "annotation") and count-based feature extraction. In this chapter, I will show how to use sklearn to construct feature matrices with term counts or TF-IDF weights, followed by a discussion of some descriptive and exploratory methods of text analysis. In particular, I'll emphasize the difference between high-level patterns of language use that we can observe directly (e.g., words used, not used), and latent patterns that we can't observe directly. You will learn how to explore "latent semantic space" using a method called Singular Value Decomposition (SVD), which is closely related to the latent variable and dimensionality reduction methods introduced in Chapter 9. 

### Package Imports


```python
import pickle
from pprint import pprint

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import spacy
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.preprocessing import Normalizer

from dcss import set_style
from dcss.text import bigram_process, bow_to_df, get_topic_word_scores, preprocess
from dcss.utils import sparse_groupby

set_style()

nlp = spacy.load('en_core_web_sm', disable=['ner'])
```

### TF-IDF Vectorization

When analyzing *content*, we are rarely interested in the most and least frequent words, as the former tend to be domain- or group-specific stop words and the latter are too rare. As discussed in Chapter 11, the main benefit of using TF-IDF is that it preserves *all* tokens (words) in the corpus, but decreases the weights of tokens that are at the exremes of the frequency distribution. 

When we call `TfidfVectorizer` instead of `CountVectorizer`, the values assigned to each token (i.e. features) are TF-IDF scores rather than binary presence / absence or frequency counts. Similar to the example in Chapter 12, we can use this vectorizer to produce a DTM. (Alternatively, we could use sklearn's `TfidfTransformer()` to convert the count-based DTM from Chapter 12 to TF-IDF.) The shape of the resulting matrix would be identical to before, but only because we are passing the exact same arguments to the vectorizer, which is deterministic. If parameters were different, we would obtain different results.


```python
with open ('data/british_hansard_processed_sample.pkl', 'rb') as fp:
    preprocessed = pickle.load(fp)
```


```python
tfidf_vectorizer = TfidfVectorizer(
    max_df=.1,
    min_df=3,
    strip_accents='ascii'
)

tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed) 
tfidf_matrix.shape
```


To help clarify the differences between the count data and the TF-IDF scores, let's construct a dataframe with the counts from the previous chapter and the above TF-IDF scores for each token across all documents. Given that the TF-IDF vectorized documents here have the same matrix shape as the count vectorized ones -- so, `num_documents x num_features` -- it follows that we have the same documents and features here. This means the vocabulary is identical and we can use the same one for both matrixes!


```python
with open ('data/british_hansard_sample_dtm.pkl', 'rb') as fp:
    count_matrix = pickle.load(fp)

tfidf_scores = np.ravel(tfidf_matrix.sum(0))
tfidf_scores = tfidf_scores/np.linalg.norm(tfidf_scores)
term_counts = np.ravel(count_matrix.sum(0))
term_counts = term_counts/np.linalg.norm(term_counts)
vocabulary = tfidf_vectorizer.get_feature_names_out()


df = pd.DataFrame({'Term': vocabulary, 'TFIDF': tfidf_scores, 'Count': term_counts})
df.sort_values(by='TFIDF', ascending=False, inplace=True)
```

The code below creates a scatterplot showing each token in the corpus by count and TF-IDF. The result is @fig-12_01.

```python
sns.jointplot(data=df.head(5000), x='Count', y='TFIDF', kind='hist')
plt.savefig('figures/12_01.png', dpi=300)
```

![png](figures/12_01.png){#fig-12_01}
    
When you inspect this plot, you should notice that:

1. most tokens in the vocabulary are used very rarely, and so the marginal distribution of counts is skewed towards low values,
2. most tokens in the vocabulary have relatively low TF-IDF scores,
3. the tokens with high count values almost always have low TF-IDF values, and 
4. the tokens with high TF-IDF scores tend to have lower counts.

If you understand the TF-IDF formula, this should make intuitive sense. If it doesn't, I recommend reviewing the formula, as you don't want to proceed with a text analysis that relies on a token scoring method that you don't understand. 

To visualize the relationship between counts and TF-IDF weights, we used two matrices (`count_matrix` and `tfidf_matrix`) with the same shape. The reason why those two matrices have the same shape is because we passed the same arguments to both vectorizers. But actually, *we should not really be using the `min_df` and `max_df` arguments with `TfidfVectorizer`*. The reason is because TF-IDF assigns very low scores to the tokens at the top and bottom of the frequency distribution, so removing them is unhelpful and can change the actual scores that are computed. Before continuing to analyze our dataset with tokens weighted by TF-IDF, let's construct a final TF-IDF DTM without the `min_df` and `max_df` arguments.


```python
tfidf_vectorizer = TfidfVectorizer(strip_accents='ascii', sublinear_tf=True)

tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed)
tfidf_matrix.shape
```


### Computing Semantic Similarity and Clustering Documents

In the previous chapter, I mentioned that each feature and each document has an associated vector of numbers. The documents, or row vectors, assign a specific value to the document for each feature in the DTM. The features, or column vectors, tell you how a specific feature is distributed across documents. Because each document is represented by a vector, this approach is also known as a **vector space model**; the documents are represented by 'long and sparse' vectors that position them *in relation to one another in multi-dimensional vector space*. This makes it relatively simple to assess the semantic similarity between documents using measures of the distance or similarity between their vectors representations. 

Perhaps the most basic of these measures is **Euclidean distance**, which is a flat measure of the distance between two points in a Euclidean space like you find in classical geometry. This measure works fine when you just want to compare the literal text contained in documents, as in plagiarism detection software. For that purpose, the importance of individual tokens in a document matters less than the degree to which two documents have similar tokens that appear with similar frequency. This is a pretty reliable measure of how similar the text strings are between documents.

Euclidean distance has some limitations when it comes to measuring semantic similarity, however, and especially when we are working with term counts rather than TF-IDF weights. This is because Euclidean distance tends to overestimate the importance of tokens that appear frequently, as they increase the magnitude of the vectors in space. For example, imagine two documents about natural language processing in some larger corpus. If the term "language_processing" appears 100 times in the first document about that topic but only once in the other, then there's a good chance the the Euclidean distance between these two documents will be large, suggesting (incorrectly) that they are about totally different topics! This overestimation is most pronounced when comparing texts of different lengths, as longer documents will of course have higher token counts. One benefit of using similarity and distance-based measures to compare vectors of TF-IDF scores is that the vectors themselves take these document length differences into account. 

Unlike Euclidean distance, **cosine similarity** compares the angle between two vectors; whereas Euclidean distance measures how far the vector has extended into space in a given direction, cosine distance considers only the direction the vector is headed in. To paraphrase Gregory Bateson, it's the difference that makes a difference. The result is a document similarity score that better compares the two documents in terms of their conceptual/abstract similarity, rather than their physical make-up. Let's see what this looks like by comparing the cosine similarity between speeches by MPs from different parties using the `tfidf_matrix` produced above.

We'll use the same `sparse_groupby` function we used in the previous chapter to aggregate the TF-IDF scores into a dataframe where each row is a vector for the entire party.

```python
# with open ('data/british_hansard_processed_sample.pkl', 'rb') as fp:
#     speech_df = pickle.load(fp)
    
speech_df = pd.read_csv('data/sampled_british_hansard_speeches.csv')

party_names = speech_df['party']
tfidf_vocabulary = tfidf_vectorizer.get_feature_names_out()
party_scores = sparse_groupby(party_names, tfidf_matrix, tfidf_vocabulary)
```


```python
len(party_names)
```


Because we've aggregated the TF-IDF scores by summing them, we should normalize them again to unit norm. We can use the `Normalizer()` preprocessing utility from sklearn to handle the math for us here. The main benefit of doing it this way is that the sklearn code is highly optimized (it's actually running C code in the background, which is super efficient) and operates on the whole matrix at once. 


```python
normalize = Normalizer()
party_scores_n = normalize.fit_transform(party_scores)
```

Now that we've normalized the matrix, we'll compute the cosine similarity between each pair of vectors. The maths are beyond the scope of this chapter, but what you need to do to compute the cosine similarity between political parties here is to compute the product of our rectangular party-by-feature and a transpose of that same matrix. The result with be a square "self-to-self" cosine similarity matrix. In the code below, the `@` symbol is used to compute the product of two matrices.


```python
sim_matrix = party_scores_n @ party_scores_n.T
sim_df = pd.DataFrame.sparse.from_spmatrix(sim_matrix).sparse.to_dense()
```

The top-left to bottom-right diagonal will always be 1 in a self-to-self cosine similarity matrix because the diagonal reports how similar each entity (in this case, political party) is to itself. Perfect similarity every time! You might also notice that the data below the diagonal is mirrored above the diagonal. We can use Numpy to clean it up a bit for us by filling the diagonal and one of the triangles (above or below the diagonal, it doesn't matter which) with `np.nan`. If we use the `.values` attribute for Pandas dataframes, we can use Numpy array functions directly without doing any conversions from Pandas to Numpy.


```python
np.fill_diagonal(sim_df.values, np.nan)
sim_df.values[np.tril_indices(sim_df.shape[0], -1)] = np.nan
```

Now let's add in the party names as the index and column names for our fresh, shiny, new cosine similarity matrix. 


```python
sim_df.index = party_scores.index
sim_df.columns = party_scores.index
```

With a matrix this size, it's possible to eyeball what's going on, but when you have a lot of comparisons to make it can be handy to write a bit of code to show you the highlights. For example, we might want to print the 3 most similar and the 3 least similar party pairings for each party. We can do this by using Pandas' `.stack()` method to flatten the dataframe dimensions so that `.nlargest()` and `.nsmallest()` return results for the entire matrix rather than row by row.


```python
print(sim_df.stack().nlargest(3))
```


```python
print(sim_df.stack().nsmallest(3))
```


We can see that Labour and Labour (Co-op) have very high similarity, and that both have similarities with the Liberal Democrats (who from time to time have had pacts with Labour). All three of these parties are considered left-of-centre. On the other hand, the Green Party and Plaid Cymru are also considered left-leaning, but Plaid Cymru is a Welsh nationalist party seeking independence from the UK, so we should expect to see that they differ from the other parties despite being social democratic. The Democratic Unionist Party is a right-leaning socially conservative party in Ireland, so their lower similarity to the other two parties also makes some sense. 

We know that there are similarities between the *content* of what Labour and Lib Dem MPs have focused on in their speeches, and that Plaid Cymru and the Democratic Unionist Party differ from the others. One way to gain a bit of insight into these comparisons is to look at the tokens that are most strongly associated with each party. Below, we'll print the 10 most associated tokens for each of the four parties. Note that these will differ a bit from the scores we previously computed because we are working with TF-IDF scores, not counts. 


```python
party_scores_df = pd.DataFrame.sparse.from_spmatrix(party_scores_n)
party_scores_df.index = party_scores.index
party_scores_df.columns = tfidf_vectorizer.get_feature_names_out()

for party in ['Labour','Liberal Democrat', 'Democratic Unionist Party', 'Plaid Cymru']:
    print(party + '\n')
    print(party_scores_df.loc[party].nlargest(10))
    print('\n')
```

The highest scoring tokens for Labour and the Liberal Democrats are not particularly noteworthy in this case. The Irish and Welsh parties, on the other hand, have very high scores for the terms that refer to their respective countries. Remember that TF-IDF scores terms highly if they appear frequently in a given document but don't appear in many documents in the corpus. The high scores in this case may indicate that these parties often refer to their home countries in parliament or that the rest of the parties don't talk about them much - it's likely some combination of these two factors. 

While cosine similarity performed on token count (e.g., `count__matrix` from the `CountVectorizer()`) and TF-IDF weights (e.g., `tfidf_matrix` from the `TfidfVectorizer()`) does a better job of measuring meaningful similarity between documents, it still relies on exact term matches to calculate the spatial distances. This is a significant limitation of using long sparse vector representations. In later chapters, we will discuss short dense vector representations called word embeddings that allow us to go beyond identical token matches to compare the semantic similarity of tokens and documents that are *conceptually* close. Using these short and dense word embeddings in similarity analysis means that seemingly identical words with different meanings can be differentiated based on their usage contexts, while other words that are not identical can be considered *conceptually* close. For now, we'll move onto another set of exploratory text analysis methods: Latent Semantic Analysis, or LSA. 

## EXPLORING LATENT SEMANTIC SPACE WITH MATRIX DECOMPOSITION

So far, we've discussed how to represent unstructured text data as long and sparse vectors. These vectors are stored as structured matrices, where the rows are documents, the columns are tokens, and the cells are either Boolean (a token is present or absent), frequencies, or TF-IDF weights. You saw how we can use token frequencies and proportions to do some preliminary comparisons of language-use across document collections, and how to perform some simple semantic similarity comparisons. While useful, there are some limitations in using these methods to learn about the actual topical content of the documents in our corpus. 

Remember, our DTMs are made up of *long and sparse vectors*. One of the first substantial breakthroughs in contemporary topic modelling was the realization that one could use matrix decomposition methods (i.e., dimensionality reduction) to construct a set of latent variables that could be used to position documents in relation to one another in **latent semantic space**. These latent variables could be interpreted as **latent topics** or **concepts** that are more abstract than the individual tokens that make them up. 

These are the same type of latent variable and dimensionality reduction methods that you learned about in Chapter 9, only applied to matrices that represent the text content of a document collection. Remember that latent variables are variables that exist but which cannot be directly observed or measured. Instead, we use dimensionality reduction methods to infer them from the lower-level features that *can* be observed and measured. In the case of text analysis, the observed and measured low-level features are specific tokens. We can interpret the latent variables several ways, but generally, we refer to them as *latent topics*. 

In Chapter 9 you learned about dimensionality reduction methods with an emphasis on Principal Component Analysis (PCA). As a brief reminder, PCA and other dimensionality reduction methods reduce the number of features in a dataset by combining highly correlated, or covarying, features into principal components. These principal components represent *latent* dimensions of a dataset, always at a higher level of abstraction than the original features. When we are performing dimensionality reduction on text data, we often used a method called **truncated Singular Value Decomposition (SVD)** rather than PCA. SVD and PCA are very similar, but SVD is an extension that can be used for non-square matrices (recall that PCA starts by converting data to a square matrix, e.g., correlation) and it handles sparse matrices efficiently. When SVD is used in the context of text analysis and the vector space model, it is called **Latent Semantic Analysis (LSA)**.

### Latent Semantic Analysis (LSA) with Singular Value Decomposition (SVD)

Singular Value Decomposition is a dimensionality reduction method comparable to PCA introduced in Chapter 9. Other than relatively small differences in how PCA and SVD decompose matrices, the salient difference between the latent variable analyses here and from Chapter 9 is *interpretation*. When the original features are individual tokens from the vocabulary, the latent components that are produced via the linear combination of highly-correlated features are *interpreted as topics*. This is just another latent variable problem where we attempt to learn about the latent variables by decomposing matrices using methods from linear algebra. 

We won't get deep into the mathematics here, but it important that you have at least a conceptual understanding of how SVD works. It all starts with a feature matrix, which in a text analysis like this will typically be a DTM. In this example, we will use our `tfidf_matrix` DTM. 

SVD decomposes the DTM into three smaller matrices, each of which contains essential information that can be used to reconstruct the original matrix: 

- $U$, which is a matrix with documents in the rows and latent topics in the columns,
- $S$, which is a diagonal matrix of singular values indicating the importance of each topic, and
- $V$, which is a matrix with latent topics in the rows and tokens from the vocabulary in the columns.

The columns in matrix $U$ are orthogonal to one another, and the rows in matrix $V$ are orthogonal to one another. When you multiply these three matrices, you get a matrix that is extremely close, or approximately equivalent, to the original matrix (i.e., our DTM). This is represented in the equation:

$$
\text{DTM} \thickapprox U \cdot S \cdot V
$$

Figure @fig-13_02 below further clarifies the relationships between these three matrices. 

![](figures/svd.pdf)

We can use these three matrices to interpret the latent topics in our dataset. We can use $V$ to learn about the tokens most strongly associated with each latent topic, enabling us to interpret what that latent topic represents. We can use $S$ to learn roughly how important the topic is relative to the other topics. Finally, we can use $U$ to better understand how the discovered topics are distributed across the documents in our corpus.

As with PCA, when we perform an SVD we will get back a number of latent components equal to the number of features in the original matrix, and those components will be sorted such that the ones explaining the most variance come first, and those explaining the least amount of variance come last. We are almost never interested in using all of the topics, so we usually select some subset of the latent components to interpret. This is called **truncated SVD**, and is the approach implemented in sklearn. This means we have to tell sklearn in advance how many components we want. 

#### LSA via SVD in sklearn

To conduct an LSA analysis with sklearn, we first initialize a `TruncatedSVD()` object and indicate the number of latent topics we want by using the `n_components` argument. In this case, we will set the number of components to work with to 20. 


```python
lsa = TruncatedSVD(n_components=100, n_iter=6, random_state=12)
```

Now we can fit it to our `tfidf_matrix` (or `count_matrix`, for that matter) to actually execute the LSA.


```python
lsa = lsa.fit(tfidf_matrix)
```

As previously mentioned, the singular values (the diagonal matrix $S$) summarize the relative importance of each of the latent dimensions. We can access these values from the `.singular_values_` attribute of the fitted `lsa` model object. Plotting them gives us a quick view of how important each latent dimension is. Let's look at the top 20 singular values. 


```python
svs = lsa.singular_values_[:20]
svs
```

Each dimensions contains a *little bit* of pretty much every term in the vocabulary. When you are interpreting the meaning of the dimensions, what you want to look for is the terms that have the highest values. 

To make this a bit easier, we can transpose the `lsa.components_` matrix (`V` Figure @fig-13_02) to create a dataframe where the rows are terms in the vocabulary and each column represents one of the latent dimensions. The score in each cell tells you how strongly associated that word is for the given topic.


```python
# transpose the dataframe so WORDS are in the rows
word_topics = pd.DataFrame(lsa.components_).T 
column_names = [f'Topic {c}' for c in np.arange(1,101,1)]
word_topics.columns = column_names

word_topics.shape
```


Let's get a list of the tokens in the vocabulary and use them as an index for our dataframe. 


```python
terms = tfidf_vectorizer.get_feature_names_out()
word_topics.index = terms

word_topics.sort_values(by='Topic 2', ascending = False)['Topic 2'].head(20)
```

Now we can easily use `.loc[]` on our dataframe to get the scores for any specific word across all latent topics. To get the topic scores for England, we would pull the row vector for the row indexed with `england`. Since the result is simply a Pandas Series, we can sort it to print the topics in order of most to least strongly associated. Note that we have to make-do with `wale` rather than `wales` because the word has been lemmatized during pre-processing. Using named entity recognition, which you will learn about later chapters, you can ensure that this doesn't happen during pre-processing. 


```python
compare_df = pd.DataFrame()

compare_terms = ['england', 'scotland', 'wale', 'ireland']

for i, term in enumerate(compare_terms):
    scores = word_topics.loc[term].sort_values(ascending=False)
    compare_df[i] = scores.index
    compare_df[term] = scores.values
```


```python
compare_df.head()
```

Note that for many terms (including `england` and `ireland`) the requested terms are not strongly loaded for any particular theme. This is different for `scotland` and `wale`, however. This suggests that there may be a topic here that is focused on issues related to Ireland, but perhaps not for Scotland and Wales. Now, it turns out this is a bit tricky, so let's think things through for a moment. Perhaps most importantly, we need to understand what these loadings (weights) tell us. When looking at a given topic, the loading of a word between -1 and 1 is the contribution it makes to the composition of that latent topic. A score closer to 1 means the *presence* of that word contributes, while a score closer to -1 means the *absence* of that word contributes to the definition of the topic. Scores around 0 have very little impact. In LSA, words are considered in relation to the words they appear with, so a focal word might only indicate a certain topic if some other word isn't in the document. 

An example of an ideal outcome can be helpful here: if your focal word was `escape` and it appeared in the same document as `backslash`, you could assume the topic of the document was related to computers or programming. If instead the word `prison` appeared in the document, it would suggest the topic of the document was a prison escape. So for the latent topic of "computing", the word `prison` could end up fairly negatively loaded, as it distinguishes between the two different uses of `escape`. LSA is capable of distinguishing between different uses of the same word, but it's important to put some thought into what the negative loadings mean in relation to the positive ones. 

In both of these cases, we can find out what topics a given word is most associated with, but since there is no guarantee that the word we are interested in is actually important (or even really relevant) to the topic, this is not an especially helpful way to discover what the topic is about. Instead, if we want to know what words are most strongly associated with each topic, we can pull the top (and bottom!) scoring words for each. 

To do so, we can use the utility function `get_topic_word_scores()` from the dcss package. One of the arguments is `all_topics`. By default, this argument is set to `False`, which means the function will return data on the top $n$ words and their scores for the specified topic. If changed to `True`, the function returns a full dataframe with all the other topics alongside the topic of interest. The word scores for these other topics tell you the relationship that the top words for the topic of interest have across the other topics, so it is important to interpret this properly. Let's explore the topic that's most relevant to "scotland".

```python
word_topics.loc['scotland'].sort_values(ascending=False)
```

```python
get_topic_word_scores(word_topics, 10, 'Topic 8')    
```

So with LSA, we can have a topic that is significantly "latent" that we have to decipher. A key topic related to Scotland is about business and school but is also distinguished by *not* being about crime and police. It's important to know that computational methods are improving all the time, so earlier ones may have revealed very solid topics but without telling us what they are. In this case, we might have to examine why Scotland, business, police, and school would be a topic of British parliamentary debate. Thankfully, new methods are being developed all of the time, for market goals rather than social science ones, so we'll explore those in later chapters. Before moving on, let's briefly take stock of what we've done here. First, we constructed a document term-matrix using sklearn's `TfidfVectorizer()`. Then we decomposed the DTM with truncated SVD, which produced a matrix with the component coefficients for each of the 67,204 sampled political speeches on 100 latent dimensions, which we can interpret as representing *latent topics*. The final step is to interpret the results by inspecting the terms that contribute the most to each latent dimension. 

  
> **Further Reading**    
>   
> If you want to deepen your understanding of latent semantic analysis, and what it was originally developed to do, I would suggest reading papers by some of the major contributors to the methodology. I recommend @dumais2004latent and @deerwester1990indexing. This work is an important foundation for some of the machine learning models used for text analysis that we discuss later in the book.
> 


## CONCLUSION

### Key Points 

- Used SpaCy's TfidfVectorizer to compute TF-IDF scores for weighting tokens
- Vector space models represent documents using vectors that are long (many features) and sparse (few non-zero values)
- Learned about semantic similarity measures including Euclidean Distance and Cosine similarity
- Conducted a Latent Semantic Analysis (LSA) using Singular Value Decomposition (SVD)

     
\part{Fundamentals of Network Analysis}
