# ~~Sequential~~ iterative modelling

<!-- a key idea I want to develop here is that models are our **instruments** for detecting what we can't see (latent constructs, etc.) just like GW and other things in science -->

::: {.callout-warning}
## In Progress

This is a "[shitty first draft](https://www.amazon.ca/Bird-Some-Instructions-Writing-Life/dp/0385480016)," with some sections being shittier than others. Normally I wouldn't share work so early in it's development, but I'm interested in your feedback. Feel free to skim, but know that I am actively developing this chapter and it will be going through some very extensive changes in the coming weeks.
:::
<!-- 
## Sequential modelling

Sequential modeling in **quantitative sociology** refers to a process where models are built and analyzed in stages, with each step incorporating additional variables, complexity, or theories in a systematic way. This approach allows researchers to investigate how different factors contribute to an outcome incrementally, helping to disentangle the relationships between variables and assess the robustness of findings as more layers of information are added. It's a common technique in sociology, particularly in **survey research**, where researchers aim to understand social behaviors, attitudes, or outcomes through the lens of various interacting factors.

### How Sequential Modeling Works:
1. **Initial Model (Baseline Model)**: Typically, a simple model is built first, including only key independent variables (e.g., demographic variables like age, gender, education).
   
2. **Adding Layers**: In subsequent steps, additional variables (e.g., social or economic factors) are added in a sequence, often based on theory or prior research. Each step reveals how the introduction of these new variables changes the coefficients and relationships found in the initial model.

3. **Assessment and Comparison**: At each stage, the new model is compared to the previous one to assess how the additional variables impact the model’s explanatory power (e.g., using measures like adjusted R² or AIC/BIC in regression models).

4. **Interpreting Changes**: The focus is not just on the final model but on understanding the changes in relationships between variables as new layers of complexity are introduced. For example, a variable that is significant in the initial model may lose its significance once another variable is added, suggesting that the initial relationship was spurious or confounded.

### Connection to Paul Lazarsfeld and Columbia University:

Sequential modeling has deep roots in the **survey research tradition** pioneered by Paul Lazarsfeld and his colleagues at Columbia University in the mid-20th century. Lazarsfeld, a leading figure in quantitative sociology, developed several methods for analyzing survey data that emphasized careful, stepwise modeling and interpretation.

Lazarsfeld and his collaborators were known for developing **sequential or stepwise approaches** to modeling that aimed to uncover causal pathways in social behavior, especially using **path analysis** and **latent variable modeling** techniques. They were particularly interested in how different variables mediated or moderated relationships between social phenomena, such as how media exposure influenced voting behavior or how social class affected educational outcomes.

This tradition, especially in the **Columbia School of sociology**, emphasized:
- **Incremental Analysis**: Lazarsfeld's approach involved testing hypotheses in stages, starting with basic models and progressively adding more nuanced variables to see how they influenced the relationships observed. This mirrors the contemporary idea of sequential modeling.
- **Mediation and Moderation**: Lazarsfeld's work frequently examined how intermediary variables (mediators) or external factors (moderators) altered the direct effects observed in survey research. This layered modeling approach is aligned with the modern practice of sequentially adding variables to better understand complex causal relationships.
- **Survey Research Tradition**: Sequential modeling in sociology is closely tied to **survey data analysis**, a method that Lazarsfeld and his colleagues pioneered. Surveys provided rich data that could be analyzed in layers, allowing sociologists to isolate the impact of various social forces on individual behavior.

### Example of Sequential Modeling in Sociology:
Consider a study on educational attainment. A sequential modeling approach might work like this:

1. **Model 1**: Start with a model including only basic demographic variables (age, gender, race).
2. **Model 2**: Add socio-economic status (SES) variables like parental education and income to see how much they contribute to explaining differences in educational attainment beyond demographics.
3. **Model 3**: Introduce school-related factors (school quality, peer effects) to see if the impact of SES diminishes, suggesting that these school-related factors mediate the effect of SES on educational outcomes.
4. **Model 4**: Include neighborhood characteristics or individual motivations to further unpack how these additional variables explain differences in outcomes.

The results from each step provide insights into the different layers of influence on educational attainment, helping to disentangle whether SES exerts a direct effect or whether its impact is channeled through other factors like school quality or individual motivation.

### Summary:
Sequential modeling is a systematic, layered approach to building and interpreting models, which allows sociologists to analyze how different factors influence an outcome in stages. Its roots can be traced back to the survey research tradition of Paul Lazarsfeld at Columbia University, where stepwise modeling helped illuminate complex causal relationships in social behavior. This method remains a powerful tool in quantitative sociology for disentangling the effects of multiple, interacting social factors.

> how does this compare to contemporary workflows, especially iterarative bayesian approaches?

Contemporary workflows, particularly in **iterative Bayesian approaches**, share similarities with traditional **sequential modeling** but diverge in significant ways, particularly in how uncertainty is handled and how models are updated in a more dynamic, flexible manner. Let’s explore the key differences and similarities between **sequential modeling** in the classical (frequentist) tradition, rooted in the work of Lazarsfeld and the Columbia University school of sociology, and more modern **Bayesian workflows**.

### Similarities between Sequential Modeling and Bayesian Approaches

1. **Incremental Model Building**:
   - Both sequential modeling and Bayesian workflows emphasize **building models step by step**, incorporating variables progressively.
   - In both approaches, the goal is to understand how each set of variables (e.g., demographic, socio-economic, psychological) affects the outcome, and how adding more information refines the relationships between these variables.

2. **Mediation and Moderation**:
   - The emphasis on **mediators and moderators** in sequential modeling mirrors what happens in Bayesian workflows when you assess how including latent variables or hierarchical structures refines your understanding of causal mechanisms.
   - In both frameworks, there is an interest in how adding more complexity—whether via additional variables or hierarchical structures—changes the relationships and pathways observed.

3. **Interpretation of Changes**:
   - In both methods, understanding how models evolve as you add layers is crucial. You’re not only interested in the final model but in how relationships change, coefficients shift, or whether variables lose significance once others are included.
   - Bayesian approaches, like sequential modeling, often involve iteratively revising models as new data or variables are introduced.

### Key Differences between Sequential Modeling and Bayesian Workflows

1. **Handling of Uncertainty**:
   - **Frequentist Sequential Modeling**: In classical sequential modeling, each model step generates a fixed estimate for coefficients and standard errors. Uncertainty is typically represented in terms of confidence intervals or p-values, and model comparisons often rely on point estimates and goodness-of-fit statistics like AIC/BIC or adjusted R².
   - **Bayesian Iterative Modeling**: In contrast, Bayesian approaches fully incorporate **uncertainty at every stage** of the modeling process. Rather than producing fixed parameter estimates, Bayesian models generate **posterior distributions** for each parameter, reflecting not just the best guess (mean or median of the posterior) but also the uncertainty around it. This allows for a more nuanced understanding of how much we “know” about the relationships in the model, based on the data and prior knowledge.
   
2. **Priors and Incorporation of Existing Knowledge**:
   - **Frequentist Sequential Models**: In traditional approaches, no prior information is used unless explicitly modeled (e.g., in mixed-effects models). You rely entirely on the data to estimate parameters, and there is no direct mechanism for incorporating previous knowledge.
   - **Bayesian Approaches**: Bayesian workflows allow for the **inclusion of prior knowledge** through the use of **priors**. Each model iteration can refine these priors based on new data, progressively updating beliefs. In a Bayesian framework, you can explicitly model your uncertainty before even adding data, and adjust it dynamically as more data or model layers are added.

3. **Model Comparison and Convergence**:
   - **Frequentist Sequential Models**: Sequential modeling often involves formal hypothesis testing between steps (e.g., nested model tests, likelihood ratio tests) or the comparison of AIC/BIC values to determine which model fits best. These comparisons are static, in the sense that you test one model at a time and move forward with it.
   - **Bayesian Models**: In Bayesian workflows, model comparison is typically done via **posterior predictive checks**, **WAIC**, or **LOO-CV** (Leave-One-Out Cross-Validation), which offer more dynamic ways to assess how well a model predicts new data. Bayesian models are often more flexible and iterative in that you can continue to refine models, incorporating more complexity as new data or insights arise, without discarding the uncertainty of previous models.

4. **Incorporating Hierarchies and Multilevel Structures**:
   - **Sequential Models**: While classical approaches can incorporate multilevel or hierarchical models, they often do so as a fixed structure once decided (e.g., a mixed-effects model). Hierarchies are incorporated once the basic relationships have been modeled, and they are often added toward the end of the modeling process.
   - **Bayesian Hierarchical Models**: In Bayesian workflows, **hierarchical modeling** is more naturally integrated from the start. Hierarchical structures can be part of the initial model specification, and Bayesian methods excel at modeling **partial pooling**, which allows for more flexible handling of group-level effects. Bayesian methods also handle uncertainty in these group-level estimates in ways that frequentist methods cannot.

5. **Flexibility and Iteration**:
   - **Frequentist Sequential Models**: Once a model is estimated and fitted, there is often a point at which no further modification is made. Models are compared at specific checkpoints, but past this point, you don’t revisit prior steps or include new evidence without re-estimating models from scratch.
   - **Bayesian Workflows**: Bayesian modeling is inherently **iterative**. Models are constantly refined based on new data, and it’s easier to revisit previous steps, incorporate new knowledge, and even compare models with the same or different priors to see how robust the results are. This flexibility allows for a more dynamic workflow, as you can continually refine models as new data, theory, or even model diagnostics evolve.

6. **Philosophy of Inference**:
   - **Frequentist Sequential Models**: The sequential modeling approach often relies on **null hypothesis testing** at each stage. The goal is to assess whether the additional variables provide statistically significant improvements or changes.
   - **Bayesian Models**: Bayesian approaches, on the other hand, focus on **probabilistic inference** rather than hypothesis testing. The emphasis is on **estimating credible intervals** and the **posterior distributions** of parameters rather than testing a null hypothesis at each stage. In this framework, every stage of the model contributes to a clearer picture of the data, without the rigid framework of significance testing.

### Example: Education and Income

Let’s say we’re interested in modeling the effect of **education** on **income**, using additional covariates like **age** and **gender**. 

#### In a Frequentist Sequential Model:
1. **Model 1**: Start with education as the sole predictor of income.
2. **Model 2**: Add gender to control for gender differences in income.
3. **Model 3**: Add age to control for generational effects.
4. **Model Comparison**: Use AIC or p-values to assess whether the model fit improves with each added variable.

#### In a Bayesian Workflow:
1. **Model 1**: Start with education, with priors reflecting previous knowledge of education’s effect on income.
2. **Model 2**: Add gender, updating the posterior distributions from Model 1, and refining prior beliefs if necessary.
3. **Model 3**: Add age, incorporating uncertainty at every step. 
4. **Model Comparison**: Use WAIC or LOO-CV to evaluate model fit, while retaining the flexibility to refine priors and revisit earlier steps.

In this Bayesian example, you maintain a more dynamic process, where you constantly update beliefs based on the evidence from the data, rather than locking in each stage of the model with hypothesis tests or fixed point estimates.

### Conclusion

**Sequential modeling** in the Lazarsfeld tradition provides a powerful framework for building models incrementally, particularly in survey-based research. However, contemporary **iterative Bayesian workflows** take this a step further by incorporating uncertainty at each stage, allowing for flexible updates of prior knowledge, and focusing on probabilistic inference. Both approaches share the core principle of systematically building complexity into models, but Bayesian methods offer more nuanced handling of uncertainty and a dynamic, iterative process that is well-suited to the complexities of modern data analysis.


## Iteration

Harry Collins' *Gravity’s Shadow: The Search for Gravitational Waves* provides a detailed ethnographic account of the decades-long process that led to one of the most significant scientific discoveries of the 21st century. The story of gravitational waves offers a powerful analogy to latent variable modeling in the social sciences, especially when we focus on the iterative nature of both endeavors and the role of instruments in refining our understanding of unseen phenomena.

Iterative Work: Learning and Refining through Constant Revision

Collins’ Gravity’s Shadow emphasizes the iterative nature of scientific work. The detection of gravitational waves was not a one-time event, nor was it a simple matter of waiting for better instruments to arrive. It was the result of decades of trial and error, refinement, and revision. Collins meticulously documents the process by which scientists developed increasingly sensitive instruments, created models to interpret their data, and collaborated across institutions and disciplines to reach the point of detection.

One of the central themes of Gravity’s Shadow is how scientists continually revised their models and instruments based on new findings. Each new iteration brought them closer to their goal, but none was considered final. Even after the construction of LIGO (the Laser Interferometer Gravitational-Wave Observatory), the project did not immediately lead to a discovery. Instead, it involved years of calibration, data collection, and refinement of both instruments and the models used to interpret the data. Scientists had to deal with noise, ambiguity, and the possibility of false detections, all of which demanded constant revision and refinement of both their physical instruments and their conceptual models of gravitational waves.

This iterative process is strikingly similar to the way Richard McElreath describes model-building in the social sciences. McElreath advocates for an iterative approach to statistical modeling: we build a model, test it, refine it based on the evidence, and repeat. Just as gravitational wave scientists couldn’t expect LIGO to detect waves on the first try, social scientists can’t expect their first model to perfectly capture the latent constructs they’re studying. The process is one of continual refinement—improving our models, testing their fit with data, and revising them as new evidence and insights emerge.

Example: The Iterative Development of LIGO

One of the most powerful examples from Gravity’s Shadow is the iterative development of LIGO itself. Collins documents how early versions of the interferometer faced significant challenges—there was too much noise in the data, the sensitivity wasn’t high enough, and there were countless external factors (like seismic activity) that had to be accounted for. Each iteration of LIGO improved upon the previous one, with more advanced technology, better calibration methods, and more precise models of what gravitational wave signals would look like.

In the social sciences, latent variable models undergo a similar process of iteration. When studying constructs like populism or class structures, our first models are often too simplistic. They fail to account for important factors, or they overfit the data. Like the early versions of LIGO, these models don’t work perfectly, but they provide a foundation to build upon. We refine them based on new data, adjust our assumptions, and incorporate new variables that better capture the complexity of the latent construct. The iterative process is essential to both fields—whether we’re detecting waves in space-time or estimating abstract social constructs, progress comes from constant refinement.

Latent Variable Models as Telescopes: Better Instruments for Understanding

Building on the iterative theme, let’s take the analogy of latent variable models as instruments—or even as telescopes—further. Just as LIGO and other telescopes allowed scientists to “see” previously unobservable phenomena, latent variable models are the social scientist’s instruments for making abstract concepts visible.

In Gravity’s Shadow, Collins highlights how the development of better instruments was key to the detection of gravitational waves. LIGO didn’t detect gravitational waves on its first day of operation—scientists had to design and continually improve a device that was sensitive enough to pick up the incredibly faint ripples in space-time. Each iteration of LIGO improved its ability to detect these waves, much like how the development of more powerful telescopes allowed us to observe distant galaxies that were once invisible to the human eye.

Similarly, in the social sciences, latent constructs like populism, belief systems, or social class are not directly observable. We can’t “see” populism in the same way we can observe a physical object. Instead, we build models that act as telescopes—tools that allow us to make these hidden constructs visible through their indirect manifestations in observable variables. Survey responses, voting behavior, political rhetoric—these are the “signals” that we observe, but they are only proxies for the latent construct we’re trying to understand. Just as LIGO detects the effects of gravitational waves on space-time, latent variable models detect the effects of latent constructs on the data we can observe.

Example: Detecting Populism through Latent Variable Models

Let’s take an example from the social sciences: the latent construct of populism. Much like gravitational waves, populism doesn’t have a material form—it’s an abstract concept that exists as a set of beliefs, attitudes, and behaviors. We can’t directly observe populism in the same way we can observe a physical event. Instead, we detect its presence through indirect indicators: speech patterns in political leaders, voting behavior in certain segments of the population, or responses to survey questions about trust in elites and institutions.

A latent variable model is our equivalent of LIGO in this case. It allows us to detect the presence of populism by modeling the relationships between the observable indicators (survey responses, speeches, voting patterns) and the latent construct itself. Just as LIGO had to filter out noise and refine its detection methods, our latent variable models need to account for measurement error, confounding variables, and ambiguity in the data.

As we refine these models, much like the iterative development of LIGO, we improve our ability to “see” populism more clearly. Each new dataset, each new refinement of the model, helps us to sharpen the lens through which we view this abstract construct, much like how each new version of LIGO allowed scientists to detect ever fainter gravitational waves.

Iteration as a Core Theme: From Gravitational Waves to Social Constructs

The iterative process described in Gravity’s Shadow resonates deeply with the way McElreath and other modelers approach latent variable modeling. Iteration is at the heart of both scientific discovery and model-building. In both fields, we begin with imperfect models or instruments, refine them based on new data and insights, and move incrementally closer to uncovering the truth about latent phenomena.

This iterative process has two key components:

1. Model-building and refinement: In both the physical sciences and social sciences, our first models are rarely correct. Collins describes how gravitational wave scientists continually refined their models of what a gravitational wave signal would look like, just as social scientists refine their latent variable models based on new data and theoretical insights.
2. Validation through indirect evidence: Whether it’s gravitational waves or populism, we rely on indirect evidence to validate our models. In Collins’ case, scientists used complex simulations and comparisons with existing physical theories to validate their models of gravitational waves. In social sciences, we use simulation, fit indices, and external validation (like cross-study comparisons) to ensure that our latent variable models are capturing the underlying construct we’re interested in.

Where This Fits: An Epistemological Bridge

This section would fit well in a chapter on latent variable modeling and scientific inference—it could serve as a philosophical bridge between discussions of measurement and validity and the iterative nature of scientific work. By connecting the iterative process in both gravitational wave detection and latent variable modeling, you can emphasize the theme of gradual, methodical improvement in our understanding of unseen constructs.

This could also be an introduction to a more detailed discussion on latent variable modeling techniques. By positioning latent variable models as instruments for detecting the unseen, you provide a compelling motivation for the use of these techniques, linking back to the larger philosophical discussion about how we build and refine our scientific models over time.

Conclusion: Instruments for the Invisible

Both gravitational wave detection and latent variable modeling demonstrate that scientific discovery is an iterative process—we start with imperfect models and instruments, revise them based on new data, and move closer to understanding phenomena that are invisible to direct observation. Just as LIGO allowed physicists to “see” gravitational waves, latent variable models allow social scientists to detect abstract constructs like populism or belief systems.

By positioning latent variable models as the social science equivalent of telescopes or instruments, we underscore their importance in refining our understanding of the world. And, just as the detection of gravitational waves required constant iteration and refinement, so too does the process of detecting latent constructs in the social sciences. This iterative, instrument-based approach helps us navigate the complexities of unseen phenomena, whether they exist in the fabric of space-time or in the abstract structures of human society.
 -->