# Latent structure in networks

Full rewrite (basically new) chapter coming in fall 2024.

<!-- - **TODO**: The old NSBM content is appended to the end. Merge these, etc. -->

<!-- Nested Stochastic Blockmodels -->

> Fold in relevant content from _generative-network-analysis-with-bayesian-stochastic-blockmodels.qmd
<!-- 
# Nested Stochastic Blockmodels with Graph-Tool (Enron Email Network)

In this tutorial, you'll learn how to use `graph-tool` to iteratively fit, improve, and compare Nested Stochastic Blockmodels (NSBMs) by analyzing a directed email communication network between Enron employees. You also learn how to conduct rigorous posterior inference about the network from a generative modelling perspective.

The models we'll develop here start simple and gradually increase in complexity, incorporating additional information about the Enron network, or applying refinements to better estimate it's structure. We will visualize, assess, and compare these models, and finally, analyze the posterior distribution of block partitions to quantify uncertainty, compute marginal probabilities for node block assignments, and determine if there are other plausible explanations for the structure of our observed network.

This will be a bit of a modelling marathon, so if all of this is new to you, *focus on high-level logic*. Whether this is new to you or not, it will take a while for this content to really sink it. Give it time, take breaks, and come back to this material as often as you need to.

Below, we'll use `graph-tool` to:

- iteratively fit, improve, and compare Nested Stochastic Blockmodels by analyzing a directed email communication network between Enron employees
- conduct rigorous posterior inference about the network from a generative modelling perspective

## Imports and Setup

```python
import math
import pickle
import random
from pprint import pprint

import graph_tool.all as gt
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.metrics import homogeneity_score

import icsspy
from icsspy.networks import (
    get_consensus_partition_from_posterior,
    plot_line_comparison,
)
from icsspy.paths import enron


icsspy.set_style()
print(f'Using graph-tool version {gt.__version__}')
```


## Load the Enron Data

We can load the Enron email data [@crick2022enron] from the `icsspy` course package. The network itself has already been prepared and can be loaded directly into `graph-tool`.


```python
enron_email_network = str(enron / 'enron_graph.gt')
g = gt.load_graph(enron_email_network)
print(g)
```


Like the political blogs network, this network has internal property maps containing data about node and edge attributes, as well as the graph itself. We can list the available property maps:

```python
g.list_properties()
```

- `label` is a string variable containing the email address.
- `position` is a string variable containing information about the job position associated with the email account.
- `edge weight` are counts of the number of emails that vertex $i$ sent vertex $j$ (since this is a directed network).


## Prepare for Modelling

Since we'll be creating a series of identical visualizations for each model, let's define a simple visualization function so we can avoid repeating ourselves. [â˜‚ï¸Ž](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)

```python
def draw_state(state, g, hvprops, heprops, filename=None):
    state.draw(
        vertex_text = g.vp['position'],
        hvprops=hvprops,
        heprops=heprops,
        vertex_size=5,
        output_size=(1200, 1200),
        bg_color=[1, 1, 1, 1],
        output=filename,
)
```



We'll also initialize a couple of empty dictionaries that we'll use to collect information about our models once they've been fit. The first will store each model's Minimum Description Length (MDL), the second will store homogeneity scores.

```python
model_mdl_scores = {}
model_homogeneity_scores = {}
```

## Model 1: A Nested SBM with Binary Edges

As before, let's start with a simple nested SBM and visualization. We'll use `gt.minimize_nested_blockmodel_dl()` with the default parameters, which is the same default nested SBM we fit in our analysis of the political blogs networks in the previous notebook. This model will select the best fitting posterior partition, where "best fitting" means the posterior partition with the shortest Minimum Description Length (MDL).

Since we have not passed an argument for edge weights, the model will treat this network as binary; the only information we are using to estimate the latent hierarchical block structure of this network is the presence or absence of edges.

```python
model_1 = gt.minimize_nested_blockmodel_dl(g)
```

Now that we've fit the model, let's visualize the results.


## Model 1: A Nested SBM with Binary Edges

```python
hvprops = {"fill_color": "black", "size": 30}
heprops = {"color": "black", "pen_width": 2}
draw_state(state=model_1, g=g, hvprops=hvprops, heprops=heprops)
```

This visualization conveys a lot of information about the estimated network structure. Each node's job title is printed on the node (although you'll really have to squint to read it). You may notice that some blocks contain clusters of folks with the same or similar job titles. This makes some intuitive sense, especially when considered in terms of **positional network analysis** and the concept of **stochastic equivalence**. It's plausible, likely even, that the formal roles people hold in the organization generate different relational patterns, however noisy, and that these patterns would be picked up by the blockmodel.

## Model 1: A Nested SBM with Binary Edges

To get a better sense of how homogeneous blocks are with respect to job titles, we can get the position labels from our internal property map `position` and compute a homogeneity score for the combination of job positions and block membership. To do so, we'll iterate over the vertices in the graph and collect the information we need from the appropriate property maps.

```python
m1_l0_prop_map = model_1.levels[0].b # block assignment property map

vertex_data = []
for v in g.vertices():
    vertex_data.append((int(v), g.vp.position[v], m1_l0_prop_map[v]))

vertex_data = pd.DataFrame(vertex_data)
vertex_data.columns = ["VertexID", "JobPosition", "M1BlockID"]
```

## Model 1: A Nested SBM with Binary Edges

```python
vertex_data.head(10)
```

## Model 1: A Nested SBM with Binary Edges

We'll use `sklearn`'s homogeneity score to calculate the uncertainty (via entropy) of the job positions within each block ID. The closer the score is to 1, the more homogeneous the blocks are with respect to job titles.


```python
m1_homogeneity = homogeneity_score(
    vertex_data['JobPosition'], vertex_data['M1BlockID']
)

model_homogeneity_scores['Model 1'] = round(float(m1_homogeneity), 4)
pprint(model_homogeneity_scores)
```

Remember, this is not supervised learning! We are not *trying* to group nodes based on their job titles, we are just hypothesizing that at least some of the network structure is driven by the formal roles people in the network hold. In that sense, the score doesn't tell us anything about how good or bad our model is, but once we've fit a few more models we can compare homogeneity scores to see whether some models result in more homogeneous blocks than others, and whether patterns show up across models.


## Model 2: A Nested SBM with Edge Weights

We fit Model 1 using binary edges. In Model 2, we'll complicate things a wee bit by including edge weights, which are counts. Before we model our edges, let's get an initial sense of how they are distributed by plotting them as an [ECDF](https://en.wikipedia.org/wiki/Empirical_distribution_function) (basically a cumulative histogram).

```python
sns.ecdfplot(g.ep.edge_weight.a)
plt.xlabel("\nNo. of emails sent from $i$ to $j$")
plt.ylabel("Proportion of edges\n")
plt.title("EDCF of edge weights\nin the Enron email network\n", loc="left")
plt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{int(x):,}')) # comma format the x-axis
plt.savefig('output/enron_edge_weight_ecdf.png', dpi=300)
```

## Model 2: A Nested SBM with Edge Weights

In the SBM framework, we can include edge weights as covariates in our generative model and assume that they are sampled from some probability distribution conditioned on the block partitions (i.e., a **prior distribution**).

Given that this data is over-dispersed (i.e., the variance is greater than the mean), a [Negative Binomial distribution](https://en.wikipedia.org/wiki/Negative_binomial_distribution) would be an appropriate choice to model these edges, but as of fall 2024 [it is not implemented in `graph-tool`](https://graph-tool.skewed.de/static/doc/demos/inference/inference.html#edge-weights-and-covariates). While we can extend the distributions available in `graph-tool` by applying transformations,^[For example, we could approximate the Negative Binomial distribution by using the Poisson distribution with a Gamma-distributed rate parameter. One way to do this would be to sample a rate parameter $\lambda$ from a Gamma distribution for each edge, use that parameter to sample the edge weight from the Poisson distribution, and then fit the SBM using the Poisson-distributed edge weights.] we'll keep things relatively simple here by modelling the edges with a [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution). This should still provide a reasonable approximation, though it may not do the best job of characterizing the heavy tail of the distribution. Be mindful of this model limitation!

## Model 2: A Nested SBM with Edge Weights

We'll use the `minimize_nested_blockmodel_dl()` function again to fit Model 2, but this time we'll add an argument called `state_args`. `state_args` contains two important pieces of information:

- the edge weight property map, passed to `recs`, and
- the probability distribution we want to use to model the edge weights, passed to `rec_types`.

```python
model_2 = gt.minimize_nested_blockmodel_dl(
    g,
    state_args = dict(
        recs=[g.ep.edge_weight],
        rec_types = ['discrete-poisson']
    )
)
```


As we did with Model 1, let's plot the network's hierarchical block structure (using the same `hvprops` and `heprops` customization for our `draw_state()` function).

```python
draw_state(model_2, g, hvprops, heprops)
```

We'll also add the new block assignments to the `vertex_data` dataframe we created above.

```python
m2_l0_prop_map = model_2.levels[0].b  # block assignment property map

m2_vertex_data = {}
for v in g.vertices():
    m2_vertex_data[int(v)] = m2_l0_prop_map[v]

vertex_data['M2BlockID'] = vertex_data['VertexID'].map(m2_vertex_data)
```

Remember that the integer block IDs are just labels; they aren't meaningful in any numerical sense. The labels that get assigned depend on the model itself, so it doesn't mean anything if a new model puts a node in a block with a different integer label. It may be the case that the node has been assigned to a different block, but the labels do not tell us if that is the case.


#### MDL and Homogeneity

Visually (@fig-enron_model_2) we can already see that the inclusion of edge weights has changed the model's best estimate of the latent block structure. What did it do the MDL?

```python
m2_mdl = model_2.entropy()
model_mdl_scores["Model 2"] = int(round(m2_mdl))
pprint(model_mdl_scores)
```

The MDL score is much higher! However, *this is to be expected*, as the jump from a binary to a weighted network requires a huge increase in bits to adequately compress the network. We'll see how this model compares to the other weighted models we'll fit below. 

MDL is Always Larger for Models with Edge Weights and Other Covariates

Recall that MDL is computed by compressing the model *and* the data. Models with more parameters will always have larger MDLs, and models with edge weights always have more parameters than binary networks. Weighted networks contain more information than binary networks, and therefore require more bits to compress.

Although MDL is a model-agnostic way of comparing model fits, this doesn't mean that the models using binary edges are to be preferred over weighted networks. We want to strike a good balance between having simple parsimonious models on the one hand and models that are complex enough to adequately explain our data on the other hand. This will often mean that, given the choice, we'll want to a select models with the lowest MDL given a set of models that sufficiently capture our network, and it may be that models of binary networks are not considered sufficient when compared to their weighted counterparts. This is a matter of judgement.

## Model 2: A Nested SBM with Edge Weights

Let's plot the MDLs for the models we've fit so far for easy comparison (@fig-model_comparison_mdl_M1_M2). We'll use the `plot_line_comparison()` function from the `icsspy` course package.

```python
plot_line_comparison(
    models_dict=model_mdl_scores,
    title="Model Comparison\nMinimum Description Length (MDL)\n",
    xlabel="\nMDL",
    padding=5_000,
    filename = 'output/model_comparison_mdl_M1_M2.png')
```


## Model 2: A Nested SBM with Edge Weights

What about the homogeneity of job titles within blocks?

```python
m2_homogeneity = homogeneity_score(
    vertex_data['JobPosition'], vertex_data['M2BlockID']
)

model_homogeneity_scores['Model 2'] = round(float(m2_homogeneity), 4)
pprint(model_homogeneity_scores)
```

When we model edge weights rather than just presence/absence, the best posterior partition aligns more closely with the formal job descriptions themselves.


## Model 2: A Nested SBM with Edge Weights

Let's plot these as well.

```python
plot_line_comparison(
    models_dict=model_homogeneity_scores,
    title="Model Comparison\nBlock homogeneity wrt formal job titles\n",
    xlabel="\nHomogeneity",
    xrange=(0, 1),  # homogeneity scores range from 0 to 1
    print_decimals=True,
    filename = 'output/model_comparison_homogeneity_M1_M2.png')
```


## Model 3: Weighted, Poisson-distributed, Refined

Once again, we are modelling our edges as Poisson-distributed. The difference between Models 2 and 3 is that Model 3 includes some additional **r**efinements on the partition we found with Model 2. Let's take a moment to understand what this means.

#### Why Refine Nested SBMs?

Minimizing a model's description length is equivalent to maximizing it's posterior probability. In theory, when we find a good partition with minimal description length, we've also found the partition with maximum posterior probability. But this isn't always the case, as our search space -- the posterior distribution of all possible partitions -- is vast and complex. It's possible that the partition we find with `minimize_nested_blockmodel_dl()` came from a local minima rather than the global minima. If it came from a local minima, it may be very good but not the best.

To improve out estimates, we'll further explore the posterior distribution, enough to escape any local minima and to find the global minima. This involves making iterative refinements using Markov Chain Monte Carlo (MCMC) sampling. We sample from our posterior distribution to find partitions that further lower MDL and maximize posterior probability. This process *starts* with our initial partition, rather than randomly, which makes the MCMC sampling process in `graph-tool` very efficient.

In short, you can think of the initial partition returned from the `minimize_nested_blockmodel_dl()` function as the best guess about the best partition, and the refined estimate as the best partition,^[Well, unless you count the partition in the next model as a single partition, in which case this would probably be the second best guess!] identified by more thoroughly exploring the posterior distribution.

How do we do this in `graph-tool`? In short, we want to run a merge-split MCMC refinement algorithm multiple times, each time storing the resulting blockstates and description lengths. At the end, we select the model with the shortest description length. Let's fit Model 3!

## Model 3: Weighted, Poisson-distributed, Refined

Since this model is a refinement of Model 2, we use Model 2 as a base and accept new partitions only if they have shorter description lengths than the Model 2 partition.

```python
model_3 = model_2.copy() # will update if there are improvements to be had
m3_mdl = model_2.entropy()

num_refinements = 10 # no. of times to refine the initial state using merge-split MCMC
num_mcmc_calls = 2_000 # no. of times to call multiflip_mcmc_sweep within a refinement loop
num_mcmc_iters_per_call = 10 # no. iterations to perform per single multiflip_mcmc_sweep call

for _ in range(num_refinements):
    temp_state = model_2.copy()
    for _ in range(num_mcmc_calls):
        temp_state.multiflip_mcmc_sweep(beta=np.inf, niter=num_mcmc_iters_per_call)

    if temp_state.entropy() < m3_mdl:
        model_3 = temp_state
        m3_mdl = temp_state.entropy()

# draw the model
draw_state(state=model_3, g=g, hvprops=hvprops, heprops=heprops)
```

That code block will take a while to run. As you wait for it to finish, let's unpack the code a bit. Above, we set three variables that have a massive impact on how extensively we search the posterior distribution: `num_refinements`, `num_mcmc_calls`, and `num_mcmc_iters_per_call`. Let's break down what each of these are, and how they work together to influence our search.

The first parameter, `num_refinements`, represents the number of separate searches of the posterior distribution we will conduct. Each starts from the same initial state -- Model 2! -- but is independent from the other searches. By performing a number of independent searches, in this case {{ num_refinements }}, we make it much less likely that our final result comes from a local minima.

The second, `num_mcmc_calls`, determines the number of times we call the `multiflip_mcmc_sweep()` function within a single refinement loop. If we set to `10_000`, each refinement loop will execute `multiflip_mcmc_sweep()` 10,000 times, and hence will explore more of the parameter space. Once again, this increases the chances of finding improvements for our partition.

Finally, `num_mcmc_iters_per_call` determines the number of iterations that `multiflip_mcmc_sweep()` performs in each individual call. Each iteration attempts to refine the current partition by proposing changes to the MCMC algorithm, and then either accepting or rejecting the change. When we increase this number, we again search the posterior distribution more thoroughly, increasing the chances of improving our partition.

How thoroughly we search the posterior distribution depends on all three of these parameters. The larger the numbers we provide our three search parameters, the more thoroughly we search the posterior distribution, and the more likely we are to find refinements for our model. The cost, of course, is increased computation time.


```python
m3_l0_prop_map = model_3.levels[0].b  # block assignment property map

m3_vertex_data = {}
for v in g.vertices():
    m3_vertex_data[int(v)] = m3_l0_prop_map[v]

vertex_data['M3BlockID'] = vertex_data['VertexID'].map(m3_vertex_data)
```

How does Model 3 compare to Models 1 and 2?

```python
model_mdl_scores["Model 3"] = int(round(m3_mdl))
pprint(model_mdl_scores)

plot_line_comparison(
    models_dict=model_mdl_scores,
    title="Model Comparison\nMinimum Description Length (MDL)\n",
    xlabel="\nMDL",
    padding=5_000,
    filename = 'output/model_comparison_mdl_M1_M2_M3.png')
```


```python
m3_homogeneity = homogeneity_score(
    vertex_data['JobPosition'], vertex_data['M3BlockID']
)

model_homogeneity_scores['Model 3'] = round(float(m3_homogeneity), 4)
pprint(model_homogeneity_scores)

plot_line_comparison(
    models_dict=model_homogeneity_scores,
    title="Model Comparison\nBlock homogeneity wrt formal job titles\n",
    xlabel="\nHomogeneity",
    xrange=(0, 1),
    print_decimals=True,
    filename = 'output/model_comparison_homogeneity_M1_M2_M3.png')
```

## Posterior Inference

In this next section of the notebook, we'll do two things that require more intensive analysis of the posterior distribution of block partitions.

**First**, we'll develop another Nested SBM that creates a "consensus partition" by averaging over a large sample of partitions from the posterior distribution, weighted by their posterior probabilities. This quantifies uncertainties in the blockmodelling process, including determining the number of blocks at each level, and block membership at each level (i.e. the block assignment marginal probabilities).

**Second**, we'll perform some additional analyses of the posterior distribution with the goal of figuring out whether it contains other high-probability partitions that could represent plausible competing explanations, or "data stories," for the network structure we've observed.


### Model 4: Weighted, Poisson-distributed, Consensus Partition

In Model 3, we refined the partition from Model 2 by more thoroughly searching the posterior distribution of partitions. Those refinements are *optimizations* of the Model 2 partition. In other words, we searched the posterior distribution for ways to improve our best guess about the best partition. We can improve our analysis even further by **averaging over many posterior partitions**, weighted by their posterior probabilities, rather than attempting to find the single best fitting partition. This quantifies uncertainty in our node-level block assignments and accounts for variability across many different but plausible partitions. We can use `get_consensus_partition_from_posterior` from the course package to simplify this process.


```python
model_4 = get_consensus_partition_from_posterior(model_3, g, force_niter=2000)
```

Let's take a look at our model's MDL, and compare it to Models 1-3.


```python
m4_mdl = model_4.entropy()
model_mdl_scores["Model 4"] = int(round(m4_mdl))

plot_line_comparison(
    models_dict=model_mdl_scores,
    title="Model Comparison\nMinimum Description Length (MDL)\n",
    xlabel="\nMDL",
    padding=10_000,
    filename = 'output/model_comparison_mdl_M1_M2_M3_M4.png')
```

The `get_consensus_partition_from_posterior()` function assigns each node to a block in the consensus partition (averages from partitions in the posterior weighted by posterior probability). We can extract that block information to consider homogeneity with respect to job titles for this model (below).

As we go through the usual process below, we'll also extract information from a newly created internal property map that contains counts the times each node was assigned to each partition in our posterior samples. We can normalize these counts to get the each node's marginal probability for it's block assignment. We'll store that information in our `vertex_data` dataframe as well and will look at it shortly.

```python
m4_l0_prop_map = model_4.levels[0].b
marginal_counts_consensus_partition = g.vertex_properties["pv"]

m4_vertex_assignments = {}
m4_vertex_marginal_probs = {}

for v in g.vertices():
    assigned_block = m4_l0_prop_map[v]
    m4_vertex_assignments[int(v)] = assigned_block
    # get the count for the assigned block
    count_of_assigned_block = marginal_counts_consensus_partition[v][assigned_block]
    # normalize the count to get the probability
    total_count = sum(marginal_counts_consensus_partition[v])
    prob_of_assigned_block = count_of_assigned_block / total_count
    m4_vertex_marginal_probs[int(v)] = prob_of_assigned_block

vertex_data['M4BlockID'] = vertex_data['VertexID'].map(m4_vertex_assignments)
vertex_data['MargProbsConsPart'] = vertex_data['VertexID'].map(m4_vertex_marginal_probs)
```


What do our homogeneity look like now, using the consensus partition?

```python
m4_homogeneity = homogeneity_score(
    vertex_data['JobPosition'], vertex_data['M4BlockID']
)

model_homogeneity_scores['Model 4'] = round(float(m4_homogeneity), 4)

plot_line_comparison(
    models_dict=model_homogeneity_scores,
    title="Model Comparison\nBlock homogeneity wrt formal job titles\n",
    xlabel="\nHomogeneity",
    xrange=(0, 1),
    print_decimals=True,
    filename = 'output/model_comparison_homogeneity_M1_M2_M3_M4.png')
```

We can also use the `pv` property map to change the appearance of the nodes in our visualizations to fractions of a pie, reflecting the marginal probabilities of block assignments. We won't use our `draw_state()` function this time, since we're using some new arguments that we didn't include in that function: `vertex_shape` and  `vertex_pie_fractions`.

```python
hvprops['size'] = 15

model_4.draw(
    vertex_shape="pie",
    vertex_pie_fractions=g.vp['pv'],
    hvprops=hvprops,
    heprops=heprops,
)
```


### Competing Explanations?

When developing generative models, the posterior distribution may contain multiple explanations of the network structure with nearly equal probability. This is exactly why Bayesian generative modelling emphasizes analysis of the *full* posterior distribution rather than selecting a single best estimate.

In the case of SBMs, even when we create a consensus partition from the posterior, we may still want to know whether the posterior contains clusters of similar partitions that differ from the one we selected, but which are still very plausible. Each these different partitions may be offering different generative explanations of the observed network, which is definitely something we would want to know!

To determine whether this is the case, we can infer the modes of the posterior distribution to identify clusters of graph partitions that are similar to one another but different from those in other clusters (see [Peixoto (2021)](https://journals.aps.org/prx/pdf/10.1103/PhysRevX.11.021003)).


### Competing Explanations?

```python
n_partitions_sample = 5000 # the larger the number, the more accurate the estimates

state = gt.NestedBlockState(g) # initialize
gt.mcmc_equilibrate(state,
    force_niter=1000,
    mcmc_args=dict(niter=10)
)

bs = []

def collect_partitions(s):
   global bs
   bs.append(s.get_bs())

gt.mcmc_equilibrate(state,
    force_niter=n_partitions_sample,
    mcmc_args=dict(niter=10),
    callback=collect_partitions
)

# infer partition modes in posterior
pmode = gt.ModeClusterState(bs, nested=True)

# minimize the mode state itself
gt.mcmc_equilibrate(pmode, wait=1, mcmc_args=dict(niter=1, beta=np.inf))

# get inferred modes
modes = pmode.get_modes()

for i, mode in enumerate(modes):
    b = mode.get_max_nested()    # mode's maximum
    pv = mode.get_marginal(g)    # mode's marginal distribution

    print(f"Mode {i} with size {mode.get_M()/len(bs)}")
    state = state.copy(bs=b)
    state.draw(
        vertex_shape="pie",
        vertex_pie_fractions=pv,
        output_size=(1200, 1200),
        output=f"output/enron-partition-mode-{i}.png")
```


ðŸ˜Ž We don't have competing explanations in our posterior distribution! While there are minor differences in the partitions -- as we've seen from our initial models in this notebook -- there are no competing clusters with comparable high probability.

This code will list each mode in the posterior along with the [collective posterior probability of the partitions in each mode]{.kn-pink}. The number of modes detected will depend on the posterior distribution for any given network, but of course the probabilities of all the modes combined will sum to 1. It will also create a series of plots, one for each mode in the posterior, that plots the marginal node block assignment probabilities by drawing the nodes as small pie charts, just like we did above. If there are multiple modes, this will help us compare the different stories or explanation that each partition tells.

Tomorrow, we'll develop your generative modelling toolkit to include simulation and agent-based modelling!

- **TODO**: This is the original text from the print edition. Update it / replace it with the new content I wrote for FCIT / GESIS. It's much better.

### Bayesian Hierarchical Stochastic Blockmodels

Unlike their deterministic counterparts, Bayesian stochastic blockmodels conceptualize network structure as a latent variable problem to be addressed with a generative model. Just as LDA assumes that specific combination of words observed in documents are *generated* from shared latent themes, SBMs assume that specific patterns of ties between nodes in social networks are *generated* from some latent network structure that influences the formation and dissolution of relationships. The types of latent structure that we are interested in varies, and we can develop models for specific types of structure. 

Having a probabilistic model of how this works, grounded in plausible generative mechanisms, is an important part of developing models that don't under or overfit our data. It helps us differentiate structure from random noise in the process of moving from concrete connections between concrete nodes to general connections between categories of nodes. This allows us to overcome some of the limitations of deterministic approaches, which can be tripped up by structure that is caused by random fluctuations rather than some meaningful network-driven social process. 

Tiago @peixoto2019bayesian summarizes the Bayesian response to this problem in one pithy paragraph:

> "The remedy to this problem is to think probabilistically. We need to ascribe to each possible explanation of the data a probability that it is correct, which takes into account modeling assumptions, the statistical evidence available in the data, as well as any source of prior information we may have. Imbued in the whole procedure must be the principle of parsimony -- or Occam's razor -- where a simpler model is preferred if the evidence is not sufficient to justify a more complicated one" (page 4).

As with LDA, the underlying logic of developing a Bayesian generative model here is the same as in other contexts. To continue drilling that underlying logic:

1. we have observed data (connections between nodes in a network) and unobserved latent variables (block or community membership);
2. we want to infer the distributions of the latent variables (i.e., the assignment of nodes into latent blocks) conditional on the observed data;
3. to do so, we construct a joint probability distribution of every possible combination of values for our latent and observed variables (i.e., the numerator in Bayes theorem) and then perform approximate posterior inference to determine the probabilities of different distributions on the latent variables conditional on the observed data. 

We are after the posterior probabilities of many different partitions of the network conditioned on the connections we observe. In other words, we want to know the conditional probability that some node partition $b$ could have plausibly generated an observed network $G$,

\begin{align}
P(\text{b}|\text{G})
\end{align}

As with all Bayesian models, we need to play the "what's that" game, providing priors for all latent variables. The natural tendency here is to prefer uniform priors. If you recall from Chapter 28, using a uniform distribution for our priors means assigning an equal probability to every possible value of the latent variable. @peixoto2019bayesian has shown, however, that this strategy often results in suboptimal results with network models, as it has an a priori preference for solutions with number of blocks comparable to the number of nodes in the network. Who wants that? Nobody. Instead, @peixoto2019bayesian proposes a three-level hierarchical Bayesian approach where we sample (1) the number of blocks, (2) the sizes of each block, and the (3) the partition of the observed network into those blocks. 

This hierarchical model is much less likely to overfit our data, and it does so without requiring us to determine the number of groups in advance, or indeed making any assumptions about the higher-order structure of the networks we are interested in. We will use this model exclusively below. It's known as a **nested Stochastic Blockmodel**. @peixoto2014hierarchical describes a number of interesting variations on inference algorithms for this hierarchical model. One *very* important thing to know about the SBM implementation in graph-tool is that rather than strictly considering equivalence, it also considers the probability of nodes connecting to other nodes, in the more standard sense of network models we've looked at previously. This means that the network partitions from graph-tool will be based on a mixture of assortative community structure (as we've seen in Chapter 15 with Louvain and Leiden) along with disassortative (structural equivalence). Incorporating edge weights into the SBM estimation tends to push the balance in the results towards the assortative side, which makes some intuitive sense - a highly weighted connection between two nodes could drown out the latent influence of structural equivalence. We will examine this shortly.

This has all been very abstract. Let's get our hands dirty with some code. 

## BLOCKMODELLING WITH GRAPH-TOOL

When it comes to the fitting Bayesian stochastic blockmodels, there's no beating Tiago Peixoto's graph-tool, in Python or otherwise. It has astonishing performance in terms of both speed and memory, and as a result it can handle exceptionally large networks efficiently. This performance is achieved by offloading most of the heavy lifting to C++ on the back-end. The cost of these performance improvements, however, is that using graph-tool is less "Pythonic" than you might be used to by this point in the book. Graph-tool is considerably more complex than the network analysis packages we've seen so far (Networkx and NDLib).

The additional overhead and less Pythonic nature that gives graph-tool it's superior performance capabilities also means that I have to spend more time upfront describing how things work. It is entirely possible some of this won't really "sink in" until you start working with graph-tool. That's OK! Once you get your hands dirty with some models and have built up a bit of intuition, you can always come back to this content to deepen your understanding.

### Installing graph-tool

The easiest way to get up and running with graph-tool is to install it via conda-forge with the following command. Because of its numerous dependencies, I strongly recommend that you do this inside a Conda environment (such as the dcss environment, if you've been following along with the supplementary learning materials). As a reminder, Conda environments were introduced in Chapter 2.

`conda install -c conda-forge graph-tool`

If you haven't been using an environment already, you can also install graph-tool inside a conda environment designed specifically for graph-tool. You can use that environment the same way you use any other conda environment. To download and activate the graph-tool environment, simple execute the following from the command line:

`conda create --name gt -c conda-forge graph-tool`

When conda prompts you for permission to download and install the required packages, agree. When it's finished, activate the environment with 

`conda activate gt`

When you do so, you should see your command prompt change; it will now start with `(gt)` (as opposed to `dcss` if you've been using the conda environment for this book). If you are using Jupyter, note that you'll have to launch your Jupyter Notebook server inside that environment to access the packages inside the environment.

### Understanding Property Maps

The most important graph-tool concept to understand is how its array-like "property maps" work. Rather than attaching information about a node (e.g., its ID / label or degree centrality) to the node itself, each node in the network is assigned a unique index. That same index is contained in a property map, and whenever we want to know some information about a node, we use the node index to find the relevant information in the property map. There's a bit of extra friction here, though: because of the C++ backend, each property map object contains *only one type of data*, that you have to declare in advance. This is a pain, but it's what allows us to enjoy some pretty remarkable performance improvements.

Because graph-tool makes such heavy use of these array-like property maps, it's easiest to think of a network in graph-tool as a collection of *associated arrays*. For example,  in a network with three nodes -- `['Lebron James', 'Anthony Davis', 'Kentavious Caldwell-Pope']` -- and an associated property map of colours -- `[Red, Green, Blue]` -- `Lebron James` would be `Red`, `Antony Davis` would be `Green`, and `Kentavious Caldwell-Pope` would be `Blue`. We can encode just about anything in a property map, including vectors of values. For example, the `[Red, Green, Blue]` property map could also be stored as RGB values, `[[255,0,0], [0,128,0], [0,0,255]]`, which would associate `[255,0,0]` with `Lebron James`.

It's also very important to note that 

1. graph-tool does not automatically label nodes, and 
2. it is possible for multiple nodes can have the same label. 

This can result in some unwelcome surprises. For example, if your edgelist contains strings **as opposed to numbers** -- such as 

```
[
    ('Karamo', 'Tan'), 
    ('Karamo', 'Tan')
]
```

then graph-tool will create four different nodes and two edges rather than creating two nodes and aggregating the edges into a weight of 2 for the tie between Karamo and Tan. 

You might recall from Chapter 14 that different disciplines tend to use different words to refer to nodes and edges. In graph-tool, **nodes are referred to as vertices**. They are *exactly* the same. When we create a new vertex in graph-tool -- `v = g.add_vertex()` -- `v` becomes a `vertex` class object, which we can refer to as a **vertex descriptor**. Vertex descriptors are alternative to node indices and can be used to access information about a node from a property map. If we assigned our `[Red, Green, Blue]` property map to an object called `colour_map`, we could retrieve the information for node `v` with `colour_map[v]`. 

Edge property maps, which can contain useful information such as edge weight, behave somewhat differently. They are accessed using **edge descriptors**, which can be obtained from the source and target nodes. For example, we might obtain and store an edge descriptor between nodes `Karamo` and `Tan` with `e = g.edge('Karamo','Tan')` or `e = g.edge(1, 2)` if you've assigned Karamo and Tan integer IDs to benefit from faster compute times. 

Finally, entire networks can themselves can have property maps. These network-level property maps can be accessed by passing the graph object itself. For example, if we have a network object called `g` and a property map called `graph_property_map`, we could access the properties with `graph_property_map[g]`. 

This might sound like a lot of additional overhead to worry about when conducting a network analysis, but you'll likely find the impact fairly minimal once you get used to things. As with other network analysis packages, it makes it relatively easy to do a large amount of data processing outside of the package itself. For example, you can do a lot of work with the data that will eventually be stored as property maps using Pandas and Numpy. My main advice here is *take great care that all of the data in your lists and arrays are in the same order, and of equal lengths.*

Now, let's model. 

### Imports


```python
from graph_tool.all import *
import pandas as pd
pd.set_option("display.notebook_repr_html", False)
import matplotlib
import numpy as np
import math
import pickle
from dcss.networks import label_radial_blockmodel, get_block_membership
```

### Data

As usual, I suggest refreshing yourself on the data we are using here by returning to the overview of datasets from Chapter 1. In brief, the Enron email data is provided as two CSV files, one with the edges between employees who have exchanged emails with one another, and one with the organizational position of Enron employees. 

When developing a blockmodel, we typically do so without having some external set of positions or roles that we want to approximate; the goal here is not supervised learning. However, for learning purposes, our goal will be to develop a blockmodel using relational data that mirrors job titles. The purpose of doing things this way is to illustrate the power of this approach to network analysis, as well as make the discussion of "positions" a bit less abstract. So, remember that when we talk about "positions" and "roles," we don't always (or even often) mean *official* positions or roles such as job titles. 

The two datasets below contain the relational data from employee email communications and information about the job title each employees held in the organization. 


```python
edges_df = pd.read_csv('data/enron/enron_full_edge_list.csv')
edges_df.head()
```




                          source                             target
    0    press.release@enron.com            all.worldwide@enron.com
    1  office.chairman@enron.com             all.downtown@enron.com
    2  office.chairman@enron.com      all.enron-worldwide@enron.com
    3    press.release@enron.com            all.worldwide@enron.com
    4  office.chairman@enron.com  all_enron_north.america@enron.com



As you can see, our edgelist has two columns, `source` and `target`. We don't have any edge weights (though we will compute them below) or other edge attributes. 


```python
employee_df = pd.read_csv('data/enron/enron_employees_updated.csv')
employee_df.head()
```




                               id                  position
    0        liz.taylor@enron.com  Administrative Assistant
    1    michelle.lokay@enron.com  Administrative Assistant
    2  holden.salisbury@enron.com                   Analyst
    3        kam.keiser@enron.com                   Analyst
    4   matthew.lenhart@enron.com                   Analyst



The information about each employee's *official* position in the organization is provided in a column called `'position'`. Let's count the number of employees in each role. 


```python
employee_df['position'].value_counts()
```




    Trader                      35
    Vice President              26
    Director                    17
    Manager                     15
    In House Lawyer             11
    Senior Specialist            8
    Specialist                   6
    Managing Director            6
    Analyst                      5
    Employee                     5
    President                    4
    CEO                          4
    Administrative Assistant     2
    Associate                    2
    Senior Manager               1
    COO                          1
    CFO                          1
    Name: position, dtype: int64



#### Constructing the Communication Network

To create our network, let's construct a weighted communication network between core employees using the edgelist and node attribute files above. First, we'll aggregate and count edges to compute a weight. We'll ignore any nodes that are not in the `employee_df` dataframe, narrowing our focus to core employees only. The "core employees" are those who were involved the legal proceedings following the Enron scandal.

Since this is a **directed communication network**, `i,j` ties are different than `j,i` ties, so we can simply aggregate our edges dataframe by the combination of `'source'` and `'target'` columns and treat the count of their occurrences as our edge weight. We'll also filter the resulting dataframe so that it only includes nodes that are part of the core employee subset. 


```python
edges_df = edges_df.value_counts(['source', 'target']).reset_index(name='count').copy()
core_employees = set(employee_df['id'].tolist())

core_edges_df = edges_df[edges_df['source'].isin(core_employees) & 
                         edges_df['target'].isin(core_employees)]
```

With our weighted directed edgelist created, we can initialize a directed network.


```python
eG = Graph(directed = True)
```

We can add the core employees to this network as nodes, add their job titles to a property map, and add the edge data (weights) to a property map. We'll do that in three steps:

1. get the information into lists,
2. initialize the property maps and tell graph-tool what type of data they we are going to provide, and 
3. loop over our two lists to add the employees to the networks and their node and edge attributes (job titles, edge weights) to property maps. 

First, create the lists!


```python
employee_list = employee_df['id'].tolist()
title_list = employee_df['position'].tolist()
```

Second, initialize the property maps! Note that in addition to the property maps themselves, we are creating a dictionary called `vertex_lookup`. As mentioned earlier in the chapter, we can use this to dictionary to simplify the 'lookup' process to select nodes using string values that carry some meaning about the node, rather than the integer identifier used by graph-tool.

Since we are going to use email addresses as node labels, we'll initialize a property map called `labels` and tell graph-tool to expect strings (because email addresses are strings). Similarly we will initialize a property map for job titles, called `titles`, and also containing strings. Finally, we will create an `edge_weight` property map. Since edge weights are integers in this case, we will tell graph-tool to expect integers. 


```python
vertex_lookup = {}

label = eG.new_vertex_property('string')
title = eG.new_vertex_property('string')
edge_weight = eG.new_edge_property('int')
```

Now we're ready to add information to the property maps! Let's zip up our `employee_list` and `title_list` and then iterate over it. For each pairing of elements from the two lists, we'll add the core employees to the network as nodes, their email addresses to the `labels` property map, and their job titles to the `titles` property map. Finally, we will add the information about the node index to the `vertex_lookup` dict we created above.


```python
for vertex in zip(employee_list, title_list):
    # create a new vertex instance
    v = eG.add_vertex()

    # add attributes to the property maps in the index position of the vertex
    label[v] = vertex[0]
    title[v] = vertex[1]

    # add the vertex to the lookup dictionary, converting it to an integer 
    vertex_lookup[vertex[0]] = int(v)
```

As you probably anticipated, the next thing we need to do is process the edges between nodes. We can do that by using lists pulled from the edges dataframe, but remember we *also* need to consult `vertex_lookup` to ensure we are assigning the right edges between the right nodes! 


```python
source_list = core_edges_df['source'].tolist()
target_list = core_edges_df['target'].tolist()
weight_list = core_edges_df['count'].tolist()

for nodes in zip(source_list, target_list, weight_list):
    from_idx = vertex_lookup[nodes[0]]
    to_idx = vertex_lookup[nodes[1]]

    # Let's ignore self-loops
    if from_idx != to_idx:
        edge = eG.add_edge(from_idx, to_idx)
        edge_weight[edge] = nodes[2]
```

We've now reached the very final bit of preparation. We'll make each of the property maps we've just initialized and populated with information *internal to the graph* and save the graph in `graph-tool`'s own format. That way we don't need to recreate the network again later, we can just load up the network with all the relevant property maps already defined. 


```python
eG.vertex_properties['label'] = label
eG.vertex_properties['title'] = title
eG.edge_properties['edge_weight'] = edge_weight

lookup = eG.new_graph_property('object')
lookup[eG] = vertex_lookup
eG.graph_properties['vertex_lookup'] = lookup
```

And with that, we're ready to start developing stochastic blockmodels!

### Developing Stochastic Blockmodels

In the introduction, we discussed how there are some properties that stochastic blockmodels share with LDA. One of those properties is the process for developing, critiquing, improving, and eventually selecting the best model in an iterative fashion: Box's loop. For example, in this case, after approximating the posterior distribution of the latent variables, we can test the fit of that posterior on the data, and repeat the process using the insight gained about what is and isn't working in the model. In theory, enough iterations would produce the best model possible in terms of representing the data (*not* in terms of the usefulness of the results). In practice, we have to make a choice about when we're satisfied with the results, because there's no good way to know how many iterations it would take to produce the best model you can given the data you have. 

As I mentioned earlier, our goal here is to develop a blockmodel that will partition our network into a set of positions that mirror the job titles that the core employees held within Enron. The catch, of course, is that we want to do this using only information from the relational data itself. 

graph-tool has a very handy function, `minimize_nested_blockmodel_dl()`, that takes care of all the hard work for us. It's fast to run, and tends to produce good results right out of the box. `minimize_nested_blockmodel_dl()` attempts to minimize something called the "**description length**" of a nested blockmodel. Let's break this down, starting with the nested part. As you hopefully recall from earlier in this chapter, a *nested* stochastic blockmodel is a hierarchical Bayesian model. In other words, it embeds blocks inside other blocks in a **multi-level hierarchy**. Doing things this way makes it easier to find small blocks in a network that may contain a small number of nodes. 

The `minimize` and `dl` parts of `minimize_nested_blockmodel_dl()` are a shorthand for **minimize the description length**. Minimum description length is an operationalization of Occam's razor; it suggests that the best model is one that can represent all of the data with the least amount of information required. It helps us select a model that fully explains the data but is as simple as possible given the observed data.

Finally, the blockmodel we will fit here is also **degree-corrected** [@karrer2011stochastic]. A standard baseline SBM assumes that nodes within any given block tend to have very similar, if not identical, degrees. Since this is *extremely unrealistic* in real world networks, it is almost always better to use the degree-corrected implementation. 


```python
state = minimize_nested_blockmodel_dl(eG, deg_corr = True)
```

With that one line of code, we've executed our 3-level Hierarchical Bayesian Stochastic Blockmodel! 

The function we just executed created something called a **blockstate**, which is an object containing the results of partitioning the network running our blockmodel. We can print a summary of the blockstate for our nested degree-corrected description-length-minimized blockmodel to find out

- the number of blocks that nodes were assigned to, 
- the number of levels in the nested hierarchy, and 
- the number of "**meta-blocks**" at each of those levels (blocks within blocks in the nested hierarchy).


```python
state.print_summary()
```

    l: 0, N: 149, B: 13
    l: 1, N: 13, B: 4
    l: 2, N: 4, B: 1


Remember that the model we just ran is a *stochastic generative model*, so the number of blocks will vary for each run of the model, but it typically finds 12-14 blocks at the bottom level. Remember, this is a nested variant where the "bottom level" consists of all the individual nodes, while the upper levels of the hierarchy are aggregate blocks, found by creating a new network where each block is a node and estimating a blockmodel based on that network. After some consideration, 12-14 blocks seems fairly reasonable. We have 17 job titles in the data but if we combined "Manager + Senior Manager", "Senior Specialist + Specialist", "Administrative Assistant + Employee", and "CEO + CFO + COO", we'd have 12 titles. This kind of combination would not impact the computation of the model at all and can be left until it's time for interpretation.

Finally, we can get a quick sense of how things went by visualizing the blockmodel (@fig-31_01). I'm limited to a narrow colour palette in print, but you can access a full resolution colour version of the image (and others like it) in the supplementary online materials. I recommend looking at the color versions of these images, as colour is used very effectively in these blockmodel visualizations.


```python
state.draw(
    layout = "sfdp", 
    vertex_text = eG.vertex_properties['title'], 
    eorder = eG.edge_properties['edge_weight'],
    vertex_text_position = 315,
    bg_color=[255,255,255,1],
    output_size=[4024,4024],
    output='figures/core_enron_blockmodel_sfdp.pdf'
    )
```

![Cap](figures/core_enron_blockmodel_sfdp1.pdf)

In this figure, each node is represented by an individual point (as in other network visualiztions), only the nodes are organized into blocks. The squares are points where blocks converge up the hierarchy to form the nested structure - the structure of email exchanges between blocks will decide whether a block should be grouped with another one. For example, if you look at the group of 6 blocks in the top left of the image, you might notice that there are only two traders present, but there are a lot of lawyers and vice presidents, as well as a CEO. 

This first attempt is already looking pretty good. We have 3 of the 4 CEOs in the same block near the right-hand side, along with three presidents. Note for later: the remaining CEO isn't in the same meta-block - one level up the hierarchy - as the other CEOs.

As with other generative models, *we need to think through generative mechanisms here*. If you recall from Chapter 25, all this really means is that we need to think through simple social and interactional processes that may have resulted in (i.e., generated) the patterns we see in our data. *What's a plausible story of how this data was generated?* 

Remember that we are detail with *email communication* between employees in an organization here. There are many ways to imagine the social mechanisms that best predict structure in a network like this. In this case, it could be that emails between the core employees predicts the relationship between those employees, or it could be that the emails they send *to other non-core employee Enron email addresses* are more predictive. This is an exploratory process that can't fit reasonably in this chapter, but you can see a bit of it in the online supplement.

Let's see what the outcome is with different blockmodel estimation criteria. Stochastic blockmodels in graph-tool are able to incorporate edge weights into the estimation.


```python
state_w = minimize_nested_blockmodel_dl(eG, deg_corr = True, 
                                              state_args=dict(
                                                  recs=[eG.edge_properties['edge_weight']],
                                                  rec_types=["discrete-binomial"]))
```


```python
state_w.print_summary()
```

    l: 0, N: 149, B: 67
    l: 1, N: 67, B: 10
    l: 2, N: 10, B: 2
    l: 3, N: 2, B: 1


We can see already that we end up with far too many blocks to be useful here! There's no need to visualize this graph, but we have another option - let's try setting the number of blocks to be the same as it was for the unweighted model, then see what the weights do for the results.


```python
state_w2 = minimize_nested_blockmodel_dl(eG, deg_corr = True, B_min=12, B_max=12,
                                              state_args=dict(
                                                  recs=[eG.edge_properties['edge_weight']],
                                                  rec_types=["discrete-binomial"]))
```


```python
state_w2.print_summary()
```

    l: 0, N: 149, B: 12
    l: 1, N: 12, B: 3
    l: 2, N: 3, B: 2
    l: 3, N: 2, B: 1


At first glance (@fig-31_02), incorporating edge weight seems as though it produces more tightly-knit, smaller blocks, and only two distinct groups of blocks one level up the hierarchy where we had four with the first model. The larger blocks are also more heterogenous, with CEO's grouped alongside many traders and even "employees". 


```python
state_w2.draw(
    layout = "sfdp", 
    vertex_text = eG.vertex_properties['title'], 
    eorder = eG.edge_properties['edge_weight'],
    vertex_text_position = 315,
    bg_color=[255,255,255,1],
    output_size=[4024,4024],
    output='figures/core_enron_blockmodel_sfdpw.pdf'
    )
```

![Cap](figures/core_enron_blockmodel_sfdpw1.pdf)

The use of edge weights in a blockmodel is a theoretical consideration more than it is a technical one, so it takes some careful thought and experimenting to see what the impact is. In our case, we have people with quite different roles in the company, so their email volume will be quite different. If we don't use edge weights, we stick to a stricter definition of equivalence, closer to structural, and here this produces the most intuitive results. Nonetheless, we should have a way to compare the results beyond just looking at a graph - these graphs won't be very helpful for huge networks. We can use the `get_block_membership` utility from the dcss package to add block assignment information to the employee dataframe.


```python
employee_blocks_df = get_block_membership(state, eG, employee_df,
                                         'model_uw_1')
employee_blocks_df = get_block_membership(state_w2, eG, employee_blocks_df,
                                         'model_w_2')
```

Let's take a look at some of the job titles that one would expect to be more well-defined. 


```python
df_by_position = employee_blocks_df.groupby('position').agg(list)
df_by_position[df_by_position.index.isin(['CEO','President', 'In House Lawyer'])].head()
```




                                                                    id  \
    position                                                             
    CEO              [david.w.delainey@enron.com, jeff.skilling@enr...   
    In House Lawyer  [bill.rapp@enron.com, carol.clair@enron.com, d...   
    President        [greg.whalley@enron.com, jeffrey.a.shankman@en...   
    
                                     model_uw_1_block_id  \
    position                                               
    CEO                                     [5, 5, 0, 0]   
    In House Lawyer  [1, 9, 10, 10, 9, 7, 5, 3, 9, 9, 3]   
    President                              [5, 5, 0, 12]   
    
                                    model_w_2_block_id  
    position                                            
    CEO                                   [0, 5, 1, 5]  
    In House Lawyer  [9, 9, 0, 8, 2, 8, 3, 5, 6, 1, 6]  
    President                             [2, 2, 1, 5]  



You might be able to get a sense of things from some of the smaller lists here. For example, in the `model_uw_1_block_id` column, we can see that one block has 3 of the 4 CEOs, as well as 3 of the 4 Presidents, while another has the remaining CEO and President. 6 of the lawyers also tend to end up in the same block on this run (again, this is stochastic so results might vary a little bit).  With the weighted model, only two of the CEOs end up in the same block, although they are joined by a President and a lawyer.

Alternatively, we can count the number of unique block assignments by role (job title) and calculate the average, based on the number of people with those roles. A lower value here would be a loose indicator of accuracy, with two caveats: a 0.5 value for CEO would be the same if the 4 CEOs were divided equally into two blocks, rather than 3 in one block and 1 in another. This block assignment difference is conceptually significant, so a more robust metric might be desirable. Job titles that apply to only 1 employee will also, necessarily, have a perfectly poor score of 1.0 every time.


```python
employee_blocks_df.groupby(['position'])['model_uw_1_block_id'].agg(lambda x: x.nunique()/x.count())
```




    position
    Administrative Assistant    1.000000
    Analyst                     0.400000
    Associate                   1.000000
    CEO                         0.500000
    CFO                         1.000000
    COO                         1.000000
    Director                    0.411765
    Employee                    0.600000
    In House Lawyer             0.545455
    Manager                     0.466667
    Managing Director           0.666667
    President                   0.750000
    Senior Manager              1.000000
    Senior Specialist           0.875000
    Specialist                  0.500000
    Trader                      0.200000
    Vice President              0.423077
    Name: model_uw_1_block_id, dtype: float64




```python
print(employee_blocks_df.groupby(['position'])['model_uw_1_block_id'].agg(lambda x: x.nunique()/x.count()).sum())
print(employee_blocks_df.groupby(['position'])['model_w_2_block_id'].agg(lambda x: x.nunique()/x.count()).sum())
```

    11.338629507747154
    11.916386064915477


We can do the exact inverse to roughly assess the homogeneity of the blocks, by reversing the columns in the groupby operation.


```python
employee_blocks_df.groupby(['model_uw_1_block_id'])['position'].agg(lambda x: x.nunique()/x.count())
```




    model_uw_1_block_id
    0     0.750000
    1     0.583333
    2     0.277778
    3     0.476190
    4     0.555556
    5     0.416667
    6     0.230769
    7     0.625000
    8     0.714286
    9     0.400000
    10    0.666667
    11    0.500000
    12    0.666667
    Name: position, dtype: float64




```python
print(employee_blocks_df.groupby(['model_uw_1_block_id'])['position'].agg(lambda x: x.nunique()/x.count()).sum())
print(employee_blocks_df.groupby(['model_w_2_block_id'])['position'].agg(lambda x: x.nunique()/x.count()).sum())
```

    6.862912087912089
    7.970732305329079


This loose evaluation suggests that the unweighted model might be preferred, but we can do better with this evaluation. Sci-kit learn provides *many* classification evaluation metrics and the problem we're solving here is essentially a clustering classification. There are metrics within sklearn's clustering section that provide the above evaluations but with more nuance (remember the equivalent 0.5 score if the CEOs were clustered with different proportions but the same number of blocks). A `homogeneity_score` evaluates, you guessed it, the homogeneity of the detected clusters, so if clusters contain more of the same type of job title, the results will score higher. Scores here are on a scale from 0 to 1, with 1 being the best.  


```python
from sklearn.metrics import homogeneity_score, completeness_score, adjusted_mutual_info_score
```

Let's compare homogeneity scores for the unweighted network and then the weighted one. As with the rough evaluation above, the unweighted model has a better score.


```python
homogeneity_score(employee_blocks_df['position'], employee_blocks_df['model_uw_1_block_id'])
```




    0.353428152904928




```python
homogeneity_score(employee_blocks_df['position'], employee_blocks_df['model_w_2_block_id'])
```




    0.25528558562493037



The `completeness_score` inverts the previous score, instead assessing the homogeneity of block assignments for each job titles, so the degree to which nodes are assigned to blocks with other nodes that have the same title. The result is actually very similar in this case!


```python
completeness_score(employee_blocks_df['position'], employee_blocks_df['model_uw_1_block_id'])
```




    0.3435558493343224




```python
completeness_score(employee_blocks_df['position'], employee_blocks_df['model_w_2_block_id'])
```




    0.2771316517440044



Finally, we can also do both of the above in a unified score, `adjusted_mutual_info_score`, where homogeneity and completeness are considered together and the position of the ground-truth and predicted labels doesn't matter. This can also be used to calculate agreement between two labelling methods, when there is no known ground-truth, but unfortunately our block assignment classifications will not be the same between models - `block 1` in one model is not necessarily the same as `block 1` in the next, or even in repeat runs of the same model. Note that this method is a version of `normalized_mutual_info_score` that is adjusted to account for chance, because the standard mutual information score tends to overestimate the shared information between models that have a larger number of clusters.

For this score, the maximum is 1 but it is possible to have a negative score if the predicted clusters are nonsensical enough. We can see that the adjusted mutual info score below is roughly half of the individual scores above, for the unweighted network. For the weighted network, the score is *much* lower. If we compare the two block assignments together, they actually have more agreement with each other than the weighted model has with the ground truth job titles.


```python
adjusted_mutual_info_score(employee_blocks_df['position'], employee_blocks_df['model_uw_1_block_id'])
```




    0.15309516996415473




```python
adjusted_mutual_info_score(employee_blocks_df['position'], employee_blocks_df['model_w_2_block_id'])
```




    0.0756412457785869




```python
adjusted_mutual_info_score(employee_blocks_df['model_w_2_block_id'], employee_blocks_df['model_uw_1_block_id'])
```




    0.15563023649762936



With this information in mind, let's continue on with the unweighted network to see if we can optimize it more, then examine the end result.

###  Model Selection and Optimization

Given the stochastic nature of these models, it is always advisable to run them a number of times and then select the model with the least entropy. Higher entropy is not *inherently* bad. **Properly discuss entropy here, and why we care**. For example, a compressed JPEG image with only two colours will have a lot less entropy than one with a thousand colours. 

In the case of stochastic block models, entropy returns the minimum description length, which is the amount of information the model needs to recreate the entire network. The goal of reducing entropy is fundamental to these models, with the assumption that minimizing entropy results in simpler models that do a better job of uncovering latent similarities in the data without overfitting. Below, we'll execute 10 runs of `minimize_nested_blockmodel_dl` and print the entropy for each. 


```python
states = [minimize_nested_blockmodel_dl(eG, deg_corr=True) 
          for n in range(10)]

for s in states:
    print(s.entropy())
```

    6162.281933127059
    6187.135324492942
    6168.918484063684
    6161.190122173799
    6163.517013260514
    6162.876759036053
    6178.052196472743
    6154.1481501809185
    6166.798460034726
    6154.869718381805


We can automatically grab the lowest entropy state using `np.argmin`.


```python
state = states[np.argmin([s.entropy() for s in states])]
```

### More MCMC

At the expense of increased runtime, we can also follow-up the above model selection process by sampling from the posterior distribution and running `mcmc_equilibrate`, which performs random changes in the block assignments of the nodes, automatically handles the entropy calculations, and chooses the optimum values at the end. This step is also required to collect the block assignment posterior marginals, which tell us the likelihood (if any) that a node belongs to each block, based on the assignments it was given during the iterations. More iterations here will always improve the model, but with decreasing improvement/run-time payoffs.

First, we will use the object `S1`, defined below, to keep track of the original entropy score to see how much we improved the model.


```python
S1 = state.entropy()
S1
```




    6154.1481501809185



To collect marginal probabilities with MCMC, the blockstate needs to have been prepared for sampling, rather than for minimizing description length, which we can achieve by copying the blockstate and setting sampling to `True`. At the same time, we will add an additional 4 empty levels to the nested hierarchy so that the model has a chance to assign more levels. If these hierarchy levels don't improve the model, the equilibration method will collapse them.


```python
state = state.copy(bs=state.get_bs() + [np.zeros(1)] * 4,sampling = True)
```

We're going to perform many iterations of the `mcmc_equilibrate` function, where nodes are moved between different blocks. Importantly, the MCMC method used in graph-tool doesn't perform fully random moves, which would be a fairly typical MCMC approach. By taking advantage of the assumption that networks are made up of heavily interdependant observations, the MCMC estimation only has to randomly sample from probable block assignment moves - to the blocks that a node's alters are members of. 

We create a callback function to pass to `mcmc_equilibrate` so that we can collect a set of block assignment choices from each iteration. The `bs` values can be thought of as votes for block re-assignment, and constitute the posterior marginal probability of each node's assignment to each block.


```python
bs = []

## OUR CALLBACK FUNCTION THAT APPENDS EACH ESTIMATED BLOCKSTATE TO THE ARRAY
def collect_partitions(s):
    global bs
    bs.append(s.get_bs())
        
mcmc_equilibrate(state, force_niter=10000, mcmc_args=dict(niter=10), callback=collect_partitions)
```




    (6159.927069603201, 37378153, 4808499)



Note that this will sometimes result in higher entropy for the block model solution! That's because we need to select the best partition from the ones added to the `bs` list by the callback function.


```python
state.entropy() - S1
```




    5.778919422760737



The `PartitionModeState` function takes our set of labeled partitions and tries to align them into a single set of common group labels. We can then use the `get_marginal()` method of the returned object to create a vertex property map of marginal probabilities for our original network graph. This property map can be used for calculations as well as for visualization of probable block memberships.


```python
pmode = PartitionModeState(bs, nested=True, converge=True)

pv = pmode.get_marginal(eG)
eG.vertex_properties['pv'] = pv
```

Finally, the convenience function `get_max_nested()` returns the most likely block assignment for each node as a single final blockstate, which will group nodes in proximity to each other in our visualization, based on their most likely membership. We apply this result back to our original blockstate object by providing it to the `copy()` method of the state object. Note that our entropy has improved a bit more here!


```python
bs = pmode.get_max_nested()
state = state.copy(bs=bs)
state.entropy()
```




    6153.278269237107



Let's re-calculate the same mutual information scores we used earlier to see if things have improved on those criteria.


```python
employee_blocks_df = get_block_membership(state, eG, employee_blocks_df, 'model_uw_mcmc')
homogeneity_score(employee_blocks_df['position'], employee_blocks_df['model_uw_mcmc_block_id'])
```




    0.38131989351325507




```python
completeness_score(employee_blocks_df['position'], employee_blocks_df['model_uw_mcmc_block_id'])
```




    0.3526819549124348



Homogeneity improves from 0.35 to almost 0.39, while completeness only improves a small amount.


```python
adjusted_mutual_info_score(employee_blocks_df['position'], employee_blocks_df['model_uw_mcmc_block_id'])
```




    0.1561547346951431



But the adjusted mutual info score below is actually slightly worse than it was before! This doesn't necessarily mean the results are worse, though. We'll take a look at a different layout for the blockmodel below and discuss some potential explanations for this.

### Visualizing Block Connections as a Radial Tree

While the sfdp layout does a nice job of positioning nodes (and blocks) in spatial relation to each other, the radial tree layout can be very helpful for getting a sense of the connection patterns between the blocks and also keeps nodes together in a way that makes individual blocks very easy to distinguish. Since it is the default layout for printing a block state, we can easily obtain a simple representation using the `.draw()` method (see @fig-31_03).


```python
state.draw()
```


    
![png](chapter_31_latent_network_structure_stochastic_block_models_files/chapter_31_latent_network_structure_stochastic_block_models_94_0.pdf)
    





    (<VertexPropertyMap object with value type 'vector<double>', for Graph 0x7f74c8ac3670, at 0x7f74c43f4a00>,
     <Graph object, directed, with 172 vertices and 171 edges, at 0x7f74c433b9d0>,
     <VertexPropertyMap object with value type 'vector<double>', for Graph 0x7f74c433b9d0, at 0x7f74c431ea90>)



As is often the case, there are a few preparation steps we can do to improve the visualization of edges, as well as to add node labels to our figure. This process is a bit complex and is an adaptation of one that was devised by the author of graph-tool. The details aren't particularly important, so we can use the utility function `label_radial_blockmodel` from the dcss package to take care of most of it. 


```python
eG = label_radial_blockmodel(eG, state)
```

The resulting figure is much improved (@fig-31_04), and clearly shows the relations between blocks, while also making it easier to examine which job titles were assigned to each block.


```python
state.draw(
    vertex_text = eG.vertex_properties['title'], 
    eorder = eG.edge_properties['edge_weight'],
    vertex_shape='pie',
    vertex_pie_fractions=eG.vertex_properties['pv'],
    edge_control_points = eG.edge_properties['cts'],
    pos=eG.vertex_properties['pos'], 
    vertex_size=10, 
    edge_pen_width = 0.2,
    bg_color=[255,255,255,1],
    vertex_text_rotation=eG.vertex_properties['text_rot'],
    vertex_text_position=0,
    output='figures/core_state_radial_tree_labels.pdf'
    )
```




    (<VertexPropertyMap object with value type 'vector<double>', for Graph 0x7f74c8ac3670, at 0x7f7550a574f0>,
     <Graph object, directed, with 172 vertices and 171 edges, at 0x7f7550a4d700>,
     <VertexPropertyMap object with value type 'vector<double>', for Graph 0x7f7550a4d700, at 0x7f7550a55970>)



![Cap](figures/core_state_radial_tree_labels1.pdf)

You'll notice that some of the nodes are broken up into pie fractions - these indicate their probability of being assigned to a different block. In the full colour version, these fractions are coloured the same as the alternative block that the node might have been assigned to. You'll also notice that the blocks have become significantly more heterogenous! Traders are in blocks with other traders, most lawyers are in a block that two other lawyers had some probability of being assigned to, and the CEOs are in fairly exclusive blocks. Although we no longer have 3 CEOs in one block with the COO, the block that one of the CEOs was moved to contains the other CEO, and their two respective blocks form a single block one level up the hierarchy! Earlier I mentioned that there are possible explanations for a decreased adjusted mutual information score and this is one example - that score doesn't incorporate the higher levels of the hierarchy. Even though it's probably actually a better model to have the four CEOs split evenly among two blocks, then put those two blocks together at the next hierarchy level, this would still negatively impact the mutual info score compared to the model where 3 CEOs were in one block. 

It's quite clear from the results of these stochastic blockmodels that there's some very powerful estimation going on, and that the Bayesian aspects of it allow a great deal of nuance. The versatility of the modeling that drives graph-tool has led to a collaborative extension for topic modeling. Given the relational nature of words in text, which is often analyzed in the same way as social relations, topics can be blockmodelled from text documents to great effect. We'll explore this method in the section that follows.


## CONCLUSION

### Key Points 

- Hierarchical Stochastic Blockmodels are remarkably powerful models that provide a nearly unparalleled degree of insight into the structure of a network and nodes' roles within it 
- SBMs build on the Bayesian intuitions established earlier in this book; they employ a similar approach of using latent variables and prior distributions to model unknown/unobserved 
- TopSBM is really cool. Once you're comfortable with the material in this chapter and the previous one, you should explore TopSBM on your own, or using the supplementary material online.
 -->