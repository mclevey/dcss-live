# Generative Network Analysis with Bayesian Stochastic Blockmodels

## LEARNING OUTCOMES

- Explain how stochastic equivalence builds upon, yet is distinct from, structural equivalence
- Describe the Bayesian logic of Stochastic BlockModels (SBMs) and how they aggregate nodes into stochastically equivalent blocks
- Use `graph-tool` to produce posterior distributions of nodes in hierarchically nested blocks
- Understand how TopSBM unifies SBMs and LDA models

## LEARNING MATERIALS

You can find the online learning materials for this chapter in `doing_computational_social_science/Chapter_31`. `cd` into the directory and launch your Jupyter Server.

## INTRODUCTION

In the previous chapter, we used Bayesian generative models to identify latent topic structure in text data. In this chapter, we use the same underlying Bayesian logic to tackle some difficult problems in network analysis. Just as LDA addresses topic modelling as a latent variable problem using generative models, the models we introduce here -- stochastic block models (SBMs) -- approach network structure a similar way. Whereas LDA assumes that documents contain a mixture of latent topics that are made up of words, SBMs assume that networks have latent modular structure, and the goal is to figure out how to partition a network according to these low-level building blocks. 

I'll start by briefly introducing some new theoretical ideas about "equivalence" and structural positions in networks. Then, I'll emphasize the theoretical and the generative logic of SBMs and their hierarchical Bayesian design. Then I'll discuss the role of Bayesian inference in SBMs, following a similar explanatory process as in the previous chapter. 

In the second half of the chapter, we'll focus on three related things. First, I'll introduce the Python package graph-tool. Second, we will fit a series of SBMs to an email communication network of Enron employees involved in the legal proceedings of the Enron scandal. Third and finally, we will return to the comparison of LDA and SBM. I'll emphasize the shared conceptual logic of the problems of topic modelling and community detection. Then I'll introduce TopSBMs as a shared modelling framework for both tasks. This combination marks the end of our journey through Bayesian generative modelling and probabilistic programming.

### Latent Network Structure: Connected Communities and Structural Positions

Generally speaking, network researchers think about the structure of social networks in one of two ways, each of which posits different social mechanisms to produce an outcome of interest (such as whether someone adopts a belief or behaviour), or give rise to different kinds of patterns in network structure. Both perspectives subsume a wide variety of specific methods, measures, and models. 

The first of these two perspectives focuses mainly on issues related to **connection, cohesion, and network flow**. We work within this **general perspective** anytime we are thinking through the processes by which ties and cohesive subgroups form and dissolve, or when concerned with the role of networks in facilitating or mitigating the *spread* of a contagion or the creation of opportunities from one's direct ties or the structure of ties in their neighbourhood. Everything we have discussed in relation to networks to this point in the book has assumed this general perspective. Because of the role of direct connection and walk-structure, research that adopts this perspective measures and models *specific* relationships between *specific* nodes. 

By contrast, **positional approaches** are premised on the notion that we can reduce the complexity of networks that represent specific relationships between specific nodes to a smaller set of connections between **blocks** (i.e., groups) of nodes that are judged to be "equivalent" in some meaningful way. In other words, each "block" in a network consists of a group of structurally interchangeable nodes, each of which share a common **structural position** in the network. The research goals here focus on understanding the *general* relationships between *general structural positions* rather than understanding the specific connections between specific people. 

Why would we care to do this in the first place? It depends on whom you ask, but you a common motivation for working with this positional approach is **generalization**: we want to abstract away the concrete details of specific connections between specific people to focus on understanding *the big picture, in general*. As we will soon see, this way of thinking lends itself well to probabilistic models of network structure, and is more robust to problems that arise from the imprecise and imperfect observation and measurement of relational data. 

Let's imagine a simple scenario. Imagine two professors, each principal investigators of research labs at a university but in completely different departments and with no overlap in their research agendas or lab members. Let's also assume that both labs have a fairly hierarchical organizational structure, as is historically typical of many sciences. In both cases, the professors have a group of students and lab technicians who report directly to them. Regardless of how we measure relevant network ties (e.g., communication, collaboration, advice-giving), the community detection we methods we discussed in Chapter 15 would place both professors into different communities. From a connection, community, and flow perspective, these two professors and their labs are completely distinct, with the only relationships between them being indirect connections along the walk structure of the network. By contrast, the models we're going to discuss in the rest of this chapter would identify the two professors as being **equivalent** despite the fact that they are not connected to one another and they have no connections in common. This is because the focus is on *general* patterns of *aggregate* relations between *categories* of nodes [@jscott; @cprell].

Nodes that occupy the same structural positions in a network tend to have behavioural similarities even when they are not connected to one another. Structurally equivalent nodes may behave in similar ways because they are conforming, to varying degrees, to the expectations associated with their position and social roles. Professors do what professors do, grad students do what grad students do, undergrad students do what undergrad students do, and so on. Another mechanistic explanation is that equivalent nodes have to negotiate similar types of problems, situations, and institutional environments, and they exhibit behavioural similarities due to these common experiences and contexts rather than the influence of their direct social ties (bringing us back to that idea of *shared institutional environments* and the connection to hierarchical models first introduced in Chapter 29). 

Finally, people who occupy the same structural positions in networks might look to mirror the behaviours of other people who share their structural positions. For example, if they are uncertain about how to behave, or what is needed for success, they may model their behaviour on someone who is in the same position and seems to be thriving (similarly, they may observe behaviours and outcomes they want to *avoid*). Returning to our hypothetical example: it doesn't stretch credibility to claim that a professor is more likely to make decisions about running their research lab based on observations of how similar professors run their labs than the interpersonal influence of their friends and family (who are not professors running research labs).

The first thing we need to do in any positional analysis is specify an exactly what we mean by "equivalence." We need an operational definition. 

#### Equivalence?

@lorrain1971structural theorized that nodes in a network could be considered **structurally equivalent** if they connected to the rest of a network in *identical* ways. In this definition, equivalent nodes are literally substitutable for one another, which was the initial inspiration behind the move to aggregate nodes into abstract blocks that reveal an underlying structure of generic positions and social roles.

It is exceedingly rare to find nodes that meet this strict definition of equivalence when analyzing real social networks. As a result, we typically use some measure of *approximate equivalence* instead. Since Lorraine and White's classic article, researchers have proposed many different ways of conceptualizing equivalence executing positional analyses, one of the most influential of which was White and Reitz's [-@white1983graph] **regular equivalence**. To be "regularly equivalent," nodes must have similar connection profiles to other regular equivalence classes, rather than the identical connection profile of structural equivalence. 

Structural and regular equivalence are both deterministic. Probability offers us yet another way of operationalizing the idea of equivalence, and one that is well-aligned with Bayesian probabilistic modelling. Nodes are **stochastically equivalent** if they have the same probabilities of connecting with other nodes. In other words, nodes are grouped into the same blocks if they are statistically indistinguishable from one another due to sharing similar probabilities of connecting with other nodes according to their stochastically equivalent blocks. 

  
> **Further Reading**    
>   
> Like the deterministic conception of equivalence, this stochastic conception was initially developed in the social sciences and statistics [see @holland1983stochastic; @snijders1997estimation; @wang1987stochastic; @nowicki2001estimation], but over the same time period were developing in other sciences (a textbook example of "multiple discovery"); it is now very much an interdisciplinary affair at the cutting edge of network science [see @peixoto2019bayesian].
>


#### Blockmodels

Once we've adopted some operational definition of equivalence, the second step is to use that definition to cluster nodes into different equivalence classes. As previously mentioned, these classes represent generic **positions** and are typically referred to as "**blocks**." The process of identifying these blocks is called "**blockmodelling**," and there are a number of different ways it can be done [see @doreian2005generalized; @doreian2002positional; @ferligoj2011positions; @peixoto2019bayesian; @snijders1997estimation]. The most important distinction is between deterministic and stochastic blockmodels. Unsurprisingly, deterministic blockmodels are used alongside deterministic understandings of equivalence, and stochastic blockmodels are used alongside stochastic understandings of equivalence.

The results of deterministic blockmodels are fully determined by (*i*) the parameters of the model, such as the number of blocks to look for, and (*ii*) the input data itself. Given the same parameter values and the same input data, the models will always produce the same results. Typically this is done by constructing a similarity or distance matrix from an adjacency matrix, and then performing some form of cluster analysis, such as hierarchical clustering, on that matrix. There have been many specific algorithms for blockmodelling introduced since Harrison White and his students first introduced the idea of structural equivalence and blockmodelling in the 1970s, the most famous of which are CONCOR and REGE. A discussion of these and other deterministic blockmodels is beyond the scope of this chapter, but they are described in most social network analysis methods texts. 

By contrast, the blockmodels we will discuss in the rest of this chapter are *stochastic*, and are based on notions of *stochastic equivalence*. Let's shift the discussion to them now. 

### Bayesian Hierarchical Stochastic Blockmodels

Unlike their deterministic counterparts, Bayesian stochastic blockmodels conceptualize network structure as a latent variable problem to be addressed with a generative model. Just as LDA assumes that specific combination of words observed in documents are *generated* from shared latent themes, SBMs assume that specific patterns of ties between nodes in social networks are *generated* from some latent network structure that influences the formation and dissolution of relationships. The types of latent structure that we are interested in varies, and we can develop models for specific types of structure. 

Having a probabilistic model of how this works, grounded in plausible generative mechanisms, is an important part of developing models that don't under or overfit our data. It helps us differentiate structure from random noise in the process of moving from concrete connections between concrete nodes to general connections between categories of nodes. This allows us to overcome some of the limitations of deterministic approaches, which can be tripped up by structure that is caused by random fluctuations rather than some meaningful network-driven social process. 

Tiago @peixoto2019bayesian summarizes the Bayesian response to this problem in one pithy paragraph:

> "The remedy to this problem is to think probabilistically. We need to ascribe to each possible explanation of the data a probability that it is correct, which takes into account modeling assumptions, the statistical evidence available in the data, as well as any source of prior information we may have. Imbued in the whole procedure must be the principle of parsimony -- or Occam's razor -- where a simpler model is preferred if the evidence is not sufficient to justify a more complicated one" (page 4).

As with LDA, the underlying logic of developing a Bayesian generative model here is the same as in other contexts. To continue drilling that underlying logic:

1. we have observed data (connections between nodes in a network) and unobserved latent variables (block or community membership);
2. we want to infer the distributions of the latent variables (i.e., the assignment of nodes into latent blocks) conditional on the observed data;
3. to do so, we construct a joint probability distribution of every possible combination of values for our latent and observed variables (i.e., the numerator in Bayes theorem) and then perform approximate posterior inference to determine the probabilities of different distributions on the latent variables conditional on the observed data. 

We are after the posterior probabilities of many different partitions of the network conditioned on the connections we observe. In other words, we want to know the conditional probability that some node partition $b$ could have plausibly generated an observed network $G$,

\begin{align}
P(\text{b}|\text{G})
\end{align}

As with all Bayesian models, we need to play the "what's that" game, providing priors for all latent variables. The natural tendency here is to prefer uniform priors. If you recall from Chapter 28, using a uniform distribution for our priors means assigning an equal probability to every possible value of the latent variable. @peixoto2019bayesian has shown, however, that this strategy often results in suboptimal results with network models, as it has an a priori preference for solutions with number of blocks comparable to the number of nodes in the network. Who wants that? Nobody. Instead, @peixoto2019bayesian proposes a three-level hierarchical Bayesian approach where we sample (1) the number of blocks, (2) the sizes of each block, and the (3) the partition of the observed network into those blocks. 

This hierarchical model is much less likely to overfit our data, and it does so without requiring us to determine the number of groups in advance, or indeed making any assumptions about the higher-order structure of the networks we are interested in. We will use this model exclusively below. It's known as a **nested Stochastic Blockmodel**. @peixoto2014hierarchical describes a number of interesting variations on inference algorithms for this hierarchical model. One *very* important thing to know about the SBM implementation in graph-tool is that rather than strictly considering equivalence, it also considers the probability of nodes connecting to other nodes, in the more standard sense of network models we've looked at previously. This means that the network partitions from graph-tool will be based on a mixture of assortative community structure (as we've seen in Chapter 15 with Louvain and Leiden) along with disassortative (structural equivalence). Incorporating edge weights into the SBM estimation tends to push the balance in the results towards the assortative side, which makes some intuitive sense - a highly weighted connection between two nodes could drown out the latent influence of structural equivalence. We will examine this shortly.

This has all been very abstract. Let's get our hands dirty with some code. 

## BLOCKMODELLING WITH GRAPH-TOOL

When it comes to the fitting Bayesian stochastic blockmodels, there's no beating Tiago Peixoto's graph-tool, in Python or otherwise. It has astonishing performance in terms of both speed and memory, and as a result it can handle exceptionally large networks efficiently. This performance is achieved by offloading most of the heavy lifting to C++ on the back-end. The cost of these performance improvements, however, is that using graph-tool is less "Pythonic" than you might be used to by this point in the book. Graph-tool is considerably more complex than the network analysis packages we've seen so far (Networkx and NDLib).

The additional overhead and less Pythonic nature that gives graph-tool it's superior performance capabilities also means that I have to spend more time upfront describing how things work. It is entirely possible some of this won't really "sink in" until you start working with graph-tool. That's OK! Once you get your hands dirty with some models and have built up a bit of intuition, you can always come back to this content to deepen your understanding.

### Installing graph-tool

The easiest way to get up and running with graph-tool is to install it via conda-forge with the following command. Because of its numerous dependencies, I strongly recommend that you do this inside a Conda environment (such as the dcss environment, if you've been following along with the supplementary learning materials). As a reminder, Conda environments were introduced in Chapter 2.

`conda install -c conda-forge graph-tool`

If you haven't been using an environment already, you can also install graph-tool inside a conda environment designed specifically for graph-tool. You can use that environment the same way you use any other conda environment. To download and activate the graph-tool environment, simple execute the following from the command line:

`conda create --name gt -c conda-forge graph-tool`

When conda prompts you for permission to download and install the required packages, agree. When it's finished, activate the environment with 

`conda activate gt`

When you do so, you should see your command prompt change; it will now start with `(gt)` (as opposed to `dcss` if you've been using the conda environment for this book). If you are using Jupyter, note that you'll have to launch your Jupyter Notebook server inside that environment to access the packages inside the environment.

### Understanding Property Maps

The most important graph-tool concept to understand is how its array-like "property maps" work. Rather than attaching information about a node (e.g., its ID / label or degree centrality) to the node itself, each node in the network is assigned a unique index. That same index is contained in a property map, and whenever we want to know some information about a node, we use the node index to find the relevant information in the property map. There's a bit of extra friction here, though: because of the C++ backend, each property map object contains *only one type of data*, that you have to declare in advance. This is a pain, but it's what allows us to enjoy some pretty remarkable performance improvements.

Because graph-tool makes such heavy use of these array-like property maps, it's easiest to think of a network in graph-tool as a collection of *associated arrays*. For example,  in a network with three nodes -- `['Lebron James', 'Anthony Davis', 'Kentavious Caldwell-Pope']` -- and an associated property map of colours -- `[Red, Green, Blue]` -- `Lebron James` would be `Red`, `Antony Davis` would be `Green`, and `Kentavious Caldwell-Pope` would be `Blue`. We can encode just about anything in a property map, including vectors of values. For example, the `[Red, Green, Blue]` property map could also be stored as RGB values, `[[255,0,0], [0,128,0], [0,0,255]]`, which would associate `[255,0,0]` with `Lebron James`.

It's also very important to note that 

1. graph-tool does not automatically label nodes, and 
2. it is possible for multiple nodes can have the same label. 

This can result in some unwelcome surprises. For example, if your edgelist contains strings **as opposed to numbers** -- such as 

```
[
    ('Karamo', 'Tan'), 
    ('Karamo', 'Tan')
]
```

then graph-tool will create four different nodes and two edges rather than creating two nodes and aggregating the edges into a weight of 2 for the tie between Karamo and Tan. 

You might recall from Chapter 14 that different disciplines tend to use different words to refer to nodes and edges. In graph-tool, **nodes are referred to as vertices**. They are *exactly* the same. When we create a new vertex in graph-tool -- `v = g.add_vertex()` -- `v` becomes a `vertex` class object, which we can refer to as a **vertex descriptor**. Vertex descriptors are alternative to node indices and can be used to access information about a node from a property map. If we assigned our `[Red, Green, Blue]` property map to an object called `colour_map`, we could retrieve the information for node `v` with `colour_map[v]`. 

Edge property maps, which can contain useful information such as edge weight, behave somewhat differently. They are accessed using **edge descriptors**, which can be obtained from the source and target nodes. For example, we might obtain and store an edge descriptor between nodes `Karamo` and `Tan` with `e = g.edge('Karamo','Tan')` or `e = g.edge(1, 2)` if you've assigned Karamo and Tan integer IDs to benefit from faster compute times. 

Finally, entire networks can themselves can have property maps. These network-level property maps can be accessed by passing the graph object itself. For example, if we have a network object called `g` and a property map called `graph_property_map`, we could access the properties with `graph_property_map[g]`. 

This might sound like a lot of additional overhead to worry about when conducting a network analysis, but you'll likely find the impact fairly minimal once you get used to things. As with other network analysis packages, it makes it relatively easy to do a large amount of data processing outside of the package itself. For example, you can do a lot of work with the data that will eventually be stored as property maps using Pandas and Numpy. My main advice here is *take great care that all of the data in your lists and arrays are in the same order, and of equal lengths.*

Now, let's model. 

### Imports


```python
from graph_tool.all import *
import pandas as pd
pd.set_option("display.notebook_repr_html", False)
import matplotlib
import numpy as np
import math
import pickle
from dcss.networks import label_radial_blockmodel, get_block_membership
```

### Data

As usual, I suggest refreshing yourself on the data we are using here by returning to the overview of datasets from Chapter 1. In brief, the Enron email data is provided as two CSV files, one with the edges between employees who have exchanged emails with one another, and one with the organizational position of Enron employees. 

When developing a blockmodel, we typically do so without having some external set of positions or roles that we want to approximate; the goal here is not supervised learning. However, for learning purposes, our goal will be to develop a blockmodel using relational data that mirrors job titles. The purpose of doing things this way is to illustrate the power of this approach to network analysis, as well as make the discussion of "positions" a bit less abstract. So, remember that when we talk about "positions" and "roles," we don't always (or even often) mean *official* positions or roles such as job titles. 

The two datasets below contain the relational data from employee email communications and information about the job title each employees held in the organization. 


```python
edges_df = pd.read_csv('data/enron/enron_full_edge_list.csv')
edges_df.head()
```




                          source                             target
    0    press.release@enron.com            all.worldwide@enron.com
    1  office.chairman@enron.com             all.downtown@enron.com
    2  office.chairman@enron.com      all.enron-worldwide@enron.com
    3    press.release@enron.com            all.worldwide@enron.com
    4  office.chairman@enron.com  all_enron_north.america@enron.com



As you can see, our edgelist has two columns, `source` and `target`. We don't have any edge weights (though we will compute them below) or other edge attributes. 


```python
employee_df = pd.read_csv('data/enron/enron_employees_updated.csv')
employee_df.head()
```




                               id                  position
    0        liz.taylor@enron.com  Administrative Assistant
    1    michelle.lokay@enron.com  Administrative Assistant
    2  holden.salisbury@enron.com                   Analyst
    3        kam.keiser@enron.com                   Analyst
    4   matthew.lenhart@enron.com                   Analyst



The information about each employee's *official* position in the organization is provided in a column called `'position'`. Let's count the number of employees in each role. 


```python
employee_df['position'].value_counts()
```




    Trader                      35
    Vice President              26
    Director                    17
    Manager                     15
    In House Lawyer             11
    Senior Specialist            8
    Specialist                   6
    Managing Director            6
    Analyst                      5
    Employee                     5
    President                    4
    CEO                          4
    Administrative Assistant     2
    Associate                    2
    Senior Manager               1
    COO                          1
    CFO                          1
    Name: position, dtype: int64



#### Constructing the Communication Network

To create our network, let's construct a weighted communication network between core employees using the edgelist and node attribute files above. First, we'll aggregate and count edges to compute a weight. We'll ignore any nodes that are not in the `employee_df` dataframe, narrowing our focus to core employees only. The "core employees" are those who were involved the legal proceedings following the Enron scandal.

Since this is a **directed communication network**, `i,j` ties are different than `j,i` ties, so we can simply aggregate our edges dataframe by the combination of `'source'` and `'target'` columns and treat the count of their occurrences as our edge weight. We'll also filter the resulting dataframe so that it only includes nodes that are part of the core employee subset. 


```python
edges_df = edges_df.value_counts(['source', 'target']).reset_index(name='count').copy()
core_employees = set(employee_df['id'].tolist())

core_edges_df = edges_df[edges_df['source'].isin(core_employees) & 
                         edges_df['target'].isin(core_employees)]
```

With our weighted directed edgelist created, we can initialize a directed network.


```python
eG = Graph(directed = True)
```

We can add the core employees to this network as nodes, add their job titles to a property map, and add the edge data (weights) to a property map. We'll do that in three steps:

1. get the information into lists,
2. initialize the property maps and tell graph-tool what type of data they we are going to provide, and 
3. loop over our two lists to add the employees to the networks and their node and edge attributes (job titles, edge weights) to property maps. 

First, create the lists!


```python
employee_list = employee_df['id'].tolist()
title_list = employee_df['position'].tolist()
```

Second, initialize the property maps! Note that in addition to the property maps themselves, we are creating a dictionary called `vertex_lookup`. As mentioned earlier in the chapter, we can use this to dictionary to simplify the 'lookup' process to select nodes using string values that carry some meaning about the node, rather than the integer identifier used by graph-tool.

Since we are going to use email addresses as node labels, we'll initialize a property map called `labels` and tell graph-tool to expect strings (because email addresses are strings). Similarly we will initialize a property map for job titles, called `titles`, and also containing strings. Finally, we will create an `edge_weight` property map. Since edge weights are integers in this case, we will tell graph-tool to expect integers. 


```python
vertex_lookup = {}

label = eG.new_vertex_property('string')
title = eG.new_vertex_property('string')
edge_weight = eG.new_edge_property('int')
```

Now we're ready to add information to the property maps! Let's zip up our `employee_list` and `title_list` and then iterate over it. For each pairing of elements from the two lists, we'll add the core employees to the network as nodes, their email addresses to the `labels` property map, and their job titles to the `titles` property map. Finally, we will add the information about the node index to the `vertex_lookup` dict we created above.


```python
for vertex in zip(employee_list, title_list):
    # create a new vertex instance
    v = eG.add_vertex()

    # add attributes to the property maps in the index position of the vertex
    label[v] = vertex[0]
    title[v] = vertex[1]

    # add the vertex to the lookup dictionary, converting it to an integer 
    vertex_lookup[vertex[0]] = int(v)
```

As you probably anticipated, the next thing we need to do is process the edges between nodes. We can do that by using lists pulled from the edges dataframe, but remember we *also* need to consult `vertex_lookup` to ensure we are assigning the right edges between the right nodes! 


```python
source_list = core_edges_df['source'].tolist()
target_list = core_edges_df['target'].tolist()
weight_list = core_edges_df['count'].tolist()

for nodes in zip(source_list, target_list, weight_list):
    from_idx = vertex_lookup[nodes[0]]
    to_idx = vertex_lookup[nodes[1]]

    # Let's ignore self-loops
    if from_idx != to_idx:
        edge = eG.add_edge(from_idx, to_idx)
        edge_weight[edge] = nodes[2]
```

We've now reached the very final bit of preparation. We'll make each of the property maps we've just initialized and populated with information *internal to the graph* and save the graph in `graph-tool`'s own format. That way we don't need to recreate the network again later, we can just load up the network with all the relevant property maps already defined. 


```python
eG.vertex_properties['label'] = label
eG.vertex_properties['title'] = title
eG.edge_properties['edge_weight'] = edge_weight

lookup = eG.new_graph_property('object')
lookup[eG] = vertex_lookup
eG.graph_properties['vertex_lookup'] = lookup
```

And with that, we're ready to start developing stochastic blockmodels!

### Developing Stochastic Blockmodels

In the introduction, we discussed how there are some properties that stochastic blockmodels share with LDA. One of those properties is the process for developing, critiquing, improving, and eventually selecting the best model in an iterative fashion: Box's loop. For example, in this case, after approximating the posterior distribution of the latent variables, we can test the fit of that posterior on the data, and repeat the process using the insight gained about what is and isn't working in the model. In theory, enough iterations would produce the best model possible in terms of representing the data (*not* in terms of the usefulness of the results). In practice, we have to make a choice about when we're satisfied with the results, because there's no good way to know how many iterations it would take to produce the best model you can given the data you have. 

As I mentioned earlier, our goal here is to develop a blockmodel that will partition our network into a set of positions that mirror the job titles that the core employees held within Enron. The catch, of course, is that we want to do this using only information from the relational data itself. 

graph-tool has a very handy function, `minimize_nested_blockmodel_dl()`, that takes care of all the hard work for us. It's fast to run, and tends to produce good results right out of the box. `minimize_nested_blockmodel_dl()` attempts to minimize something called the "**description length**" of a nested blockmodel. Let's break this down, starting with the nested part. As you hopefully recall from earlier in this chapter, a *nested* stochastic blockmodel is a hierarchical Bayesian model. In other words, it embeds blocks inside other blocks in a **multi-level hierarchy**. Doing things this way makes it easier to find small blocks in a network that may contain a small number of nodes. 

The `minimize` and `dl` parts of `minimize_nested_blockmodel_dl()` are a shorthand for **minimize the description length**. Minimum description length is an operationalization of Occam's razor; it suggests that the best model is one that can represent all of the data with the least amount of information required. It helps us select a model that fully explains the data but is as simple as possible given the observed data.

Finally, the blockmodel we will fit here is also **degree-corrected** [@karrer2011stochastic]. A standard baseline SBM assumes that nodes within any given block tend to have very similar, if not identical, degrees. Since this is *extremely unrealistic* in real world networks, it is almost always better to use the degree-corrected implementation. 


```python
state = minimize_nested_blockmodel_dl(eG, deg_corr = True)
```

With that one line of code, we've executed our 3-level Hierarchical Bayesian Stochastic Blockmodel! 

The function we just executed created something called a **blockstate**, which is an object containing the results of partitioning the network running our blockmodel. We can print a summary of the blockstate for our nested degree-corrected description-length-minimized blockmodel to find out

- the number of blocks that nodes were assigned to, 
- the number of levels in the nested hierarchy, and 
- the number of "**meta-blocks**" at each of those levels (blocks within blocks in the nested hierarchy).


```python
state.print_summary()
```

    l: 0, N: 149, B: 13
    l: 1, N: 13, B: 4
    l: 2, N: 4, B: 1


Remember that the model we just ran is a *stochastic generative model*, so the number of blocks will vary for each run of the model, but it typically finds 12-14 blocks at the bottom level. Remember, this is a nested variant where the "bottom level" consists of all the individual nodes, while the upper levels of the hierarchy are aggregate blocks, found by creating a new network where each block is a node and estimating a blockmodel based on that network. After some consideration, 12-14 blocks seems fairly reasonable. We have 17 job titles in the data but if we combined "Manager + Senior Manager", "Senior Specialist + Specialist", "Administrative Assistant + Employee", and "CEO + CFO + COO", we'd have 12 titles. This kind of combination would not impact the computation of the model at all and can be left until it's time for interpretation.

Finally, we can get a quick sense of how things went by visualizing the blockmodel (Figure @fig-31_01). I'm limited to a narrow colour palette in print, but you can access a full resolution colour version of the image (and others like it) in the supplementary online materials. I recommend looking at the color versions of these images, as colour is used very effectively in these blockmodel visualizations.


```python
state.draw(
    layout = "sfdp", 
    vertex_text = eG.vertex_properties['title'], 
    eorder = eG.edge_properties['edge_weight'],
    vertex_text_position = 315,
    bg_color=[255,255,255,1],
    output_size=[4024,4024],
    output='figures/core_enron_blockmodel_sfdp.pdf'
    )
```

![Cap](figures/core_enron_blockmodel_sfdp1.pdf)

In this figure, each node is represented by an individual point (as in other network visualiztions), only the nodes are organized into blocks. The squares are points where blocks converge up the hierarchy to form the nested structure - the structure of email exchanges between blocks will decide whether a block should be grouped with another one. For example, if you look at the group of 6 blocks in the top left of the image, you might notice that there are only two traders present, but there are a lot of lawyers and vice presidents, as well as a CEO. 

This first attempt is already looking pretty good. We have 3 of the 4 CEOs in the same block near the right-hand side, along with three presidents. Note for later: the remaining CEO isn't in the same meta-block - one level up the hierarchy - as the other CEOs.

As with other generative models, *we need to think through generative mechanisms here*. If you recall from Chapter 25, all this really means is that we need to think through simple social and interactional processes that may have resulted in (i.e., generated) the patterns we see in our data. *What's a plausible story of how this data was generated?* 

Remember that we are detail with *email communication* between employees in an organization here. There are many ways to imagine the social mechanisms that best predict structure in a network like this. In this case, it could be that emails between the core employees predicts the relationship between those employees, or it could be that the emails they send *to other non-core employee Enron email addresses* are more predictive. This is an exploratory process that can't fit reasonably in this chapter, but you can see a bit of it in the online supplement.

Let's see what the outcome is with different blockmodel estimation criteria. Stochastic blockmodels in graph-tool are able to incorporate edge weights into the estimation.


```python
state_w = minimize_nested_blockmodel_dl(eG, deg_corr = True, 
                                              state_args=dict(
                                                  recs=[eG.edge_properties['edge_weight']],
                                                  rec_types=["discrete-binomial"]))
```


```python
state_w.print_summary()
```

    l: 0, N: 149, B: 67
    l: 1, N: 67, B: 10
    l: 2, N: 10, B: 2
    l: 3, N: 2, B: 1


We can see already that we end up with far too many blocks to be useful here! There's no need to visualize this graph, but we have another option - let's try setting the number of blocks to be the same as it was for the unweighted model, then see what the weights do for the results.


```python
state_w2 = minimize_nested_blockmodel_dl(eG, deg_corr = True, B_min=12, B_max=12,
                                              state_args=dict(
                                                  recs=[eG.edge_properties['edge_weight']],
                                                  rec_types=["discrete-binomial"]))
```


```python
state_w2.print_summary()
```

    l: 0, N: 149, B: 12
    l: 1, N: 12, B: 3
    l: 2, N: 3, B: 2
    l: 3, N: 2, B: 1


At first glance (Figure @fig-31_02), incorporating edge weight seems as though it produces more tightly-knit, smaller blocks, and only two distinct groups of blocks one level up the hierarchy where we had four with the first model. The larger blocks are also more heterogenous, with CEO's grouped alongside many traders and even "employees". 


```python
state_w2.draw(
    layout = "sfdp", 
    vertex_text = eG.vertex_properties['title'], 
    eorder = eG.edge_properties['edge_weight'],
    vertex_text_position = 315,
    bg_color=[255,255,255,1],
    output_size=[4024,4024],
    output='figures/core_enron_blockmodel_sfdpw.pdf'
    )
```

![Cap](figures/core_enron_blockmodel_sfdpw1.pdf)

The use of edge weights in a blockmodel is a theoretical consideration more than it is a technical one, so it takes some careful thought and experimenting to see what the impact is. In our case, we have people with quite different roles in the company, so their email volume will be quite different. If we don't use edge weights, we stick to a stricter definition of equivalence, closer to structural, and here this produces the most intuitive results. Nonetheless, we should have a way to compare the results beyond just looking at a graph - these graphs won't be very helpful for huge networks. We can use the `get_block_membership` utility from the dcss package to add block assignment information to the employee dataframe.


```python
employee_blocks_df = get_block_membership(state, eG, employee_df,
                                         'model_uw_1')
employee_blocks_df = get_block_membership(state_w2, eG, employee_blocks_df,
                                         'model_w_2')
```

Let's take a look at some of the job titles that one would expect to be more well-defined. 


```python
df_by_position = employee_blocks_df.groupby('position').agg(list)
df_by_position[df_by_position.index.isin(['CEO','President', 'In House Lawyer'])].head()
```




                                                                    id  \
    position                                                             
    CEO              [david.w.delainey@enron.com, jeff.skilling@enr...   
    In House Lawyer  [bill.rapp@enron.com, carol.clair@enron.com, d...   
    President        [greg.whalley@enron.com, jeffrey.a.shankman@en...   
    
                                     model_uw_1_block_id  \
    position                                               
    CEO                                     [5, 5, 0, 0]   
    In House Lawyer  [1, 9, 10, 10, 9, 7, 5, 3, 9, 9, 3]   
    President                              [5, 5, 0, 12]   
    
                                    model_w_2_block_id  
    position                                            
    CEO                                   [0, 5, 1, 5]  
    In House Lawyer  [9, 9, 0, 8, 2, 8, 3, 5, 6, 1, 6]  
    President                             [2, 2, 1, 5]  



You might be able to get a sense of things from some of the smaller lists here. For example, in the `model_uw_1_block_id` column, we can see that one block has 3 of the 4 CEOs, as well as 3 of the 4 Presidents, while another has the remaining CEO and President. 6 of the lawyers also tend to end up in the same block on this run (again, this is stochastic so results might vary a little bit).  With the weighted model, only two of the CEOs end up in the same block, although they are joined by a President and a lawyer.

Alternatively, we can count the number of unique block assignments by role (job title) and calculate the average, based on the number of people with those roles. A lower value here would be a loose indicator of accuracy, with two caveats: a 0.5 value for CEO would be the same if the 4 CEOs were divided equally into two blocks, rather than 3 in one block and 1 in another. This block assignment difference is conceptually significant, so a more robust metric might be desirable. Job titles that apply to only 1 employee will also, necessarily, have a perfectly poor score of 1.0 every time.


```python
employee_blocks_df.groupby(['position'])['model_uw_1_block_id'].agg(lambda x: x.nunique()/x.count())
```




    position
    Administrative Assistant    1.000000
    Analyst                     0.400000
    Associate                   1.000000
    CEO                         0.500000
    CFO                         1.000000
    COO                         1.000000
    Director                    0.411765
    Employee                    0.600000
    In House Lawyer             0.545455
    Manager                     0.466667
    Managing Director           0.666667
    President                   0.750000
    Senior Manager              1.000000
    Senior Specialist           0.875000
    Specialist                  0.500000
    Trader                      0.200000
    Vice President              0.423077
    Name: model_uw_1_block_id, dtype: float64




```python
print(employee_blocks_df.groupby(['position'])['model_uw_1_block_id'].agg(lambda x: x.nunique()/x.count()).sum())
print(employee_blocks_df.groupby(['position'])['model_w_2_block_id'].agg(lambda x: x.nunique()/x.count()).sum())
```

    11.338629507747154
    11.916386064915477


We can do the exact inverse to roughly assess the homogeneity of the blocks, by reversing the columns in the groupby operation.


```python
employee_blocks_df.groupby(['model_uw_1_block_id'])['position'].agg(lambda x: x.nunique()/x.count())
```




    model_uw_1_block_id
    0     0.750000
    1     0.583333
    2     0.277778
    3     0.476190
    4     0.555556
    5     0.416667
    6     0.230769
    7     0.625000
    8     0.714286
    9     0.400000
    10    0.666667
    11    0.500000
    12    0.666667
    Name: position, dtype: float64




```python
print(employee_blocks_df.groupby(['model_uw_1_block_id'])['position'].agg(lambda x: x.nunique()/x.count()).sum())
print(employee_blocks_df.groupby(['model_w_2_block_id'])['position'].agg(lambda x: x.nunique()/x.count()).sum())
```

    6.862912087912089
    7.970732305329079


This loose evaluation suggests that the unweighted model might be preferred, but we can do better with this evaluation. Sci-kit learn provides *many* classification evaluation metrics and the problem we're solving here is essentially a clustering classification. There are metrics within sklearn's clustering section that provide the above evaluations but with more nuance (remember the equivalent 0.5 score if the CEOs were clustered with different proportions but the same number of blocks). A `homogeneity_score` evaluates, you guessed it, the homogeneity of the detected clusters, so if clusters contain more of the same type of job title, the results will score higher. Scores here are on a scale from 0 to 1, with 1 being the best.  


```python
from sklearn.metrics import homogeneity_score, completeness_score, adjusted_mutual_info_score
```

Let's compare homogeneity scores for the unweighted network and then the weighted one. As with the rough evaluation above, the unweighted model has a better score.


```python
homogeneity_score(employee_blocks_df['position'], employee_blocks_df['model_uw_1_block_id'])
```




    0.353428152904928




```python
homogeneity_score(employee_blocks_df['position'], employee_blocks_df['model_w_2_block_id'])
```




    0.25528558562493037



The `completeness_score` inverts the previous score, instead assessing the homogeneity of block assignments for each job titles, so the degree to which nodes are assigned to blocks with other nodes that have the same title. The result is actually very similar in this case!


```python
completeness_score(employee_blocks_df['position'], employee_blocks_df['model_uw_1_block_id'])
```




    0.3435558493343224




```python
completeness_score(employee_blocks_df['position'], employee_blocks_df['model_w_2_block_id'])
```




    0.2771316517440044



Finally, we can also do both of the above in a unified score, `adjusted_mutual_info_score`, where homogeneity and completeness are considered together and the position of the ground-truth and predicted labels doesn't matter. This can also be used to calculate agreement between two labelling methods, when there is no known ground-truth, but unfortunately our block assignment classifications will not be the same between models - `block 1` in one model is not necessarily the same as `block 1` in the next, or even in repeat runs of the same model. Note that this method is a version of `normalized_mutual_info_score` that is adjusted to account for chance, because the standard mutual information score tends to overestimate the shared information between models that have a larger number of clusters.

For this score, the maximum is 1 but it is possible to have a negative score if the predicted clusters are nonsensical enough. We can see that the adjusted mutual info score below is roughly half of the individual scores above, for the unweighted network. For the weighted network, the score is *much* lower. If we compare the two block assignments together, they actually have more agreement with each other than the weighted model has with the ground truth job titles.


```python
adjusted_mutual_info_score(employee_blocks_df['position'], employee_blocks_df['model_uw_1_block_id'])
```




    0.15309516996415473




```python
adjusted_mutual_info_score(employee_blocks_df['position'], employee_blocks_df['model_w_2_block_id'])
```




    0.0756412457785869




```python
adjusted_mutual_info_score(employee_blocks_df['model_w_2_block_id'], employee_blocks_df['model_uw_1_block_id'])
```




    0.15563023649762936



With this information in mind, let's continue on with the unweighted network to see if we can optimize it more, then examine the end result.

###  Model Selection and Optimization

Given the stochastic nature of these models, it is always advisable to run them a number of times and then select the model with the least entropy. Higher entropy is not *inherently* bad. **Properly discuss entropy here, and why we care**. For example, a compressed JPEG image with only two colours will have a lot less entropy than one with a thousand colours. 

In the case of stochastic block models, entropy returns the minimum description length, which is the amount of information the model needs to recreate the entire network. The goal of reducing entropy is fundamental to these models, with the assumption that minimizing entropy results in simpler models that do a better job of uncovering latent similarities in the data without overfitting. Below, we'll execute 10 runs of `minimize_nested_blockmodel_dl` and print the entropy for each. 


```python
states = [minimize_nested_blockmodel_dl(eG, deg_corr=True) 
          for n in range(10)]

for s in states:
    print(s.entropy())
```

    6162.281933127059
    6187.135324492942
    6168.918484063684
    6161.190122173799
    6163.517013260514
    6162.876759036053
    6178.052196472743
    6154.1481501809185
    6166.798460034726
    6154.869718381805


We can automatically grab the lowest entropy state using `np.argmin`.


```python
state = states[np.argmin([s.entropy() for s in states])]
```

### More MCMC

At the expense of increased runtime, we can also follow-up the above model selection process by sampling from the posterior distribution and running `mcmc_equilibrate`, which performs random changes in the block assignments of the nodes, automatically handles the entropy calculations, and chooses the optimum values at the end. This step is also required to collect the block assignment posterior marginals, which tell us the likelihood (if any) that a node belongs to each block, based on the assignments it was given during the iterations. More iterations here will always improve the model, but with decreasing improvement/run-time payoffs.

First, we will use the object `S1`, defined below, to keep track of the original entropy score to see how much we improved the model.


```python
S1 = state.entropy()
S1
```




    6154.1481501809185



To collect marginal probabilities with MCMC, the blockstate needs to have been prepared for sampling, rather than for minimizing description length, which we can achieve by copying the blockstate and setting sampling to `True`. At the same time, we will add an additional 4 empty levels to the nested hierarchy so that the model has a chance to assign more levels. If these hierarchy levels don't improve the model, the equilibration method will collapse them.


```python
state = state.copy(bs=state.get_bs() + [np.zeros(1)] * 4,sampling = True)
```

We're going to perform many iterations of the `mcmc_equilibrate` function, where nodes are moved between different blocks. Importantly, the MCMC method used in graph-tool doesn't perform fully random moves, which would be a fairly typical MCMC approach. By taking advantage of the assumption that networks are made up of heavily interdependant observations, the MCMC estimation only has to randomly sample from probable block assignment moves - to the blocks that a node's alters are members of. 

We create a callback function to pass to `mcmc_equilibrate` so that we can collect a set of block assignment choices from each iteration. The `bs` values can be thought of as votes for block re-assignment, and constitute the posterior marginal probability of each node's assignment to each block.


```python
bs = []

## OUR CALLBACK FUNCTION THAT APPENDS EACH ESTIMATED BLOCKSTATE TO THE ARRAY
def collect_partitions(s):
    global bs
    bs.append(s.get_bs())
        
mcmc_equilibrate(state, force_niter=10000, mcmc_args=dict(niter=10), callback=collect_partitions)
```




    (6159.927069603201, 37378153, 4808499)



Note that this will sometimes result in higher entropy for the block model solution! That's because we need to select the best partition from the ones added to the `bs` list by the callback function.


```python
state.entropy() - S1
```




    5.778919422760737



The `PartitionModeState` function takes our set of labeled partitions and tries to align them into a single set of common group labels. We can then use the `get_marginal()` method of the returned object to create a vertex property map of marginal probabilities for our original network graph. This property map can be used for calculations as well as for visualization of probable block memberships.


```python
pmode = PartitionModeState(bs, nested=True, converge=True)

pv = pmode.get_marginal(eG)
eG.vertex_properties['pv'] = pv
```

Finally, the convenience function `get_max_nested()` returns the most likely block assignment for each node as a single final blockstate, which will group nodes in proximity to each other in our visualization, based on their most likely membership. We apply this result back to our original blockstate object by providing it to the `copy()` method of the state object. Note that our entropy has improved a bit more here!


```python
bs = pmode.get_max_nested()
state = state.copy(bs=bs)
state.entropy()
```




    6153.278269237107



Let's re-calculate the same mutual information scores we used earlier to see if things have improved on those criteria.


```python
employee_blocks_df = get_block_membership(state, eG, employee_blocks_df, 'model_uw_mcmc')
homogeneity_score(employee_blocks_df['position'], employee_blocks_df['model_uw_mcmc_block_id'])
```




    0.38131989351325507




```python
completeness_score(employee_blocks_df['position'], employee_blocks_df['model_uw_mcmc_block_id'])
```




    0.3526819549124348



Homogeneity improves from 0.35 to almost 0.39, while completeness only improves a small amount.


```python
adjusted_mutual_info_score(employee_blocks_df['position'], employee_blocks_df['model_uw_mcmc_block_id'])
```




    0.1561547346951431



But the adjusted mutual info score below is actually slightly worse than it was before! This doesn't necessarily mean the results are worse, though. We'll take a look at a different layout for the blockmodel below and discuss some potential explanations for this.

### Visualizing Block Connections as a Radial Tree

While the sfdp layout does a nice job of positioning nodes (and blocks) in spatial relation to each other, the radial tree layout can be very helpful for getting a sense of the connection patterns between the blocks and also keeps nodes together in a way that makes individual blocks very easy to distinguish. Since it is the default layout for printing a block state, we can easily obtain a simple representation using the `.draw()` method (see Figure @fig-31_03).


```python
state.draw()
```


    
![png](chapter_31_latent_network_structure_stochastic_block_models_files/chapter_31_latent_network_structure_stochastic_block_models_94_0.pdf)
    





    (<VertexPropertyMap object with value type 'vector<double>', for Graph 0x7f74c8ac3670, at 0x7f74c43f4a00>,
     <Graph object, directed, with 172 vertices and 171 edges, at 0x7f74c433b9d0>,
     <VertexPropertyMap object with value type 'vector<double>', for Graph 0x7f74c433b9d0, at 0x7f74c431ea90>)



As is often the case, there are a few preparation steps we can do to improve the visualization of edges, as well as to add node labels to our figure. This process is a bit complex and is an adaptation of one that was devised by the author of graph-tool. The details aren't particularly important, so we can use the utility function `label_radial_blockmodel` from the dcss package to take care of most of it. 


```python
eG = label_radial_blockmodel(eG, state)
```

The resulting figure is much improved (Figure @fig-31_04), and clearly shows the relations between blocks, while also making it easier to examine which job titles were assigned to each block.


```python
state.draw(
    vertex_text = eG.vertex_properties['title'], 
    eorder = eG.edge_properties['edge_weight'],
    vertex_shape='pie',
    vertex_pie_fractions=eG.vertex_properties['pv'],
    edge_control_points = eG.edge_properties['cts'],
    pos=eG.vertex_properties['pos'], 
    vertex_size=10, 
    edge_pen_width = 0.2,
    bg_color=[255,255,255,1],
    vertex_text_rotation=eG.vertex_properties['text_rot'],
    vertex_text_position=0,
    output='figures/core_state_radial_tree_labels.pdf'
    )
```




    (<VertexPropertyMap object with value type 'vector<double>', for Graph 0x7f74c8ac3670, at 0x7f7550a574f0>,
     <Graph object, directed, with 172 vertices and 171 edges, at 0x7f7550a4d700>,
     <VertexPropertyMap object with value type 'vector<double>', for Graph 0x7f7550a4d700, at 0x7f7550a55970>)



![Cap](figures/core_state_radial_tree_labels1.pdf)

You'll notice that some of the nodes are broken up into pie fractions - these indicate their probability of being assigned to a different block. In the full colour version, these fractions are coloured the same as the alternative block that the node might have been assigned to. You'll also notice that the blocks have become significantly more heterogenous! Traders are in blocks with other traders, most lawyers are in a block that two other lawyers had some probability of being assigned to, and the CEOs are in fairly exclusive blocks. Although we no longer have 3 CEOs in one block with the COO, the block that one of the CEOs was moved to contains the other CEO, and their two respective blocks form a single block one level up the hierarchy! Earlier I mentioned that there are possible explanations for a decreased adjusted mutual information score and this is one example - that score doesn't incorporate the higher levels of the hierarchy. Even though it's probably actually a better model to have the four CEOs split evenly among two blocks, then put those two blocks together at the next hierarchy level, this would still negatively impact the mutual info score compared to the model where 3 CEOs were in one block. 

It's quite clear from the results of these stochastic blockmodels that there's some very powerful estimation going on, and that the Bayesian aspects of it allow a great deal of nuance. The versatility of the modeling that drives graph-tool has led to a collaborative extension for topic modeling. Given the relational nature of words in text, which is often analyzed in the same way as social relations, topics can be blockmodelled from text documents to great effect. We'll explore this method in the section that follows.

### TopSBM: A Unified Bayesian Approach to Latent Variable Modelling for Text and Networks

At the start of this chapter, I noted that there is a deep underlying similarity between SBMs and LDA. Both are Hierachical Bayesian models developed to overcome the limitations of widely-used deterministic methods. However, @gerlach2018network have generalized SBMs to text data as an alternative to LDA, and in doing so have addressed some of the limitations inherent in the baseline LDA models. Specifically, TopSBM, as the generalization is known, makes use of the full set of additional inferential techniques described above that enable TopSBM to:

- explore a broader variety of topic mixtures by avoiding the use of a Dirichlet prior;
- removes the need to select a number of topics in advance due to the specific hierarchical design of the model, and the use of hyperpriors;
- uses efficient MCMC posterior inference rather than variational inference, which means that the model can handle large-scale data without relying on an optimization strategy.

The TopSBM approach models text datasets as a **bipartite network**, which is a network that has two types of nodes, and where connections are only allowed across types. In this case, the two types of nodes are words and documents, where we assign an edge between words and documents if a word appears in a document, but we do *not* assign edges from word to word, or document to document. 

One of the deep conceptual similarities here is that the matrix form of a bipartite network is analogous to a document-term feature matrix, and when the nodes in the bipartite network are words and documents, *the matrices are identical*. The identical underlying data structures and the conceptual similarity of the goals of recovering latent topics in text data and latent structure in network data allows us to easily develop SBM models for text data. In doing so, we benefit from the additional inferential techniques that have been developed for SBMs. 

A full discussion of TopSBM is beyond the score of this chapter, and sadly so is developing a model, but this will be available in the supplementary online material. However, I strongly encourage you to put your developing skills to use here and try your hand at developing one! Meanwhile, if you want to learn more about the theory and mathematics behind TopSBM, or better understand the relationship between topic modeling and community detection and blockmodelling, I recommend reading @gerlach2018network. If you just want to take a look at some results, there is a pickle available in the `data/pickles` directory, and a few example topics in a dataframe below. This model was run on a 100K random sample of the Canadian Hansards and took quite a long time to complete.


```python
topSBM_model = pickle.load( open( 'data/can_hansard_100k_sample_topSBM.pkl', 'rb'))
```


```python
topic_dict = topSBM_model.topics(l=1,n=20)
```


```python
df_list = []
for topic in [76,91,200,228,104,126]:
    df = pd.DataFrame.from_records(topic_dict[topic], columns = ['words_' + str(topic), 'scores_' + str(topic)])
    df_list.append(df)
topic_df = pd.concat(df_list, axis=1)
topic_df.head(20)
```




           words_76  scores_76    words_91  scores_91       words_200  scores_200  \
    0           gas   0.184736      regime   0.206437  infrastructure    0.259398   
    1       climate   0.141000   violation   0.109540    municipality    0.131606   
    2        carbon   0.101561        Iran   0.064138            road    0.109510   
    3      emission   0.088723     torture   0.049770        building    0.093287   
    4         clean   0.075559     protest   0.032759       municipal    0.077184   
    5    greenhouse   0.069086     iranian   0.029655    construction    0.059265   
    6          fuel   0.065223        Cuba   0.026782         transit    0.058902   
    7     pollution   0.048523    activist   0.023678           mayor    0.042920   
    8         green   0.046075      brutal   0.019885             bus    0.026273   
    9       ethanol   0.013056       Egypt   0.018966          design    0.018221   
    10      heating   0.012566    dictator   0.017126          stream    0.013742   
    11      warming   0.012185  systematic   0.015632      councillor    0.011926   
    12       fossil   0.012131   Venezuela   0.014253  Municipalities    0.011320   
    13  temperature   0.011913       Khadr   0.013448           sewer    0.008112   
    14      Climate   0.009846   execution   0.012644           Mayor    0.007022   
    15     polluter   0.009248     embargo   0.011494            pass    0.006659   
    16       Change   0.009084        Omar   0.011034         upgrade    0.004903   
    17    pollutant   0.006582     Amnesty   0.010920          shovel    0.004480   
    18      dioxide   0.006473       Burma   0.010575          subway    0.003753   
    19       diesel   0.006093     Myanmar   0.009195            Road    0.003753   
    
           words_228  scores_228      words_104  scores_104       words_126  \
    0      emergency    0.360683         social    0.351863         society   
    1        vaccine    0.064984        poverty    0.166007       principle   
    2          virus    0.054231           poor    0.128998           value   
    3       outbreak    0.047920           rich    0.067926     institution   
    4           SARS    0.046517         living    0.055031     fundamental   
    5       pandemic    0.040673        welfare    0.051680           basic   
    6         spread    0.040673         wealth    0.038989           equal   
    7       epidemic    0.039037         decent    0.020104           voice   
    8   preparedness    0.031791     inequality    0.016753         concept   
    9          Ebola    0.030856  disadvantaged    0.016347        powerful   
    10          dose    0.023142           load    0.012793           ideal   
    11          H1N1    0.022207         hungry    0.008529  characteristic   
    12           flu    0.021272      disparity    0.007869            like   
    13     influenza    0.020570          needy    0.006650     cornerstone   
    14    quarantine    0.020103         steady    0.005584           noble   
    15    infectious    0.018700        kitchen    0.005026       Coalition   
    16           kit    0.014960            pie    0.005026   institutional   
    17          mask    0.014493         clothe    0.005026   philosophical   
    18   containment    0.013558           tory    0.004366       motivated   
    19         avian    0.012856           soup    0.002741        hallmark   
    
        scores_126  
    0     0.191839  
    1     0.164089  
    2     0.145013  
    3     0.115906  
    4     0.085317  
    5     0.066920  
    6     0.052844  
    7     0.048091  
    8     0.043677  
    9     0.016174  
    10    0.008890  
    11    0.008581  
    12    0.007964  
    13    0.007007  
    14    0.005772  
    15    0.005062  
    16    0.004784  
    17    0.003519  
    18    0.002469  
    19    0.002346  



As you can see, these topics end up being pretty coherent! There were 267 topics found from this run and the majority of them are intuitive enough that they seem a bit obvious. This is a good thing.

## CONCLUSION

### Key Points 

- Hierarchical Stochastic Blockmodels are remarkably powerful models that provide a nearly unparalleled degree of insight into the structure of a network and nodes' roles within it 
- SBMs build on the Bayesian intuitions established earlier in this book; they employ a similar approach of using latent variables and prior distributions to model unknown/unobserved 
- TopSBM is really cool. Once you're comfortable with the material in this chapter and the previous one, you should explore TopSBM on your own, or using the supplementary material online.

    
\part{Embeddings, Transformer Models, \& Named Entity Recognition}
