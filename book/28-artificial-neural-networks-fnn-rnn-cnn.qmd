# Neural networks 101

<!-- 
Maybe incorporate bits from: 

_developing-neural-network-models-with-keras.qmd
 -->

## LEARNING OBJECTIVES

- Describe the basic operation of early neural network models, the Perceptron and the Boltzmann machine.
- Explain the basic components of a neural network and how they work together to learn from data and make predictions. 
- Explain how 'forward propagation,' 'backward propagation,' 'gradient descent,' and 'autoencoders' improve the performance of neural networks

## LEARNING MATERIALS

You can find the online learning materials for this chapter in `doing_computational_social_science/Chapter_23`. `cd` into the directory and launch your Jupyter Server.

## INTRODUCTION

This chapter introduces artificial neural network models and deep learning. We will build on the distinctions we drew between the symbolic and connectionist paradigms in Chapter 20. We'll start by introducing the Perceptron, which was one of the first artificial simulations of a biological neron. We will use the Perceptron as a relatively simple entry point into the more complex world of contemporary neural network modelling. 

Once we discussed how neural network models work at the level of individual artificial neurons, we will shift our focus to the basic components and algorithms involved in contemporary neural network modelling. We will emphasize the basic components of a multilayer Perceptron model, as well as the algorithms involved in training these models. More specifically, we will learn how neural network models are organized into layers, with information about our data feeding *forward* through those layers and information about errors flowing *backwards*. We will learn about activation functions, backpropagation, gradient descent, and learning curves. We will conclude with a high-level discussion of more advanced "deep learning" neural network architectures and some ethical and political challenges that we need to consider when using them, or when evaluating other research that uses them.

By the end of this chapter, you will have a solid conceptual foundation in neural network modelling, and a sense of what makes deep learning so challenging.  

## THE PERCEPTRON

To really understand how neural networks work, it's necessary to understand what happens at the level of individual artificial neurons. For that reason, we'll start our introduction to neural networks by discussing the first successful attempt to simulate a biological neuron. Recall from Chapter 20 that neural networks are the model family of choice within the **connectionist** paradigm, which is loosely inspired by biological cognition.

The Perceptron was a highly simplified model of a biological neuron, proposed by psychologist Frank @rosenblatt1958perceptron. The simulated neuron would receive numerical inputs from multiple sources (i.e., other neurons). To simulate differences in the strength of each of those incoming signals, the Perceptron would multiply each by a **weight**. Then it would sum all of the weighted inputs and, if the sum exceeded a specific threshold, then the Perceptron would output 1 (it "fires"); if not, 0 (it does not fire). Recall the threshold model of complex contagion from Chapter 18 as an analogy. If enough neighbours are activated and sending strong enough signals, ego is likely to activate as well. 

For a relatively simple model like this, then, the main questions are:

1. How do you come up with the **weights** that each simulated neuron uses to multiply incoming signals, and
2. what **thresholds** should you place on the simulated neurons to determine whether the sum of the weighted inputs is enough to cause it to "fire" (output 1) or not (output 0). 

Rosenblatt's solution to these problems was influenced by behaviouralist notions of operant conditioning that were dominant in psychology at the time. In brief, he proposed teaching the Perceptron to learn the connection weights itself using a process that we would now call supervised learning.

To illustrate the process, imagine you are training a Perceptron to differentiate between black and white photos of cats and dogs (which is a pretty tall order for the Perceptron, but we'll proceed anyway). In this scenario, your input features are individual pixel values. The number of initial inputs would be equal to however many pixels are in the original image. If each image was 28 pixels by 28 pixels (*much less* than the images we make these days), it would be represented by a total of 784 numerical input features. 

To keep things really simple, in this example we will work with just four input features. @fig-22_01 illustrates the basic model. First, we start by assigning weights using a random number generator. It doesn't matter what our initial weights are *as long as they are not all the same*. Positive weights are "excitatory" and negative weights are "inhibitory." The Perceptron makes an initial prediction by multiplying each input value with its randomly assigned weight and then summing all of these weighted inputs. For example, in this case, it would perform the following calculation: 

$$(0.2 \cdot 0.9) + (0.1 \cdot 0.1) + (0.08 \cdot 0.7) + (0.92 \cdot 0.2) = 0.43$$

If $0.43$ is greater than the simulated neuron's fixed threshold, it fires (outputs 1, predicts cat). If not, it does not fire (outputs 0, predicts dog). 

![The Perceptron received numerical inputs, multiplied them by weights to simulate differences in signal strength, and then "fired" if the sum of weighted inputs was greater than a fixed threshold.](figures/perceptron_threshold.png){#fig-22_01}

Drawing on operant conditioning, if the Perceptron makes an incorrect prediction -- the image was a dog (output = 0) but the Percetron guessed cat (output = 1) -- then it makes a *minor* adjustment to the weights, raising some and lowering others (giving us an early version of supervised learning). Then it makes another prediction on another image using these adjusted weights. It makes further minor adjustments to the weights whenever it makes an incorrect prediction and leaves them as they are when it makes correct predictions. 

This is a dramatically simplified version of the biological networks in our brain. Although we will see how these models can be made more considerably more complex, even the more complex models are simple relative to the biological networks that inspire them. 

It should now be clear what we mean when we say that Symbolic AI and Connectionist AI have very different ways of modelling cognition and "learning" from data. In Symbolic AI, declarative knowledge is represented as symbols and rules, and "cognition" is about performing operations like logical inference on those symbols, as we might when engaging in slow and controlled cognition. Conversely the connectionist paradigm starts below the level of symbols, with perception itself. Cognition is modelled in the form of massive and dense networks of relatively simple neurons, with "learning" being a process of adjusting the weights between neurons. Higher-level mental representations are the products of many neurons firing together. 

Brains are "black boxes" because, as I mentioned in the context of comparing Symbolic AI and Connectionist AI in Chapter 20, you can't peek inside them and "see" concepts like cats, Russian Banyas, hardwood flooring, or toxic masculinity. You won't see them if you look inside a Perceptron (which is far too simple for anything like this) or a contemporary ANN either (you would just see a massive densely-connected network of simulated neurons sending numbers back and forth to one another another). These processes are considered *sub-symbolic* because they *underlie* or *give rise* to the higher-level symbols that we *can* communicate about, for example with natural language. That said, in simpler ANN models, it is possible to see individual neurons influencing one another. You can find an excellent interactive example of this can be found at playground.tensorflow.org. 

The early Perceptron was a simple machine, but it generated an enormous amount of intellectual hype when Rosenblatt first demonstrated it in the 1950s. Despite the obvious limitations of the model, Rosenblatt and others envisioned networks of these Perceptrons processing low-level input signals to perform increasingly difficult high-level tasks, including the kind of computer vision task in our hypothetical example of differentiating between images of cats and dogs. Let's now consider some of the ways that this initial example developed and became more complex, eventually leading to the development of the kinds of neural network models that are dominant today. 

## MULTILAYER PERCEPTRONS

The Perceptron provided an important foundation for contemporary ANN models. Let's take a closer look at a powerful extension that is currently in wide use -- the **Multilayer Perceptron (MLP)**.

### Making Sense of Layers

As you might expect, MLPs organize networks of simulated neurons into multiple layers: an **input layer**, one or more **hidden layers**, and an **output layer**. This layered organization is illustrated in @fig-22_02.

![Simulated neurons in a Multilayer Perceptron are assigned to specific layers: an **input** layer, one or more **hidden** layers, and an **output** layer. Information propogates **forward** (left to right) through the network from the raw data in the input layer to the predictions made in the output later. In training, information about errors are propagated *backward* (right to left) from the output later to the input layer.](figures/mlp.png){#fig-22_02}

Our data enter the MLP via the neurons in the input layer. Just like in the Perceptron, the input values are numerical. Every neuron in the input layer is connected to every neuron in the hidden layer, and every neuron in the hidden layer is connected to every neuron in the output layer. Therefore, the hidden layer in this MLP is **dense**. Later we will learn that there are other ways of organizing the connections between neurons. This model architecture is also **sequential** because information has to pass through each layer one at a time to go from one end of the network to another. Non-sequential model architectures are also possible, but we will not discuss them here.

Analogous to a biological neural network, the neurons in the input layer send their output values to the nodes in the hidden layers. The neurons in the hidden layer multiplies each incoming input value by its weight, which simulates differences in the connections weight between neurons in a biological network. The simulated neuron in the hidden layer then sums all of these weighted inputs to determine what it will output. 

### Activation Functions

At this point, we encounter another importance difference between MLPs and their predecessor, the Perceptron. Whereas the Perceptron had a fixed threshold and would make deterministic binary decisions about whether to fire (1 for yes, 0 for no), the MLP applies an **activation function** to the sum of weighted inputs and outputs a continuous number. The continuous number is then passed along to all connected neurons in the next layer, which in this case are the neurons in the output layer. There are many possible activation functions one might use, but the most common three are sigmoid, tanh, and ReLU. We will learn about each in a moment, but first let's clarify what activation functions are simulating *in general*.

Recall from Chapter 20, during our brief introduction to biological neural networks, we learned that action potentials are triggered in postsynaptic neurons when they receive a level of stimulation that exceeds some threshold. The Perceptron simulated this with an actual fixed threshold, and then the neuron would deterministically "fire" if the weighted sum of inputs exceeds the neurons threshold. In other words, this is a binary step function that outputs 0s for every number before a threshold and 1s for every number above the threshold. But what if the sum of weighted inputs is just a *wee bit above* or below the threshold?

In a biological neural network, relatively low levels of stimulation are insufficient to trigger an action potential, but once the level of stimulation reaches a certain point the probability of an action potential jumps way up. Once that point has been reached, additional stimulation has little additional effect. In the 1980s, researchers realized the many benefits of making this simulated process a bit more like the biological process using activation functions. @fig-22_03 illustrates the differences between four different types of activation functions. For each, the x-axis represents the total input from other neurons, and the y-axis represents the neuron's output.

![Four common activation functions for hidden layer neurons: a binary step function with a hard threshold (A), the sigmoid function (B), the hyperbolic / tanh function (C), and the rectified linear unit function / ReLU (D).](figures/activation_functions.png){#fig-22_03}

The first activation function (@fig-22_03, Subplot A) is the original binary step function used in the Perceptron: if the sum of weighted inputs exceeds some threshold (in this hypothetical example, the threshold is set to 2), then the neuron output 1 (i.e., it fires), else it outputs 0 (i.e., does not fire). In a famous paper from 1986, Rumelhart, Hinton, and Williams [-@rumelhart1986learning] replaced the binary step function with a sigmoid activation function. (You may recognize the sigmoid function, @fig-22_03 Subplot B, from the context of logistic regression.) One of the many advantages of their revised approach is that it eliminates entirely the need to use hard thresholds and binary outputs. Instead, the sigmoid function returns a real number between 0 and 1, which can be interpreted as a probability. Similarly, the hyperbolic tangent activation function (@fig-22_03, Subplot C) is continuous and S-shaped, but outputs values between -1 and 1. 

The fourth activation function shown in @fig-22_03 is the Rectified Linear Unit Function, or ReLU, shown in Subplot D. ReLU simply accepts the sum of weight inputs and if the sum is equal to or less than 0, it output a 0. If the sum of weighted inputs is greater than 0, it outputs whatever that positive value is. In other words, it also output continuous values that have a lower bound of 0, but no upper bound.

The sigmoid, hyperbolic tangent, and ReLU activation functions are all vastly superior to the binary step / hard threshold activation function. Aside from making ANNs slightly closer to their biological counterparts, these functions all output continuous values, which turns out to be *very* useful when training ANNs.

### What About the Output Layer?

Once we've added as many layers consisting of as many neurons as we care to include, we're ready to create an **output layer**. Neural networks provide you with a remarkable degree of latitude when constructing them; the output layer is no exception. It's up to you to decide what the best output later for your ANN will be; that decision will rest, in large part, on whether you're interested in performing, for example, a Regression task (numerical approximation of a continuous variable) or a Classification task (probabilistically sorting observations/cases into two or more different classes). If you're performing a Regression task, you can get away with using a simple single-neuron output layer with a fully linear activation function; this will effectively end up producing a (scaled) numerical output representing the summed-up weighted signals from each of the neurons in the preceding layer. If you're performing a classification task with 2 labels/classes, you can use an output layer that replicates the functionality of a logistic regression. For classification tasks with more than 2 labels/classes, you can use a 'softmax' activation function, which is a generalization of logistic regression into larger numbers of categories. An intuitive interpretation of a softmax-powered output layer might view it as allowing each part of the preceding layer to 'vote' on which of the categories that part thinks the item belongs to. The softmax function then normalizes each of the 'scores' for each category into a series of probabilities that sum to 1; the item is classified as belonging to the class with the highest probability.

## TRAINING ANNS WITH BACKPROPAGATION AND GRADIENT DESCENT

One of the major challenges that held back advancing this model for quite some time was that training a multi-layered ANN is far more complex than a single neuron (e.g., a Perceptron). When ANNs make incorrect predictions, how do we know which weights in the hidden layer of the network to adjust? The possible paths from input through the hidden layer to the output is very complex, and every mistaken prediction could be the result of any number of thousands or millions of weights and thresholds. Which ones should change, in what direction, and by how much? Backpropagation is an algorithm that enables us to answer those questions and better train our ANNs.

### Backpropagation

When neurons in the hidden layers of an ANN output continuous values, rather than a binary decision, we can quantify the extent to which each individual weighted connection contributed to an overall prediction error. Just as information flows forward from an input to a final prediction, it is possible to send information about errors *backwards* from the final prediction to the input layers. This is called **backpropagation**. The algorithm itself was developed in the 1970s, but Rumelhart, Hinton, and Williams [-@rumelhart1986learning] famously showed its usefulness in the context of neural network modelling, and it has greatly improved the ANN training process. In fact, backpropagation may be the most important algorithm in ANN modelling. 

Working backwards from a prediction, the backpropagation algorithm starts by using a loss function to compute an overall measure of prediction error. The specific loss function used depends on context. If you are training a neural network on a regression problem, then Mean Squared Error (MSE) is a good choice. If you are training a neural network for a multi-class classification problem, Categorical Cross Entropy is a better choice. In short, the loss function you use depends on the type of model you are developing. Once the overall error has been computed, the next step is to calculate how much each individual weight in the ANN contributed to that error. Each weight can then be adjusted in the direction that would best minimize the overall prediction error. These adjustments are *minor* and *local*. Very small adjustments are made to weights based on the impact those changes would have only on connected neurons. 

Training an ANN with backpropagation involves two processes for each example provided in training: a **forward pass** and a **backward pass**. During the forward pass, information is sent through the network from the input layer, through the hidden layer, and out the output layer. As in Figure XXX, you can picture this as information moving from left to right. The neurons in the hidden layer output continuous values that result from applying an activation function to the inputs they receive from the neurons in the layer below them. The neurons in the output layer apply a different activation function, but we will discuss that later. 

The backward pass starts after a prediction has been made. First, an overall prediction error is computed using a loss function such as Mean Squared Error or Categorical Cross Entropy. Then the contribution that each connection weight makes to that overall error is computed, and then small adjustments are made to the weights such that the overall error is minimized. Once the adjustments are made, training can proceed with the next example. 

### Gradient Descent 

The changes made to connection weights are governed by an optimization algorithm called Gradient Descent. *In general*, Gradient Descent is used to find parameter values for some function that minimizes the loss, or cost, as much as possible. In the case of ANN modelling, our goal is to find the optimal values for all of the connection weights across all layers in our ANN. With Gradient Descent, we do that by making small modifications to the connection weights over *many* iterations. We start with our randomized weights and adjust them iteratively during training, example after example, until we have values that minimize the overall loss measured by our loss function. 

In @fig-22_04, loss is represented on the Y axis, and the range of possible weight values is represented on the X axis. The connection weights range from -1 to 1, as they might if we were using the hyperbolic tangent activation function (`tanh`). Let's say that our randomly selected starting point is the point S1 (for "Step 1"). If this was the value used for the connection weight, the loss for that weight would be the corresponding `y` value (which in this hypothetical example is not actually shown). If the connection weight was a little higher (e.g., shifted to the right a bit), the loss would decrease. Therefore, we increase our connection weight a bit in the next step, from -0.79 to -0.59 and reduce our loss a little bit.

At each step, the algorithm calculates how much the loss would change if the weight value was slightly different. Each potential new weight value has a **gradient**, or slope. Since we want to minimize loss, gradient descent will select a weight value that lets it *descend* (the opposite would be gradient ascent). 

We continue these relatively small steps for S3, S4, S5, and S6, gradually increasing the connection weight and reducing the loss. By S7, we reach a connection weight that minimizes the loss. We have *converged* to the minimum loss. From this position, any further adjustments to the connection weight *regardless of the direction* would increase the loss (and would be gradient *ascent*). We can't minimize the loss any further.

![An illustration of Gradient Descent to iteratively find connection weight values that minimize loss. In this example the optimal weight is 0, but it could have been any value between -1 and 1.](figures/grad_desc_1.png){#fig-22_04}

Each step in @fig-22_04 is a **learning step**. The size of those steps is determined by a learning rate learning parameter in our ANN. In general, small steps are better because they reduce the chance of accidentally stepping over the optimal weight value. In our example, it might mean stepping over the optimal value at the bottom of the curve (0), climbing back up the curve on the other side, and then bouncing back and forth without ever stopping on the optimal value. The downside of using small learning rates is that it takes longer, and more iterations, to find the value that minimizes loss. 

There is no reason to assume that the loss function has a nice single valley shape like the one in @fig-22_04. It could a wide variety of shapes, such as the one shown in @fig-22_05. This illustrates another issue that we might encounter when using Gradient Descent to find the optimal value for the connection weights in our ANN. First, imagine you have a random starting point: in this case, the grey S1 point. From here, Gradient Descent makes adjustments, each time reducing the loss by gradually increasing the weights with steps 2 - 6 (all represented with the grey points). After the 6th step, any further increases in the connection weight start *increasing* the loss. Gradient Descent *thinks* it has found an optimal weight, but it hasn't. It's stuck on a **local minimum** and doesn't know about the **global minimum** that it might have found if the random starting point had been the black S1 point further to the right of the Figure. 

Clearly the black S6 point in @fig-22_05 is better than the grey S6 point because it has a lower loss value. The more complex the terrain produced by the loss function, the more challenging it is to converge on the *global* minimum. There are variations on the basic Gradient Descent algorithm that can help with this problem. **Stochastic Gradient Descent (SGD)**, for example, computes the gradients based on a randomly selected subset of the training data. This causes the loss values to jump around a lot more at each step, but over many iterations, it greatly reduces the risk of getting stuck in local minimums and increases the chance we will get at least very close to the global minimum. We will see an example of using SGD in an ANN later in this chapter.

![An illustration of Gradient Descent to iteratively find connection weight values that minimize loss.](figures/grad_desc_2.png){#fig-22_05}

### Learning Curves

As an ANN trains, it uses backpropagation and gradient descent to iteratively tune the weights until it converges on an locally optimal solution (which isn't always the globally optimal solution). It goes through data *repeatedly* to do this. Each pass through the data is a single **epoch** of training. As we will see in the next chapter, we can calculate various performance measures during each epoch of training; it is often helpful to plot these measures. @fig-22_06 illustrates this for a hypothetical ANN. In this example, we can see that the accuracy rates for both the training and validation data increases with each epoch, and that the two rates are very close together (which suggests the model is not overfitting). 


![Accuracy and loss metrics for the test and validation data at each epoch of training.](figures/learning_curves_ann.png){#fig-22_06}

## MORE COMPLEX ANN ARCHITECTURES

Now that we've discussed some fairly simple neural network models and I've explained how some key algorithms (such as Gradient Descent) work, let's turn our attention to some more complex ANN architectures. Focus on the big picture here. The goal here is still conceptual; we want to understand, at a high-level, what these types of models do and how they work. I will refer back to these models in later chapters (at which point you might want to flip back to these pages to remind yourself of the big picture).

### Stacked Autoencoders

**Autoencoders** are a type of ANN that attempt to produce an output identical to whatever input was received, which is not as pointless as it might sound. Autoencoders have hidden layers that are smaller than their input and output layers. By trying to produce an output that is identical to their inputs, they to learn how to create a high-quality representation with a smaller number of bits. (You can think of this as analogous to file compression; when you zip a file, the same file contents are represented using fewer bits.) In practice this introduces a lot of computational problems, so instead we can use a clever trick. We make the hidden layer *bigger* than the input and output layers, but at any given moment only a small portion of those neurons are allowed to be active, meaning the autoencoder is still forced to learn a more compact representation, but the math is easier. 

Increasingly sophisticated representations can be learned when autoencoders are stacked together, with the outputs of one becoming the inputs for another. With each autoencoder, the representations are less like low-level perceptual patterns and more like higher-level mental representations analogous to the types of symbols that feature in cognitive science and Symbolic AI, such as rules, frames, schemas, and scripts. In this way, we can use Autoencoders as a form of neural network-based dimensionality reduction; their low-dimensional representations of high-dimensional objects can be very useful! We will see some examples of this later in the book.

## CONVOLUTIONAL NEURAL NETWORKS

Convolutional Neural Networks (CNNs, or ConvNets) are ubiquitous in computer vision -- working with image data -- and are increasingly also used in natural language processing. They were first introduced in the 1980s by Yann LeCun, who was inspired by the work of Kunihiko Fukushima on some of the first deep neural networks developed for computer vision tasks, in this case recognizing hand written digits. Two of the most common computer vision tasks CNNs are currently used for are:

1. Image classification
2. Object detection / recognition

Like neural networks more generally, CNNs are biologically *inspired*. In particular, they are inspired primarily by our brain's vision system. Let's take a moment to understand some basics of the biology to better understand how the artificial version works. It is important to remember that the artificial versions depart from their biological inspirations in important ways; CNNs don't need to work like their biological counterparts anymore than a plane should fly the same way birds do, but the biology helps provide a framework for making sense of CNNs. 

### From Biological Vision to Artificial Vision

How do we see? The eyes play a very important role, of course, but the brain doesn't simply receive images provided by the eyes. It actively *constructs* those representations. Our retinas convert patterns of light into neural signals using photoreceptors (consisting of rod and cone cells). Special neurons in the retina (bipolar cells) detect light areas on dark background and dark areas on bright backgrounds. Then, at a higher level of neural processing, retinal ganglion cells respond to differences in light within their own **receptive field**. These cells may activate depending on whether there is light in the centre or the edges of their receptive field, or in some particular orientation (e.g. horizontal), to which they are especially sensitive. Together they enable the detection of edges and other low-level building blocks of vision (Ward 2020). 

After this fairly minimal amount of processing, a neural signal is sent from the eyes to the brain via the optic nerve, where it splits into several different pathways. The most important, for our purposes, is the pathway to the primary visual cortex, located in the back of our brain. Starting in the late 1950s, David Hubel and Torsten Wiesel made a series of discoveries that revealed the *hierarchical* organization our visual system, in which complex abstract representations are built from the bottom up by combining simple low-level representations (e.g. edges). In addition, there is a *top down* process where our brains are actively involved in the construction of those representations, for example by filling in missing bits of information and creating a 3-dimensional model of our environment from 2-dimensional inputs (Ward 2020). These *backward* connections are important -- they outnumber the *forward* connections -- but are not currently well-understood (Mitchell 2019). 

In the primary visual cortex, then, neurons are organized into hierarchical layers, with those on the bottom detecting low-level features, like edges. The layers above it detect more complex features, from relatively simple shapes to more complex ones, eventually resulting in our conscious perception of faces, objects, and the rest of our environments (Ward 2020).

#### How Convolutional Neural Networks Process Data: From Pixels to Predictions

CNNs, like other artificial neural networks, are inspired by this biology, but they don't mirror it exactly. Drawing on the hierarchical organization of the visual system, CNNs are made up of a sequence of *layers* of neurons, with each layer in the sequence sending its output to the layers of neurons that come next, where they are processed as inputs. As with the multilayer Perceptron and other neural networks, each of these artificial neurons has an activation value that is computed from an input value and a weight. 

Let's say we have an image of a cat. We can represent that image as a matrix that encodes information about the brightness and colour of each individual pixel in the image. Each neuron in the first layer corresponds to one of those pixels in the image, so they must be the same size. In other words, if there are 12 million pixels (as there are in an image from an iPhone in 2021, for example), then there must be 12 million neurons in the CNN's first layer. 

Each hidden layer in the CNN is itself made up of multiple **activation maps** (also called **feature maps**), directly inspired by the hierarchical nature of the vision system. The neurons in each of these activation maps is organized like a grid, with each neuron responding to specific features within specific regions of the image like retinal ganglion cells responding to specific patterns of light within their individual receptive fields. Each activation map is focused on different types of visual features. As in the brain, some activation maps are focused on very low-level features such as edges, detected by variations in the distribution and intensity of reflected light. In layers focused on edge-detection, the simulated neurons activate when they detect an edge *within their narrow receptive field* that matches some specific orientation: horizontal, vertical, or any other angle. Their receptive field is a specific pixel location in the input image and a small surrounding area that overlaps with the receptive fields of other simulated neurons in the same activation map. 

This is the most important way in which CNNs differ from other types of neural networks that have *dense* layers. The *convolutional* layers in a CNN are designed to learn patterns that are local, in other words within a narrow receptive field. In contrast, the type of neural networks we have learned about before now had *dense* layers, where every neuron in one layer feeds into the every neuron in the next layer, learning more *global* patterns from the entire image. As Chollet (2018) notes, this means that CNNs only has to learn a pattern once; if it learns it in one part of an image, it will recognize it in other parts of the image without having to relearn it, as a densely connected layer would. As a result, CNNs are more efficient with image training data than networks with densely connected layers are.

Each neuron has an activation value that represents the extent to which the input numbers in its receptive field match its expectation, such as for a horizontal edge. Let's say that the receptive field of a given neuron is a grid of 3 by 3 pixels. The numbers for each of the 9 pixels represent how bright the pixel is, from 0 to 255. As with more basic neural network models, these pixel values are multiplied by a given weight. All of those weighted inputs are summed and the resulting activation value can be passed on. 

Within any given activation map inside any given layer, the simulated neurons all use the same weights. In other words, they all multiply the inputs from the pixels within their receptive fields by the same weights before summing them to produce their activation value. Each of these processes of multiplying the inputs in a receptive field by the weights shared by neurons in the same activation map is called a **convolution**. 

Each layer of the CNN has its own set of activation maps, each of which is a grid of neurons looking for a particular pattern within its narrow receptive field. These layers are called **convolutional layers**. The activation values that result from summing each weighted input are passed from that layer into the next as a new set of input values. Inspired by our biological vision system, the initial layers are focused on very low-level features such as edges, and subsequent layers combine these low-level patterns into more complex shapes and objects. The number of activation maps in each layer, and the number of layers in any given CNN, vary and are controlled by the researcher. 

If we are training our CNN to perform a classification task -- like classifying whether images containing a patch of trees or not -- then the activation values from the penultimate layer of the CNN are passed to a classification module. This module is itself a neural network that will predict the likelihood of a patch of trees given the input values from the final layer in the CNN, which encodes information about the most high-level features in the image (e.g. grass, leaves, tree branches). The classification model outputs the probability of the image containing a patch of trees. If it were an object detection model, it would output probabilities of that the image contains any of the types of objects it knows about. 

Other than the organization of layers into activation maps and the process of performing convolutions on the inputs of each neurons receptive field, CNNs operate like ANNs. The weights, for example, are learned using back propagation during a supervised learning training process. Each pass through the data is a training **epoch**, and typically many of these are required to train a CNN. When the network "converges" on a good set of learned weights, the error is diffused as much as possible via back propagation, training is complete, and if the model is a good one, you can start using it to make predictions on unseen data. 

### Biased Training Data for Computer Vision: Ethical and Political Concerns 

Earlier, I mentioned that CNNs were initially developed by Yann LeCun in the 1980s, but only recently became ubiquitous in computer vision. The models have changed little since they were first introduced. Rather, the massive explosion of data and the rapid increases in computing power brought CNNs into the spotlight. A number of widely-used training datasets, such as ImageNet, have played an important role in this development, and more recently, social media platforms like Flickr and Facebook that serve up massive collections of labeled datasets harvested from users, such as people tagged in photos posted to Facebook. 

There are some *major* ethical and political issues to consider here. First, there is the market for your data; it is entirely possible that your images (as in photos you upload to sites like Facebook or Flickr), uploaded and tagged, are in a training dataset somewhere, perhaps even a public one like ImageNet. But a less obvious, and even more consequential, set of issues concern biases embedded in these massive training datasets, and more importantly appaling misogyny and racism [@crawford2019excavating; @gebru2020race; @buolamwini2018gender; @de2019does; @steed2021image]. @crawford2019excavating state the problem plainly at the start of "Excavating AI: The Politics of Images in Machine Learning Training Sets:"

> You open up a database of pictures used to train artificial intelligence systems. At first, things seem straightforward. You’re met with thousands of images: apples and oranges, birds, dogs, horses, mountains, clouds, houses, and street signs. But as you probe further into the dataset, people begin to appear: cheerleaders, scuba divers, welders, Boy Scouts, fire walkers, and flower girls. Things get strange: A photograph of a woman smiling in a bikini is labeled a “slattern, slut, slovenly woman, trollop.” A young man drinking beer is categorized as an “alcoholic, alky, dipsomaniac, boozer, lush, soaker, souse.” A child wearing sunglasses is classified as a “failure, loser, non-starter, unsuccessful person.” You’re looking at the “person” category in a dataset called ImageNet, one of the most widely used training sets for machine learning. 
>
> Something is wrong with this picture. 

Sometimes the biases are in the relationship between the image and its label, as with the examples that Crawford and Paglen cite. In other situations it is due to asymmetries in who is represented in images and how they are represented [e.g., @gebru2020race; @buolamwini2018gender]. White men, for example, are generally far more represented in these training datasets than other people, and as a result CNNs trained on these datasets tend to perform far better for white men than they do for women or racialized people. For example, a CNN could classify a person as a women because the person is standing in a kitchen, and the training data contains many more images of women in kitchens than men in kitchens. 

A good example of these issues is the 80 Million Tiny Images dataset that was created by and formerly hosted by researchers at MIT [see @prabhu2020large]. The dataset consisted of images scraped from the web and was annotated using crowd sourcing. However, after being in wide use for 14 years, it was discovered that the training data contained thousands of images annotated with racial slurs, not to mention labels such as "rape suspect." The dataset was also found to include many deeply problematic images, such as pictures taken up skirts, that were clearly taken (and of course circulated) without consent.

One especially high-profile illustration of the implications of racially biased training data happened in 2015 when Google released a new feature in their photo app that would tag images with captions derived from a CNN trained to classify the primary contents of an image. Because of training data biased towards white people, the CNN tagged a selfie of two black people with "Gorillas" (Mitchell 2019). Obviously this is unacceptable, and any applications of CNNs on image data -- including for research in computational social science, not just commercial applications -- need to directly address the issues of training data with racial and other biases. 

As you know from Chapter 19, there is an ongoing debate about what we should do, given that biases in training data -- not just image training data -- reflect real biases and inequalities in the real world. On the one hand, we can learn more about these biases and inequalities from the problems that arise from models trained on those biased data. While there may be some merit to this idea within a purely scientific context, datasets used to train these models are very difficult and expensive to collect and build. It's not like we could easily separate out training datasets for commercial applications, in which we work hard to reduce biases, from those intended for scientific research on bias, where we let those biases remain. The same training data is used in both contexts. So at best, using biased training data to study bias is making the best of a bad situation. These biases are amplified by the models and feed back into society, and as we saw in Chapter 19 these negative feedback loops create and solidify inequalities, especially when they are part of facial recognition systems or are part of opaque automated decision-making processes. 

While CNNs have many positive benefits in the world, including healthcare applications assisting in diagnoses using medical imaging data, others are obviously deeply problematic and rightfully controversial. None moreso than facial recognition (or really anything that involves classifying people), which is used not just to help you find pictures of your friends and adorable cats in your personal digital photo library, but by police departments and many others. Privacy concerns, as well as the negative consequences of mistakes resulting from hidden biases that disproprtionately affect racialized people. Regulation is clearly needed, but we are in the early days of this political debate. Most of the action in Europe right now is focused on transparency and the "right to explanation" in the context of automated decision making, such as whether you get a loan or probation. As Mitchell (2019), @timnetTedTalk, and many others have pointed out, debates about the ethics, politics, and risks of deep learning has been far too concerned with the potential threats of intelligent machines and far too unconcerned with the very real and immediate threats of opaque errors rooted in deeply embedded racial and gender biases. 

Data isn't the only culprit here. Nor is it the only solution. As computational social scientists, it is incumbent upon us to actively interrogate the fairness of the models that we build. If we find them lacking, it is not enough to simply blame the data and move on; we must confront the problem head on. It is up to us to fix our data, proactively build fairness into what we do.



  
> **Further Reading**  
> 
> Chapter 10 from G{'e}ron's [-@geron2019hands] *Hands-on Machine Learning* and the first several chapters of Chollet's [-@francois2017deep] *Deep Learning with Python* offer a deeper dive into neural network modelling than the introduction in this book. Chollet is the original developer of Keras, the Python package we'll use to develop a simple neural network model in the next chapter.
>


## CONCLUSION

### Key Points 

- This chapter introduced neural network models, shallow and deep.
- Examined some early neural network models -- the perceptron and Bolztmann machine --
- Learned how the 'backward propagation' and 'gradient descent' addressed critical limitations in their modelling frameworks
- Explored the difference between 'shallow' and 'deep' neural networks,
- Learned about an approach to deep learning that involves stacking together 'autoencoders.' 