# Text as Data

::: {.callout-warning}
## Planned Revision

This chapter is being thoroughly revised in fall 2024. The contents from two chapters from the print edition -- "Iterative text analysis" and "Exploratory text analysis" (see below) -- will be merged and condensed. New content will be added from two new books: *Text as Data: A New Framework for Machine Learning in the Social Sciences* and *Mapping Texts*. It will set up the next chapter on LSA and latent semantic space. With this change and the changes to the network analysis chapters, this part of the book will feature 6 chapters in pairs of 2: 2 for structured data, 2 for text data, and 2 for network analysis. The first chapter of each pair will be more introductory and text-heavy, and the second more data-driven. They should end implying the models that come later in the book won't actually get into inference. 
:::

## LEARNING OBJECTIVES

- Describe how text preprocessing, exploratory text analysis, close reading, and computational modelling all connect in larger text processing pipelines and workflows
- Explain the difference between manifest and latent content in text data
- Explain why there is disagreement about whether coding (also known as annotating or labelling) or count-based feature extraction methods are the best tools for constructing quantitative representations of text data
- Describe the "bag-of-words" approach to representing text 
- Explain what a Document-Term Matrix is, and compare matrices with term counts and term weights (e.g. TF-IDF)
- Explain how TF-IDF word weights are computed
- Explain the role of close reading in computational text analysis
- Describe the computational grounded theory framework

## LEARNING MATERIALS

You can find the online learning materials for this chapter in `doing_computational_social_science/Chapter_11`. `cd` into the directory and launch your Jupyter Server.

## INTRODUCTION

The previous chapter introduced some basic methods for processing natural language data stored as unstructured text. Typically, these methods are part of a much larger project; we are preparing text data for some other downstream analysis. Before we get there, this chapter offers a bigger picture view of generic text processing pipelines and workflows. The goal is to understand how the various text analytic methods that are introduced in this book fit together, and to highlight a few core challenges in text analysis.

Before we get started, I want to clarify exactly what I mean by "pipelines" in this chapter. As a reminder, we briefly discussed SpaCy's text processing pipeline in the previous chapter. In this chapter, I am using "pipelines" to refer to the same general idea; it's the sequence of operations that we are pushing our data through a series of steps, transforming the data and fitting various kinds of models along the way. However, we are focusing on an idealized text analysis pipeline for an entire project.

## EXPLORATION IN CONTEXT: TEXT ANALYSIS PIPELINES

The methods introduced in the previous chapter are rarely used on their own. Instead, they are paired with other methods and models in larger text analysis pipelines. Let's start by discussing these larger pipelines to provide context for what you've already learned and what is still to come. This chapter will focus on summarizing and describing the *content* of many different documents. We will consider other possible goals later in the book. 

Figure @fig-11_01 is a high-level overview of a typical computational text analysis pipeline focused on describing the content of many documents in a corpus. Keep in mind that this is a *typical* project pipeline and the details may differ in any specific project. At the top left of the figure is the "original data;" let's use a data set consisting of 236,074 speeches made by UK MPs between 2016 and 2019 as an example to make this more concrete. Working with this full dataset is going to be fairly slow on most machines, and at this stage, we don't want to be sitting around waiting for our code to execute. Instead, we want to enable quick, iterative, and multi-method analyses, so we draw a sample to work with instead. 

Once we have our sample, we perform some initial processing, or **preprocessing**, which usually involves a combination of cleaning and pre-screening text that we want to analyze. The cleaning tasks vary by project, but may include converting characters to lowercase, removing punctuation, and normalization via lemmatization. I think of pre-screening as the selection of relevant text, rather than filtering of unwanted text, because we are not modifying the original data; our research workflows are always non-destructive. 

The methods introduced conceptually in this chapter and concretely in the next are represented in the next stage of the pipeline, which is the construction of a **feature matrix**. There are two main ways to do this: by extracting features from the text itself, or by "coding" the data (also known as labelling and annotation). This is a somewhat controversial stage in the process, as researchers and methodologists disagree about the "best" way to accomplish this task. Both approaches have their merits and demerits, and you should select the approach that will best enable you to answer your research questions. 

The next step in the pipeline is exploratory analysis, the focus of the next chapter. The main purpose of these exploratory methods is to develop a deeper understanding of both the manifest and latent content in a corpus. **Manifest content** is plainly communicated, whereas **latent content** is "below the surface" of the text and therefore requires more intrepretation from the researcher. I want to emphasize that this interpretive work is done iteratively, by going back and forth between data-driven exploration of the kind introduced here, formal modelling (discussed in later chapters), and careful close readings of individual documents. Recall the discussion of Box's loop from Chapter 8. Exploratory text analysis serves the same purpose as the techniques from Chapter 8: better understanding our data and analysis so we can iteratively critique, revise, and improve our models. 

![](figures/text_eda.pdf)

### Counting, Coding, Reading

Social scientists have been answering questions about our social, political, psychological, and economic lives by systematically collecting, interpreting, and drawing inferences from text data for over a hundred years, long before anyone had the kind of computational powerful at their fingertips that we do now. (Humanists have been doing it even longer, of course.) Formal content analysis techniques have been a core part of the social sciences' methodological toolkits since shortly after the First World War, when researchers such as @lasswell1927propaganda started developing methods for analyzing propaganda and political discourse in newspapers [@krip]. It should hardly come as a surprise that the explosion of possibilities afforded by *computation* and large-scale textual data is viewed in part through the lens of this long history, much of which has revolved around competing ideas about the best way to analyze latent content. 

For many years these differences divided text analysts, with some being more oriented toward scientific approaches and others toward the humanities. These divisions are not so clear-cut in practice, and they involve far more rigor and depth than their oversimplified names suggest. The methods used by these groups are sometimes summarized as counting (identifying patterns in the manifest content of text), coding (identifying latent content through careful specification of concepts), and reading (of the painstaking variety practiced by our friends in humanities departments).

Rather than rehashing comparisons of specific approaches [see @ignatow2016text], we will focus on understanding why manual coding and the role of interpretation has been so divisive, and how these debates have informed multiple scientific approaches to content analysis, be they quantitative, qualitative, computational, or hybrid [@krip; see also @neuendorf2016content]. 

The distinction between manifest and latent content played an important role in the early development of mainstream quantitative approaches to content analysis [@krip; @neuendorf2016content; @berelson1952content]. Focusing on manifest content is often considered more *objective* because it's measures are "closer" to its observations (eg. there is little theory and interpretation distance between observing words on the page and **counting** the number of times two words co-occur). With manifest content, meanings are unambiguous and sit at the surface level of text. Analyzing latent content, however, is a little too close to *subjective* judgement for some. The distance from words on the page to the latent meanings and messages behind them requires a greater leap of interpretation. Any analysis of latent content necessarily requires us to use our human brains -- wired as they are with preconceived notions, theories, cultural schemas, and prone to cognitive biases like conformation bias and motivated reasoning -- to interpret ambiguous meanings. This is unfortunate, as latent content tends to be much more interesting than manifest content. To be clear, counting techniques are in no way free of subjectivity; the main goal of the "counting" is feature extraction under different constraints (eg. count occurrences, count co-occurrences), *which can then be modeled*. No serious social scientist should be satisfied with a table of word co-occurrences and no further interpretation. The major difference is where the interpretations take place, and how accessible and transparent they are.

  
> **Further Reading**    
>   
> @evans2016machine provide a great review of the intersection of natural language processing and social scientific content analysis. If you want to learn more about the general methodological foundations of quantitative content analysis in the social sciences, @krip and @neuendorf2016content are widely-used sources. @ignatow2016text provide a broader methodological discussion that includes high-level discussions of text analysis methods from the social sciences and humanities as well as computer science. 
>


Differences in interpretations of latent content are bound to arise. For a very long time, the mainstream solution for dealing with this problem has been **specification**, which we've already discussed in the context of working with latent factors (Chapter 30), and manual **coding**. Researchers specify precise operational definitions that indicate what concepts mean, and what types of things would constitute an observation of that concept in a document. Once defined, researchers *manually* construct the quantitative representation of their text data by coding each document. 

In this context, "coding" is the process of transforming unstructured documents into structured datasets by manually labeling data according to some set of variables that are coupled to theoretical concepts via the specification process. While there are different coding styles, they tend to generally follow a similar pattern. First, you have a research question you want to answer. Usually you also have some idea of what you expect, grounded in some larger theory (i.e., a hypothesis). If you want to compare the tone and argumentative style of letters to the editor addressing local or non-local issues [e.g. @perrin2008parallel], you would first decide what types of tones and argumentative styles are relevant, and then you would carefully operationalize those tones and styles based, at least in part, on theory. Then you would read each text and assign codes based on the presence or absence of specific tones and argumentative styles. If resources allow, you would have multiple trained researchers (including yourself) code the documents. This makes it possible to compare the codes assigned to documents by different researchers and compute an inter-coder reliability rate [@krip]. Codes with a reliability rate above a given threshold (e.g. 90% agreement between coders) are retained, shifting the coding process from one based on *subjective* interpretation to *inter-subjective* agreement. In short, the coding approach is one that hinges on good specification. 

Though widely practiced, and despite plenty to love, there are some valid concerns about manual coding that go beyond the time (and money) it requires. The difference between approaches that "code" and those that count and map was the subject of an animated debate in the *American Journal of Cultural Sociology* following the publication of Monica Lee and John Levi Martin's [-@lee2015coding] "Coding, Counting, and Cultural Cartography." (I've provided the references for this debate in the "Where to Go Next" section at the end of the chapter.) Lee and Martin start by engaging with an argument made by Richard Biernacki [-@biernacki2012reinventing; -@biernacki2015after] that manual coding just makes things worse. Biernacki thinks that any content analysis requires the kind of careful interpretation that our colleagues in the humanities practice. From his perspective, manual coding both lowers the quality of the interpretation (by virtue of being coupled to theoretical concepts and hypotheses) and obscures it. 

Consider an example. If I were to code the presence or absence of different types of political arguments in a collection of news stories about immigration reform, I would start specifying the types of political arguments I think are relevant and likely to be found. I would have to be explicit about what constitutes an observation of one type of political argument versus another (i.e., operationalization). Researchers who question the validity of the coding approach would likely point out that my (or any) choice of coding scheme would invariably misrepresent the texts themselves. As a result, my codes could be contested by researchers who see the same text differently, and any results I obtained from analyzing the final dataset would likely not be replicated by another researcher. Their second objection would be that this potential interpretive chaos is hidden away behind the codes, where other researchers and readers can't see it. 

Biernacki's [-@biernacki2015erratum] solution is to reject coding altogether, and to replace it with humanistic approaches to interpretation. Somewhat surprisingly, he argues that this approach is actually *more* scientific because it "better engages standards for validity, transparency, producing competing hypotheses, generalizing and hypothesis-testing by recalcitrant detail" (page 313). Lee and Martin [-@lee2015coding; @lee2015response] accept Bernacki's critique that manual coding *hides* the essential, but messy, work of interpretation rather than eliminates it, but they disagree that turning to humanistic approaches is the only, or the best, response to the problem. Instead, they propose a refinement of the "counting" methods that begins by representing original texts in a simplified form, like a map represents terrain in simplified form. To be a good "map," these simplified representations need to remove a lot of information from the texts while still faithfully representing the core features of the original texts. Lee and Martin offer semantic networks (discussed in later chapters) as an approach, which work by exploiting the low-level relationships between words within semantic units like sentences and paragraphs.

Lee and Martin's goal is not to eliminate interpretation, but rather to move it out into the open where it can be seen, evaluated, and potentially contested. The idea is that this becomes possible if we have formal procedures for producing map representations from text. This leaves the researcher to openly and transparently interpret the map rather than hiding interpretive judgements behind codes, and then analyzing relationships among the codes as if no really challenging interpretation had taken place at all. 

This debate boils down to whether, and how, to make complex interpretive research, which is absolutely unavoidable, more open and transparent. The debate between coding and count-based approaches is largely a debate about where the inevitable interpretation should happen, and who should be able to see and assess it. Those who code and those who count both break with Bernacki, and personally I think that's a good thing because the approach he recommends -- close reading -- is *not an alternative* to counting or coding. Coding and counting both have many strengths, but should *always* be paired with close reading of a subset of documents. In other words, Bernacki is right that close reading and interpretation are essential, but it doesn't follow that manual coding has no place in text analysis, or in social science more broadly. For the same reason, Lee and Martin are right to shift interpretation out into the open, but their critique of manual coding is also overly dismissive and "maps" don't just magically reveal their unambiguous meanings to us. We should not abandon manual coding in favour of an exclusive commitment to humanistic interpretation or formalism; we should combine close reading, manual coding, formal approaches, and other methods. 

In the rest of this chapter, and in subsequent chapters focused on text data, I will assume the following: 

1. close reading is not an alternative to any other method, it must be paired with other methods;
2. "coding" and "counting" approaches need not be pitted against each other either, as they can be used together to mitigate the limitations of employing either approach in a vacuum; and 
3. any *computational* approach to text analysis benefits from combining all of these approaches in some way. 

In the rest of this chapter, we will introduce some important count-based feature extraction methods for constructing quantitative representations of text, and we will see how to use these representations to compare high-level differences in manifest language use and to explore the *latent* dimensions of text data. Like the methods you learned in the previous chapter, the methods you learn here are useful regardless of whether you want to interpret a "map" or model your data a bit further downstream. In later chapters, we will discuss several ways of doing this using different types of machine learning. We will also return to the idea of close reading, and how to integrate it into larger text analysis workflows.

## COUNT-BASED FEATURE EXTRACTION: FROM STRINGS TO A BAG OF WORDS

Any quantitative or computational text analysis requires some sort of quantitative representation of the text to operate on. Once you've constructed that representation, the analysis typically involves going back-and-forth between algorithmic manipulations and modelling of the quantitative representation on the one hand and careful interpretation of the textual representation on the other hand. For that reason, it is very useful to have the following four things accessible to you at any point in the analysis process:

1. the original texts
2. any relevant metadata about the texts, such as who produced them
3. the pre-processed versions of the texts
4. a quantitative representation of the texts

There are two main types of quantitative representations of text that you will learn in this book: (*i*) long sparse vectors and (*ii*) short dense vectors. The long and sparse vector representation is usually referred to as a **bag-of-words**, and the most widely-used data structure is the **Document-Term Matrix (DTM)**. The short dense vector representations have come to be know as **embeddings**. Alternative ways of representing texts quantitatively, such as networks, can easily be interpreted as variations on these two types of representation. We will set embeddings aside for now and focus on Document-Term Matrices.

### Long and Sparse Representations with Document-Term Matrices (DTMs)

The first step in constructing a quantitative representation of text is to learn the "**vocabulary**," which is the set of unique terms (i.e., words and short phrases) that are used across the entire corpus. In our example of political speeches by UK MPs between 2016 and 2019, for example, the **corpus** would consists of the full text across all speeches by all political parties in our *sampled* dataset. 

Note that the vocabulary depends on how we define the corpus. If we define it as the original speech data, then the vocabulary will consist of every unique token used across all speeches. If we define it as our *pre-processed* speech data, then the vocabulary will consist of all the unique words that make it through our pre-processing step in the text analysis pipeline. This process of defining the corpus, learning the vocabulary, and constructing the DTM is an example of automated **count-based feature extraction**.

When we create a DTM representation of our text data, each unique term in the corpus vocabulary will become an individual feature (i.e., column) unless we specifically set some sort of condition that filters terms out (e.g., must appear in a minimum of 5 documents). 

The cells in a DTM typically represent one of three things: 

- the presence or absence of a token in the relevant document (`0` or `1`),
- a count of the number of times a token appears in the relevant document (integers), or
- some measure of word importance or relevance, such as TF-IDF (floats), which we will discuss below.

The DTM shape will always be equal to the number of unique tokens in the vocabulary (minus any that we screen out in the process of constructing the DTM) and the number of documents (i.e., rows). The table below is a hypothetical example of a DTM with term counts in each cell. 

| Documents    | Token 1 | Token 2 | Token 3 | Token 4 | Token ... | Token $n$ |
|:-------------|:--------|:--------|:--------|:--------|:----------|:----------|
| Document 1   | 0       | 0       | 3       | 0       | 2         | 8         |
| Document 2   | 2       | 0       | 1       | 1       | 0         | 0         |
| Document 3   | 1       | 0       | 0       | 0       | 1         | 4         |
| Document 4   | 0       | 2       | 1       | 0       | 1         | 3         |
| Document ... | 0       | 0       | 0       | 1       | 2         | 1         |
| Document $n$ | 1       | 0       | 0       | 1       | 5         | 1         |

Table: A hypothetical Document-Term Matrix

In this case, each row of the matrix is a vector representation for a document and each column is a vector representation for a token in the vocabulary. The long sparse vector representation for Document 1, then, would be all of the numbers in the first row of the table (`Document 1`: `[0,0,3,0,2,8]`) and the long sparse vector representation for Token 1 would be all of the numbers in the column `Token 1` (`[0,2,1,0,0,1]`). 

When we describe vectors as "long and sparse" we are typically referring to the document vectors, which are long because each element in the vector (i.e., feature in the matrix) represents a unique term in the vocabulary. Vocabularies are always large, and most words in the vocabulary do not appear in most documents. As a result, these vector representations are mostly full of 0s; hence sparse.

### Weighting Words with Term Frequency Inverse Document Frequency (TF-IDF)

In many approaches to computational text analysis, working with simple count data is rarely ideal because the words that occur the most frequently are **function words** (e.g., 'the', 'and', 'of') that carry very little information about the actual *content* of a document. Extremely rare words are also generally uninformative. What we really want are the words that are somewhere in between those two extreme ends of the frequency distribution. This is generally done by computing some sort of word weight, and the most common by far is a measure called TF-IDF.

**TF-IDF** stands for "Term-Frequency Inverse Document Frequency," and it is intended to measure the usefulness of any given token for helping reveal what a document is about relative to other documents in a corpus. It *weights* words rather than counts them, and the weights are lower for words that are either too common or too rare. To understand how it works, let's break it down and look at term frequency and inverse document frequency separately, and then the full measure. To do so, we will use a hypothetical example of a dataset of 150 journal article abstracts.

As you might expect, **Term Frequency** is a measure of how common a word is in some document. Rather than using a straight count (which would be biased towards longer documents), we multiply the number of times the word appears in a document by the inverse ratio of the number of documents that have the term compared to the total number of documents in the corpus. Let's say, for example, that the word "environment" appears four times in a 200 word abstract for a journal article about environmental activism. The term frequency $TF_{i,j}$ for "environment" *in this specific document* would be 0.02.

$$
TF_{environment} = 4/200 = 0.02
$$

Now let's say there are a total of 150 abstracts in our dataset and the word "environment" appears 42 times in the full dataset. We want to know how important the word "environment" is across the whole collection, so we calculate the inverse document frequency, IDF, using the following equation:

$$
IDF = \log\Big(\frac{N}{DF_i}\Big)
$$

Where $N$ is the total number of documents in the dataset, and $DF_i$ is the number of documents that the word $i$ appears in. The IDF score for "environment" is the log of this value, which is 0.55.

$$
IDF_{environment} = \log\Big(\frac{150}{42}\Big)
$$

To compute the TF-IDF weight for any word in any document in a corpus, we multiply $TF$ with $IDF$.

$$
W_{i,j} = TF_{i,j} \times \log\Big(\frac{N}{DF_i}\Big)
$$

Putting it all together, TF-IDF is as its name suggests: Term Frequency times Inverse Document Frequency. The TF-IDF weight of a word in a document increases the more frequently it appears in that document but decreases if it also appears across many other documents. Rare, but not *too* rare, words are weighted more than words that show up across many documents. The result is a set of words that, while not the most common, tell us a lot about the content of any one document relative to other documents in the collection. This measure is far more useful than raw counts when we are attempting to find meaningful words. In the next chapter, we will further clarify TF-IDF by comparing word weights with their frequencies in our political speech dataset.

## CLOSE READING?

So far, I've sketched out a pretty high-level and idealized pipeline that explains how different types of text processing, analysis, and modelling fit together. I've also explained the challenges involved in one crucial step: the approach used to represent text *quantitatively*. This has led to some disagreements over the various approaches to this problem, with some arguing in favour of coding over counting, others counting over coding, and others for throwing the baby out with the bathwater. Now let's turn our attention to another issue, which is the role of close reading in a computational text analysis. The idea, illustrated in the pipeline, is that you engage in deep reading *as you iteratively explore your data and develop models.*

In computational text analysis, methodologists are beginning to think through ways of *systematically* combining various inductive and deductive approaches to computational text analysis with good old fashioned reading. Why? Because:

1. mixed-methods research [@small2011conduct] is especially valuable when one of the methodologies is less familiar to the scientific community (as computational text analysis often is), and / or when it pulls the researcher further away from the original data than more familiar methods (validation and triangulation);
2. there is a lot to be gained by thoughtfully combining induction and deduction; and 
3. machines and humans are good at different types of things, and we want to use both our brains and our computers for the things they are best at. 

Most computational text analyses involve machine learning of one kind or another, and the impressive results that these models produce, combined with the use of metaphors like "reading" and "learning," can make it easy to forget, at least temporarily, that computers are not *actually* reading; they don't understand words, sentences, or meaning (manifest or latent) in the same way that humans do. When computers "read," they are applying mathematical operations to internal representations of data. More inductive computational models, such as probabilistic topic models (introduced in Chapter 30) identify patterns in documents that, hopefully, correspond to what we humans recognize as reasonably coherent themes and "topics." Despite finding the pattern, the computer doesn't know what a topic is, or what a word is for that matter. Behind the scenes, it's all probability distributions. To really know, understand, and assess the validity of the computational analysis, we humans need to read things carefully. Systematic comparisons of manual and computational text analysis support this combination [@nelson2018future]. There is no way around it; whatever our specific interests or text analysis methodology, we have to read carefully. That's a good thing.

Humans with domain knowledge should do the things that humans are good at and computers are bad at (e.g. interpretation, critical thinking), and that computers should do the things that computers are good at but humans are comparably bad at (e.g. computing the similarity of two massive vectors of numbers); In the next section, we explore one practical implementation of  human-computer division of labour: computational grounded theory.

### "Computational Grounded Theory"

One of the most exciting and promising examples of a mixed-approach framework is Laura Nelson's [-@nelson2017computational] "computational grounded theory." It is, to date, the most systematic and sophisticated approach to combining machine learning and computation more generally with deep reading and interpretation by humans. As the name of the framework suggests, Nelson's approach builds on the qualitative foundations of grounded theory [@grounded; @charmaz2006constructing], which is (somewhat confusingly) both a process and a product. To risk oversimplifying things, the process involves *inductively* identifying, integrating, and refining categories of meaning in text. This is accomplished through a variety of specific procedures, the most common of which is the method of "constant comparison" of cases. The product is a set of relatively abstract concepts and statements (i.e. theory) that are "grounded" in the data.

Nelson builds on this methodological foundation because it is well-established, unapologetically inductive, and emphasizes the interpretive work that is unavoidable in text analysis. But, as she points out, grounded theory does not scale well to large datasets, and the results can be difficult to validate and replicate. Her computational framework is designed to address these problems while retaining the good parts of the original approach.

Computational grounded theory involves three basic steps. The first is pattern detection using exploratory and computationally inductive methods -- such as those introduced in the next chapter, as well as Chapters 30 and 33 -- to discover latent themes and topics in a corpus. This is a shift in the logic of the grounded theory method. In classic grounded theory, the researcher is doing interpretive work to develop and refine categories of meaning. In computational grounded theory, the computer identifies potential categories of meaning (i.e. topics) using unsupervised methods that can be replicated; the researcher interprets and evaluates those categories.

This is the starting point for the second step -- "guided deep reading" -- in which the researcher makes informed decisions about specific texts to read and interpret. The *guided* part is key here, because it allows the researcher to select texts that are representative of some larger theme or topic, not an unusual outlier. This helps mitigate the effects of confirmation bias and other cognitive biases that can affect the judgements of even the most well-intentioned researcher. It also makes the interpretive part of the analysis easier to validate and replicate. Think of it as the difference between exploring an unfamiliar city with and without a map. Without a map, you may end up seeing the same amount of the city, but if you have a map you can make more informed decisions about where to go and you will have a better sense of what you did and did not see. You can also trace your route on the map, making it easier for someone else to understand where you went and potentially to go there themselves.

To summarize:  we use computationally inductive methods to discover some potential themes and estimate how they are distributed within and across texts in our corpus. We then use the results of that analysis to select a sample of texts that are representative of specific themes and, through a process of "deep reading," use our human brains to develop a better and more sophisticated understanding of what those themes are. This enables us to come to an understanding of the text that is better than any one method could have produced on its own.

The third and final step of the computational grounded theory framework is pattern confirmation. For Nelson, this step forces the researcher to operationalize concepts and ideas discovered in the first two steps, and then check to see how common they are across the corpus. One way to do this is to go through the supervised learning process covered in Chapters 21 and 22, but we will set further discussion of supervised learning methods aside for now.

The full process is summarized in Figure @fig-11_02, which is based on a figure from Nelson's [-@nelson2017computational] article. I encourage you to read her article carefully, in part because she thoroughly illustrates each step with examples from her work on the political logics underlying the women's movement in New York and Chicago from 1865 to 1975 [@nelson2015political]. It's an excellent article with fascinating examples.

![](figures/cgt.pdf)

  
> **Further Reading**    
>   
> If you are interested in the debate over coding and counting that was discussed in this chapter, I would recommend reading the original articles by @lee2015coding, @biernacki2015erratum, @reed2015counting, @spillman2015ghosts, and @lee2015response. 
>
> In addition, I recommend reading Laura Nelson's [-@nelson2017computational] original article on computational grounded theory. You can also learn more about the original grounded theory method by consulting and @grounded or @charmaz2006constructing. Finally, @small2011conduct offers a great overview of various different ways of doing mixed methods research.
>


## CONCLUSION

### Key Points 

- Outlined a generic text analysis pipeline that starts with sampling and preprocessing text, constructing quantitative representations using manual coding and/or automated count-based feature extraction
- Demonstrated how to perform exploratory analysis of the manifest and latent content of those texts, combined with guided close reading and model development
- Discussed the challenge of transparently interpreting latent content and the tensions between the coding, counting, and close reading approaches
- Highlighted Laura Nelson's computational grounded theory framework as an exemplar of the foregoing

# Exploratory Text Analysis

## LEARNING OBJECTIVES

- Build a 'bag-of-words' representation of unstructured text
- Use feature extraction tools from Sklearn
- Build familiarity with chunks, triplets, and n-grams
- Explain 'Document-Term Matrices' and how they can be used
- Describe high-level patterns of language use in a corpus, and across subsets of documents in a corpus, using counts, frequencies, and term weights

## LEARNING MATERIALS

You can find the online learning materials for this chapter in `doing_computational_social_science/Chapter_12`. `cd` into the directory and launch your Jupyter Server.

## INTRODUCTION

The generic text analysis pipeline introduced in the previous chapter stresses the interconnectedness of data exploration and iterative model development, in Chapter 8, I stressed the importance of exploratory data analysis to this kind of iterative development. However, exploratory text analysis requires some extra tools in addition to the ones we introduced earlier. I'll start by showing you how to scale up preprocessing methods to a large text dataset, and discuss using gensim's `Phraser` module alongside spaCy in order to detect n-grams. We will then consider how to use Sklearn to construct feature matrices with term counts or frequencies. This enables a broad range of exploratory analyses and sets the stage for starting to explore the latent thematic dimensions of text datasets, which we will turn to in the next chapter.

### Package Imports


```python
import pickle
from pprint import pprint

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import scipy
import spacy
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.preprocessing import Normalizer

from dcss import set_style, download_dataset
from dcss.text import bigram_process, bow_to_df, preprocess
from dcss.utils import sparse_groupby

set_style()

nlp = spacy.load('en_core_web_sm')
```

## SCALING UP: PROCESSING POLITICAL SPEECHES

In this chapter, we're going to work with text data from speeches made by British Members of Parliament (MPs) between 2016 and 2020, available in full from the [British Hansards](https://hansard.parliament.uk/) dataset. We will drop any observations that are missing values from the `party`, `speakername`, or `speech` columns. 

```python
years = [2016, 2017, 2018, 2019, 2020]
columns = [
    'speech', 
    'speakername', 
    'party', 
    'constituency', 
    'year'
]
dfs = []

for year in years:
    # download the data using Dropbox share link
    download_dataset(
        data_url=f'https://www.dropbox.com/scl/fi/c9d1aqzrage1juf276nvd/british_hansard_{year}.csv?rlkey=ilyn06y4hw4jocr4fhq6w6olk&st=ioslogzq&dl=1',
        save_path=f'data/british_hansard/bh{year}.csv'
    )

    # load the data
    df = pd.read_csv(
        f'data/british_hansard/bh{year}.csv', low_memory=False,
        usecols=columns
    )

    dfs.append(df)

uk_df = pd.concat(dfs)
uk_df.dropna(
    subset=['party', 'speakername', 'speech'], inplace=True
)

uk_df.reset_index()
uk_df.info()
```

```python
uk_df['party'].value_counts()
```

The Conservative Party has made far more speeches than other parties within this time frame due to the fact that they were the governing party for that entire window, first under Theresa May (2016-2019), later under Boris Johnson (2019-2022).

We will also ignore speeches made by the Speaker of the House and Independents. We will focus only on parties whose MPs collectively made more than 400 speeches within our four year window.

```python
parties_keep = [
    'Conservative', 
    'Labour', 
    'Scottish National Party', 
    'Labour (Co-op)',
    'Liberal Democrat',
    'Democratic Unionist Party',
    'Plaid Cymru',
    'Green Party'
]

party_subset = uk_df[uk_df['party'].isin(parties_keep)].copy()
party_subset.reset_index(drop=True, inplace=True)

total_speech_counts = party_subset['party'].value_counts()
total_speech_counts
```

This leaves us with 224,016 speeches. 

So far, all of the text processing we have done has been on a very small amount of text. When scaled up to data of this size, things inevitably take a lot longer. Powerful computers help a lot, of course, but even then you can spend a lot of time just waiting around for code to finish running, and that's not ideal when you are rapidly iterating over many different analyses. Instead, it can be helpful to work with a smaller representative sample of the full dataset -- you can always execute your code against the full dataset when your code is developed. The best way to do this is by drawing a random sample, of course. 

We will draw a **stratified random sample** where the **strata** are political parties. In the code block below, we do this by grouping the dataframe by political party and then drawing a random sample of 30% from each strata. This is done without replacement; once a speech has been sampled, it can't be sampled again. We set the `random_state` to ensure that your sample matches mine.


```python
sampled_speeches = party_subset.groupby('party')

sampled_speeches = sampled_speeches.sample(
    replace=False,
    frac=.3,
    random_state=23
)

len(sampled_speeches)
```

```python
sampled_speeches.to_csv(
    'data/sampled_british_hansard_speeches.csv', index=False
)
```

```python
sampled_speech_counts = sampled_speeches['party'].value_counts()

sample_sizes = pd.DataFrame(
    zip(total_speech_counts, sampled_speech_counts),
    columns=['Total', 'Sample'],
    index=parties_keep)
```

There are now 67,204 speeches in our dataset, sampled from 8 political parties (if we treat Labour Co-op as if it were a separate party, which it *sort of* is) proportional to the number of speeches each made within our 4 year window. 

Let's start by quickly taking a look at the length of speeches by politicians from each party. We will do so by computing the length of each string (i.e., the number of tokens in each speech). 



```python
sampled_speeches['speech_len'] = sampled_speeches['speech'].apply(lambda x: len(x.split(" ")))
```

Now we can group by political party, extract each group from the grouped object, and plot the kernel density estimate for our new speech length variable. We will put each plot side by side, as small multiples, to facilitate comparisons. Note that in the graph below, the kernel density estimates shows the density for speeches *within each party*, not across parties.

We will define a function called `party_subplot()` to avoid needlessly repeating code. The result is shown in Figure @fig-11_01.


```python
#| warning: false

def party_subplot(subgroup, title, position):
    sns.kdeplot(
        ax=position, 
        data=subgroup, 
        x='speech_len',
        log_scale=True, 
        fill=False, 
        alpha=1, 
        linewidth=3, 
        color='C0'
    )
    
    position.set(
        xlabel='Number of tokens (log scale)', 
        title=title
    )


parties = sampled_speeches.groupby('party')

fig, ax = plt.subplots(
    2, 4, 
    sharex=True, sharey=True, 
    figsize=(20, 6)
)

party_subplot(
    parties.get_group('Conservative'), 
    'Conservative', 
    ax[0, 0]
)

party_subplot(
    parties.get_group('Labour'), 
    'Labour', 
    ax[0, 1]
)

party_subplot(
    parties.get_group('Scottish National Party'), 
    'Scottish National Party', 
    ax[0, 2]
)

party_subplot(
    parties.get_group('Labour (Co-op)'), 
    'Labour (Co-op)', 
    ax[0, 3]
)

party_subplot(
    parties.get_group('Liberal Democrat'), 
    'Liberal Democrat', 
    ax[1, 0]
)

party_subplot(
    parties.get_group('Democratic Unionist Party'), 
    'Democratic Unionist Party', 
    ax[1, 1]
)

party_subplot(
    parties.get_group('Plaid Cymru'), 
    'Plaid Cymru',
    ax[1, 2]
)

party_subplot(
    parties.get_group('Green Party'), 
    'Green Party', 
    ax[1, 3]
)

plt.tight_layout()
plt.savefig('figures/speech_length_by_party.png', dpi=300)
```


![png](figures/speech_length_by_party.png){#fig-11_01}
    
```python
parties['speech_len'].median()
```

We can see that the distributions for each party follow roughly the same pattern of proportions. The distribution of speech lengths is strongly skewed, with the median length generally being in the ballpark of 70-90 terms for all parties.


### From Rule-Based Chunks and Triplets to Statistically Dependant n-grams

Previously, you learned how to extract phrases contained in spaCy docs by accessing the noun chunks attribute (`.noun_chunks`), and you saw how to leverage the syntactic dependency labels assigned to each token to extract information such as verb-object pairs. You also saw how to generalize that knowledge to semantic triplets, also known as SVOs or subject-verb-object triplets. Although the results of an automated SVO extraction often involve a lot of noise, there is a fair amount we can do to improve the results by customizing them to our research contexts (e.g., changing how we walk through the dependency trees when working with social media data).

Each of those methods are especially helpful when we are exploring text data or trying to extract specific pieces of information. Often, however, we want to identify **n-grams**, which are phrases that denote some sort of concept that we want to treat *as if they were a single token*. The n in n-gram refers to the number of tokens in the phrase. For example, **bigrams** are two tokens that make up a phrase that ostensibly has a different meaning than the two constituent tokens. For example, `climate` and `change` tokens could be transformed into a single `climate_change` token. Don't forget, your computer has *no idea* what the tokens "climate," "change," or "climate change" mean, so it can only estimate when two tokens co-occur frequently enough to be considered a phrase, rather then simply being adjacent tokens from time to time. It's important to keep this in mind at all times when working with advanced computational techniques.

The `Phrases` model class  in gensim is widely-used for this task and is the recommended go-to complimentary n-gram package for spaCy. It's a well-optimized way to detect bigrams in a corpus without a lot of effort or processing time. Ultimately though, it's a statistical model that calculates maximum likelihood estimates for token co-occurrences (pairs of tokens that co-occur too frequently to be random). In other words, it scores tokens that appear next to each other based on their *statistical* dependencies rather than their *syntactic* dependencies (i.e., not based on linguistic rules and domain expertise). 

gensim's `Phrases` includes two scoring functions for the likelihood estimation, **Pointwise Mutual Information (PMI)** and **Normalized Pointwise Mutual Information (NPMI)**. Neither scoring method is inherently better or worse, and the choice between them depends on your objective. NPMI is generally better at prioritizing frequent co-occurrences, while the PMI scorer tends to give high probabilities to less frequent cases. As you may have guessed from the name, NPMI scores modify PMI ones by normalizing them to a scale from -1 to 1, where a score of 1 would mean that the two tokens only ever appear together and negative scores indicate that they appear together less than expected by chance. The normalized values are also easier to interpret in comparison to each other and as you will see, the trained `Phraser` model class, which is a leaner form of the `Phrases` class when you no longer need to update the model, can return a dictionary of all bigrams and their associated scores. This can be helpful to get a better sense of the parameters that result in higher scores for the bigrams that you expect. 

In this example, I'll use the `npmi` scorer because we will be training the model on a very specific domain (political speeches), so we can reasonably expect that meaningful bigrams in that context will be repeated frequently. With that said, it's always worth comparing the results of the various options and configuration parameters. There are ways to quantitatively evaluate the model, but often it's enough to look at the text itself with the merged tokens because the poor results tend to be noticeable right away if the parameters weren't set to capture the results you want, or if the input data wasn't pre-processed correctly.  


As of 2021, gensim is transitioning to a major new version, and some of the planned changes impact the `Phraser` class implementation. Rather than publish gensim code that will soon be out of date, I've included the relevant code in the dcss package (enabling it to be updated as appropriate) in the form of two functions, `bigram_process()` and `preprocess()`. The former is simply a few lines of code that passes our text into `Phrases` in the form gensim expects and returns the exact same text but with pairs of words detected as bigrams joined (ie. `word1_word2`). 

I've set the scoring threshold pretty high: 0.75 out of a maximum of 1.0. Sometimes it's preferable to process the text with a strict threshold like this and miss some bigrams rather than worry about handling too many nonsense results from a relaxed score minimum. 

The `preprocess()` function also removes stop words, which are words that are important in communication but do not convey content, such as function words (e.g., and, the).Stop words can be a bit tricky because of socio-linguistic variation within and across cultural groups. The idea here is that different cultural groups, large or small, tend to have their own culture-specific stop words that we want to disregard in any text analyses that are focused on *content*. "Social" might be a stop word in a dataset of documents produced by sociologists, but not for chemists, classicists, or East Anglian dwile flonkers. In a domain or culture-specific application, we want to be able to identify words like this and exclude them along with more language-specific stop words (e.g., English, Spanish, Korean). 

We won't actually call the `bigram_process()` function directly. Instead, we will call the `preprocess()` function from the dcss package that *includes* the bigramming process as an option alongside other pre-processing steps. All of those steps are things you've learned how to do in this chapter. Below, we call the function using the speeches from all the selected parties, rather than a random sample. Fair warning, *this is gonna take a while*. We've got a lot of text to process. 


```python
bigram_model, preprocessed = preprocess(
    sampled_speeches['speech'], 
    nlp=nlp, 
    bigrams=True, 
    detokenize = True, 
    n_process=4
)

len(preprocessed)
```

*Some time later*, you'll be left with a list of ~67,000 speeches that have been thoroughly prepared for downstream analysis. spaCy is ridiculously fast *relative* to comparable packages. This much text will still take time to analyze. That's why we worked with a stratified random sample earlier and why you'll want to while prototyping. 

When your code finishes running, you'll want to save the results to disk so they can easily be re-loaded later. Below, we do this with `pickle`, which stores Python data structures in binary format, which is OK for data generated within a larger pipeline, and which could easily be re-generated if necessary. The `pickle` package is remarkably adaptable and can safely interact with *most* python objects, but it's important not to rely on it unless you've thoroughly tested whether or not what you want to save can be converted to and from binary without ill effect. We already know that this is going to work out just fine. We can save and load our `preprocessed` and `bigram_model` objects to and from memory, respectively, using the `dump()` and `load()` functions from the `pickle` package:

```python
with open(
    'data/british_hansard_processed_sample.pkl', 'wb') as fp:
    pickle.dump(preprocessed, fp)

    
with open('data/british_hansard_processed_sample_bigram_model.pkl', 'wb') as fp:
    pickle.dump(bigram_model, fp)
```

```python
with open ('data/british_hansard_processed_sample.pkl', 'rb') as fp:
    preprocessed = pickle.load(fp)
```

To briefly recap, we've just used a function called `preprocess()` that applied a series of operations to a sample of political speeches. Specifically, it

1. detected bigrams using gensim's `Phraser` class and merged them into single tokens;
2. filtered out English-language stopwords and tokens containing fewer than 2 characters;
3. from the remaining tokens, selected nouns, proper nouns, and adjectives; and
4. replaced each selected token with it's lemma.

In the rest of this chapter, we will primarily work with the data that resulted from that process. We can re-access that data *anytime* by loading the pickle we created, which is very handy because you don't want to be sitting around needlessly re-preprocessing your data all the time.

It's generally a good idea to do your text analysis in a non-destructive way, and to always have on hand:

1. The original text data, in full;
2. Any relevant metadata, such as who created the text data;
3. The preprocessed text data, pre-transformation into a feature matrix or other quantitative representation; and
4. The feature matrix itself (created later in this chapter).

Let's add the pre-processed speech data to our `sampled_speeches` dataframe, to help keep everything together. As you can see, it will contains two Series with text data, one with the original full speech text, such as this remark from Theresa May:


```python
sampled_speeches.iloc[700]['speech']
```

and another with the version that was produced by our pre-processing function:

```python
sampled_speeches['preprocessed'] = preprocessed
sampled_speeches.iloc[700]['preprocessed']
```

As you can see, our preprocessing has removed a *lot* of information. When working with small data sets or individual documents, this would make little sense. But, when you are trying to understand the content of a large *collection* of documents, it's enormously helpful. It helps us understand the forest for the trees. 

Now that our data is ready, let's move to the next step in our pipeline. If you recall from the previous chapter, our next task is to construct a quantitative representation of our text data. We're going to use feature extraction methods in Sklearn. We'll start with simple term counts.

## CREATING DTMS WITH SKLEARN

In Sklearn, we can construct DTMs with Boolean or count data using `CountVectorizer()` and with TF-IDF weights using `TfidfVectorizer()`. The process of learning the vocabulary is a method of the vectorizer itself, so the first thing we will do is make a decision about which vectorizer to use and how to tune it. Let's start with the `CountVectorizer`. 

Once we initialize a vectorizer object, Sklearn learns the vocabulary in our corpus using the `fit()` method. It can then transform our raw unstructured text data into a DTM using the `transform()` method. In the resulting DTM, each document is a row and each token (i.e. word) in our corpus vocabulary is a column.

As always, the quality of any machine learning analyses depends in large part on the quality of the data we provide. In the context of feature extraction methods such as the construction of a DTM from text data, we can control this by (a) pre-processing our data and / or (b) customizing the feature extraction process itself by changing specific parameters in our vectorizer. You've already learned how to do the first part. We can use our `preprocessed` list from earlier in the vectorization process below. 

### Count Vectorization

Sklearn's `CountVectorizer` has a number of parameters that we can tune. For a simple example: we often want to avoid words that are too generic to the corpus, so we can use the `max_df` parameter to specify that we don't want to keep tokens that appear more than $n$ times, or in more than $n$% of the documents in our collection. This can be especially helpful when working with text datasets that include a lot of specialist language. Similarly, we can use the `min_df` parameter to specify that we do not want to keep tokens that appear in fewer than 3 documents in our collection. While some parameters might be useful, others will be irrelevant to your task. I encourage you to read the documentation to get an better idea of what you can do with Sklearn. 

Which parameters should you use? These decisions are part of a large and complex literature on "feature selection," and there is no one rule you can follow that will get the best results every time. The best advice I can give you is to keep things as simple as you can and align your decisions with your research needs. If it makes sense to do something given the question you are trying to answer, then do it and report the decision when you report on the rest of your methodological decisions. If it doesn't, don't do it just because you can. In this case, our spaCy pre-processing and bigram detection with gensim took care of most of what we would want to do. However, given the volume of data we are working with, we will also:

- ignore tokens that appear very frequently and very infrequently,
- strip accents from characters.

Make note of the parameters we are using here; consider the the effects they will have, given the data.


```python
count_vectorizer = CountVectorizer(
    max_df=.1,
    min_df=3,
    strip_accents='ascii'
)
```

Once we have instantiated our `CountVectorizer` with the relevant arguments, we want to learn the vocabulary and construct the DTM. We can use the `fit_transform()` method to do this, which simply combines the `fit()` and `transform()` methods. Below, we do this for `preprocessed` texts. 


```python
count_matrix = count_vectorizer.fit_transform(preprocessed)
vocabulary = count_vectorizer.get_feature_names_out()

count_matrix.shape
```

Let's pickle both of these objects for future use.

```python
with open('data/british_hansard_sample_dtm.pkl', 'wb') as fp:
    pickle.dump(count_matrix, fp)
    
with open('data/british_hansard_sample_vocabulary.pkl', 'wb') as fp:
    pickle.dump(vocabulary, fp)
```

Our vectorizer has produced a DTM with 16,428 unique tokens (all of which met the criteria specified in the arguments passed to `CountVectorizer()`) from 67,204 documents (i.e., speeches). We can also use the `ngram_range` argument to return ngrams up to three tokens long if we're using the default word analyzer, or a chosen number of letters if we're using the character analyze. Our "vocabulary" would then include these ngrams. Again, we've already done this using the statistical model in gensim but there are times when you just want to stick to one library for easier interopability between its functions and object types, so it is convenient to have so many options available. There are two versions of the character n-gram analyzer: `char_wb` will respect token boundaries while `char` could result in a trigram whith the last letter of one token, a space, and the first letter of the next token.

#### Comparing Token Frequencies and Proportions

We can start discovering some very high-level patterns in our text data just by working with these simple frequencies, akin to doing exploratory data analysis prior to modelling. For example, we can convert the `count_matrix` to a dataframe and add a column indicating the party of the speaker, group the dataframe by party, and then compare some simple aggregate patterns in word usage across each political party. We'll start by creating the dataframe, which with data this size will require staying within the sparse matrix framework unless you're working with a system that has a great deal of memory resources. This is made quite clear below, where 67K speeches is not a particularly huge text dataset by modern standards, but keeping track of 26K features for *each* of those speeches becomes a huge memory burden when most of the values for those features are zeroes.


```python
count_data = pd.DataFrame.sparse.from_spmatrix(count_matrix)
count_data.columns = vocabulary

count_data.index = sampled_speeches['party']
count_data.shape 
```


The sparse form of the count vectorizer data uses only about 21MB of memory, because the density is around 0.001 - only 0.1% of the values are non-zero and sparse matrices don't actually store the zero or np.nan values. In fact, you are able to select whatever value you like to "fill" the empty areas of the matrix.


```python
print('sparse size: ' + str(count_data.memory_usage().sum()/1048576) + "MB")
print('sparse density : ' + str(count_data.sparse.density))
```

The dense version, on the other hand, occupies up a straight-up remarkable 8400MB of memory! The code block below will turn a sparse matrix into a dense one then calculate the size. You probably won't want to run it yourself!

```python
count_data_d = count_data.sparse.to_dense()
print('dense size: ' + str(count_data_d.memory_usage().sum()/1048576) + "MB")
```

The next step is to group the dataframe by the subset of parties, aggregate the token frequencies, and calculate their proportions within each party. We will use some full matrix manipulations for this, storing the percentages in the `results` dataframe and then transposing it so that each row is a token (indexed by the token string itself) and each column contains the token proportions for each party. With sparse matrix handling in the current version of pandas, aggregation with a groupby operation is unfortunately extremely slow. The function `sparse_groupby` from `dcss.utils` is a handy trick that at least works for doing a sum aggregation, and is very fast.

Now we can create the dataframe, transpose it, and look at a random sample of word proportions.

```python
party_counts = sparse_groupby(
    sampled_speeches['party'], count_matrix, vocabulary
)

results = party_counts.div(party_counts.sum(axis=1), axis=0)
results_t = results.T

results_t.sample(20, random_state=10061986)
```

With this dataframe, we can easily retrieve (and compare) the proportions for any given token across each of our parties. For example, if we search for `scotland`, we find that the Scottish National Party comes out on top. Note how small the differences in scores are across Plaid Cymru, Labour (Co-op), Labour, Conservative, and SNP. 


```python
search_term = 'scotland'
results_t.loc[search_term].sort_values(ascending=False)
```


While it is useful to compare the proportion of *specific tokens* of interest across each group, we can also compare parties by inspecting the top $n$ tokens for each. 


```python
n_top_words = 5
top_words_per_party = {}

for party in results_t.columns:
    top = results_t[party].nlargest(n_top_words)
    top_words_per_party[party] = list(zip(top.index, top))
     
for k, v in top_words_per_party.items():
    print(k.upper())
    for each in v:
        print(each)
    print('\n')
```

<!-- 
    CONSERVATIVE
    ('bill', 0.006218382261590537)
    ('service', 0.0050770014305403285)
    ('business', 0.004968748251783816)
    ('deal', 0.004288714860400624)
    ('lady', 0.004075841159892851)
    
    
    DEMOCRATIC UNIONIST PARTY
    ('northern_ireland', 0.024837738090187928)
    ('party', 0.007169219021762186)
    ('united_kingdom', 0.0058117337632036655)
    ('constituency', 0.0051754125482543585)
    ('decision', 0.005111780426759428)
    
    
    GREEN PARTY
    ('environmental', 0.010475651189127973)
    ('bill', 0.010050962627406568)
    ('eu', 0.009484711211778029)
    ('standard', 0.008210645526613816)
    ('deal', 0.007219705549263873)
    
    
    LABOUR
    ('bill', 0.005798510334341296)
    ('service', 0.0055913094008465634)
    ('child', 0.005147307400500709)
    ('prime_minister', 0.005064738607453937)
    ('deal', 0.00441665147712455)
    
    
    LABOUR (CO-OP)
    ('service', 0.006464849798699288)
    ('bill', 0.006426138123257975)
    ('public', 0.0050634871477237536)
    ('child', 0.004908640445958501)
    ('deal', 0.004831217095075875)
    
    
    LIBERAL DEMOCRAT
    ('brexit', 0.005289392526715915)
    ('deal', 0.005235602094240838)
    ('business', 0.004894929355232016)
    ('prime_minister', 0.004876999211073657)
    ('bill', 0.0048232087785985795)
    
    
    PLAID CYMRU
    ('wale', 0.02312352245862884)
    ('welsh', 0.015218676122931441)
    ('british', 0.011894208037825059)
    ('brexit', 0.0076832151300236405)
    ('uk', 0.007166075650118203)
    
    
    SCOTTISH NATIONAL PARTY
    ('scotland', 0.013614973572070461)
    ('scottish', 0.011039820657267923)
    ('uk', 0.009860951405463383)
    ('bill', 0.006522432335803806)
    ('prime_minister', 0.006189063571973833)
    
     -->

Finally, we can compute the *difference* of proportions between any given pair of document groups. This will result in a single vector of positive and negative numbers, where tokens with the largest positive values are associated with the first group and not the second, and tokens with the largest negative values are associated with the second group but not the first.


```python
diff_con_snp = results_t['Conservative'] - results_t['Scottish National Party']
diff_con_snp.sort_values(ascending=False, inplace=True)
```


```python
con_not_snp = diff_con_snp.head(20) # Conservatives but not SNP
con_not_snp
```

<!-- 


    lady           0.003259
    local          0.002403
    school         0.002009
    course         0.001568
    area           0.001491
    council        0.001423
    sure           0.001383
    business       0.001360
    clear          0.001265
    police         0.001261
    great          0.001230
    service        0.001081
    number         0.001061
    funding        0.001009
    opportunity    0.000992
    nhs            0.000955
    able           0.000932
    prison         0.000921
    hospital       0.000910
    department     0.000880
    dtype: Sparse[float64, nan]
 -->



```python
lab_not_snp = diff_con_snp.tail(20) # SNP but not Conservatives
lab_not_snp
```

<!-- 


    power            -0.000971
    office           -0.000986
    week             -0.001019
    pension          -0.001083
    poverty          -0.001137
    family           -0.001164
    conservative     -0.001184
    eu               -0.001205
    woman            -0.001214
    leader           -0.001250
    glasgow          -0.001608
    snp              -0.001793
    party            -0.001872
    tory             -0.002508
    parliament       -0.003324
    prime_minister   -0.003792
    brexit           -0.004404
    uk               -0.006066
    scottish         -0.009727
    scotland         -0.012228
    dtype: Sparse[float64, nan] -->



We can concatenate these two series to more easily visualize their differences. The results are show in Figure @fig-11_02.


```python
dop = pd.concat([con_not_snp, lab_not_snp])
```


```python
fig, ax = plt.subplots(figsize=(6, 6))
sns.swarmplot(x=dop, y=dop.index, color='black', size=4)
ax.axvline(0) 
plt.grid() 
ax.set(
    xlabel=r'($\longleftarrow$ Scottish National Party)        (Conservative Party $\longrightarrow$)',
    ylabel='',
    title='Difference of Proportions'
)
plt.tight_layout()
plt.show('figures/british_hansard_difference-of-proportions.png')
```
    

![png](figures/british_hansard_difference-of-proportions.png){#fig-11_02}
    

As you can see, simple token frequencies and proportions can be very useful when we are starting to explore our text data. Before moving on to the larger problem of modelling latent topics, let's discuss an alternative way of scoring tokens in a DTM. In the next chapter we will take a look at Term Frequency-Inverse Document Frequency (TF-IDF) weights.

  
> **Further Reading**   
>
>   The count-based methods we discussed in this chapter are the foundation of "Dictionary-based" approaches that are widely-used in the literature. For example, @bonikowski2016populist uses count-based dictionary methods to study populist claimsmaking in the 2016 American general election. @nelson2021future discusses dictionary-based methods alongside machine learning methods that we will cover later in the book.
>


## CONCLUSION

### Key Points 

- Learned about chunks, triplets, bi-grams, and n-grams
- Used Gensim's Phraser with SpaCy to detect n-grams
- Used Sklearn to create a Document Term Matrix (DTM)
- Discussed differences between using token counts vs proportions
