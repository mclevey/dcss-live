# Research Ethics, Politics, and Practices

## LEARNING OBJECTIVES

- Explain the challenges with informed consent in computational social science
- Describe the tensions between the competing ethical principles of privacy and transparency
- Explain how algorithmic biases and biased training datasets can amplify and exacerbate inequalities
- Explicitly articulate the normative and political values that underlie your research
- Identify the types of computational social science that you will and *will not* do

## LEARNING MATERIALS

You can find the online learning materials for this chapter in `doing_computational_social_science/Chapter_19`. `cd` into the directory and launch your Jupyter Server.

## INTRODUCTION

The chapters following this one will introduce a variety of machine learning models. Before we get there, we're going to consider some of the ethical and political challenges that arise in the context of computational social science. One of the many themes in this chapter is that we are working in unsettled times when it comes to research ethics in computational social science and data science. Many of the methods and models in this book provide access to power that we are not accustomed to dealing with, and for which there are few guidelines and standards. The recent advances in computational methods have far outstripped what we, as social scientists, have been historically capable of, and our ethical standards and practices have not yet caught up. As professional researchers, we need to make ethical decisions in our work. That means doing *more* than making sure we don't violate *currently established* ethical principles. Throughout this chapter, I will emphasize that current ethical standards are not adequate for much of what is introduced in this book (e.g., machine learning).

Rather than being *reactive* (e.g., changing practices and establishing standards after people have already been harmed), we should be *proactive* (e.g., anticipating and mitigating potential harms). We must adopt practices that help ensure we are doing our work in ways that *enable* us to be transparent and accountable to the right people at the right times. It means asking ourselves hard questions about the types of things we will and won't do and making a serious effort to anticipate the potential unintended negative consequences of the work we do. There is no avoiding constant reflexive practice. Nor can we avoid the politics of research. We must confront difficult political issues head on and make our normative values explicit and visible in our work. We do this not only to protect ourselves, our participants, and anyone who might be affected by our work once it leaves our hands, but because it also produces better science: science that is transparent, accountable, and reproducible. 

We'll start by considering the context of social network analysis, which we covered in the preceding chapters, followed by matching issues we have to negotiate as we work with machine learning in following chapters. 

## RESEARCH ETHICS AND SOCIAL NETWORK ANALYSIS 

As researchers, we are not detached from the social and political world we study, and we need to remember that our position as researchers puts us in unique positions of power. In network analysis, knowledge of a network imparts power over it in concrete and tangible ways. Most of us have limited understanding of the structure of the networks we live our daily lives in, and whatever understanding we do have diminishes rapidly as we move beyond our immediate connections. As researchers, we have privileged access to intimate details of the lives of real people and the unique relational contexts that shape their lives, for better and for worse. This information is often sensitive and has the potential to cause harm if not handled properly. 

At some point in your research career you will gain information that is very important, valuable, or compromising to the people participating in your study, and in network analysis that can happen *without any one individual realizing it*. Part of the value of studying networks comes from the ways that micro-level interactions (e.g. friendships, advice, communication) combine to produce highly consequential network structures that are not immediately obvious. When we collect relational data, we gain access to information about an emergent network structure that, though lacking in details, can reveal a picture that's very difficult to see from the inside. 

The decisions we make when we collect relational data, construct and analyze networks, and present our findings *all* have important ethical dimensions. For example, in a commentary from a 2021 special issue of *Social Networks* on ethical challenges in network analysis, Bernie @hogan2021networks recounts several experiences where presentations of simple network visualizations caused unintentional harm. In one case, a student gave a presentation that included a visualization of a network of their classmates, revealing that everyone was connected in one large group except a single isolated student. Similarly, after presenting visualizations of an academic network, Hogan describes being contacted by disconcerted academics who were located on the periphery of the network (implying they were marginal), but who felt this unfairly painted them in a poor light as they were primarily active in *other* networks that didn't fall within the presented network's boundaries. These were not necessarily "marginal" academics, but the definition of network boundaries *portrayed* them as marginal. We don't just *reveal* networks as they really exist, we *construct* them, and in ways that feed back into the world.

  
> **Further Reading**    
>   
> To learn more about some salient ethical issues in comtemporary network analysis, I recommend readind the 2021 special issue of *Social Networks* edited by @tubaro2020social. 
>


Cases such as these are a reminder that unavoidable and seemingly benign measurement decisions play a significant role in determining who is portrayed as central or marginal within the boundaries of a network *as we define it*; we have a certain amount of control over influential representations of the world that cast some people as more central (and therefore more powerful, influential, and high-status) than others. This is what I meant when I said we *construct* networks, we don't just reveal them. Since it is possible to cause harm with our constructions, we should consider the important ethical dimensions of the decisions involved, such as which ties we measure among which people. And since many harms can come from portraying *specific* people as central or marginal, we should also consider the ethical implications of how we share information about networks, whether we are sharing data or presenting results in some other form. All of this is especially problematic for people who are already marginalized. Cases like these are likely more common than we realize. 

There are concrete things we can do to help mitigate the negative effects of situations such as those described above, but many problems persist. For example, properly anonymizing network data *can* go a pretty long way. However, this is not a just a matter of "give everyone numeric IDs" because people are often able to make pretty good inferences about who's who in a network they are involved in even if they don't have all the information needed to construct the network in the first place. If someone showed you a visualization of a friendship network that you're part of, I'd wager that with some time and thought you could make very good guesses as to who was where in the network. The ability to use extremely surface-level data to know, with relative certainty, information about individuals is *powerful*. 

So how can we present data while protecting anonymity? There are a variety of options. Consider the Faux Magnolia High network data available in the statnet R library [@handcock2003statnet], for example. It describes a fake high school with 1461 students with attribute data for grade, sex, and race. While it was based on real data, and those variables could potentially have been used to identify individual students, an exponential random graph model was used to infer the broad patterns between these variables and the network structure. Those patterns were then used to create a *randomly generated network* that became the dataset provided to the public. (Unfortunately, I couldn't make space for exponential random graph models (ERGMs) in the networks chapters, but @lusher2013exponential provide a good starting point if you are interested in delving further into ERGMs.) Unfortunately, this won't work for all network data, nor for all data in general; the Faux Magnolia High data is primarily used for learning and testing network models. It poses little value for further network research because it is so far divorced from the original data. It makes no claims to represent any relationship between the original data and network structure beyond that captured in the model used to generate it. 

This raises difficult questions about the tension between privacy and transparency. We'll turn to these issues directly in a moment, but for now, I want to emphasize that network data collection can sometimes result in information about people who have not provided consent, or specifically *informed consent*. For example, if you collect data on an organization's management team and ask employees to name the people they give advice to and receive advice from, you will likely end up with information about someone who simply wasn't in the office that day, and all the little bits of relational information from many different people add up to paint a picture of that person's position in the advice-sharing network.

As with other ethical challenges we will discuss below, do not assume that you are in the clear because your research passes an ethics review. As I've mentioned, current ethical standards are lagging behind advancing methods, and they are not well suited to judging how cutting-edge work might be used by others. One of the driving forces for the recent explosion of network analysis derives from the generalizability of methods, measures, and models. At their heart, networks are mathematical constructs. Anything that can be reasonably conceptualized as a collection of things connected to other things is within its purview. A measure that can be used to describe "popularity" or "influence" in sociology can be used for "risk of exposure" in an epidemiological model or "importance" in a criminal or terrorist network. Knowledge about networks *in general* is powerful because network analysis itself is so generalizable. You shouldn't assume that your work will only be used in the way you intended it to be used.

While I have focused on how researchers need to consider the ethics of working with networks, we aren't the only ones working on them. Google built one of the most valuable tech companies in the world on the foundation of PageRank (a centrality-like algorithm drawing on network analysis to estimate the relative "quality" of a website based on the links leading to and from it). Similarly police forces and intelligence agencies profit from information about the structure and dynamics of our social networks, and it doesn't especially matter if they have any information about the explicit *content* of those ties. You can make powerful inferences using only your knowledge of the structure of the network as Kieran @khfpr cleverly showed in a blog post following revelations in 2012 about the extent of NSA metadata-based surveillance [e.g., @metadata]. These non-academic groups do not have the same standards we hold ourselves to, but they have access to everything we publish, more data, and far more money and computing power. When we develop new network tools or data, we need to consider what others with more resources might be able to do with it. 

In the following section, I move from network data to discussing data more generally, and I will focus more closely on issues of informed consent and balancing the principles of privacy and transparency. 

## INFORMED CONSENT, PRIVACY, AND TRANSPARENCY

Digital data collection (including the collection methods we discussed in Chapters 5 and 6) poses greater ethical challenges than more traditional data collection methods, and issues with **voluntary informed consent** are especially salient. This is largely because we can observe (and interfere) from a great distance, and without the knowledge of the people and groups we are observing (and potentially interfering with, for example in online experiments). The massive availability of data online also poses new challenges for **privacy**, as information that is anonymous in one dataset can quickly become uniquely identifiable when combined with other data. This necessitates a considerable amount of careful ethical thinking and decision-making for individual researchers and teams [@salganik2019bit; @beninger2017social], as there are no pre-established rules or ethical checklists to rely on in these and many other situations we might find ourselves in when collecting digital data. In research with social media data, where issues around informed consent are ubiquitous [@sloan2017retrospective], some have argued for increased ethical standards [@goel2014data] while others have argued that this is unnecessary for minimal risk research on data in the public domain [e.g., @grimmelmann2015law]. 

One of the reasons why these debates rage on is because the boundaries between public and private are much more porous with data collected from social media platforms and the open web [see @sloan2017retrospective]. And while people may realize that much of what they do and say online can be read by anyone, they are not necessarily thinking about the fact their words and actions are being recorded, stored, and used for something other than their own intended purpose. And even if they are thinking about that, people may not anticipate how the data collected about them from social media platforms and the open web may be linked up with other data, just like they may not anticipate the richness of the network knowledge that can be gleaned from lots of seemingly trivial details, like the name of the person you call when you need to vent about your insufferable coworker. 

For example, from 2006 to 2009, @lewis2008tastes collected a huge volume of Facebook data from a cohort of students over four years. With this, they created network datasets with information about the students' home states, cultural tastes such as preferred books and musical genres, political affiliations, the structure of their friendship networks, photos, and so on. All of the Facebook data they collected was from public profiles, but it was *not* collected with informed consent. The researchers linked the Facebook data with data from the college (e.g., on academic major). That's quite the collection of intimate portraits of some 1,700 unaware people.

As part of the terms of funds they received from the National Science Foundation, @lewis2008tastes made an anonymized version of their data publicly available via Dataverse; they did not identify the institution by name, used identification numbers instead of names, and they delayed releasing personal information like interests in movies, books, and so on. Within days of the first wave of release, @zimmer2010but and others were able to identify Harvard as the source of the data and show that enough unique information was available to identify individual students. 

There is nothing inherently wrong with linking datasets. Researchers do it all the time, and for good reason. But where there is a lack of consent, the data is *extensive and sensitive*, and there is a lack of agreed-upon ethical standards, the risks should be readily apparent. While people know their actions are public, they can't reasonably be expected to anticipate all the things that researchers (or government or industry) will do with that data, what they will link it to, and what the resulting picture of them will look like. So, while they may have consented to publicly releasing certain data on certain platforms, they have not consented to the various ways that we might recombine those data in ways they never considered, and which they may not fully realize is even possible. Common privacy protection methods are little defense against dedicated research methods, and we may easily de-anonymize individuals without realizing it in our pursuit of more robust data. 

As with network data, anonymized names are not enough to protect people. In the 1990s, a government agency called the Group Insurance Commission collected state employees' health records for the purposes of purchasing health insurance, and released an anonymized dataset to researchers [@salganik2019bit]. This data included things like medical records, but also information like zip code, birth date, and sex. By combining this data with voting records (that also had zip code, birthdate, and sex) purchased for $20, Latanya Sweeney, a grad student, was able to attach the name of the governor of Massachusetts to specific medical records, and then mailed him a copy. By linking records, data that is *internally* anonymous can be used to identify personal information that no one source intended to allow. Whenever you release anonymized data, you need to think very carefully about not just your own data, but what other kinds of data might exist that could be used in harmful ways. 

Medical records are an obvious example of **informational risk**: the potential for harm from the disclosure of information, but this is far from the only example. Latanya @sweeney2002k has shown, for example, that 87\% of the US population could be reasonably identified with just their 5-digit ZIP, gender, and date of birth. The risk posed by record linkage means that even seemingly innocuous data can be used to unlock much riskier data elsewhere. Even attempts to perturb the data, by switching some values around, may not be enough if enough unchanged data is still available. Given the power of machine learning to make inferences about unseen data, which we will cover later in this book, I will echo @salganik2019bit and stress that you should *start* with the assumption that any data you make available is potentially identifiable, and potentially serious. 

As researchers, we tend to hyper-focus on the aspects of our data that pertain to our specific research projects, as if we were only responsible for what we ourselves do with the data we collect. After all, we collected the data for a particular purpose, and that purpose can define how we perceive its uses. We should also consider what *other* questions might be answerable with our data, both as a matter of good research and as a matter of protecting the data we have direct responsibility over, and the indirect data that it might unlock. 

One response to this type of problem is to simply share nothing; *lock down all the data*. But this collides with another very important ethical principle and scientific norm: **transparency**, which is a necessary but insufficient condition for **accountability**. We don't want black box science that nobody can question, challenge, or critique. We will later discuss how datasets can contain racist and sexist data that are learned by models, put into production, and further propagated, for example. Privacy and transparency are in direct contradiction with one another. So where on the scale should the needle to be? There is no perfect solution for completely transparent research and completely protected privacy, so we consider the importance of both according to the situation. There is no avoiding difficult decision-making and constant ethical reflection and reflexive practice.

"According to the situation" is key here. As @diakopoulos2020transparency sums up the key idea about the ethical importance of transparency:

> "Transparency can be defined as 'the availability of information about an actor allowing other actors to monitor the workings of performance of this actor.' In other words, transparency is about *information*, related both to outcomes and procedures used by an actor, and it is *relational*, involving the exchange of information between actors. Transparency therefore provides the informational substrate for ethical deliberation of a system's behavior by external actors. It is hard to imagine a robust debate around an algorithmic system without providing the relevant stakeholders the information detailing what that system does and how it operates. Yet it's important to emphasize that transparency is not sufficient to ensure algorithmic accountability." (Page 198)

But as @diakopoulos2020transparency points out, we can't understand algorithmic transparency in a binary -- transparent or not -- as there are many different *types* of transparency, including what types of information and how much is provided, to whom, and for what purposes. The nature of disclosure can also matter, as self-disclosures are self-interested and present things in a certain light. Not all transparency is good transparency, and not all types of transparency lend themselves to accountability. He identifies a number of things we need to consider when trying to strike this delicate balance between privacy and transparency:

- *Human Involvement*: Some machine learning algorithms require human input. Supervised machine learning may require data that have been annotated by humans, while others require humans to provide feedback on results, or during operation. Wherever humans have non-trivial input into the process, their decisions should be made open and available.
- *The Data*: Machine learning often involves "training" an algorithm on some set of data. This data, and how it was produced, can have significant impacts how the algorithm functions. If photo data has been used to train a facial recognition algorithm, biases in the original data, like a disproportionate number of white men on some social media sites, can taint any subsequent work that doesn't match the training data. If we don't know the training data, we can't examine it for biases. 
- *The Model and Code*: While algorithms are executed by computers, humans wrote them. They were written to solve specific problems, sometimes with specific data and goals in mind. Decisions were made about what variables to optimize, and much more. Researchers decide the values of parameters, or decide not to decide and use default values. These decisions should be open and available for review. 

In an ideal world, no important *decisions* about our data or models would need to be hidden to protect privacy or confidentiality. In practice, that is often not the case, and we must navigate as best we can our obligations to safeguard our data while making our work as open and transparent as possible. Both are essential; we cannot completely abandon one for the other while still meeting a high standard for ethical research. The answer is not to make all information available; there are too many factors to balance, risks to assess, privacy to protect, and so on. Nor is the answer full transparency, which is not good for anyone. It's *contextually-appropriate transparency*, where decisions are made close to the specific cases with the relevant stakeholders. These are the kinds of transparency that are most important to ensuring algorithmic accountability.

In addition to contextual ethical considerations, we can look for ways to build fairness into our practices more deeply [@pracFair], and adopt develop new privacy-oriented practices such as Sweeney's [-@sweeney2002k] proposed $k$-anonymity. This notation should be familiar based on our discussion of $k$-cliques in the networks chapters. The idea behind $k$-anonymity is that no one individual in a dataset can be distinguished from *at least* $k$ other individuals in the same data using a combination of unique "quasi-identifiers" (e.g. 5-digit ZIP, gender, and date of birth). The goal here, like in Faux Magnolia High, is to protect privacy by hiding needles in identical needle stacks, but we manage how transparent/anonymous our data is with the value of $k$. With especially sensitive data, we may choose higher values, while lower values may be more appropriate for low-risk stakes. This may mean generalizing some data to make it less specific: if only one person is from Glasgow in your dataset, that might mean replacing their location data with Scotland, or you could remove their location data, or remove them from the data altogether. In every case, we make our data *less transparent*, but we try to preserve the contextually appropriate transparency of the data *while also protecting individual privacy and anonymity*. 

As computational scientists, we *must* wield our power responsibly. That means doing our work in ways that are transparent and facilitate accountability while also ensuring privacy and respecting the people represented in our datasets. It also means doing our work in ways that are auditable and which which enable us to be accountable for the work we do and the impacts it has. That may manifest in any number of ways, the most obvious of which are to use tools that record every decision, every step that takes and input and produces an output, are recorded and can be understood. There are systems that enable this, and using them is the cost of entry.

However, being aware of the political power we wield and adopting tools and workflows that attempt to make our work as transparent and accountable as possible are, as I mentioned earlier, necessary *but insufficient*. To wield power responsibly, it is necessary to go beyond abstract ethical principles to think more deeply about how and why we do science, and what kinds of science we want to contribute to and advance, and which we want no part of. In the next section, we'll discuss bias and algorithmic decision-making as examples of why it is so important to ask ourselves these kinds of questions.

> In addition to @diakopoulos2020transparency, I suggest looking into other articles on transparency and accountability by Diakopoulos and others, such as @diakopoulos2017enabling and @ananny2018seeing.

## BIAS AND ALGORITHMIC DECISION-MAKING

In a widely-viewed talk "How To Stop Artificial Intelligence From Marginalizing Communities," Timnit @timnetTedTalk raises two very important questions about the many machine learning algorithms that are invisibly woven into virtually every aspect of our lives. For any given algorithm: 

1. Should it exist at all?
2. If it is to exist, is it robust enough to use in high-stakes contexts (e.g., in the criminal justice system, healthcare, education, etc.)?

Gebru's questions take aim directly at high-stakes algorithmic decision-making (ADM); rightfully so, as ADM is one of the most insidious mechanisms through which systemic inequality is perpetuated. But more importantly, these questions are especially relevant to us as researchers; you will likely have opportunities to contribute to technologies such as these, or others that are similar in one way or another. Given that you could easily find yourself in a situation where that's a possible outcome, it's important for us to ask ourselves these questions early and often so we can better understand what kinds of technologies we are uncomfortable contributing to, whether because we think they are inherently dangerous or simply too prone to abuse to be worth the risk. 

If you don't spend a lot of time thinking about, critiquing, or developing algorithms, it might *seem* like incorporating algorithms into decision-making is reasonable and perhaps even more impartial than the alternative. After all, algorithms are just a series of steps consistently carried out by computers, following mathematical rules and precision. And a computer is incapable of thought, let alone bigotry. 

This is a complete fantasy; algorithms don't spring into existence fully formed out of nowhere. They're written by humans to enforce human rules, and I doubt anyone would say that the rules we make are always fair. When our biases are encoded into algorithms, those biases are perpetuated and amplified, often with very serious consequences. These biases disproportionately affect people who are already marginalized. There is a rapidly growing literature [e.g., @west2019discriminating; @gebru2020race; @propub; @o2016weapons; @eubanks2018automating; @benjamin2019race; @nelson2021leveraging; @noble2018algorithms; @de2019does; @buolamwini2018gender; @hamidi2018gender] and there is no excuse for ignorance. 

Who can we turn to when an algorithm discriminates? Rarely ever one person. ADM technologies are thought up, planned, developed, and implemented by *many* people, diffusing any direct responsibility and allowing any one person or group to somewhat reasonably claim that they cannot be held personally responsible for specific negative outcomes. If you think something is wrong, you can always try to get the organization to change the rules, right?

This is one small part of Virginia Eubanks' [-@eubanks2018automating] description of the evolution of what she calls the "Digital Poorhouse:" technological systems born from conservative hysteria over welfare costs, fraud, and inefficiency as the 1973 recession hit. With recent legal protections put in place to protect people needing welfare from discriminatory eligibility rules, politicians and state bureaucrats were caught between a desire to cut public assistance spending and the law. So, they found a way to cut spending, and gave it a spin that was hard to dispute at face value. They commissioned new technologies to save money by "distributing aid more efficiently." After all, computers could ensure that every rule was being followed, welfare fraudsters couldn't sneak through the cracks in the algorithms, and everyone would be getting equal treatment. Welfare assistance had rules, and computers would simply enforce the rules that were already there. By the 1980s, computers were collecting, analyzing, and storing incredibly detailed data on families receiving public assistance. And they were sharing this data with agencies across the US government, including the Department of Defence, state governments, federal employers, civil and criminal courts, local welfare agencies, and the Department of Justice. 

Algorithms trawled these data for indications of fraud, criminal activity, or other inconsistencies. Through a combination of new rules and technologies, Republican legislators in New York state set about solving the problem of "cheats, frauds, and abusers" of the welfare system [@eubanks2018automating]. In 1972, almost 50% of citizens living under the poverty line were on public assistance; as of 2018, it was less than 10%. Every new set of rules could be justified if they found a few examples of misuse, which could then be amplified and used to justify the next round of rules. When failure to be on time for an appointment or otherwise missing any caseworker-prescribed therapeutic or job-training activity can be met with sanctions that result in temporary or permanent loss of benefits, this feeds into a cycle of poverty. People in need of assistance are then punished for illness, taking care of dependents, or occupational obligations, which in turn produces greater pressures on health, family, and finances. In protecting against people becoming "dependent" on the government, algorithms become the walls of the Digital Poorhouse, actively hindering people from escaping privation and perpetuating the cycle of poverty.

Think back to Gebru's questions. Should these algorithms exist? Are they robust enough to handle high-stakes contexts? The first question is always difficult, in part because the same algorithms can be used in so many different contexts and to so many different ends. The second question is easier to answer: no, they are not good enough to rely on in these high-stakes contexts. These are questions that we should *always* be thinking about when we produce algorithms that make decisions where humans would otherwise. We need to ask these questions because we are working in areas with important unsettled ethical dimensions where the decisions we make have material consequences on people lives. These questions should help us determine what kinds of work we will do, and what kinds we will *not*. 

In addition to consent, informational risk, the tensions between competing principles such as privacy and transparency, and the highly consequential risks of algorithmic bias and decision paired with algorithmic decisions making, we have to be deeply concerned with the *data* we train our models with, and whether those data contain biases that would be perpetuated if used in an applied context. We'll discuss the details in the next chapter and many that follow, but for now what you need to know is that machines only "learn" what we teach them via *many* examples. Certain kinds of machine learning make it very hard to understand what exactly the machine has learned, which contributes to a lack of accountability in a context where what the model learned has *very* significant consequences for the lives of real people. Here's the problem, having been collected from the real world, they reflect the biases that exist wherever they were first collected. And of course any biases of the people who collected them, which is a problem given the extent to which marginalized people are underrepresented in fields like machine learning and artificial intelligence research [e.g., @west2019discriminating; @gebru2020race]. Many of these models learn, *or are explicitly trained to learn* [e.g., classification models for social categories such as race, gender, and sexuality], those biases, which are then amplified and further propogated. Sometimes these biases are blatently obvious once you know to look for them [@buolamwini2018gender]. Othertimes they can be much more illusive, even though there are plenty of good reasons to suspect they are there in some form [@bolukbasi2016man; @gonen2019lipstick; @nissim2020fair].

  
> **Further Reading**    
>   
> There is a lot of excellent work on ethics and politics of machine learning and artificial intelligence that is important to know. I strongly recommend @o2016weapons, @eubanks2018automating, and @propub for general introductions to issues related to systemic social inequality and algorithmic decision making. Timnit @gebru2020race provides a good overview of questions related to race and gender in machine learning and ethics. @west2019discriminating provide a close look at issues related to diversity and representation issues in machine learning and artificial intelligence that includes a critique of "pipeline" research on diversity in STEM fields. 
> 
> Abeba Birhane and Fred Cummins [-@birhane2019algorithmic] "Algorithmic injustices" offers a perspective grounded in philosophical work on relational ethics, and @hanna2020towards offers a guidelines for work on algorithmic fairness that is grounded in critical race theory and sociological and historical work on the social construction of race and systemic social inequality. @denton2020bringing tackle of issues of algorithmic unfairness in benchmark machine learning datasets, which are biased towards white, cisgender, male, and Western people. 
>


## DITCHING THE VALUE-FREE IDEAL FOR ETHICS, POLITICS, AND SCIENCE

We've discussed a lot of major challenges in this chapter so far, but we've barely scratched the surface. One thing I hope has been clear so far is that data are not inherently objective descriptions of reality that *reveal* the truth to us, like some sort of mythical view from nowhere; they are things that we *construct*. It's not a matter of collecting and drawing insights from "raw data;" *it's models all the way down.* Deciding to collect data in *any* way, is in effect a modelling decision that is propagated forward into other models (like univariate distributions), which in turn is propagated forward into more complex models (like machine learning models). At the end of all this, we design digital infrastructure that further entrenches our models in the world, whether it's in the algorithms that recommend friends and news articles, or predictive models that we come to understand and game over time, further re-structuring and re-imagining our societies. 

While we should reflect on whether the data we collect and encode represents the world in some statistical sense, this is only the most obvious dimension of the problem of fair representation. It is also crucial to think about how the data we collect, and how we encode it, works *back* on the world. In other words, we need to think about how the ways we collect and encode data represent people, and whether the potential impacts from our work are *fair* and *just*. If the idea of doing computational social science with justice in mind is a bit too much for you, then I recommend, at the very least, starting with a commitment not to do computational social science in ways that contribute to *injustices*, which, as the algorithmic injustice literature makes patently clear, is *very easy to do.* In the end, the decision about what kind of work you will or will not do is up to you and any ethics board/stakeholders you must answer to, but this decision should be *intentional*. Refusing to make a decision *is a decision*, so it's better to know what you're comfortable contributing to so you don't get a nasty surprise later on. 

I hope this resonates, but even if it does, it may not sit very well with everyone's understanding of how science is supposed to be done. Shouldn't we strive for impartiality? Shouldn't we be pursuing the "value-free ideal?" This debate has raged on in some form or another in the sciences and humanities for centuries, and a full discussion is beyond the scope of this chapter. But the point I want to emphasize here is an obvious one whose full implications are rarely appreciated: science is fundamentally a human and cultural activity. For better or for worse, *there is no getting rid of values in science* [@douglas2009science].

  
> **Further Reading**    
>   
> There is plenty of work in science and technology studies as well as the sociology, history, and philosophy of science that is relevant to this discussion. I recommend reading Heather Douglas' [-@douglas2009science] *Science, policy, and the value-free ideal* and @collins2020experts *Experts and the Will of the People*. Both books articulate realistic normative models for science in social and political context. Finally, @green2021data (discussed below) is worth reading for a more expliticly political take on the practice of data science. 
>


Not only is it impossible and pointless to try to get rid of values in science, *neutrality itself is an illusion*. Every decision that we make in the context of collecting data, applying models, interpreting outputs, and making decisions is part of imagining and structuring the world in particular ways, and to the extent that those decisions impact who gets what, *these decisions are political*. Neutrality is not an answer here. As @green2021data points out, efforts to *resist* reform are just as political as any effort *for* reform, and the only people who get to claim "neutrality" are the ones whose perspective and interests are already widely entrenched. Everyone else is denied that stance. There really is no getting out of politics, whether we want out or not. 

@green2021data uses the case of predictive policing and systemic racism to make an argument we will return to when considering what and how we will and will not do computational social science. 

> "... the very act of choosing to develop predictive policing algorithms is not at all neutral. Accepting common definitions of crime and how to address it does not allow data scientists to remove themselves from politics -- it merely allows them to *seem* removed from politics, when in fact they are upholding the politics that have led to our current social conditions." (Page 16)

and 

> "Whether or not the data scientists ... recognize it, their decisions about what problems to work on, what data to use, and what solutions to propose involve normative stances that affect the distribution of power, status, and rights across society. They are, in other words, engaging in political activity." (Page 20)

There are three core related insights here: (1) it is not possible to be "neutral;" (2) striving for neutrality is fundamentally conservative in that it maintains the status quo, whatever that may be; and (3) while you are entitled to conservatism if that's what you want, you should be honest and call it what it is: conservativism, not neutrality. You don't need to adopt a specific political stance to do good science, but doing good science, doing *ethical* and professionally *responsible* science, means articulating those values and making them explicit. You can see this as an extension of transparency if you like: you have values that shape your science, whether you know it or not. It is incumbent upon you to identify those values, understand their role, to make them explicit, and use that reflexive knowledge to do better science in service of your articulated and carefully considered values. 

Green [-@green2021data] argues that abstract ethical principles are not enough, we also need explicit normative values. *But doesn't that run against the value-free ideal?* Yes? Doesn't that make for bad science? *No. Quite the opposite, actually.* Nothing good can come from pretending that science is not fundamentally a human and cultural endeavor [@collins2020experts; @douglas2009science]. There is no being free from social standpoints or political and cultural contexts. And that does *not* devalue or diminish science in any way. The problem is *not* that we find values in places (i.e., sciences) where they don't belong, it's that those values are usually hidden, intentionally or unintentionally; they are not *recognized* as values, they are implicit, smuggled in. And they affect people's lives. 

### Critical Questions to Ask Yourself

We do not just make neutral tools that reveal some value-free Truth about the world. How will your tools be used? Are you developing or improving tools that could be used to violate people's rights? That could infringe on their privacy or manipulate their informational environments and emotional/affective states? Could it undermine their autonomy, identify, or self-presentation? Could it out their secrets, or expose intimate details of their lives? Does it assign them membership in groups they don't identify themselves with, such as methods that automatically estimate membership in some sort of social category? 

If you consider these questions, you will quite possibly find yourself with the start of your very own "what I won't build" list, articulated so clearly by [Rachael @tatmam](http://www.rctatman.com/files/Tatman_2020_WiNLP_Keynote.pdf) in her Widening NLP keynote. What will *you* not build? How will you *not do* computational social science or data science?

I am framing this as a question of professional *responsibility* in part because much of the mess that data scientists and computational social scientists can find themselves in, wittingly or unwittingly, stems directly from defining our scientific work and roles in society as *lacking* agency, power, and responsibility for the way our work is used, and how it acts back on the world, and for avoiding politics as if it tainted our science rather than making it better. By framing it as a *professional* responsibility, I'm casting it as the cost of entry: ignoring these issues or defining them as not our/your responsibility is professionally irresponsible at best.

It is not enough to think about these things, they have to have an impact on our professional practice. Some of that, most in fact, is not a matter of technical skill. As we've already discussed, much is a matter of explicating your own values, whatever they might be, and making them more explicit. It's about making decisions about what you *will* and *won't* do for explicitly-articulated ethical and political reasons. Doing so does not mean injecting values into "science" that would otherwise be "value-free," nor does it mean compromising the integrity of our research work. Doing so results in *better* science, but more importantly it contributes to a world that is better for everyone, including us. 

In addition to decisions about what you will and won't do in data science and computational social science, you will need to make specific decisions about *how* to do the things you've decided you will do. At a minimum, the cost of entry here should be to do your work in ways that are as transparent, accountable, and reproducible as possible. 

  
> **Further Reading**    
>   
> There is a growing movement in the machine learning community, and more recently computational research in general, towards embedding fairness, transparency, and accountability (see, for example, the FAccT conference) into concrete research practices. It has also motivated discussions of prioritizing interpretable and causal models [e.g., @rudin2019stop; @kusner2020long] and better standards and documentation for data and models [e.g., @gebru2018datasheets; @mitchell2019model; @pracFair; @pdpp; @holland2020dataset], and research with secondary data [e.g., @weston2019recommendations].
> 


In the kinds of cases that Cathy @o2016weapons and others discuss, the central idea is that to be *accountable* one has to be able to explain to those whose lives we affect how decisions where made not just in general, but *in their case*. If a bank uses a model that denies you a loan, you have a right to know why. Yet many widely-used cutting edge models used in the field, like most contemporary neural network models, can include thousands or millions of parameters that are learned from data and extraordinarily difficult to understand. Some of the really large-scale language models that make the news headlines have billions. And the variables these models use -- generally known as features -- are often low-level, like individual words or pixels. This has prompted two movements: (1) towards using less complex models that produce directly interpretable results, from humble logistic regressions to hierarchical Bayesian models instead of more complex models; and (2) developing new "explainability" models that attempt to inject a bit of interpretability into more complex models. 

Part of doing ethical, fair, and just computational and data science is about using models in ways that are appropriate for the problem at hand. Often this will mean putting down your neural network and picking up your logistic regression. But that doesn't mean that the more complex models don't have a place, they do! In fact, as @nelson2021leveraging and others have argued, they can even enable approaches to computational research that are informed by intersectionality theory. 

As always, part of what makes this a challenge is that there is no checklist here. That said, here's a non-exhaustive checklist to get you *started* thinking thorough some of these ethical and political considerations in computational social science and data science.

- Have the people represented by my data provided informed consent? If not, have I fully justified its use?
- How important is privacy? Are any participants particularly at risk? Are any data particularly sensitive?
- How important is transparency? How much of my data and process can I reveal to increase accountability and reproducibility?
- What kind of data might my data be linked with? Does this pose any risks?
- What could other people who have more resources do with my work?
- Should this work exist? Is it robust enough to be used in high-stakes contexts?
- What values have I used to guide this research? Have I made those explicitly clear?
- What kind of work will I do? What kind of work will I not do? How does this research fit into that?

If you can provide answers to these questions (and any more that apply) that would satisfy you coming from others, as well as yourself, you will be taking a much more proactive approach to conducting ethical and principled computational social science. 

## CONCLUSION

### Key Points

- Knowledge of network structure can provide information that can be used to influence, for good or ill, that network.
- Anonymizing data is not a matter of removing names. The vast wealth of data in the digital age provides many ways to de-anonymize data, so more advanced techniques are needed to protect privacy.
- Transparency in research is important for producing better science that is reproducible, accountable, and more open to critique. 
- Privacy and transparency are in direct opposition to each other; we must balance the two principles according to the contextual importance of both.
- Algorithms are not impartial. They reproduce human biases and goals, and they hide individual accountability.
- Science is a human and cultural endeavour. It has never been value-free. We can make science even better by making our values explicit, rather than hiding them.
- While ethical standards lag behind new technologies, doing ethical and principled computational social science requires holding ourselves to higher standards than are the current norm. 
