# Association and latent factors

::: {.callout-note}
## Learning Objectives

By the end of this chapter, you should be able to:

- Explain what latent variables, components, dimensions, and clusters are
- Explain what the the 'curse of dimensionality' is, and why it matters in computational social science
- Explain what dimensionality reduction methods are, and describe the basic logic of linear dimensionality reduction using theory-driven approaches (Exploratory Factor Analysis) and data-driven approaches (Principal Component Analysis)
- Perform linear dimensionality reduction using Principle Component Analysis (PCA)
:::

<br>

This chapter introduces additional methods for conducting exploratory data analysis and inductive discovery. You may have encountered them previously when learning multivariate statistics. In machine learning, these methods are typically referred to as "unsupervised" because we discover and interpret latent patterns in the data *inductively*, but for now, you can think of them as a form of multivariate exploratory data analysis. 

We will begin with an introduction to the idea of latent variables: abstract constructs that we cannot observe, but which we hypothesize to have effects in the data. We will distinguish between two main ways of dealing with this problem: Exploratory Factor Analysis (EFA) for theory-driven efforts to measure latent variables, and Principal Component Analysis (PCA) for data-driven efforts to mitigate the "curse of dimensionality," facilitate more inductive and interpretive work, and improve the quality of downstream analyses. 

## Imports and Data Preparation

We will make extensive use of the Sklearn package in this chapter. Sklearn is very important and widely-used in machine learning, and it features heavily in the rest of this book. Some of the methods we introduce here are also implemented in other Python packages, including statsmodels, which implements a wide-variety of statistical models. If it suits your needs better, you should feel free to use statsmodels instead.


```python
import pandas as pd
import numpy as np
from scipy.stats import zscore
import random

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples

import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

from dcss import set_style
set_style()
```

We will continue working with the VDEM data in this chapter, filtered to contain observations from 2019 only.


```python
# Data downloaded in Chapter 6
df = pd.read_csv(
    'data/vdem/V-Dem-CY-Full+Others-v10.csv', low_memory=False
) 

df = df.query('year == 2019').reset_index()
df.shape
```

Now that we have the VDEM data from 2019 loaded up, we can select the columns we will use in our analyses. In this case, we want the country name as well as a series of variables related to political deliberation, civil society, media and internet, private and political liberties, and the executive. The specific variables we will use in each of these categories are given in @tbl-vdemvars. Given that I don't have space to discuss each variable (there are 35 of them in total), I recommend that you consult the VDEM codebook to ensure you know what each represents.

| Deliberation | Civil Society | Media & Internet | Private & Political Liberties | The Executive |
|:-------------|:--------------|:-----------------|:------------------------------|:--------------|
| `v2dlreason` | `v2cseeorgs`  | `v2mecenefm`     | `v2cldiscm`                   | `v2exrescon`  |
| `v2dlcommon` | `v2csreprss`  | `v2mecenefi`     | `v2cldiscw`                   | `v2exbribe`   |
| `v2dlcountr` | `v2cscnsult`  | `v2mecenefibin`  | `v2clacfree`                  | `v2exembez`   |
| `v2dlconslt` | `v2csprtcpt`  | `v2mecrit`       | `v2clrelig`                   | `v2excrptps`  |
| `v2dlengage` | `v2csgender`  | `v2merange`      | `v2clfmove`                   | `v2exthftps`  |
| `v2dlencmps` | `v2csantimv`  | `v2mefemjrn`     |                               |               |
| `v2dlunivl`  | `v2csrlgrep`  | `v2meharjrn`     |                               |               |
|              | `v2csrlgcon`  | `v2meslfcen`     |                               |               |
|              |               | `v2mebias`       |                               |               |
|              |               | `v2mecorrpt`     |                               |               |

: Table: VDEM variables used in this chapter. {#tbl-vdemvars}

We will create a list of these indicator variables names that we can use to subset the larger dataframe. 

```python
indicators = [
    'v2dlreason', 
    'v2dlcommon', 
    'v2dlcountr', 
    'v2dlconslt', 
    'v2dlengage',
    'v2dlencmps', 
    'v2dlunivl', 
    'v2cseeorgs', 
    'v2csreprss', 
    'v2cscnsult',
    'v2csprtcpt', 
    'v2csgender', 
    'v2csantimv', 
    'v2csrlgrep', 
    'v2csrlgcon',
    'v2mecenefm', 
    'v2mecenefi', 
    'v2mecenefibin', 
    'v2mecrit', 
    'v2merange',
    'v2mefemjrn', 
    'v2meharjrn', 
    'v2meslfcen', 
    'v2mebias', 
    'v2mecorrpt',
    'v2exrescon', 
    'v2exbribe', 
    'v2exembez', 
    'v2excrptps', 
    'v2exthftps',
    'v2cldiscm', 
    'v2cldiscw', 
    'v2clacfree', 
    'v2clrelig', 
    'v2clfmove'
]
```

We can now subset the original dataframe so that it includes only these variables, and then use the country names as the dataframe index. 


```python
countries = df['country_name'].tolist()
df = df.set_index('country_name')[indicators]
df.shape
```

The resulting dataframe has 179 observations (each one a country in 2019) and our 35 variables. Before moving on, we can do a quick check to see whether we have any problems with missing data. The code below counts the number of variables with missing (1) and non-missing (0) data. None of our variables have missing data.


```python
df.isna().sum().value_counts()
```


All we need to finish preparing our data is to get our indicator variables into a Numpy array. We will do some additional cleaning a bit later in the chapter.


```python
X = df.to_numpy() 
```

## LATENT VARIABLES AND THE CURSE OF DIMENSIONALITY

Before getting into "latent variables," let's clear up a bit of terminology. In this chapter, and many that follow, we'll talk about the "dimensions" and "dimensionality" of dataframes and matrices. All of this talk of "dimensions" is really just about the number of variables we are using. If we have 10 variables, we have 10 dimensions; 147,002 variables, 147,002 dimensions. When we have a *lot* of variables, we often describe our dataset as "high-dimensional." 

High-dimensional datasets pose all sorts of problems for statistical and machine learning models that low-dimensional datasets do not. That's why we refer to this situation as the **curse of dimensionality** even if it might seem like an embarrassment of riches. Typically, we reduce the number of variables we are working with by manually selecting the variables of interest, or by performing some sort of **dimensionality reduction** on the dataset that mitigates problems associated with the curse of dimensionality. Below, you will learn about two different but related approaches to dimensionality reduction, one driven by theory and measurement, the other by data-driven induction.

### Theory First: Measuring Latent Variables with Exploratory Factor Analysis

We social scientists spend a huge amount of time trying to measure things that we can't directly observe. If you take a moment, you can probably list dozens of such things: political ideology, religiosity, well-being, job satisfaction, social capital, opportunity costs, social anxiety, confirmation bias, populism, introversion, personality types, emotional labour, resource mobilization, and so on. In earlier chapters of this book we spent a fair amount of time working with five variables that measure things we can't directly observe:

- a country-level measure of the principle of electoral democracy (polyarchy),
- a country-level measure of the principle of liberal democracy,
- a country-level measure of the principle of participatory democracy,
- a country-level measure of the principle of egalitarian democracy, and
- a country-level measure of the principle of deliberative democracy.

When I say that none of these five "principles" of democracy can be directly observed, I mean that quite literally: you can't look at a country and eyeball the amount of deliberative democracy you see, as if it had material form. That's because deliberative democracy is not a material thing in the world, it's an abstract concept developed by social scientists and political philosophers in the context of theoretical debate and applied research. Because these are unobservable abstract concepts, we call the variables that represent them **latent** or **hidden**.

When we develop theoretical concepts, we can't assume other researchers share our meanings, or that our meaning is somehow self-evident. Instead, we put in quite a lot of work to ensure conceptual clarity, as that is the only way to advance the state of our collective knowledge. These abstract concepts are *essential* in the quest to advance collective knowledge, but to treat them as if they were "real" material things, and not theoretical constructs, is a mistake called **reification**. 

Abstract concepts, like the principles of democracy above, are meaningful both in our everyday lives and in relation to broader theories and paradigms, but because they can't be directly observed, they are difficult to measure empirically. This requires us to adopt measurement strategies that combine careful reasoning and logic with measurement models that we carefully and transparently validate. The first step in this process is **specification**, which involves developing conceptual and operational definitions of concepts. Specification is essential because it helps ensure we are talking about the same thing; in social science, as in everyday life, you can't get very far just making up your own definitions of things, however good those definitions might be. 

A **conceptual definition** involves defining an abstract concept in relation to other concepts whose meaning is more widely shared, usually by breaking it down into more concrete aspects or dimensions. For example, my friend and colleague Igor Grossmann conducts fascinating psychological research on wisdom. What exactly, is "wisdom," and how does one study it scientifically? Even something as grand and abstract as "wisdom" can be specified in terms of concrete dimensions that, together, speak to the more general concept. For example, as @brienza2018wisdom propose, wisdom can be represented by the extent to which people demonstrate intellectual humility, recognition of uncertainty and change, consideration of the broader context at hand and perspectives of others, and the integration of these perspectives/compromise in specific situations. 

The various dimensions that are specified in the conceptual definition of wisdom are, of course, *other concepts*. The difference between an operational definition and a conceptual one is that an **operational definition** describes the specific operations that one would have to perform to generate empirical observations (i.e., data) for each of the various dimensions. The variables that contain data on these specific dimensions are typically called **indicator variables**. Operational definitions involve specifying things like the level at which to measure something, the type of variables to use (e.g. ordinal, interval, ratio, categorical), the range of variation those variables should have, and so on. A good operational definition of a concept enables one to measure the concept by measuring the concept's dimensions with a high degree of reliability and validity, and then aggregating the measures of specific dimensions into a measure of the abstract concept that *also* has high reliability and validity. In the case of measuring wisdom, for example, @brienza2018wisdom outline an explicit measurement strategy that attempts to mitigate social desirability biases (which inevitably come into play when you ask people about the wisdom of their reasoning) by assessing how people respond to specific scenarios. They provide an online supplement that includes the specific survey instruments used to collect the data according to the operational definitions laid out in their paper. 

Let's return to our abstract "principles of democracy." The principle of electoral democracy, for example, is represented by the five dimensions listed below. The set of accompanying questions come straight from the VDEM codebook [@coppedge2020v].

- **Freedom of expression**: "To what extent does government respect press and media freedom, the freedom of ordinary people to discuss political matters at home and in the public sphere, as well as the freedom of academic and cultural expression?" (variable: `v2x_freexp_altinf`)
- **Freedom of association**: "To what extent are parties, including opposition parties, allowed to form and to participate in elections, and to what extent are civil society organizations able to form and to operate freely?" (variable: `v2x_frassoc_thick`)
- **Share of adult citizens with suffrage**: "What share of adult citizens (as defined by statute) has the legal right to vote in national elections?" (variable: `v2x_suffr`)
- **Free and fair elections**: "To what extent are elections free and fair?" (variable: `v2xel_frefair`)
- **Officials are elected**: "Is the chief executive and legislature appointed through popular elections?" (variable: `v2x_elecoff`)

Each dimension is a bit more concrete than "electoral democracy," but for the most part, we still can't directly observe these dimensions. Perhaps you noticed that some contain multiple questions! The first dimension, for example, contains several questions about freedom of the press and media, freedom of ordinary people, and freedom of academic and cultural expression. In this case, each of the five dimensions that make up the higher-level measure of electoral democracy are called **indices**, which is a type of measure that is constructed by combining the values of lower-level indicator variables. 

For example, the *freedom of expression* dimension represented by the index variable `v2x_freexp_altinf` is constructed from the values of the variables government censorship effort (`v2mecenefm`), harassment of journalists (`v2meharjrn`), media self-censorship (`v2meslfcen`), freedom of discussion (`v2xcl_disc`), freedom of academic and cultural expression (`v2clacfree`), levels of media bias (`v2mebias`), how critical the media is (`v2mecrit`), and the diversity of perspectives promoted by the media (`v2merange`). These lower-level indicators are easier to observe than the higher level index variables above them, or the even higher still indices representing types of democracies. If you want to learn more about the conceptual and operational definitions of these principles of democracy, as well as the specific measurement models used, you can consult Coppedge et al. (2020 **CITE**). 

The difference between these indices and indicator variables maps directly back to the process of specification; the variables we use to record observations about the specific dimensions of concepts are indicator variables because they *indicate* part of the concept, and the overall concept is measures by combining the values for those indicators into an index. Indices are **composite measures** because they are created by systematically and transparently combining multiple indicators. 

When we want (or need) to measure something really big and abstract like a concept that is part of a larger theory (e.g. the amount of deliberative democracy that we see in a given country at some point in time), we break the big abstract concept down into various different dimensions, and sometimes we break *those* dimensions down into even smaller ones. The measures for the higher-level concepts are indices constructed by combining the values of lower-level indicator variables. 

This general idea is sketched out in @fig-09_01 below, with example indicator variables on the top feeding into mid-level index measures for latent concepts (in gray), which in turn feed in to the high-level index measures of the latent concept of the principle of electoral democracy, or "polyarchy" (also in gray). The `...`s are meant to emphasize that there are other indicators that feed into the mid-level indices in addition to those shown here.

![cap](figures/latent.png){#fig-09_01}

When trying to measure latent variables, degree of electoral democracy or freedom of association, we typically perform some sort of **factor analysis** that tells us whether the indicators we observed and measured (e.g. the share of adult citizens with voting rights and the power of the Head of State relative to the Head of Government) are indeed likely to reflect some underlying "**factor**."

A "**factor**" is simply a subset of highly correlated variables that have been combined into a single composite variable. If `v2x_freexp_altinf`, `v2x_frassoc_thick`, `v2x_suffr`, `v2xel_frefair`, and `v2x_elecoff` are all highly correlated with one another (and not strongly correlated with other variables), it might be because they are all indicating different dimensions of the same underlying latent construct: *electoral democracy*. The factor analysis lets us take a larger set of variables, of which some are highly correlated with one another, and reduce them to a smaller subset of explanatory factors. Depending on the type of factor analysis you conduct, those factors may or may not be correlated with one another, but usually they are not. 

When we conduct a factor analysis, we also compute **factor loadings** that clarify the relationship between each of the original variables in our analysis and the underlying factors extracted in the analysis. Variables that are strongly associated with the latent factor contribute more to that factor, and hence have higher loading. The specific factor loadings we can compute vary a bit depending on how we are approaching things. If we assume that the latent variables might be at least somewhat correlated with one another (which is a very reasonable assumption!), then we compute two sets of factor loadings, one being the Pearson correlation coefficients between the variables and the latent factors (a "structure matrix") and one being coefficients from a linear regression (a "pattern matrix"). 

If we assume that the latent variables are not correlated with one another (rarely a reasonable assumption, but it has its place), then there is only one set of factor loadings (either the correlation coefficients or the regression coefficients, which in this scenario would be the same). These loading scores are often "**rotated**" to help make them more substantively interpretable. Though we won't discuss them here, the type of rotation you perform depends on whether you think the factors are correlated with one another. If you suspect they are at least somewhat correlated with one another, then you would use an oblique rotation, and if you suspect they aren't, you whould choose an orthogonal rotation.

Factor loadings describe how specific *variables* (e.g., government intimidation of the opposition) contribute to a latent factor. **Factor scores**, on the other hand, tell us how specific *observations* (e.g., the United States of America in 2020) score on a given latent factor (e.g., electoral democracy). You can probably see where I'm going with this: if the latent factors represent meaningful variables that we want to measure but can't observe, then the factor scores that describe how an observation is related to that latent factor *is* the measurement of that observation for that latent factor. For example, on the egalitarian democracy measurement variable in 2019, Namibia scored 0.453, Egypt scored 0.118, France scored 0.773, North Korea scored 0.096, Vanuatu scored 0.566, Senegal scored 0.517, Canada scored 0.776, and Ukraine scored 0.316. Where did these numbers come from? The egalitarian democracy variable is a latent index variable constructed from several other indices, which are in turn constructed from more concrete low-level indicators. The latent variables and the individual country scores are mathematically constructed using factor analysis. 

In the interest of space, we will not actually conduct a theory-oriented factor analysis in this chapter. Instead, we will focus on a different approach that is more inductive and data-driven: Principal Components Analysis (PCA).

  
> **Further Reading**  
> 
> Chapter 13 from Barbara Tabachinick and Linda Fidell's [-@tabachnick2007using] *Using Multivariate Statistics* and Chapter 17 of @field2012discovering *Discovering Statistics Using R* both provide a good introduction to exploratory factor analysis and PCA as widely-practices in the social and cognitive sciences. Chapter 8 of G{\'e}ron's [-@geron2019hands] *Hands-on Machine Learning* and Chapter 3 of M{\"u}ller and Guido's [-@muller2016introduction] *Introduction to Machine Learning with Python* provide a good introduction to "dimensionality reduction" in machine learning.
>


## CONDUCTING A PRINCIPAL COMPONENT ANALYSIS IN SKLEARN

### Standardization

We did most of the necessary pre-processing at the start of the chapter when we imported our data, filtered the rows, selected the relevant columns, and then converted the data to a Numpy `ndarray`, which is a nice way of storing matrix data. There is, however, one very important piece of pre-processing that we need to do before we conduct a PCA: scaling our variables via z-score normalization, or "**standardization**."

Remember, PCA reduces the dimensionality of a dataset by constructing "principle components" from highly correlated features. If the variance contained in any one component differs from the variance contained in another *because of the scales for the features that contribute to it*, then PCA will make consequential mistakes. In short, PCA is *heavily* impacted by feature scaling. To prevent any such issues, we can use Sklearn's `StandardScaler()`, which performs z-score normalization on each feature. The z-score normalization ensures we are comparing things on the same scales.


```python
X = StandardScaler().fit_transform(X) 
```

Many statistical and machine learning models require standardization. If you need a refresher, you can consult the sub-section below. Otherwise you are free to skip over it. I recommend consulting the documentation whenever you use a model with which you are unfamiliar.

#### A Brief Refresher on Variance, Standard Deviation, and Z-score Normalization

First, let's *very* briefly revisit the concepts of variance and standard deviation. *Variance* is a statistical measure of how spread out or clustered the values in a data set are. More specifically, it's a measure of how far each value is from the mean. Variance is usually represented with the symbol $\sigma^2$. A larger variance means that the values are more spread out, while a smaller variance means that they are more clustered around the mean. Let's use some very simple examples to see how this works.


```python
ABCD = {
    'A': [1, 1, 1, 1, 1], # no variance...
    'B': [1, 2, 3, 4, 5], # some variance...
    'C': [-1, 1, 3, 5, 7], # a bit more variance...
    'D': [-10, -9, 3, 4, 4] # still more variance...
}

for k, v in ABCD.items():
    print(f'{k} has a variance of {np.round(np.var(v), 3)}.')
```


The *standard deviation* of a data set is the square root of the variance ($\sigma^2$), and is therefore represented with the symbol $\sigma$.


```python
for k, v in ABCD.items():
    print(f'{k} has a standard deviation of {np.round(np.std(v), 3)}.')
```

A z-score is a measure of how far an observation's value ($x$) is from the mean ($\mu$), standardized by dividing by the standard deviation ($\sigma$). Thus, an observation $x$ has a z-score: 

$$
z = \frac{x - \mu}{\sigma}
$$

While there are other ways of standardizing data, usually when we are standardizing our data, we are converting each observed value into a z-score. Below, we use the `zscore()` function from the stats module of a package called scipy. Note that the values in `A` all return `nan` because they have a standard deviation of 0, which means there is no variance.


```python
for k, v in ABCD.items():
    print(f'The values in {k} have the following Z-scores: {np.round(zscore(v), 3)}.')
```


### Back to PCA!

Now that our data has been standardized, we can conduct the PCA. When we initialize the model object with `PCA()`, we have the option of telling sklean to compute a specific number of components (e.g. pass the number 15 to get back the 15 principal components that account for the most variance) or a float specifying the amount of variance we want to be accounted for by the PCA (e.g. pass the number .9 to produce a solution that accounts for 90% of the variance). In this example, we will not specify either. 

Once we initialize the model object, we can use the `.fit_transform()` method on our standardized array $X$.


```python
pca = PCA()
pca_results = pca.fit_transform(X)
```

Let's take a look at the results! 

Below, we create a dataframe representation of the Numpy matrix returned by `pca.fit_transform(X)` because it's a bit easier to read. As you can see, each country is represented as a row and each principal component as a column. The cells indicate the association between each country and component pairing. These scores don't have any meaning to us just yet, but they will become more clear shortly. 


```python
res = pd.DataFrame(pca_results, index=countries)
res.columns=[f'PC {i}' for i in res.columns]

res['PC 0'].head()
```


Each of the 35 principal components we have constructed accounts for some amount of variance in the dataset. The components are ordered such that the first component accounts for the most variance, followed by the second, third, fourth, and so on. The amount of variance that each individual component accounts for is stored in the `pca` model object as an attribute (`explained_variance_ratio_`), which means we can access it using dot notation. Because we used the default parameters, rather than specifying the `n_components` parameter, the explained variance ratio scores will sum to 1, which means that *together* the principal components account for 100% of the variance in the data.


```python
evr = pca.explained_variance_ratio_
evr
```


The first value in the `evr` array above is roughly .6, which means that the first principle component contains roughly 60% of the variance in the dataset. You can interpret the rest of the numbers the same way: the second component contains roughly 9% of the variance, the third roughly 4% of the variance, and so on. In this particular example, a quick glance at this array alone suggests that the first component accounts for substantially more variance than any of the others.


```python
print(f'The sum of the array is: {np.round(np.sum(evr), 2)}')
```

Usually, we want to see how much *cumulative* variance is accounted for by some subset of principal components, starting with the first component. In other words, how much variance is accounted for by each component *and those before it*. The cumulative variance of the first three components, for example, is:


```python
np.sum(evr[:3]) 
```

Knowing how the explained cumulative variance changes with each additional principal component is useful because we typically want to work with some subset of the components rather than the entire set of variables. That is, after all, generally the point of using a data-driven dimensionality reduction method like PCA. If you are going to work with a subset, you should know how much information you kept and how much you threw away.

Let's create a Series containing information on the **cumulative explained variance** for the components in our PCA. We can do this by passing the array of explained variance ratios (`evr`) to Numpy's `cumsum()` function, which is short for cumulative sum. The Series tells us how much variance is accounted for by each component and those preceding it (remember, the index starts with 0, so the 0th element of the series represents the first component).


```python
cve = pd.Series(np.cumsum(evr))
cve[:12]
```

In this case, a simple preview of the cumulative explained variance tells us that the first two components alone account for 68% of the variance in the dataset, which is a *very* substantial amount. Similarly, we can see that the first 12 components still account for 90% of the variance -- pretty good considering we started with 35 indicator variables! 

This is only part of the picture, though. Let's plot the proportion of cumulative explained variance for each successive principal component. Notice that, by default, PCA will construct a number of components equal to the number of original variables in the dataset. You should be able to see the diminishing returns, here, even if they set in rather smoothly. The code below produces @fig-08_01.


```python
fig, ax = plt.subplots()
sns.lineplot(x=cve.index, y=cve)
plt.scatter(x=cve.index, y=cve)
ax.set(xlabel='Principal component ID',
       ylabel='Proportion of explained variance (cumulative)')
ax.set(ylim=(0, 1.1))
sns.despine()
plt.savefig('figures/08-01.png', dpi=300)
```

![png](figures/08-01.png){#fig-08_01}
    

### Matrix Decomposition: Eigenvalues, Eigenvectors, and Extracting Components

When you conduct a principal component analysis, you only want to keep some components. You're trying to *reduce dimensionality* while preserving as much information (in the form of variance) possible. So, which components do you extract? 

The above plot of cumulative variance accounted for can be helpful if you want to preserve a certain amount of variance in the data. An alternative approach is to construct a **scree plot** to determine which components are substantively important enough to use. To understand how to construct and interpret a scree plot, you need to know a little bit more about how PCA works, and more specifically what role eigenvalues and eigenvectors play in a PCA, and how those roles differ from loadings in factor analysis.

As discussed earlier, PCA creates a covariance matrix of standardized variables (or sometimes a correlation matrix). We can understand the structure and other properties of these matrices mathematically using **eigen-decomposition**. We won't get into the linear algebra here, but here's the basic idea: We can decompose the matrix into two parts. The first are the principal components themselves, which are *directions* of axes where there is the most variance (**eigenvectors**). The second part is the *amount* of variance that is accounted for by the principal components (**eigenvalues**). Every eigenvector has an eigenvalue and there is an eigenvector (principal component) for every variable in the original data. A very important feature of PCA is that the first principal component accounts for the greatest possible variance in the data set. The second principal component is calculated in the same way as the first, except that it cannot be correlated with the first. This continues for each principal component until you have as many as you do variables. It is important to remember that all of the information about scale (the amount of variance explained) is contained in the eigenvalues. The eigenvectors in a PCA do not tell us anything about the magnitude of explained variance.

Scree plots graph the eigenvalues for each component in the PCA, which you now know represents the amount of variance that each component accounts for. The higher the eigenvalue, *the more important the component*. Because eigenvalues represent the amount of explained variance, Sklearn helpfully stores them in the `explained_variance_` attribute of the PCA model object.^[If you are looking for the eigenvectors, Sklearn stores them in the `.components_` attribute.] This also makes it very straightforward to create a scree plot. The code below produces @fig-08_03.


```python
eigenvalues = pd.Series(pca.explained_variance_)

fig, ax = plt.subplots()
sns.lineplot(x=eigenvalues.index, y=eigenvalues)
plt.scatter(x=eigenvalues.index, y=eigenvalues)
ax.set(xlabel='Principal component ID', ylabel='Eigenvalue')
sns.despine()
plt.savefig('figures/08-03.png', dpi=300)
```
    
![png](figures/08-03.png){#fig-08_03}
    
This figure should be straightforward to understand. The first few components are more important than the others. In a scree plot, you are usually looking for an **inflection point**, i.e., a point where the slope of the line changes rather abruptly. Usually, that point is clear, but we can also inspect the eigenvalues themselves if we want a bit more precision, just remember that the eigenvalues are 0-indexed, so 0 represents the first component, 1 represents the second component, and so on.

```python
eigenvalues.head(10)
```

We might use the fact that dropping from 3.14 to 1.48 (a decrease of more than 50%) is significantly greater than the drop from 1.48 to 1.12, and from 1.12 to 1.01, to identify an inflection point at 1.48. The general rule is that you extract the components to the left of the inflection point, excluding the component at the inflection point itself. However, there are debates about whether it is best to keep all components with eigenvalues higher than some threshold, such as 1, the idea being that this is still quite a lot of variation *even if less than the other components*. In this example, cutting at the inflection point would be the third component, which means we would extract the first two. On the other hand, if we go with a threshold of 1, then we would take the first 5. 

When different rules suggest different courses of action, the best solution is the one most aligned with your goals. One reason why researchers perform PCA is because they want to do some sort of regression analysis but have a *bad* multi-collinearity problem. In that case, keep lots of components! It is better to keep information than throw it away unless you *really* need to throw some away. If, conversely, you are trying to visualize a high-dimensional dataset by collapsing it down to two significant dimensions, then you should only extract those two components *provided they contain a lot of variance*.  

Here, we will extract the first two because they preserve a *lot* of variance, and because the next thing I want to do is create a simple visualization of where the countries in our analysis are positioned in terms of these latent dimensions, and creating informative visualizations in three or more dimensions is a fool's errand.


```python
component_1 = pca_results[:, 0]
component_2 = pca_results[:, 1]

PC12 = pd.DataFrame(zip(component_1, component_2), columns=['PC1', 'PC2'])
```

We can now easily visualize how the countries in our dataset are positioned in relation to these two principal components. Let's grab the country names from our metadata variables to use in the visualization, which will be a simple density plot with country names indicating where each country is given these two components.


```python
PC12['Country'] = countries
```


```python
ax = sns.kdeplot(data=PC12, x='PC1', y='PC2', alpha=.8, fill=True)
for i, country in enumerate(PC12['Country']):
    ax.text(PC12['PC1'][i],
            PC12['PC2'][i],
            country,
            horizontalalignment='left',
            size=3,
            color='black',
            weight='normal')
ax.set(xticklabels=[], yticklabels=[])
ax.set(
    xlabel=
    f'$\longleftarrow$ PC1 (eigenvalue: {np.round(eigenvalues.loc[0], 2)}) $\longrightarrow$',
    ylabel=
    f'$\longleftarrow$ PC2 (eigenvalue: {np.round(eigenvalues.loc[1], 2)}) $\longrightarrow$'
)

plt.savefig('figures/08-04.png', dpi=300)
```

![png](figures/08-04.png){#fig-08_04}
    
While the text is dense in @fig-08_04 (a high resolution version is available in the online supplement), careful inspection should lead to noticing several patterns. The first principal component is defined by the opposition between countries like Norway, Denmark, Switzerland, Luxembourg, and Germany on the one hand, and by Burundi, Tukmenistan, Syria, Eritrea, and North Korea on the other. The second principal component is defined by the opposition of countries like Haiti, Dominican Republic, Nigeria, Gabon, and Honduras on the one hand, and Laos, Eritrea, United Arab Emirates, China, and Singapore on the other. The eigenvalue is *much* higher for the first principal component, suggesting that the interpretation of the differences between countries on the left and the right of the graph is most important. 

This is not a factor analysis. We have not guided the PCA towards this solution. Instead, we have obtained these two latent dimensions *mathematically*, through matrix decomposition, and projected the countries onto that latent space. These two dimensions only represent 68% of the variance in the dataset, but when you think about it, that's a lot of information for just two variables. The challenge, given that this is computationally *inductive*, is to do the qualitative and historical work necessary to interpret this representation of the latent structure in the data. However, don't forget that the only information the PCA has to work with comes from our original variables, so those variables are a great place to start. 

## CONCLUSION

---

## Key Points 

- We learned about latent variables and the differences between theory-driven and data-driven dimensionality reduction
- Discussed distinctions between Factor Analysis and Principal Component Analysis 
- Conducted a PCA

---