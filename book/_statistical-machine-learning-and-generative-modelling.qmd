# Statistical Machine Learning and Generative Models 

## LEARNING OBJECTIVES

- Compare statistical and machine learning models in terms of their underlying goals, e.g., prediction versus inference
- Explain why it can be helpful to use both statistical and machine learning models in a research project
- Compare the Frequentist and Bayesian paradigms given differences in their interpretations of the meaning of probability
- Compare discriminative and generative models

## LEARNING MATERIALS

You can find the online learning materials for this chapter in `doing_computational_social_science/Chapter_25`. `cd` into the directory and launch your Jupyter Server.

## INTRODUCTION

Our discussion of machine learning has focused on a selection of models that are central to the symbolic and connectionist paradigms, specifically linear and logistic regression models and tree-based methods such as Decision Trees, Random Forests, and Gradient Boosted Machines, and finally connectionist neural network models. These models are mathematically inductive but can be put to good use in a wide-variety of research designs, regardless of whether your goals are primarily inductive or deductive, theory-building or theory-testing, basic or applied. 

This is another high-level chapter that transitions us into new set of methods and models: probabilistic programming and generative modelling with a framework of computational Bayesian statistics and machine learning. If you don't know what any of that means, you will soon. What's coming is going to be *really* fun! The goal for this chapter is to deepen your understanding of the relationship between (*i*) statistics and machine learning in general, and (*ii*) probabilistic machine learning and generative modelling specifically. If I do a good job of those two things, you'll understand there are some differences between the two fields, but these differences are becoming less salient as both evolve over time. 

In the next chapter we will introduce some essential knowledge of probability theory. Together, these two chapters are the foundation upon which a series of chapters on generative modelling with structured, network / relational, and text data build. 

## STATISTICS, MACHINE LEARNING, AND STATISTICAL MACHINE LEARNING: WHERE ARE THE BOUNDARIES AND WHAT DO THEY BIND?

Historically, statistics and machine learning have been fairly distinct fields, with computer scientists being the main developers of machine learning methods and, well, statisticians being the main developers of statistical methods. For some time the computer scientists were skeptical of statistical methodologies and statisiticans were skeptical of machine learning methodologies, in part because they were working towards different types of goals. For the most part, computer scientists were primarily interested in developing ML models that excelled at prediction tasks on large datasets, whereas statisticians were primarly interested in developing interpretable models with the goal of facilitating *inference* under uncertainty. While this distinction is still true *in general* (most statistians are not doing machine learning, for example) the boundaries between the fields have become much more porous over time. Many of those changes have been ongoing since the early 1990s. 

To help make sense of all of this, it's useful to distinguish between 

1. specific methods and models
2. the ends those methods and models serve, chiefly **prediction** and **inference**

For a long time now, statistics and machine learning have made use of a lot of common methods and models. As you now know, linear and logistic regression models, Principal Component Analysis (PCA), Factor Analysis (FA), data clustering methods like $k$-means, are all widely used in *both* statistics and machine learning. There are many other examples, some of which are featured heavily in chapters to come, but the point is this: trying to accurately discern the boundaries between machine learning and applied statistics based on specific methods and models that "belong" to each is pointless. I suggest you think instead about what a method or model is being *used for* in any given situation. This is a better way of making sense of the many models you will learn, as the same techniques and models might be used in both ML and applied statistics, but to very different ends. It is very important to understand this if you are to make good modelling choices. 

I have stressed before, and will continue to strees: when we sit down to analyze data we do so with a goal in mind. Think back to the start of Box's loop. We have a question we want to answer. We build a model that we think can help provide that answer, use it to analyze data, critique its performance, and revise the model to account for those critiques. Perhaps our goal is to simply describe the data using simple summary statistics and data visualization. Good description is *essential* in science, but we rarely stop there. 

Instead, visualization is often a part of a larger modelling project, which will often have one of two goals: **prediction** and **inference**. When focused on prediction, we want to use a model to approximate a process -- either unobserved or observed -- with the goal of accurately anticipating the outcome of an observed event. We may do this in a way that sacrifices an understanding of the process involved in the interest of maximizing the model's ability to predict accurately and reliably. As a result, models optimized for prediction may often make use of highly complex mathematical functions, or may include a staggering number of parameters that are not well-suited to the kind of interpretation and inference that we are generally interested in as researchers. 

Inference, on the other hand, is focused on *understanding*, *interpreting*, and *explaining* the process. To do so, we develop models of the unobserved processes that we think might underly the *generation* of observed data; *how did this data come to be?*. Some models (usually the simpler ones) are well-suited for both inference and prediction, but more complex models tend to provide one whilst eschewing the other.

If you are trying to understand whether and how variables such as age or gender identity relate to some other variable of interest, such as preferred music genres, then your goal is inference. If, on the other hand, you are trying to use information about age, gender identity, and other variables to make many accurate guesses about whether someone will like a given music genre, then your goal is prediction. Clearly, the two are related; if you manage to construct a model that perfectly infers the relationship between some variables for a given population, you could use that model to make predictions about previously-unobserved individuals within that population. This is an idea we will return to frequently in coming chapters on generative modelling. 

Traditional statistical models are *typically* considered to produce interpretable inferences: how confident can we be that an observed relationship in the data is not random noise, but indicative of something deeper? Machine learning, on the other hand, *tends* to be used to find patterns in observed data that can be used to make predictions about data that has not been observed. Although we usually think of *the future* when we think of prediction, in practice, predicting "the future" is usually just making predictions about data we haven't seen before; "the future" is just out-of-sample data, and "predicting the future" is just using a model to make predictions about the values that out-of-sample data takes for a variable we are interested in. Unsurprisingly, given the history of the two fields of study, traditional statistics lend themselves better to data sets with a (relatively) smaller number of observations and variables. Machine learning, on the other hand, has developed rapidly out of a desire to make use of the huge quantities of data readily available in the digital age. 

Statistics is built on a foundation of using theory to inform decisions about constructing models to assess hypotheses. We might construct a statistical model to test hypotheses about the effects of education on political identity while controlling for the usual suspects like class, occupation / profession, age, ethnicity, and so on. Statistics has generally assumed a **deductive** orientation. We know, *based on theory* and other empirical research, what might be important to consider when we investigate the relationships between *specific* variables. 

If we were taking a machine learning approach, we *might* make considerably less use of theory and focus instead on uncovering potentially interesting patterns in the data itself. We could, for example, pass our entire dataset through an algorithm to determine which combination of variables best predicts the outcome we are interested in. Machine learning differs from most applied statistical models in that it is mathematically **inductive**. Most machine learning is not about using theory or conducting more deductive or explanatory research, but there are almost always ways to use machine learning models in ways that are informed by theory, regardless of exactly how deductive you want to be. 

*I'll give away the plot here and tell you that most of what you will learn in the coming chapters obliterates these distinctions and thrives at the intersection of statistics, machine learning, and applied data analysis*.

I have been careful to stress that these descriptions are typical or tendencies. While true *in general*, these two approaches are not at all mutually exclusive of each other, and it can be useful to make use of both. Most of the best contemporary work blends them in exciting ways! 

One of the major moments in the evolving relationship between machine learning and statistics happened in 2001, when Leo @breiman2001statistical published a now classic treatise proposing that academic statisticians make space in their work for algorithmic (e.g., machine learning) models. Nowadays, it's difficult to imagine that such a plea was necessary, but at the time, algorithmic models were much more prevalent outside of academic statistics.

The original paper is worth reading, but in short, Breiman points out that some algorithmic models, such as Random Forests, are more accurate predictors than classical statistical models. (Don't forget this was 2001, when the state-of-the-art in machine learning and statistics was very different than they are now). Breiman also argued that Decision Trees are just as interpretable as statistical models but still good predictors, even if less accurate than Random Forests. Finally, he showed that there is often more information available about the relationship between variables than just the reason they're related. For example, algorithmic models can give a more accurate sense of which variables are most *important* and *under which conditions*. As always, decisions about the type of information to look for, how to find it, and how to interpret it are all research design decisions that can be made well or poorly. 

Historically, the differing goals of statistics and machine learning have influenced their development. Many classic machine learning models are often optimized for prediction to such an extent that humans are incapable of accurately interpreting and drawing inferences from them. As a result, most supervised machine learning is useful to social scientists *when prediction is useful to social scientists*. Since "prediction" includes making predictions about unseen data, we can make plenty of good use of these models without relying on them for making statistical inferences (though that is possible too). 

Many machine learning techniques use complex and iterative techniques to analyze data, and the results they produce may be similarly complex and hard to understand. Even when we constrain the models to prevent them from becoming too complex, understanding *why* they produce specific results can still be beyond us. Yet, with experience and expertise, we can use them to learn things that are very difficult to learn from classical statistical models, and we can learn even more by using *both* statistical and machine learning models, or by venturing into the exciting hybrid world of *Bayesian data analysis, probabilistic programming, probabilistic machine learning, and generative modelling*. There are lots of different names for what happens in this fascinating hybrid world, and it's evolving at a breakneck pace. The community is moving increasingly towards the label of "probabilistic programming," so I'll use that term often, but othertimes I'll switch to talk about "Bayesian data analysis" and "generative modelling" or "probabilistic modelling" because it makes more sense in the specific context. In general, I use these terms interchangeably. 

Before we get into the details of developing various kinds of probabilistic models, we need to take a step back and talk about something much more fundamental: the meaning of probability itself. 

### WHAT IS PROBABILITY? TWO INTERPRETATIONS, TWO PARADIGMS

What *exactly* is probability? In a basic (and wholly inadequate) sense, it refers to the chance that an event occurs. While the mathematical foundations of probability theory are the same no matter who you ask, the interpretation of what probability *means* and where it applies can vary from statistician to statistician. In this part of the chapter, we will compare the two most influential philosophical perspectives on probability, the classical or "Frequentist" paradigm and the Bayesian paradigm.

If you're a social science student, I'd wager that your quantitative methods classes have been wholly Frequentist. While Frequentist approaches certainly have their merits, everything that follows is Bayesian. While Bayesian methods certainly have their limitations, I'm going to be promoting them with enthusiasm. I hope that, like me, you find the Bayesian approach liberating, powerful, intuitive, humbling, and beautiful, but it's fine if you don't. Understanding Bayesian approaches can also help you become a better Frequentist. 

On that note, a final disclaimer. In much of what follows I'll be contrasting the philosophical foundations of Frequentist and Bayesian thinking. In doing so, I'll be contrasting a "strict" Frequentist view with a Bayesian view. This "strict" view is not necessarily one that most Frequentists think much about in day-to-day practice, and when presented with it in philosophical form may find that they do not agree with it. Regardless of whether any individual Frequentist agrees with the strict philosophical interpretation or thinks about it very much at all, Frequentist statistical methodology, including **null hypothesis significance testing (NHST)** is wholly premised on it [@szucs2017null; @mcshane2019abandon]. Bayesian thinking offers a good alternative to Frequentist statistical methodology that is better-suited to certain types of problems, which happens to include just about anything of interest in the social and biomedical sciences, and likely many other fields as well. For that reason, you will find absolutely no NHST in what follows. What you'll find are Bayesian alternatives. Finally, in laying out these philosophical differences, I have emphasized their strict interpretations. This is mainly for pedagogical reasons, as I assume that you like most social science students you've probably had limited exposure to multiple statistical paradigms. It's easier to learn the differences by focusing on the big picture stuff first. Overtime you'll start to notice more of the philosophical complexities and will be capable of reasoning in the gray zone between the statistical paradigms. 

With that said, let's compare interpretations of the meaning of probability!

#### The Frequentist Paradigm and the "Rigid View" of Probability

Although this may be your first time encountering the term, the vast majority of statistical analyses are currently conducted within the "Frequentist" paradigm. Anytime you've witnessed someone evoke 'statistical significance', 'confidence intervals', 'p-values', or those little stars that show up next to regression tables, that person is almost certainly using Frequentist techniques and concepts. 

For Frequentists, probability is understood to be the long-run relative frequency of an event across repeated trials. As these trials are repeated over and over, the frequency of the event in question (relative to other possible outcomes) will trend towards to the 'true value'. Imagine performing an experiment, such as flipping a 'fair' coin, many times over. Even though the ratio of heads to tails would differ in small ways from the expected value of 1:1 (or 50% heads, 50% tails), its long run value would approach the 'true' value of the coin. 

At risk of grotesque oversimplification, the Frequentist paradigm holds that *all phenomena that can be repeatedly sampled from under nearly ideal and identical conditions* will produce certain results with a certain frequency. This is the concept that motivates Frequentists' interpretation of probability, and probability is meaningless without it. As such, a strict Frequentist would tell you that the concept of probability *can only be applied in two contexts*:

1. Experiments that can be performed repeatedly in 'nearly ideal' conditions, and whose outcomes across trials vary despite (nearly) identical conditions for each trial.
2. Populations of sufficient size such that a *practically* limitless number of samples can be randomly drawn therefrom. 

That's it. From a strict Frequentist perspective, the concept of probability cannot be applied under any conditions other than these. As a result, I will refer to the Frequentist interpretation of probability as "rigid." 

Despite these self-imposed strictures, or perhaps because of them, the Frequentist paradigm has proven itself capable of covering a whole lot of ground. Almost any experiment from fields like Physics or Chemistry could be considered repeatable (after all, un-repeatable experiments are generally of little scientific value), and is thus fertile ground for Frequentist analysis. Much of population science (i.e., demography) also lends itself reasonably well to the Frequentist paradigm as it would be practically impossible to completely exhaust the ability to randomly sample from a population of even modest size. 

The strict Frequentist approach isn't such a good fit for some types of scientific inquiry. For example: what happens when a Frequentist wants to develop a model to predict the outcome of a specific election that hasn't happened yet? A *strict* Frequentist would tell you that it can't be done. A presidential election between two candidates in a given year, for example, is a one-off event, and cannot be sampled from or repeated under experimental conditions. As such, it would be improper to apply probability to the outcome. 

Fortunately, the vast majority of the Frequentists alive today are not nearly so rigid in their interpretation of probability as the statisticians who invented the most widely-used Frequentist techniques. They would generally be willing to say that it's possible to think of a presidential election as driven by a dizzyingly large set of plausible end states (in the form of electoral college results, district-level voting trends, or even individual voters' decisions), and that there's no good reason why we can't think of all these possibilities as stemming from an nearly infinite set of hypothetical elections that we can sample from in search of the outcome we care about (in this case, who wins the election). 

A willingness to bend the rules can help, but can't always rescue Frequentism from contexts where its mathematical underpinnings really struggle. One such scenario would be a situation where data does not vary from sample to sample, and yet it is clear that the data does not present a realistic view of the underlying construct. Cases such as these generally require analysts to further relax their criteria about what is, and is not, a valid target for probabilistic description. Speaking of which...

#### The Bayesian Paradigm and the "Flexible View" of Probability

If you're interested in computational social science or data science (if you are reading this book, I hope you are!), you've likely encountered the term 'Bayesian' before, and may even have a vague sense of what it means. I'm going to proceed on the assumption that you know next to nothing about it. 

The Bayesian paradigm is premised on the notion that probability can and should be used to describe states of knowledge. This might sound like a fairly mundane and uncontroversial idea, but the consequences that stem from it are substantial. For example, one implication of this view is that data themselves are not inherently stochastic (as the Frequentists would have it), but rather that *our perception of and knowledge about a phenomenon is always imprecise, faulty, and incomplete*. As such, Bayesians use 'randomness' to describe the 'uncertainty' inherent to our understanding of a process, not of the process itself. See the difference?

Consider the simple coin flip yet again. For a Frequentist, a coin flip is a prototypical example of a simple random process, and you can determine the probability of heads by repeatedly flipping a coin and tallying the outcomes. A Bayesian, on the other hand, might argue that coin flips are not *really* random processes at all, they just seem random to us. Whether the coin lands on heads or tails is actually determined by a *huge* number of deterministic physical factors that are beyond our ability to observe and reason about, such as the coin's initial orientation, initial distance from the surface it will land on, the exact location and magnitude of the force imparted on it, ambient air pressure, wind, altitude, and so on. If we had endless time, energy, and resources, we could probably create a deterministic description of coin flip mechanics, capable of making perfect predictions provided we have perfect and complete knowledge of all the relevant factors.

This is a really important point, so here it is again, put a slightly different way: from a Bayesian point of view, the problem is not that the things we are trying to understand are inherently uncertain and random. If it were possible to know everything about every relevant factor completely and perfectly, we would find that things that seem random are not random at all. They simply *seem* random because we have imperfect and incomplete knowledge. It is our selective perception, cognitive biases, flawed beliefs, limited knowledge, and so on that are the sources of uncertainty. Therefore, we should approach the task of inference with these limitations in mind; when we iteratively design, develop, and improve models, we should do so while accounting for uncertainty in our knowledge rather than approaching inference on the assumption that 'randomness' is inherent in the things we want to understand. 

You could say that the Bayesian *paradigm* is characterized by a certain amount of intellectual humility and appreciation for the staggering complexity of the world. All knowledge is provisional, incomplete, and imperfect. The rich intellectual world that has developed around Bayesian statistics and machine learning, and which continues to evolve, is explicitly organized around doing research in these ways, and places a great emphasis on combining information from differences sources, embracing uncertainty and heterogenity, and integrating theory and data in iteratively-developed bespoke statistical models that are well-matched for different problems. OK -- enough with the unhinged admiration for Bayesian statistics. Let's move on. 

If perfectly complete and objective knowledge about the world is not possible, then the Bayesian notion of using probabilities to quantify the degree of certainty for some state of knowledge becomes *extremely* useful and powerful. Bayesians work within an intellectual framework where differing states of knowledge (or hypotheses, in a scientific setting) can be rigorously evaluated and compared in light of observed empirical data. Unlike (strict) Frequentist inference, where new evidence cannot be incorporated on the fly, Bayesian contributions to knowledge are *always open to revision given new evidence*.

By this point, you might already be thinking something along the lines of *Wait. a. second. Are you telling me that as a result of the Bayesian paradigm using probability to describe beliefs and states of knowledge, you can assign a probability to anything?"* Well, no. But yes, sort of.

Bayesians think that questions of knowledge and "belief" are the only domains in which one may legitimately apply probability theory, which means that technically one cannot assign a probability to just anything. But since literally *everything* we humans know and reason about is by definition a form of knowledge or belief, you would be hard pressed to find something that could not be assigned some sort of probability within the Bayesian paradigm. Anything a human brain can perceive and reason about falls under the purview of Bayesian probability. That's a pretty broad scope, and it's why I have opted to describe the Bayesian paradigm as having a "Flexible View" of probability. 

Bayesian analysis has historically been the underdog in academic statistics but has thrived in applied settings, including government and industry. Sharon @mcgrayne2011theory provides an interesting historical account of how Bayesian analysis has thrived in applied settings in *The Theory That Would Not Die: How Bayes' Rule Cracked the Enigma Code, Hunted Down Russian Submarines, and Emerged Triumphant from Two Centuries of Controversy*, if you are interested. There is no question that Bayesian statistics is now well within the mainstream of academic statistics and has long been used in fields like sociology, political science, and public policy [e.g., for sociology, see @western1999bayesian; @lynch2019bayesian], but in 2021 it is still the case that most researchers are trained in, and continue to work within, the Frequentist paradigm. 

Why? It's a bit odd when considered in light of the fact that the Bayesian paradigm has proven capable of deftly tackling the most difficult of statistical problems in an intuitive fashion.  While there are ongoing debates about the philosophical underpinnings of the Bayesian paradigm [e.g., @gelman2013not; @mayo2013discussion; @johnson2013comment; @gelman2013rejoinder], most commentators would argue that Bayesian approaches tend to perform well in cases where Frequentist approaches typically stumble, and in other cases they tend to produce similar types of estimates as do Frequentist approaches, only with more information about the uncertainty involved in those estimates. What gives?

One explanation is that some of the most powerful founding figures of modern statistics, such as Ronald Fisher, were diehard Frequentists who sought to sink Bayesian statistics with intense fervor, some of which was intellectual and some of which was nasty personal politics and messing with people's careers. McGrarayne's book on the history of Bayesian methods is in part the story of brilliant mathematicians, computer scientists, and statisticians working using Bayesian reasoning in secret because of the hegemonic influence of the towering statistical giants and gatekeepers of the time. 

But this is certainly not the only reason. The other, which is much less political, is the computational complexity inherent in the Bayesian paradigm. We won't spend very much time on these specifics as the problem of computational complexity in Bayesian analysis is largely obsolete, but the oversimplified answer is this: in many cases, *analytical* Bayesian inference requires the use of some truly intimidating integral calculus that sometimes fails to produce a closed-form solution even with a dizzying array of clever mathematical tricks. In bygone eras where computers were rare, slow, and expensive (or nonexistent, as they were the days of Reverend Thomas Bayes himself and the two great mathematicians Richard Price and Pierre Simon LaPlace, both of whom are largely responsible for what we now call Bayes' theorem, discussed at length in the next chapter), Bayesian inference was dually limited: it was, at best, the sole domain of inspired mathematicians, and there were some forms of inference it simply couldn't tackle. Fortunately, this is no longer the case, and the Bayesian paradigm is undergoing a renaissance. The renaissance is inherently computational.

#### Everyone's Wrong, But Sometimes We're Wrong In Useful Ways

Time for a controversial claim: the Bayesian interpretation of probability comes relatively naturally to us and the Frequentist view does not. When we evoke probability to describe something (e.g. "I think there's a 70% chance the professor is going to be late to class today," or "I think there's a roughly 55% chance Biden takes Pennsylvania in the 2020 American presidential election"), we're applying probabilities to "beliefs" or knowledge. In that sense, many of us are "natural" Bayesians. 

This is unfortunate for many reasons, one being that Bayesian analyses are generally more intuitive than Frequentist analyses, and Bayesian techniques expose (rather than obscure) more of the assumptions, decision making, and analytic procedures involved in modelling than do Frequentist techniques. More generally, Frequentists view "subjectivity" and ironically any kind of human influence on inference as an inconvenience at best, and a fatal scientific flaw at worst. Bayesians would counter that science, statistics, and inference *are all human endeavours*. To pretend otherwise is folly. Rather than strive in vain to eliminate any form of situated knowledge, Bayesians prefer to put it all out in the open: make your knowledge visible; strive to uncover your biases and assumptions; and make them all explicit, visible, and open to critique in your models. If there is uncertainty in our knowledge, *make those uncertainties explicit*.

To be clear, there are very few truly doctrinaire Frequentists alive today, and similarly there are few dyed-in-the-wool Bayesians. In their quest for intelligible insights, most contemporary statisticians, data scientists, and computational scientists will habitually draw from both the Bayesian and Frequentist paradigms. Truly competent statisticians and researchers are comfortable with both, and are intimately familiar with their relative merits, shortcomings, and idiosyncracies. I won't bother trying to hide my strong preference for the Bayesian view in what follows (really, there would be no point), but that's not intended to be an indictment of the Frequentist paradigm, although it is, perhaps, an indictment of uncritically-held institutional preferences for Frequentist approaches. Different statististical philosophies are useful in different intellectual and applied contexts.

Now that we've contrasted these two competing interpretations of probability and the statistical paradigms those interpretations are embedded in, let's turn to one final high-level distinction before we get into the guts of probability itself: generative versus discriminative modelling. 

## GENERATIVE VS. DISCRIMINATIVE MODELS

The famed physicist Richard Feynman once remarked "what I cannot create, I do not understand." This is something of an informal motto for generative modellers across many disciplines. What does it mean, why does it matter, and what is a generative model? 

The supervised learning models we have discussed in this book are all **discriminative**: they learn mathematical functions that use labeled data to map some input to an output. Put another way, we develop "discriminative models" when we are trying to solve regression and classification problems (by which I mean continuous or categorical outcomes) in the context of *supervised learning*. For example, we can train a discriminative model to classify a speech according to the political party of the politician who gave it as a function of the words used in the speech. To do this, we would provide the model with the feature matrix $X$ containing data on word usage, and we would provide the labelled vector $y$ indicating the party of the speaker. We would then train a supervised learning model to *discriminate* between political parties based on the content of the speeches. So linear and logistic regressions, $k$-nearest neighbors, Support Vector Machines, decision trees, Random Forests, and Gradient Boosted Machines are all examples of discriminative models. Provided the starting conditions don't change, discriminative models are *usually* deterministic, in that they calculate some sort of value based on an algorithmic process. If neither the inputs nor the initial conditions change between runs, then the value doesn't either -- no matter how many times you run it. 

We use discriminative models when we face discriminative problems, and supervised learning is all about discriminative problems. *Is this a speech from a Conservative or a Labour MP?* When provided with enough data, discriminative models can uncover hidden mathematical regularities that enable many different types of predictions, and because we always have the labelled values $y$ we can compute a wide variety of evaluation measures to assess the quality of those predictions; the evaluation metrics we discussed in Chapter 24 are all examples. Much of the hype about "superhuman performance" in machine learning comes from the dramatic improvements in supervised learning on these measures, none moreso than accuracy. 

In sum, discriminative models are, ultimately, just models used for prediction problems in a supervised learning framework. Consequently, some people use the two terms interchangeably, but I find this creates some confusion. Supervised learning is more than just the specific models; it's a larger process and framework that has evolved to develop high-quality discriminative models. Supervised learning is the goal, the general framework, and the process; discriminative models are the actual learning algorithms at the core of that learning process. We train the discriminative models with labeled data. The model learns how to map $X$ to $y$ such that it can make discriminative predictions about new observations (out of sample data). The machine learns from the training data *iteratively*, by correcting it's prediction mistakes.

**Generative models** are completely different. Rather then attempting to learn how to discriminate between observations given a learned mathematical function that maps $X$ to $y$, generative models are concerned with the underlying process that resulted in us seeing the data we are seeing. There are different kinds of generative models. The contagion models we developed in Chapters 17 and 18 are generative models that are anchored not only in theoretical ideas developed within network analysis, but in complex systems theory more generally and agent-based modelling [**CITES**]. Some generative models attempt to learn processes from data. With others, we develop generative models by thinking through generative mechanisms ourselves, and then evaluate the quality of the resulting models.

One way we know we have a good generative model is when it can generate data that looks a lot like the data we observe from the real world. For example, if we train models to generate images of seascapes, mountain ranges, and so on, the best models would produce realistic looking images, and the bad ones will produce fake images (or complete nonsense). We could also train a model to generate sentences; the best ones could plausibly be strung together by a human, whereas bad ones would contain obvious mistakes and things that don't make sense.  Consider models that learn generative processes, as opposed to models where we encode the generative mechanisms ourselves. If the model can generate new speeches or new images of seascapes and mountain ranges and so on that *sound and look real*, then the model has learned the underlying rules that govern what makes a speech sound authentic and an image look authentic; we know it knows those rules because it can use them to create realistic new speeches and images that have never existed in the world. 

Generative models, then, are about modelling the process that generated the data, and a *good* generative model is one that can successfully generate new data that are as close as possible to the original data. When we have successfully developed a model such as this, we can learn a lot *from the model*. 

Generative models have long been popular in computational social science, from the early roots of agent-based models [**cites**], to the current state of the field, where agent-based models continue to be used alongside the rapidly ascending generative approaches to network and text analysis. Generative models are at the core of computational social science regardless of how and where you draw boundaries around the field, or whether you consider the pre-2000s era dominated by agent-based models or the post-2000s era when the field expanded and broadened.

Generative models are thoroughly probabilistic, and depending on the type of model you and using and how you are specifying it, the results they produce include random noise and can produce slightly different results on different runs, which may or may not be noticable. The key assumption they make is that there are a variety of probability distributions that govern what we see in the data, and the goal is to the learn the parameters of those distributions. Discriminative models also make use of probability theory (e.g., a binary logistic regression outputs the probability that an observation belongs to class A or class B), *but they are not trying to learn the parameters of underlying probability distributions that cause observations to belong to one class or the other*. In other words, discriminative models try to learn the boundaries, hard or soft, that separate classes of observations, whereas generative models try to determine the probability distributions that give rise to the observations we observe. In this sense, there's a large area of overlap between the precepts of generative modelling and the kinds of models typically used in the Bayesian paradigm. As you'll soon see, the generative properties inherent to most Bayesian models will be of great use to us, if we can unlock their potential. The next step is to ensure you have some understanding of the basics of probability theory. While not the most inherently exciting part of computational social science, a little knowledge goes a *very* long way.

  
> **Further Reading**  
> 
> Murphy's [-@murphy2012machine] *Machine Learning: A Probabilistic Perspective* provides a deep and very comprehensive dive into probabilistic machine learning. Daphne Koller and Nir Friedman's  [-@koller2009probabilistic] *Probabilistic Graphical Models: Principles and Techniques*, Judea Pearl and Dana Mackenzie's [-@pearl2018book] *The Book of Why* and Pearl's [-@pearl2009causality] *Causality* provide fascinating introductions to probabilistic graphical models, Bayesian networks, and Bayesian causal inference (all topics in probabilistic modelling that, regrettably, I did not have space to cover in this book).
>


## CONCLUSION

### Key Points 

- This was a chapter of dichotomies. We contrasted:
 - Statistical Models with Machine Learning Models (primarily intended for inference and prediction, respectively)
 - The Frequentist paradigm with the Bayesian paradigm, primarily with respect to their views on probability ("Rigid" and "Flexible", respectively)
 - Generative Models with Discriminative Models (primarily intended to reconstruct observed data and produce accurate predictions, respectively)