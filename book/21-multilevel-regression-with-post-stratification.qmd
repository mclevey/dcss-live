# Multilevel regression with post-stratification

## LEARNING OBJECTIVES

- Explain what a Hierarchical linear regression model is
- Specify a Hierarchical linear regression model using mathematical notation
- Specify a Hierarchical linear regression model using PyMC code
- Explain what "pooling" is in a Hierarchical linear regression
- Differentiate between no pooling, partial pooling, and complete pooling
- Use informative priors to fix a problematic sampler
- Assess how well Hierarchical models fit the data
- Interpret the results of a Hierarchical model

## LEARNING MATERIALS

You can find the online learning materials for this chapter in `doing_computational_social_science/Chapter_29`. `cd` into the directory and launch your Jupyter Server.

## INTRODUCTION

Generally speaking, most introductory and intermediate quantitative methods classes for social science students do not teach hierarchical linear regression models except as a special case of 'default' linear regression models. This is probably due to the fact that simple linear models are much easier to teach than complex ones, and because of the wise notion that, where possible, we should favour simple models over complex models. And so hierarchical linear models are banished to "advanced" electives that you *might* get to after years of learning ANOVA-like statistical tests (now with 300+ flavours!). This is all a bit silly given the philosophical gymnastics required of "simple" statistical tests and linear models in the Frequentist tradition. We need a new normal in which our "default" regressions are hierarchical. I'm going to assume that you, like me, were not taught statistics this way and that you may not even know what a hierarchical regression is. Let's change that.

### Imports

```python
import pandas as pd
import numpy as np
import seaborn as sns
import pymc as pm
import arviz as az

import matplotlib as mpl
from matplotlib import pyplot as plt
from dcss import set_style
set_style()

from dcss.bayes import plot_2020_no_pool, plot_2020_partial_pool
```

## SO, WHAT'S A HIERARCHICAL MODEL?

Linear regression models of any variety can justifiably be called "**hierarchical**" if they use data at different 'levels' to estimate parameters and make predictions. I'll come back to 'data at different levels' in a moment, but first I want to acknowledge a very common source of confusion. For no good reason, hierarchical models go by a wide variety of different names, including '**random effects**', '**mixed effects**,' and '**multilevel**.' There are many others as well. The terminology here is hopelessly and inextricably muddled. In some fields, the various terms have specific meanings; in others, they don't. Sometimes the specific meanings are at odds with one another.

Thankfully none of that really matters for our purposes here. Most of the time I'll stick with hierarchical, but when I don't, I mean the same thing. Since you have now been introduced to two different languages for describing your models with precision: code and mathematical notation. A description of your model using either of these two languages is enough to banish any ambiguity about the type of model you're working with.

What do I mean when I say 'data at different levels?' Good question. Anytime I mention something along those lines (including references to '**pools**,' '**clusters**,' etc.), I'm referring to data where observations can be reasonably grouped together because they share some sort of **context**. (You might remember me alluding to this type of model when I introduced relational thinking in Chapter 14, where I compared network analysis to multilevel analysis.) The model we were working with last chapter -- the one that investigated if you could predict a Democratic candidate's margin of victory (or loss) from the amount by which the Democrats outspent (or were outspent by) the Republicans -- used clustered data: each of the observations was drawn from one of the United States of America's 435 Federal Congressional Districts, and each of those districts belonged to one of the 50 States. In this way, the State variable could have acted as a 'cluster', in the sense that two Congressional Districts from within the same state are more likely to share similarities with one another than two Congressional Districts chosen at random from the entire dataset.

The real power of a hierarchical model stems from its ability to balance the influence of individual level observations (individual congressional districts) with the influence of whole clusters (each individual state). Bayesian hierarchical models permit this tradeoff between countervailing influences by permitting the slopes ($\beta$) and intercepts ($\alpha$) of each cluster to vary from one another, while still forcing all of the slopes and intercepts to be drawn from a simultaneously-estimated prior. This is all getting a little too abstract, so let's get practical.

## GOLDILOCKS AND THE THREE POOLS

Whilst working your way through the previous chapter, you might have noticed the word '**pool**' showing up in our model. I didn't explain it at the time, but the nomenclature was indicative of the modelling strategy we were using. In a regression model, the 'pooling' you use determines how the various categorical variables in your model can influence one another. The election data we used had one categorical variable -- `state` -- that indicated which of the 50 US States each congressional district belonged to. It might not have seemed so at the time, but our decision to omit the `state` variable by excluding it from the model entirely was an intentional choice: by preventing our model from accessing the information contained therein, it was forced to treat every congressional district from every state as if they had all come from one giant state (or, equivalently, no state at all). Doing so might have *seemed* like the 'default' option, but the only reason we went that route was for the sake of simplicity.

Now that we've seen a full example of developing a simple Bayesian regression model, it's time to take our categorical data more seriously. The idea here, which I am taking from McElreath's classic *Statistical Rethinking*, is that our modelling choices should reflect serious consideration of the options provided to us by the information available to us in our datasets. In this case, we should be asking ourselves: "*how should we handle the various U.S. states that appear in this dataset?*" Let's discuss three primary options.

**Option 1, Complete Pooling**: All 50 U.S. States are identical.

This is the approach we took in the previous chapter, treating all congressional districts from all states as if there was no meaningful difference between them. This approach is known as 'Complete Pooling', because it puts all of the observations into one big 'pool' and estimates parameters therefrom (now you know where all those 'pool' references come from). This is a simple approach, which is nice, but it's rarely the best one. It can be overly simplistic, obliterates differences between clusters, and is prone to underfitting. It is highly unlikely, for instance, that increased Democratic election spending will do anything to sway voters in overwhelmingly Republican state of Wyoming. Ditto for Hawaii, where most voters are already committed Democrats. Best to avoid any impulse to artificially impose homogeneity.

**Option 2, No Pooling**: All 50 U.S. States are utterly unique.

This approach -- called "No Pooling" -- would allow each state to have its own intercept ($\alpha$) and slope ($\beta$), completely free from any other influences. This would mean that there would be practically no statistical commonalities between them, aside from the (very weak) regularizing influence of our priors. Going this route ensures that nothing our model learns about one state (or all of the states as a whole) can tell us anything about any of the others as individuals. Since the model is now free to create the best fit for each state based on the data available, this approach is very susceptible to overfitting.

**Option 3, Partial Pooling**: The U.S. States differ from one another, but there are commonalities about them that we can infer and apply productively.

This approach -- which we'll call 'Partial Pooling' -- allows states to differ from one another, but places limitations on how they may differ. Rather than giving each state free rein over its own parameters, this approach allows the model to simultaneously learn about each state's parameters from the data, as well as overall trends for the states in general by way of shared priors.

Logically and statistically, this approach usually makes the most sense: each state differs, but all are political entities within the United States of America, carrying all of the shared norms, values, and traditions incumbent upon belonging to the Union. This is the approach we primarily will use as we dive into hierarchical modelling. Before we do, though, let's take a brief detour to examine what a 'No Pooling' model might look like.

### Load Data

Since our exploration of Bayesian hierarchical linear models builds off of the model we developed in the previous chapter, we're going to re-use the same 2020 House of Representatives Election dataset. We'll start by loading, standardizing, and previewing the data:

```python
df = pd.read_csv(
    'data/2020_election/2020_districts_combined.csv'
)

spend_std = (df.spend - np.mean(df.spend)) / np.std(df.spend)
vote_std = (df.vote - np.mean(df.vote)) / np.std(df.vote)
state_cat = pd.Categorical(df.state)
state_idx = state_cat.codes
n_states = len(set(state_idx))
dem_inc = df.dem_inc
rep_inc = df.rep_inc
pvi_std = (df.pvi - np.mean(df.pvi)) / np.std(df.pvi)
```

```python
df.head()
```

Part of our objective in this chapter is to incorporate more of the available data into our model - as you may recall, we only utilized the `vote` and `spend` variables in the previous chapter. Now, we're going to expand our model to incorporate information from the `state`, `dem_inc`, `rep_inc`, and `pvi` variables. Before proceeding, let's take a moment to summarize each of the new variables and consider what they represent:

```python
df[['dem_inc', 'rep_inc', 'pvi']].describe()
```

The three new variables in our lineup, from left to right in the table above, represent `Democratic Incumbent`, `Republican Incumbent`, and `Cook Partisan Voting Index`, respectively.

The two incumbency variables are straightforward: both are binary categorical variables (whose only possible values are 1 or 0), and they represent which of the parties (if either) has an incumbent in the race. We can't really combine them in the same way we did with `vote` and `spend`, because some districts have no incumbent at all, and it's not yet clear that the effect of incumbency is the same for Republicans and Democrats alike. We'll have to keep them separate for now. The 'Cook Partisan Voting Index' (`pvi`) measures how strongly a given congressional district tends to lean towards one of the two major U.S. political parties. It's based on voting data gathered from the two previous presidential elections, and -- for this election -- ranges from a minimum of -33 (the deep-red Texas Panhandle), to 43 (the true-blue Bronx).

Without looking at any regression results, I'd expect all three of these variables to play a strong role in our model: collectively, they speak volumes about how each congressional district has voted in the past. In fact, I'd be willing to bet that their collective influence on the model, regardless of its final form, will be stronger than the `spend` variable's will be, but that's fine: the purpose of our model is to tell us what the `spend` variable's influence is whilst controlling for things like statewide preferences and historical trends. If, after the control variables are added, our model finds that `spend` isn't that important, that's a perfectly valid result.

Of course, we're not yet certain how things are going to turn out; there's a lot of modelling to be done between now and then! As a prelude, let's take a moment to remind ourselves about the fully-pooled model (i.e., "All 50 states are identical") we used last chapter:

\begin{align}
\text{vote}_i &\sim \text{Normal}(\mu_i, \sigma)& \text{[Likelihood]}  \\
\mu_i &= \alpha + (\beta \cdot \text{spend}_i)  & \text{[Linear Model]} \\
\alpha &\sim \text{Normal}(0, 2)                & \text{[alpha Prior]} \\
\beta  &\sim \text{Normal}(1, 2)                & \text{[beta Prior]} \\
\sigma &\sim \text{Exponential}(2)              & \text{[sigma Prior]} \\
\end{align}

The above model only uses a single value for $\alpha$ and a single value for $\beta$, which means that every observation (regardless of which state they come from) must use the same slope and intercept. When we build *hierarchical* models, we allow the slope and intercept to vary by state. Consequently, we're going to have to re-build our model such that it is capable of accommodating multiple slopes and multiple intercepts. Rather than use 'dummy' variables for each state (as would be the standard Frequentist practice), we're going to use an unordered categorical '**index variable**'. We can write it like so:

\begin{align}
\mu_i &= \alpha_{\text{state[i]}} + (\beta_{\text{state[i]}} \cdot \text{spend}_i)
\end{align}

Translated into plain English, the above line is saying that "The value of $\mu_i$ for a given observation $i$ is equal to the $\alpha$ for that observation's state plus the product of the $\beta$ for that observation's state and that observation's `spend` value." This makes it explicit that our model will now accommodate as many different values for $\alpha$ and $\beta$ as there are states in the dataset (48, in our case, since two were dropped).

Now let's update the rest of the model:

\begin{align}
\text{vote}_i &\sim \text{Normal}(\mu_i, \sigma)\\
\mu_i &= \alpha_{\text{state[i]}} + (\beta_{\text{state[i]}} \cdot \text{spend}_i)     \\
\alpha_{\text{state[i]}} &\sim \text{Normal}(0, 2)   &  \text{for state[i]} = 0 ... 47            \\
\beta_{\text{state[i]}}  &\sim \text{Normal}(1, 2)   &  \text{for state[i]} = 0 ... 47                \\
\sigma &\sim \text{Exponential}(2)               \\
\end{align}

Even though each of the 48 $\alpha$ and $\beta$ parameters are completely separate and will have no influence on one another, *they all share the same respective priors*. Additionally, the $\text{state[i]}$ that shows up everywhere. I'm particularly fond of the $\text{state[i]}$ nomenclature, because it very closely mirrors how a Python object would behave. What we're saying in the model definition above is that $\text{state}$ is a mapping that accepts an integer, $\text{i}$ (which can range from 0 to 370), and outputs an integer between 0 and 47. In so doing, it has mapped the observation number (0 - 370) into a state number (0 to 47).

We can replicate this behaviour using variables we've already defined:

```python
district_3_state = state_idx[3]
print(district_3_state)
print(state_cat.categories[district_3_state])
```

Feel free to go check what state the corresponding row in the dataset belongs to; you should see that it's a perfect match!

Now that we've specified our model mathematically, let's feed it into PyMC:

### No Pooling Model

We'll start by specifying the full model. We won't go through it step by step, though, as we're tight on space and we've only made a few changes from the model in the previous chapter. Those changes are:

- We added a `shape=n_states` parameter to our $\alpha$ and $\beta$ priors
- We added `[state_idx]` to the $\alpha$ and $\beta$ parameters in the linear model

As a result of these changes, the $\alpha$ and $\beta$ parameters are no longer one-dimensional scalars. Instead, they are each vectors of length 48 -- one for each of the states in the dataset. Second, during fitting, our model will now seek out the $\alpha$ and $\beta$ parameters that correspond to the $i$-th district's state. Here's what it looks like in action:

```python
with pm.Model() as no_pool_model:
    # Priors
    alpha = pm.Normal("alpha", mu=0, sigma=2, shape=n_states)
    beta = pm.Normal("beta", mu=1, sigma=2, shape=n_states)
    sigma = pm.Exponential("sigma", lam=2)
    
    # Linear Model
    mu = alpha[state_idx] + beta[state_idx] * spend_std
    
    # Likelihood
    votes = pm.Normal("votes", mu=mu, sigma=sigma, observed=vote_std)
    
    # Run Sample Traces
    trace_no_pool = pm.sample()
```

If everything works correctly, PyMC should sample without issues. Let's check the trace plots for the hyperparameters (@fig-28_01):

```python
with no_pool_model:
    az.plot_trace(trace_no_pool, ['sigma'], compact=True)
    plt.savefig('figures/28_01.png', dpi=300)
```

![png](figures/28_01.png){#fig-28_01}

Even though we can't display all the traces for `alpha` and `beta` (since there are 48 of each), a quick glance at our trace for `sigma` seems to indicate that everything is well and good. This assumption is backed up by the fact that the PyMC sampler didn't have any grievances to air. Operating under the assumption that our model was well-sampled, let's proceed with our examination of the results (@fig-28_02):

```python
with no_pool_model:
    ppc = pm.sample_posterior_predictive(trace_no_pool, var_names=['votes', 'alpha', 'beta', 'sigma'])
```

```python
plot_2020_no_pool(
    no_pool_model, 
    trace_no_pool,
    n_states, 
    state_idx,
    spend_std,
    vote_std,
    ppc,
    state_cat
)
plt.savefig('figures/28_02.png', dpi=300)
```

![png](figures/28_02.png){#fig-28_02}

Each state in the model has a different average regression line, and all of them seem to be doing a good job of fitting the data. While most of the states still show a positive relationship between spending differential and vote differential, not all do: Maine, Alabama, Massachusetts, and New Mexico have completely reversed the trend. Our model has determined that Democrats who outspend Republicans in these states tend to do worse than their colleagues who don't. According to our model, the only thing the Democrats would have to do to sweep Alabama is stop spending any money there! Though hilarious, this is clearly not a reasonable conclusion for the model to draw. Such is the peril of allowing a model to fit the data as closely as it can.

If you squint and look closely, you might be able to see some small bands around each of the regression lines, covering the interval that we have data for. It might come as little surprise, then, that those little bands are the exact same as the bands we used to surround our regression line from the previous chapter. As a brief refresher: the bands represent the model's uncertainty about the best-fit regression line (inner band, 94% HDI) and its uncertainty about where the data points themselves lay in relation to the regression line (outer band, 94% HDI; parameterized as '$\sigma$' in our model's likelihood, a.k.a. the standard deviation of the normal distribution in our likelihood).

We're not going to dwell too much on the specifics here: the important takeaway is that our unpooled model has allowed the data for each state to completely determine their own intercept and slope parameters, even when there are only a small number of observations. The only regularizing forces present are the relatively uninformative (and, therefore, weak) priors that we established for this model in the previous chapters (they haven't changed between now and then). With nothing stopping the model from rushing straight for the best possible fit, we've allowed it to descend into the dread valley of overfitting. Damn. Our model does an excellent job at fitting the data we have, but it is, in effect, painting a bullseye around an arrow that had already lodged itself into a wall. In order to curb these tendencies in a principled way, we're going to turn to the regularizing properties of the hierarchical linear model.

#### Partially Pooled Model

Our objective for the hierarchical election model we are developing here is to permit slopes and intercepts to vary between States, but to ensure that each is being drawn from a set of higher-level distributions that encode our model's knowledge of the States *in general*. This means that we're going to have to do away with the numbers ($\mu$ and $\sigma$) we've been using thus far to specify our $\alpha$ and $\beta$ priors and replace them with parameters. Let's do that now, even though the result will be incomplete:

\begin{align}
\text{vote}_i &\sim \text{Normal}(\mu_i, \sigma)\\
\mu_i &= \alpha_\text{state[i]} + (\beta_\text{state[i]} \cdot \text{spend}_i)     \\
\alpha_\text{state[i]} &\sim \text{Normal}(\alpha_\mu, \alpha_\sigma)   \\
\beta_\text{state[i]}  &\sim \text{Normal}(\beta_\mu, \beta_\sigma)   \\
\sigma &\sim \text{Exponential}(2)               \\
\end{align}

Okay, great! We've now configured our $\alpha$s and $\beta$s so that they'll be drawn from a common, higher-level distribution. This gives us four new variables to play the "what's that?" game with. Since 

- $\alpha_\mu$
- $\alpha_\sigma$
- $\beta_\mu$, and 
- $\beta_\sigma$ 

are all unobserved, they're going to need priors. You might be thinking to yourself "aren't $\alpha$ and $\beta$ *already* priors? Does this mean we're going to be giving priors to our priors?" 

Yes! Exactly! In order to keep things as conceptually clear as possible, a 'prior for a prior' has a special name: '**Hyperprior**'. Let's fill those in now, using similar numerical values as in earlier models. I've included line breaks to help clarify which type of prior is which.

\begin{align}
\text{vote}_i &\sim \text{Normal}(\mu_i, \sigma)\\
\mu_i &= \alpha_\text{state[i]} + (\beta_\text{state[i]} \cdot \text{spend}_i)     \\
\\
\alpha_\text{state[i]} &\sim \text{Normal}(\alpha_\mu, \alpha_\sigma)   \\
\beta_\text{state[i]}  &\sim \text{Normal}(\beta_\mu, \beta_\sigma)   \\
\sigma &\sim \text{Exponential}(2)               \\
\\
\alpha_\mu &\sim \text{Normal}(1, 2) \\
\beta_\mu &\sim \text{Normal}(1, 2) \\
\alpha_\sigma &\sim \text{Exponential}(1) \\
\beta_\sigma &\sim \text{Exponential}(1) \\
\end{align}


<!-- Alternative Option (Shorter...)

\begin{align}
\text{vote}_i &\sim \text{Normal}(\mu_i, \sigma)\\
\mu_i &= \alpha_{\text{state[i]}} + (\beta_{\text{state[i]}} \cdot \text{spend}_i)     \\
\\
\alpha_{\text{state[i]}} &\sim \text{Normal}(\alpha_\mu, \alpha_\sigma)   \\
\beta_{\text{state[i]}}  &\sim \text{Normal}(\beta_\mu, \beta_\sigma)   \\
\sigma &\sim \text{Exponential}(2)               \\
\\
\alpha_\mu &\sim \text{Normal}(1, 2) \\
\beta_\mu &\sim \text{Normal}(1, 2) \\
\alpha_\sigma &\sim \text{Exponential}(1) \\
\beta_\sigma &\sim \text{Exponential}(1) \\
\end{align} 
-->

Now that we have priors, and that our priors have priors (most of them, anyways; good ol' sigma remains untouched), let's translate everything into PyMC:

```python
with pm.Model() as partial_pool_model:
    
    # Hyperpriors
    alpha_mu = pm.Normal("alpha_mu", mu=1, sigma=2)
    beta_mu = pm.Normal("beta_mu", mu=1, sigma=2)
    alpha_sigma = pm.Exponential("alpha_sigma", lam=1)
    beta_sigma = pm.Exponential("beta_sigma", lam=1)
    
    # Priors
    alpha = pm.Normal("alpha", mu=alpha_mu, sigma=alpha_sigma, shape=n_states)
    beta = pm.Normal("beta", mu=beta_mu, sigma=beta_sigma, shape=n_states)
    sigma = pm.Exponential("sigma", lam=2)
    
    # Linear Model
    mu = alpha[state_idx] + (beta[state_idx]*spend_std) 
    
    # Likelihood
    votes = pm.Normal("votes", mu=mu, sigma=sigma, observed=vote_std)
```

Looking good! Surely, nothing will go wrong when we attempt to fit this model.

```python
with partial_pool_model:
    trace_partial_pool = pm.sample(random_seed=42)
```

Something went wrong when we attempted to fit this model.

##### The Peril is in the Priors

Roughly translated, the series of warnings we received can be interpreted as: "The sampling process didn't go well". One of the benefits to working with PyMC's default sampler is that it is *very* noisy. It will loudly complain whenever anything goes wrong. As annoying as it can be sometimes, you would do well to view this behaviour as a good thing: whenever your sampler is having trouble with your model, it means there's probably something wrong with your model.

Our largest cause for concern is the number of 'divergences' that the sampling process returned. The sampler records a divergence whenever the proverbial marble in the idiomatic skate bowl ends up somewhere that shouldn't be physically possible: it has ended up buried beneath the terrain, or bounced completely clear of the skate park and is zipping around the city causing chaos.

Let's examine our trace plot (@fig-28_03) to see the extent of the damage:

```python
with partial_pool_model:
    az.plot_trace(trace_partial_pool, ['alpha_mu', 'beta_mu', 'alpha_sigma', 'beta_sigma', 'sigma'], compact=True)
    plt.savefig('figures/28_03.png', dpi=300)
```

![png](figures/28_03.png){#fig-28_03}

This isn't a completely unmitigated disaster, but the problems are apparent enough that we should go through them in detail. The first thing you might notice here is that our traces don't meet the criteria we laid out last chapter:

1. **The chains are not stationary**: some of the traces in `beta_mu` and `beta_sigma` seem to occasionally meander away from the overall mean and then get stuck in a local minima for long periods of time.
2. **The chains are not mixing well**: some of the traces alternate between rapidly zipping from one extreme to another (which is fine) and slowly moving in a single direction for 50 samples at a time or more (which is not fine).
3. **The chains have not converged**: the lower end of `beta_sigma` has some real issues.

Despite everything, overall state of this posterior sampling trace isn't too bad; if you went ahead with the model as-is, you would probably draw inferences that are pretty close to what you would have gotten from a better-behaved trace... I wouldn't be in a hurry to submit this to any peer-reviewed journal, though. The bottom line is this: we can do better. To do so, we're going to have to find a less chaotic way of throwing our marble around this 101-dimensional skate bowl.

### Informative Priors: A Spoonful of Information Makes the Sampler Calm Down

There are many ways to tame a rebellious model. We don't have the space here to cover some of the better ones (reparameterization is the usual go-to), but we wanted to show that informative priors can be used to improve sampling.

In order to keep things simple, we've specified each of our spend-vote models thus far using only two different distributions:

1. The Normal Distribution, which we've used for parameters that could, theoretically, take on any value on the real number line, and
2. The Exponential Distribution, which we've used as priors for our standard deviation parameters, which must fall somewhere between 0 and positive infinity.

Our strategy for calming down our out-of-control model is going to involve tightening up our Normal distributions and swapping out our Exponential distributions for Gamma distributions (another distribution we did not explicitly discuss in Chapter 26, but which you now know how to learn about).

First, "tightening our normals." All I mean by this is that instead of using the wide, nearly flat priors we've been using thus far, we're going to shrink them down to cover a much smaller part of the real number line. You can see what I mean by looking at the priors for the model specified below.

The switch from an Exponential distribution to a Gamma distribution for our standard deviation parameters needs a bit more explanation. The Gamma distribution is similar to the Exponential in the sense that both can only take on values from 0 to positive infinity (a property we need for our standard deviations), and that they tend to peak early and have long tails. In the interests of avoiding technical jargon, the Gamma distribution will let us 'scoop' a bit of the probability density away from 0, which is likely the cause of our woes here.

A quick note: the priors we're using here are designed to demonstrate how information can be used to combat model degeneracy. As such, they're a little more strongly informative than you might expect to see in published literature, but not by that much. With these informative priors in place (and no other changes), let's examine how our model behaves:

```python
with pm.Model() as partial_pool_model_regularized:
    
    # Hyperpriors
    alpha_mu = pm.Normal("alpha_mu", mu=0.1, sigma=0.3)
    beta_mu = pm.Normal("beta_mu", mu=0.1, sigma=0.3)
    alpha_sigma = pm.Gamma("alpha_sigma", alpha=4, beta=0.10)
    beta_sigma = pm.Gamma("beta_sigma", alpha=4, beta=0.10)
    
    # Priors
    alpha = pm.Normal("alpha", mu=alpha_mu, sigma=alpha_sigma, shape=n_states)
    beta = pm.Normal("beta", mu=beta_mu, sigma=beta_sigma, shape=n_states)
    sigma = pm.Gamma("sigma", alpha=4, beta=0.10)
    
    # Linear Model
    mu = pm.Deterministic("mu", alpha[state_idx] + (beta[state_idx]*spend_std))
    
    # Likelihood
    votes = pm.Normal("votes", mu=mu, sigma=sigma, observed=vote_std)
    
    # Run Sample Traces
    trace_partial_pool_regularized = pm.sample(
        random_seed=42
    )
```

Boom! No more divergences! The sampler still had a few grievances to air (at least one of our parameters was sampled very inefficiently), but we should interpret the lack of divergences as permission to manually examine our trace plots (@fig-28_04):

```python
with partial_pool_model_regularized:
    az.plot_trace(trace_partial_pool_regularized, ['alpha_mu', 'beta_mu', 'alpha_sigma', 'beta_sigma', 'sigma'], compact=True)
    plt.savefig('figures/28_04.png', dpi=300)
```

![png](figures/28_04.png){#fig-28_04}

There are a couple of hiccups, and the `alpha_sigma` traces are verging on non-convergence, but there's nothing to be too concerned about.

```python
with partial_pool_model_regularized:
    ppc = pm.sample_posterior_predictive(trace_partial_pool_regularized, var_names=['votes', 'alpha_mu', 'beta_mu', 'alpha_sigma', 'beta_sigma', 'alpha', 'beta', 'sigma', 'mu'])
```

We won't include the `alpha` or `beta` parameters in our ArviZ summary because there are 96 of them in total. No sense reading them all. Instead, we'll focus on our hyperpriors:

```python
with partial_pool_model_regularized:
    summary = az.summary(trace_partial_pool_regularized, round_to=2, var_names=['alpha_mu', 'beta_mu', 'alpha_sigma', 'beta_sigma', 'sigma'])
    
summary[['mean', 'sd', 'r_hat']]
```

It appears that there was a bit of an issue with `beta_sigma`; our assessment of `beta_sigma` is backed by the r_hat value of 1.01 (anything noticeably greater than 1.00 indicates something's amiss). It's not high enough to be a true cause for concern, but it's worth pointing out.

Now that we've fit our hierarchical model, let's visualize the results (@fig-28_05):

```python
plot_2020_partial_pool(
    partial_pool_model_regularized,
    trace_partial_pool_regularized,
    trace_no_pool,
    n_states, 
    state_idx,
    spend_std,
    vote_std,
    ppc,
    state_cat
)
plt.savefig('figures/28_05.png', dpi=300)
```

![png](figures/28_05.png){#fig-28_05}

The above plots should look familiar: they're very similar to the ones we used to investigate the results from our unpooled model above. All of the elements they share in common are the same, only for our latest model:

- each of the points represents a single congressional district
- the black line represents the regression line from our partially-pooled model, and
- the small and large bands around the lines represent the 94% highest density interval of posterior probability for the regressor and the predictions, respectively.

There's one additional element here, though. The grey lines represent the regression lines from the unpooled model; I included them here to facilitate comparison between the partially-pooled and unpooled models.

### Shrinkage

Let's dig into these lines a little. First of all, a cursory glance at the previous model's more outlandish conclusions shows that things have been calmed down considerably. Each of the states with downward-sloping regression lines (predicting worse voting outcomes in districts where Democrats spent more) -- such as Alabama, Maine, and New Mexico -- have been pulled back from the brink. In the opposite direction, some of the more steeply positive states (such as Kentucky, where the unpooled model predicted that a single standard deviation increase in relative spending for the Democrats would net two standard deviations' worth of votes) have been reined in.

Another thing you might notice is that each of the single-district states^[Note that many of these states aren't actually single-district states, but rather only have one valid district in the dataset because of the filtering we had to do] (Wyoming, Vermont, Rhode Island, etc.) have had their regression lines change from perfect fits (where the grey line travels straight through the sole point of data) to more 'standard' fits (where the black line misses the point, often by a good margin). That's not to claim that all of their black lines are identical: they're not (compare Rhode Island with Montana). Instead, what the model is telling us is that the posterior distribution for each of these individual states isn't all that much different from the distribution all states are drawing from.

What's happening here is that all of the states who have had their regression lines 'calmed down' by the model are being regularized by the impact of our model's prior. Unlike a single-level model, however, *we didn't choose this prior: the model learned it from the data*!

This is the power of the hierarchical model; it adaptively learns how to straddle the line between underfitting and overfitting, leveraging regularizing probability distributions to calm down overeager predictions. The net effect is that our partially pooled model, at least compared with the unpooled model, has 'shrunk' the posterior distribution, causing the model's predictions to crowd more tightly around a more conservative predictor. This phenomenon is known as '**shrinkage**'.

A final parting thought on this topic: you may have noticed that the larger states such as Texas, New York, and California -- all of which already had fairly reasonable regression lines -- barely changed at all. Each of them were endowed with enough observations that they could largely overwhelm the regularizing influence of the priors.

### Does the Model Fit? Posterior Predictive Plots

Adding variables to a model can be an excellent way to explore relationships that simply couldn't be tackled without some form of multivariate regression. It is unfortunate, then, that adding even a small handful of variables results in a model that the human brain can no longer perceive visually. A regression with just a predictor and an outcome variable is simple: you can capture everything in a two-dimensional scatterplot, with the predictor on the horizontal axis and the outcome on the vertical. A regression with two independent variables (say, a predictor and a control) and one outcome variable is less simple, but doable: you'll occasionally see them visualized as a 3-dimensional plot with the two independents on the horizontal axes, and with a plane representing the regressor. Anything more than this is practically impossible, as the 'line' of best fit becomes a hyperplane that our brains are incapable of visualizing.

In this sad state of affairs, we're forced to turn to plots that collapse high-dimensional space back down into a 2-dimensional plane that allows us to see the distance between our model's predictions and the true value of the observations. The resulting plot is called a '**posterior predictive plot**', and is useful for assessing how well our model has fit the data. The main strength of this method is that it scales well into an arbitrarily large number of dimensions: we can picture all of the observations using a single plot. The downside is that we lose some of the nuance that we'd get from other visualizations. Here's how to make one (results shown in Figure @fig-28_06):

```python
mu_hpd = az.hdi(ppc.posterior_predictive["mu"], 0.89)
D_sim = ppc.posterior_predictive["votes"].mean(dim=("chain", "draw")).values

# Calculate the error bars using the correct dimension names from mu_hpd
yerr_lower = D_sim - mu_hpd.sel(hdi='lower').mu.values
yerr_upper = mu_hpd.sel(hdi='higher').mu.values - D_sim
yerr = np.vstack([yerr_lower, yerr_upper])

fig, ax = plt.subplots(figsize=(6, 6))

# Apply the calculated error bars
plt.errorbar(
    vote_std,
    D_sim,
    yerr=yerr,
    fmt="C0o",
)

ax = sns.scatterplot(x=vote_std, y=D_sim, s=1, color='darkgray')

min_x, max_x = vote_std.min(), vote_std.max()
ax.plot([min_x, max_x], [min_x, max_x], "k--")

ax.set_ylabel("Predicted vote differential")
ax.set_xlabel("Observed vote differential")

sns.despine()
plt.savefig('figures/28_06.png', dpi=300)
```

![png](figures/28_06.png){#fig-28_06}

Each of the points is an observation from our dataset. They're arranged along the horizontal axis according to their observed (actual) value, and along the vertical axis according to where our model thinks they should be. The vertical lines around each point indicate the range that contains 94% of the posterior probability for that particular observation (remember, in a Bayesian model, *everything comes with a healthy dose of uncertainty*).

Examining the plot above, we can see that our model does a passable job of retrodicting the voting data using nothing but the 'state' and 'spend' variables. We can, however, see some real problems at the far-right side of the plot: not only is our model incorrect about almost every district which lays more than ~1.5 standard deviations above the mean, it is *confidently* incorrect about them. In the next section, we're going to see if we can do better.

## THE BEST MODEL OUR DATA CAN BUY

Now that we have established a Hierarchical baseline and introduced a method for visualizing results from models whose regression 'lines' are in higher dimensions (and, thus, aren't lines any longer, but rather hyperplanes), we can start to add variables in an effort to improve model fit. Unfortunately, we don't have the room here to report all the proper checks every time we add a variable. Instead, I'll run you through how to add each of the remaining variables in the dataset and then present the finished model.

```python
with pm.Model() as full_hierarchical_model:
    
    # Hyperpriors
    alpha_mu_state = pm.Normal("alpha_mu_state", mu=0.1, sigma=0.3)
    alpha_sigma_state = pm.Gamma("alpha_sigma_state", alpha=4, beta=0.10)
    beta_mu_spend = pm.Normal("beta_mu_spend", mu=0.1, sigma=0.3)
    beta_sigma_spend = pm.Gamma("beta_sigma_spend", alpha=4, beta=0.10)
    
    # Priors from Hyperpriors
    alpha_state = pm.Normal("alpha_state", mu=alpha_mu_state, sigma=alpha_sigma_state, shape=n_states)
    beta_spend = pm.Normal("beta_spend", mu=beta_mu_spend, sigma=beta_sigma_spend, shape=n_states)
    
    # Priors
    beta_pvi     = pm.Normal("beta_pvi", mu=1, sigma=0.3)
    beta_rep_inc = pm.Normal("beta_rep_inc", mu=-0.5, sigma=0.2)
    beta_dem_inc = pm.Normal("beta_dem_inc", mu=0.5, sigma=0.2)
    sigma = pm.Gamma("sigma", alpha=4, beta=0.10)
    
    # Linear Model
    mu = pm.Deterministic("mu", 
                         alpha_state[state_idx] + 
                         beta_spend[state_idx] * spend_std +
                         beta_pvi * pvi_std +
                         beta_rep_inc * rep_inc +
                         beta_dem_inc * dem_inc 
                         )
    
    # Likelihood
    votes = pm.Normal("votes", mu=mu, sigma=sigma, observed=vote_std)
    
    # Run Sample Traces
    trace_full_hierarchical_model = pm.sample(
        target_accept=0.97,
        random_seed=42
    )
```

Let's produce the trace plots for our `full_hierarchical_model` (@fig-28_07).

```python
with full_hierarchical_model:
    az.plot_trace(trace_full_hierarchical_model, 
                  [
                      'alpha_mu_state', 
                      'alpha_sigma_state', 
                      'beta_mu_spend', 
                      'beta_sigma_spend', 
                      'beta_pvi',
                      'beta_rep_inc',
                      'beta_dem_inc',
                      'sigma',
                  ], compact=True)
    plt.savefig('figures/28_07.png', dpi=300)
```

![png](figures/28_07.png){#fig-28_07}

There are a few worrying signs here (the `alpha_mu_state` isn't sampling as efficiently as we would prefer), but nothing serious enough to call the model into question entirely! Time to take a peek at our model fit (@fig-28_08):

```python
with full_hierarchical_model:
    ppc = pm.sample_posterior_predictive(trace_full_hierarchical_model, var_names=['votes', 'mu'])
```

```python
mu_hpd_mean = mu_hpd['mu'].mean(dim="hdi").values

yerr = np.abs(ppc.posterior_predictive["votes"].mean(dim=("chain", "draw")).values - mu_hpd_mean)

plt.errorbar(
    vote_std,
    ppc.posterior_predictive["votes"].mean(dim=("chain", "draw")).values,
    yerr=yerr,
    fmt="C0o",
)

ax = sns.scatterplot(x=vote_std, y=D_sim, s=1, color='darkgray')

min_x, max_x = vote_std.min(), vote_std.max()
ax.plot([min_x, max_x], [min_x, max_x], "k--")

ax.set_ylabel("Predicted vote differential")
ax.set_xlabel("Observed vote differential")

sns.despine()
plt.savefig('figures/28_06.png', dpi=300)
```

![png](figures/28_08.png){#fig-28_08}

Much, much better. The addition of two categorical variables (Democratic and Republican incumbency), and one continuous variable (the Cook Partisan Voting Index) has allowed our model's predictions to fall closer to the observed values across the board.

Even though the above plot does a good job of showing us how well our model fit the data in general, it tells us nothing about the parameters of interest! For that, we're going to need to turn to a form of plot that can condense large amounts of coefficient information into a relatively small space (we have 51 to examine, after all). It's called a **forest plot**. Figure @fig-28_09 is the forest plot for our model.

```python
state_labels = list(state_cat.categories) 
additional_labels = ['PVI', 'Democratic Incumbency', 'Republican Incumbency', 'Spending Differential']
all_labels = state_labels + additional_labels

ax_array = az.plot_forest(trace_full_hierarchical_model,
                          var_names=['beta_pvi', 'beta_dem_inc', 'beta_rep_inc', 'beta_spend'],
                          combined=True,
                          quartiles=False)

ax = ax_array[0]

# print(f"Number of labels: {len(all_labels)}")
# print(f"Number of tick locations: {len(ax.get_yticks())}")

if len(all_labels) > len(ax.get_yticks()):
    all_labels = all_labels[:len(ax.get_yticks())]

ax.set_yticklabels(all_labels)
ax.set_title("Coefficients for spending differentials, incumbency, and PVI")

sns.despine(left=False, bottom=False, top=False, right=False)
plt.show()
```

![png](figures/28_09.png){#fig-28_09}

After all that, we can finally settle down to the task of interpreting our handiwork.

Looking at the forest plot above, you might be struck by two countervailing trends. The first is that two out of the three variables we added in this latest model have a strong impact on its predictions. It should come as no surprise that the Partisan Voting Index is strongly positively correlated with vote differential, and that Republican incumbency has a predictably negative effect, showing the impact of Republicans' prior experience in helping them win their races.

The other trend that you might notice is that most of the rest of the model's claims are relatively tepid. Starting at the top: Democratic incumbency has a positive impact on the Democrats' vote margins, but it isn't as significant a boost as was the case for their Republican counterparts -- a little under half. Finally, the 94% Highest Density Interval for most of the states' spending coefficients show only a weak effect, if any at all. Most of the coefficient estimates are above zero, but a good portion of their HDI ranges straddle 0, meaning that our model hasn't really ruled out the idea that spending has no (or even a negative) effect on Democratic margins of victory. Of all the States under consideration, only in Georgia, Maryland, Michigan, North Carolina, Oregon, and Wisconsin does our model see *unambiguous* evidence of a positive effect from Democrats outspending Republicans.

One interpretation of these results is that Democratic spending advantages don't often translate into vote advantages. An equally valid interpretation, and one which takes into account the specific historical context, is that Democrats did a good job of funneling their resources towards close races in states whose districts were ripe for a Democratic breakthrough. They didn't succeed in all cases, but were able to translate their funding advantage in North Carolina and Georgia into a total of three seats flipped in their favour (the Democrats neither gained nor lost seats in any of the other states mentioned above).

Is this enough? Should we, at this point, be satisfied?

No.

Anytime we create a model with more than two variables, it is incumbent upon us to think through the causal implications of what we have done. To once again paraphrase Richard McElreath, whose *Statistical Rethinking* has had a major influence on our discussion of Bayesian regression, a regression model is implicitly asking a set of questions about each of its variables simultaneously: that question almost always boils down to something like "*how much predictive information does this variable contain, once the effect of all the other variables is known?*" Since we're interested in the influence of spending differentials on vote differentials across different states, we're implicitly using our regression model to ask: "*what is the value of knowing the effect of the spending differential once the effects from incumbency and the partisan voting index are already known?*"

Here's a plausible explanation for what's happening here. The negative effect that we're seeing in Texas might be indicative of a concerted Democratic push to try and flip historically Republican-leaning districts. If you paid attention to the 2020 American election, you might know that Democrats generally underperformed in the 2020 House races, and as such, our model may be picking up on an effect wherein Democrats funneled cash into break-even or Republican-leaning districts, only to be rebuffed. To do this, they would have presumably had to move cash away from safer Democratic-leaning districts in metro Houston and Austin. Under such circumstances, it might be entirely plausible that, *once we control for all the other variables in the model*, Democrats were more likely to lose ridings they overspent on, and win those they underspent on.

Another problem is that our model is helpless to tell us if the presumed causal relationship underpinning the whole shebang (namely, that money helps win elections) is justifiable. It'd be just as easy to claim that our predictor (money spent) and outcome (votes received) variables share no direct causal link, and instead share 'political popularity' as a common cause. The logic there being that popular candidates might be more likely to attract donations AND votes. Modelling can't help us here. The best regression model in the world, be it Bayesian or Frequentist, wouldn't be able to help you determine if there's any validity to your assumptions of causality. If you're interested, computational social science is in the midst of a causal inference renaissance, and thanks primarily to people like the brilliant computer scientist Judea Pearl, that renaissance is Bayesian. Unfortunately, this is another area of computational social science that we simply don't have the space to cover in this book.

There is, in fact, a lot more we *could* get into here, but our hands are tied by space constraints and by the fact that this book is not a Bayesian regression textbook. Hopefully you've seen that it is possible, and even fun, to build an interesting, functional, and sophisticated statistical model from scratch. Its imperfections represent room for improvement, not something to be feared or ashamed of.

## THE FAULT IN OUR (LACK OF) STARS

Somewhere, someone in the world is saying something like "Hold up, John. This is a chapter on regression analysis! Where are the hypothesis tests!? Where are the little stars that tell me if I can publish my findings or not?"

As these chapters have probably made abundantly clear, that's not what we're doing here. The Bayesian statistical paradigm is capable of comparing hypotheses in the way Frequentists think of such things, but are generally loath to do so. This is because we already have access to a very broad *range* of hypotheses defined by a posterior probability distribution, and that distribution already contains all of the information we can possibly derive from our model for a given set of data. Anything else we do -- plotting, calculating HDI, hypothesis testing -- is simply a summarization of that posterior distribution.

If you feel like comparing hypotheses in the style of a Frequentist, go for it. All of the Bayesian regression models we fit today contain infinite hypotheses (in multiple dimensions!) and the probability of any individual from among them (say, $\beta = 3$) being 'true' (whatever that means) is 0. We've already covered why that's the case.

You could compare ranges of hypotheses against a 'null' of sorts, but the Bayesian paradigm ensures that a simple posterior plot is all that is needed to quickly ascertain whether or not most of the posterior probability for any given parameter is credibly distant from 0, which is all null-hypothesis significance testing does really does anyhow.

Instead of using null-hypothesis significance testing, consider treating each model as its own 'hypothesis' of sorts. Gelman and Shalizi [-@gelman2013philosophy] advocate a paradigm wherein whole models are judged by how well they fit data (both in-sample and out-of-sample). Accepted models are used until their flaws become too egregious to ignore, at which point new, better models are developed using insights from the failings of the previous one. It's a different way of doing science than you might be used to, but it's worth knowing about: the winds of change are blowing decisively away from the traditional null hypothesis testing paradigm.

> **Further Reading**
>
> As I mentioned in several previous chapters, there are a number of outstanding resources that you can now turn to to continue your journey into Bayesian regression analysis. I especially recommend @mcelreath2020statistical, @lambert2018student, @kruschke2014doing, and @martin2018bayesian. Finally, @lynch2019bayesian offer a literature review of the use of Bayesian statistics in sociology.

## CONCLUSION

### Key Points

- Any regression dataset that features at least one clustering variable should be modelled, by default, using partially pooled hierarchical regression; any simpler models should only be used if justified
- Wide, uninformative priors can cause a large number of divergences during sampling using an HMC sampler; using tighter, more informative priors can help ameliorate this.
- Higher-dimensional regression models (those with more than two variables) are difficult (if not impossible) to fully visualize -- we can instead turn to specialized visualizations to assess model fit (via retrodiction) and model parameters (via forest plots)
- The First Rule of Bayes Club is that we don't do p-values, stars, or null hypothesis significance testing (exceptions apply)
